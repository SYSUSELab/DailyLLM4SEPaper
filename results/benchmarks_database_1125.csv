source_paper,benchmark_name,benchmark_name_quote,is_original_proposal,is_original_proposal_quote,dataset_url,dataset_url_quote,task_description,task_description_quote,dimension,dimension_quote,evaluation_method,evaluation_method_quote,context_dependency,context_dependency_quote,problem_domain,problem_domain_quote,problem_difficulty,problem_difficulty_quote,language,language_quote,data_size,data_size_quote,source_type,source_type_quote,last_updated,last_updated_quote,build_type,build_type_quote,contamination_status,contamination_status_quote,dataset_license,dataset_license_quote,task_granularity,task_granularity_quote,evaluation_metrics,evaluation_metrics_quote,input_modality,input_modality_quote,output_modality,output_modality_quote,task_io_type,task_io_type_quote,execution_environment,execution_environment_quote,unique_features,unique_features_quote
2511.20403_output/content.md,CLASSES2TEST,"We introduce the CLASSES2TEST dataset, which maps Java classes under test to their corresponding test classes",Yes，本文是该数据集的原始发布论文,"We introduce the CLASSES2TEST dataset, which maps Java classes under test to their corresponding test classes, and a framework that integrates advanced evaluation metrics",https://anonymous.4open.science/r/classes2test,1https://anonymous.4open.science/r/classes2test,用于评估大型语言模型生成的Java单元测试的质量，支持研究人员和开发者比较不同LLM和提示策略,"AGONETEST does not aim to propose a novel test generation algorithm; rather, it supports researchers and developers in comparing different LLMs and prompting strategies through a standardized end-to-end evaluation pipeline under realistic conditions.",单元测试质量评估，包括编译成功率、代码覆盖率、缺陷检测能力、测试异味等,"a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment",集成高级评估指标，如变异分数和测试异味，进行综合评估,"a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment",类级别测试，涵盖方法交互和共享状态,"Our approach focuses on class-level test code evaluation, which is closer to real-world practices as it covers method interactions and shared state",软件测试，Java单元测试生成,"Unit testing is an essential but resource-intensive step in software development, ensuring individual code units function correctly.",现实世界软件项目级别，比单方法测试更复杂,This extended dataset makes it possible to assess the test performance of an LLM on a more complex scope (the entire class) than the single method.,Java,An annotated open source Java project dataset extending METHODS2TEST [9],"基于9,410个GitHub仓库的数据集","AGONETEST offers far broader applicability by using a dataset of 9,410 GitHub repositories",扩展自METHODS2TEST数据集的Java开源项目,An annotated open source Java project dataset extending METHODS2TEST [9],2025,arXiv:2511.20403v1  [cs.SE]  25 Nov 2025,官方自建，基于现有数据集扩展,"Leveraging the METHODS2TEST dataset [9], we developed a new dataset specifically aimed at comparing human-written tests with those produced by LLMs.",,,,,类级别单元测试生成,"Our approach focuses on class-level test code evaluation, which is closer to real-world practices as it covers method interactions and shared state",变异分数、测试异味、代码覆盖率,"a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment",Java类代码,which maps classes under test to their related test classes,单元测试代码,"for the subset of tests that compile, LLM-generated tests can match or exceed human-written tests in terms of coverage and defect detection",代码到代码,which maps Java classes under test to their corresponding test classes,支持所有Java LTS版本的项目环境,AGONETEST overcomes this barrier by supporting all Java LTS versions.,专注于类级别测试评估，支持多种LLM和提示策略的比较，提供端到端的自动化评估流水线,"AGONETEST shifts the focus to the generation of class-level tests. Our approach makes it possible to use up-to-date LLMs and not constrain prompt design (our prompts can be customized), thereby handling more complex, real-world scenarios."
2511.21380_output/content.md,"ROCODE, LogHub2.0","Two projects (i.e., ROCODE [15] and LogHub2.0 [16]) are selected for the following experiments.","No, 本文是使用该数据集进行评测","We evaluate Copilot, backed by GPT-4.1 and Claude Sonnet 4, on adapting SE research artifacts from benchmark repositories including ROCODE and LogHub2.0.",,,数据集适应任务 - 将软件工程研究工具自动适配到新的数据集，构建可运行的实验并获取执行结果,"Our objective is to automatically modify 𝑅𝐷or 𝑅𝑇, construct a runnable experiment, and obtain its execution results.",多智能体系统在数据集适应任务中的能力评估,This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks.,五阶段评估流程：文件理解、代码编辑、命令生成、验证和最终执行，测量成功率并分析失败模式,"Through a five-stage evaluation pipeline (file comprehension, code editing, command generation, validation, and final execution), we measure success rates, analyze failure patterns, and assess prompt-based interventions designed to enhance agent performance.",多文件项目环境，需要理解整个代码仓库的架构,"Before modifying code or generating commands for a repository, a multi-agent system must browse the essential files in a repository to understand its architecture.",软件工程研究，包括代码生成、bug修复、性能分析和安全等领域,"In areas ranging from code generation and bug fixing to performance analysis and security, researchers are constantly proposing new techniques",复杂软件工程任务，需要多步骤协调和迭代修复,"Most of the evaluated multi-agent systems failed to complete the assigned tasks. Under such circumstances, nearly all results would be classified as failures",Python,"To ensure a consistent and manageable experimental environment, we retained only artifacts implemented in Python.","从顶级软件工程会议（FSE, ICSE, ASE, ISSTA）2024-2025年接受的论文中筛选的可重用研究构件","We began by collecting all papers accepted by the top four SE (FSE, ICSE, ASE and ISSTA) in 2024-2025 that were either (i) awarded a Reusable Artifact badge or (ii) explicitly identified by the authors as providing reusable artifacts.",学术研究构件，来自顶级软件工程会议的可重用研究项目,"To construct a representative set of high-quality SE research artifacts, we followed a systematic multi-stage selection process",2025,arXiv:2511.21380v1  [cs.SE]  26 Nov 2025,官方自建的研究构件集合,"We began by collecting all papers accepted by the top four SE (FSE, ICSE, ASE and ISSTA) in 2024-2025",,,,,代码编辑、文件创建、命令生成、验证修复,Edit and create necessary files... Generate and execute the commands... Validating and repairing,成功率、结构相似度（从7.25%到67.14%）、完成状态记录,Results show that... substantially improve structural similarity to ground truth (from 7.25% to 67.14%),代码仓库、自然语言提示,Each multi-agent system is provided with a processed repository... along with a simple prompt that specifies the adaptation task,修改后的代码、生成的脚本命令、执行结果,"the code adaptations performed to ensure compatibility, the executable scripts or commands derived for running the experiment, and the results produced by executing those scripts or commands",代码到代码的适应任务,"automatically modify 𝑅𝐷or 𝑅𝑇, construct a runnable experiment, and obtain its execution results",Python环境，需要可靠的部署环境,This choice allows for reliable environment deployment and isolates our investigation from environment setup challenges,专注于多智能体系统在数据集适应任务中的表现评估，采用五阶段评估流程，研究提示级干预对性能的影响,"This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks. Through a five-stage evaluation pipeline... we measure success rates, analyze failure patterns, and assess prompt-based interventions"
2511.21022_output/content.md,EDAPIBench,"We introduce EDAPIBench, a dedicated benchmark featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances.",Yes，本文是该数据集的原始发布论文,"We introduce EDAPIBench, a dedicated benchmark... We construct EDAPIBench—the first dedicated benchmark for evaluating deprecated API knowledge editing in LLMs",,,评估大语言模型中废弃API知识编辑的性能，专门用于测试模型编辑技术能否有效更新废弃API知识并生成最新的API,"a dedicated benchmark for evaluating deprecated API knowledge editing in LLMs... whether existing model editing methods can effectively update deprecated API knowledge within LLMs, and enable the edited models to correctly replace deprecated APIs with up-to-date ones",有效性、泛化性、可移植性、特异性,"comprehensively assess model editing performance across four key dimensions: Effectiveness, Generalization, Portability, Specificity",基于四个维度的评估：有效性（编辑后模型是否生成最新API）、泛化性（语义等价但语法变化的输入）、可移植性（不同输入但涉及相同废弃API）、特异性（保持与编辑任务无关输入的行为一致性）,Effectiveness: Measuring whether the edited model generates the up-to-date API for the original inputs; Generalization: Measuring whether it generates the up-to-date API on semantically equivalent but syntactically varied inputs; Portability: Measuring whether it generates the up-to-date API across different inputs... Specificity: Measuring whether it preserves consistent pre-editing behavior on inputs unrelated to the editing task,单函数级别的代码补全上下文,Our study frames the model editing scenario as a code completion task... we extract lines preceding the API invocation as candidate editing inputs (to prompt LLM API completion) and the invocation line as the target API (ground truth),软件工程、API维护、第三方库更新,deprecated APIs from 8 popular Python libraries... covering eight popular Python libraries such as PyTorch and TensorFlow,实际工程级难度，涉及真实世界废弃API的识别和更新,"Using these mappings, we extract 65,596 real-world functions from GitHub that call the up-to-date APIs",Python,deprecated APIs from 8 popular Python libraries... covering eight popular Python libraries such as PyTorch and TensorFlow,"包含70多个废弃API，超过3000个编辑实例，从GitHub提取了65,596个真实世界函数","featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances... we extract 65,596 real-world functions from GitHub",从GitHub真实项目代码中提取，基于已验证的API映射关系,"We begin with 145 verified API mappings (deprecated →up-to-date) from Wang et al. [49]... Using these mappings, we extract 65,596 real-world functions from GitHub that call the up-to-date APIs",2025-2026,Publication date: November 2026. arXiv:2511.21022v1 [cs.SE] 26 Nov 2025,官方自建，全自动构建,which can be fully automatically constructed... with fully automated construction,,,,,API级别的代码补全,"code completion tasks... during code completion, LLMs frequently suggest deprecated API invocations",基于四个维度的定性评估：有效性、泛化性、可移植性、特异性,"Effectiveness, Generalization, Portability, Specificity",代码片段,the editing input (a code snippet to be completed),API调用代码,"the specific correct, up-to-date API that should replace the deprecated one",代码到代码,code completion task... generating code,,,首个专门针对废弃API知识编辑的基准测试，支持全自动构建，包含四个维度的综合评估,"the first dedicated benchmark for evaluating deprecated API knowledge editing in LLMs, with fully automated construction, serving as a standardized, rigorous platform"
2511.19875_output/content.md,CodeFuse-CommitEval,"We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models.",Yes，本文是该数据集的原始发布论文,"We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models.",https://figshare.com/s/21fe4ec9cb960b52bffe,The dataset is publicly available at https://figshare.com/s/21fe4ec9cb960b52bffe.,"检测提交信息与代码变更之间的不一致性（Message-Code Inconsistency, MCI）",A Message-Code Inconsistency (MCI) occurs when the natural language description in a commit message does not accurately reflect the actual modifications in the associated code diff.,提交信息与代码变更的一致性检测能力,evaluate models for MCI detection,使用Recall、Precision、Specificity等指标评估模型检测结果,"Results show models detect inconsistent commits more reliably than consistent ones (average Recall 85.95%, Precision 80.28%, Specificity 63.8%)",提交信息与对应的代码差异（diff）,"Each item of the verified dataset contains a commit message, the corresponding code diff, and the ground truth label",软件工程、版本控制、代码审查,Version control relies on commit messages to convey the rationale for code changes,需要语义理解和上下文推理的复杂任务,purpose inconsistencies require deeper semantic understanding and contextual reasoning,多种编程语言（基于ApacheCM数据集的多样性）,because of its diversity in programming languages and the high-quality commits,包含正负样本的平衡数据集,we generate a balanced dataset with both positive and negative samples,基于ApacheCM数据集，通过规则引导的突变生成不一致提交信息,"Built on the ApacheCM dataset for diversity and quality, we generate seven types of inconsistent messages through rule-guided mutations of originally consistent commits",2025,arXiv:2511.19875v1 [cs.SE] 25 Nov 2025,官方自建，结合LLM数据合成能力与现有提交语料库,We present a comprehensive pipeline for constructing MCI datasets. By combining the data synthesis capabilities of LLMs with existing commit corpora,通过双重验证确保数据质量,apply two-fold validation to verify both positive and negative samples,,,一致性检测,detecting inconsistencies between commit messages and code,"Recall, Precision, Specificity","average Recall 85.95%, Precision 80.28%, Specificity 63.8%",自然语言（提交信息）与代码差异,"Each item of the verified dataset contains a commit message, the corresponding code diff",二元分类（一致或不一致）,allowing the models to detect whether the commit is consistent or not,文本与代码到分类,detect whether the commit is consistent or not,,,首个专门用于MCI检测的基准，包含七种不一致类型，支持多种增强策略评估,CODEFUSE-COMMITEVAL is the first benchmarking work tailored for comprehensively evaluating LLM's MCI detection ability. We define seven mutation rules to guide powerful LLMs in generating various types of inconsistent commit messages
2511.20709_output/content.md,DUALGAUGE-BENCH,"we further curated DUALGAUGE-BENCH, a comprehensive benchmark suite of 154 programming tasks",Yes，本文是该数据集的原始发布论文,"We present DUALGAUGE, the first fully automated benchmarking framework... we also present DUALGAUGE-BENCH, a curated benchmark suite",https://anonymous.4open.science/r/DualBench-6D1D,"Our system source code, benchmark, and evaluation/study data can all be found at https://anonymous.4open.science/r/DualBench-6D1D",联合评估代码生成的安全性和功能性，确保生成的代码既满足功能规范又不会引入安全漏洞,rigorously evaluate the security and correctness of LLM-generated code in unison... ensuring that generated programs fulfill their specifications without introducing vulnerabilities,代码生成的安全性和功能性联合评估,designed to rigorously evaluate the security and correctness of LLM-generated code in unison,在沙盒环境中执行程序，针对功能和安全性测试套件运行，基于执行结果评估测试通过率和联合正确性-安全性指标,"executes it in a sandboxed environment against functional and security test suites, and evaluates the execution results against both test suites to report test pass rates and joint correctness-security metrics",单任务代码生成，基于自然语言提示生成完整程序,Given an pure-natural-language prompt as functional specification... generates the model's code output for the prompt,跨领域编程任务，涵盖多样化功能领域,spanning diverse functionality domains,,,语言无关，支持多种编程语言,This dataset design and curation process allow it to be agnostic to programming languages,包含154个编程任务，每个任务都配有功能和安全性测试套件,a comprehensive benchmark suite of 154 programming tasks... Each task/prompt is paired with both a functional test suite... and a security test suite,人工与LLM协同创建，通过多评分者的人类专业知识进行精炼和修正,"constructed these test suites through a human-and-LLM co-creation process—leveraging multiple LLMs to generate candidate tests, then refining and amending these with human expertise of multiple raters",2025,arXiv:2511.20709v1 [cs.SE] 24 Nov 2025,官方自建，基于规范测试范式,following a specification-based testing paradigm,,,,,代码生成,code generation... translating natural language prompts—typically serving as functional specifications—into programs,测试通过率、联合正确性-安全性指标,report test pass rates and joint correctness-security metrics,自然语言,Given an pure-natural-language prompt as functional specification,代码,generates the model's code output for the prompt,文本到代码,translating natural language prompts... into programs,沙盒隔离容器环境，支持依赖解析和环境配置,executes it in a sandboxed environment... The execution engine compiles and runs the model-generated code within isolated containers,首个支持安全性和功能性联合评估的基准套件，每个任务都配有覆盖驱动的功能和安全性测试套件，采用人工与LLM协同创建过程,"the first benchmark suite that pairs each code-generation prompt with dual (functional and security), coverage-enforced test suites... constructed through a human-and-LLM co-creation process"
2511.23408_output/content.md,Vul4J,"Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities drawn from open-source projects and covering distinct Common Weakness Enumeration (CWE) classes.","No, 本文是使用该数据集进行评测","Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities drawn from open-source projects...",,,评估大型语言模型（LLM）在自动化漏洞修复方面的有效性，具体针对真实漏洞和人工生成的漏洞。,"In this study, we empirically evaluate the patching effectiveness and complementarity of several prominent LLMs... using both real and artificial vulnerabilities.",漏洞修复的有效性、模型间的互补性与重叠性,Our paper empirically investigates the effectiveness and complementarity of several prominent LLMs in automated vulnerability patching...,"使用漏洞证明（Proof-of-Vulnerability, PoV）测试执行来验证生成的补丁是否成功修复漏洞。",Our evaluation employs Proof-of-Vulnerability (PoV) test execution to concretely assess whether LLM-generated source code successfully patches vulnerabilities.,单函数/代码片段,"Given a vulnerable code snippet, these models generate a patched version that aims to eliminate security flaws while preserving functionality.",软件安全、漏洞修复,Automated vulnerability patching is crucial for software security...,,,Java,"Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities...",15个真实漏洞及其41个人工生成的对应漏洞,"To perform this evaluation, we employed 15 real vulnerabilities and their 41 artificial counterparts (vulnerabilities).",真实漏洞来自开源项目和公共漏洞数据库（如CVE），人工漏洞由CodeBERT生成并经过验证。,"Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities drawn from open-source projects... Secondly, we augmented our evaluation with artificially generated vulnerabilities... we incorporated artificial vulnerabilities derived from the work of Garg et al.[15]. Garg et al. used CodeBERT[11] to generate thousands of candidate artificial vulnerabilities...",,,官方自建（Vul4J数据集）,"Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities drawn from open-source projects...",,,,,代码修复（漏洞补丁生成）,"Given a vulnerable code snippet, these models generate a patched version that aims to eliminate security flaws while preserving functionality.",PoV测试通过率（基于执行的验证）,Our evaluation employs Proof-of-Vulnerability (PoV) test execution to concretely assess whether LLM-generated source code successfully patches vulnerabilities.,代码（包含漏洞的代码片段）,"Given a vulnerable code snippet, these models generate a patched version...",代码（修复后的代码片段）,"Given a vulnerable code snippet, these models generate a patched version...",代码到代码,"Given a vulnerable code snippet, these models generate a patched version...",Maven/Gradle构建环境，包含PoV JUnit测试用例,"When no suitable test existed in the original project, Bui et al. [2] manually wrote one by following the exploit steps in the CVE report, thereby guaranteeing that every vulnerability in the dataset can be triggered, reproduced, and validated in a Maven/Gradle build.",该研究不仅评估LLM对真实漏洞的修复能力，还评估其对人工生成漏洞的修复能力，以测试模型的泛化性和鲁棒性。数据集（Vul4J）为每个漏洞提供了可执行的漏洞证明（PoV）测试用例，用于自动化验证补丁的有效性。,"Our paper empirically investigates the effectiveness and complementarity of several prominent LLMs in automated vulnerability patching, specifically focusing on both real vulnerabilities and their corresponding artificial vulnerabilities... For every entry Vul4J provides... one or more Proof-of-Vulnerability (PoV) JUnit test cases. A PoV test is an executable exploit oracle..."
2401.01062_output/content.md,CAASD (Capability Assessment of Automatic Software Development),we have developed a novel benchmark named CAASD (Capability Assessment of Automatic Software Development).,Yes,we have developed a novel benchmark named CAASD (Capability Assessment of Automatic Software Development).,,,评估AI辅助软件开发系统的能力，特别是针对系统级实现任务的能力,all of them either are limited to simple function-level implementation tasks or lack detailed requirements specifications for system-level implementation tasks,系统级软件开发能力评估,for assessing how well a software development task is completed,通过参考用例评估系统实现的质量和完整性,Each task of CAASD is equipped with a list of reference use cases depicting the system requirements. The reference use cases are used to evaluate the quality and completeness of a system implementation.,系统级开发（多文件项目）,system-level implementation tasks,软件开发,software development,非平凡软件项目,non-trivial software projects,,,,,,,2024,arXiv:2401.01062v1 [cs.SE] 2 Jan 2024,官方自建,we have developed a novel benchmark named CAASD,,,,,系统级实现任务,system-level implementation tasks,通过率,AISD achieves an impressive pass rate of 75.2%,自然语言需求描述,high-level (potentially vague) user requirements as inputs,系统实现,system implementation,需求到系统实现,"taking high-level (potentially vague) user requirements as inputs, generates detailed use cases, prototype system designs, and subsequently system implementation",,,首个评估软件开发任务完成质量的基准，包含详细的系统需求规范,this is the first benchmark that offers criteria for assessing how well a software development task is completed
2401.12554_output/content.md,ParEval,we propose the Parallel Code Generation Evaluation (ParEval) benchmark: a set of benchmarks (prompts) for evaluating how well LLMs generate parallel code.,Yes,we propose the Parallel Code Generation Evaluation (ParEval) benchmark,github.com/parallelcodefoundry/ParEval,ParEval is available online at: github.com/parallelcodefoundry/ParEval.,评估大型语言模型生成并行代码的能力,we study the capabilities of state-of-the-art language models to generate parallel code.,并行代码生成正确性和性能,"We evaluate several state-of-the-art open- and closed-source LLMs using these benchmarks, and report metrics that represent the correctness and performance of the generated code.",新颖的代码生成评估指标，包括speedup𝑛@k和efficiency𝑛@k，用于评估生成代码的性能和扩展性,We introduce novel code generation evaluation metrics that assess performance and parallel scaling.,,,科学和并行计算,consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing.,,,C/C++,"Traditional Python code generation benchmarks are tested by running eval on the generated code for a small number of small unit tests. On the other hand, in the case of parallel code — we must compile C/C++ code, link against one or more parallel libraries, and run the code in the proper parallel environment.",420个不同的编码任务,consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing.,手动设计,These benchmarks are challenging to test. Traditional Python code generation benchmarks are tested by running eval on the generated code for a small number of small unit tests.,2024,"HPDC ’24, June 3–7, 2024, Pisa, Italy",官方自建,we propose the Parallel Code Generation Evaluation (ParEval) benchmark: a set of benchmarks (prompts) for evaluating how well LLMs generate parallel code.,,,,,代码生成,we study the capabilities of state-of-the-art language models to generate parallel code.,speedup𝑛@k和efficiency𝑛@k,"We introduce two novel metrics, speedup𝑛@k and efficiency𝑛@k, for evaluating the performance and scaling of LLM generated code.",自然语言,consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing.,代码,we study the capabilities of state-of-the-art language models to generate parallel code.,文本到代码,consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing.,需要特定依赖（并行库）,"we must compile C/C++ code, link against one or more parallel libraries, and run the code in the proper parallel environment.",专注于并行代码生成评估，覆盖12种计算问题类型和7种执行模型,"These benchmarks cover twelve different computational problem types, and seven different execution models: serial, OpenMP, Kokkos, MPI, MPI+OpenMP, CUDA, and HIP."
2512.01010_output/content.md,Chain of Unit-Physics,"To address these limitations, this work conceptualizes an inverse approach to code design, embodied in the Chain of Unit-Physics framework: a first-principles (or primitives)-centric, multi-agent system in which human expert knowledge is encoded as unit-physics tests that explicitly constrain code generation.",Yes,"To address these limitations, this work conceptualizes an inverse approach to code design, embodied in the Chain of Unit-Physics framework: a first-principles (or primitives)-centric, multi-agent system in which human expert knowledge is encoded as unit-physics tests that explicitly constrain code generation.",,,该评测基准旨在解决科学计算中的代码生成任务，特别是针对具有严格物理约束的高风险科学问题，如燃烧科学中的计算流体动力学（CFD）求解器开发。,Developing computational scientific software from natural-language queries remains challenging broadly due to (a) sparse representation of domain codes during training and (b) the limited feasibility of RLHF with a small expert community.,物理一致性、数值稳定性、算法正确性,"The framework is evaluated on a nontrivial combustion task (12 degrees-of-freedom), used here as a representative benchmark for scientific problem with realistic physical constraints.",基于单元物理测试的验证方法，包括物理约束检查、数值一致性检查和诊断分析,The Verification Agent applies formalized unit-physics tests to assess physical and numerical consistency. These primitives yield physics-grounded verification even without reference datasets.,多步骤科学计算任务，涉及复杂的物理约束和数值计算,"The framework is evaluated on a nontrivial combustion task (12 degrees-of-freedom), used here as a representative benchmark for scientific problem with realistic physical constraints.",计算科学，特别是燃烧科学和计算流体动力学,"To overcome these limitations, this work systematically applies an inverse code design methodology (see Fig. 1), formally known as test-driven development (TDD) [26], to scientific software in combustion science, one such domain where TDD approaches have not been thoroughly evaluated.",高难度，涉及12自由度的复杂燃烧任务,"The framework is evaluated on a nontrivial combustion task (12 degrees-of-freedom), used here as a representative benchmark for scientific problem with realistic physical constraints.",,,,,基于人类专家知识构建的单元物理测试,"This work proposes Chain of Unit-Physics, an approach that embeds human expert knowledge directly into distinct reasoning chains of the agentic system via 'unit-physics'—formalized, testable constraints that encode fundamental physics (e.g., energy conservation laws) and practitioner experience (e.g., dimensional / bound checks, and floating-point diagnostics).",2025,arXiv:2512.01010v1 [cs.MA] 30 Nov 2025,官方自建，基于人类专家知识,"This work proposes Chain of Unit-Physics, an approach that embeds human expert knowledge directly into distinct reasoning chains of the agentic system via 'unit-physics'—formalized, testable constraints that encode fundamental physics (e.g., energy conservation laws) and practitioner experience (e.g., dimensional / bound checks, and floating-point diagnostics).",,,,,端到端科学代码生成,"Closed-weight systems and code-focused agentic variants fail to produce correct end-to-end solvers, despite tool and web access, exhibiting four recurrent error classes: interface (syntax/API) hallucinations, overconfident assumptions, numerical/physical incoherence, and configuration fragility.",物理一致性、数值误差、运行时间和内存使用效率,"On the benchmark task, the proposed framework converges within 5–6 iterations, matches the human-expert implementation (mean error of 3.1ˆ10´3%), with a „33.4% faster runtime and a „30% efficient memory usage at a cost comparable to mid-sized commercial APIs, yielding a practical template for physics-grounded scientific code generation.",自然语言查询和单元物理测试,"In the input, the user’s scientific question is paired with 'basis prompts' that establish execution permissions, tool availability, coding language settings, and transfer protocols. Additional input scopes the unit-physics tests that concatenated with the scientific query to form the complete input to the framework.",科学计算代码和可视化结果,"Finally, the agent consolidates the output into domain-relevant visualizations (for example, line graphs and contour graphs of key quantities of interest), closing the loop between natural-language inquiry and quantitative scientific insight.",自然语言到科学代码,Developing computational scientific software from natural-language queries remains challenging broadly due to (a) sparse representation of domain codes during training and (b) the limited feasibility of RLHF with a small expert community.,Python虚拟沙箱环境，具有隔离的执行权限,"The framework runs inside a dedicated Python virtual sandbox environment that is not pre-configured with metadata on all available libraries. The agent is granted isolated code execution privileges within this sandbox, ensuring that any dependency installs or script executions cannot affect the system root directory or global files.",基于第一性原理的单元物理测试驱动代码生成，强调物理一致性和数值稳定性,"This inverse-design method provides two key advantages in scientific software. First, human-authored tests embed deep domain expertise (first principles or primitives) into targeted validation checks, ensuring that each algorithmic component faithfully represents the underlying physics."
2512.01396_output/content.md,BackportBench,"we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem.",Yes,"we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem.",https://github.com/BackportBench/BackportBench,The BackportBench is available on https://github.com/BackportBench/BackportBench.,自动化补丁回移植。旨在为未打补丁的软件版本生成补丁，基于已有的补丁和两个版本之间的代码差异。,BackportBench defines the backporting problem as generating a patch for the unpatched software version based on the existing patch and the code differences between the two versions.,自动化补丁回移植的有效性、多语言能力、处理跨文件不兼容性的能力,"To facilitate the development and evaluation of automated backporting techniques... BackportBench is a multilingual benchmark... This formulation shifts the patch porting problem from code-hunk level or function level to repository level, offering a more realistic representation of software maintenance challenges.",在可执行的Docker环境中运行相关测试用例进行验证,"each task instance provides an executable Docker environment with scripts for running relevant test cases, thereby aligns with the criteria for a successful benchmark [5].",仓库级别，涉及跨文件的不兼容性处理,"This formulation shifts the patch porting problem from code-hunk level or function level to repository level, offering a more realistic representation of software maintenance challenges.",软件安全、软件维护、漏洞修复,BackportBench contains 202 real-world vulnerability patch backporting task instances curated from three popular OSS ecosystems... To remove such vulnerabilities... patch backporting is a helpful practice that adapts patches for other versions and makes it easier to deliver patches to users on older branches.,现实世界工程级，涉及逻辑和结构性变更,"the agentic method has outperformed traditional patch porting methods, especially on cases that require logical and structural changes.","Python, Java, JavaScript","BackportBench contains 202 patch backporting problems from PyPI, Maven, and npm, covering Python, Java, and JavaScript, respectively.",包含202个补丁回移植任务实例,BackportBench contains 202 real-world vulnerability patch backporting task instances,"从三个流行的开源软件生态系统（PyPI, Maven, npm）中收集的真实世界漏洞补丁回移植任务","BackportBench contains 202 real-world vulnerability patch backporting task instances curated from three popular OSS ecosystems, PyPI , Maven, and npm",2018 (根据论文版权日期)，但arXiv版本为2025年12月,© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ... arXiv:2512.01396v1  [cs.SE]  1 Dec 2025,官方自建,"we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem.",,,ACM版权，允许个人或课堂使用，禁止商业用途,Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.,代码修复/补丁生成,generating a patch for the unpatched software version based on the existing patch and the code differences between the two versions.,通过测试用例验证补丁有效性，而非基于等价性或相似性的指标,each task instance provides an executable Docker environment with scripts for running relevant test cases... We argue that whether backported patches achieve equivalence is not the only way to prove the patch is effective.,代码（两个版本的代码库及原始补丁）,generating a patch for the unpatched software version based on the existing patch and the code differences between the two versions.,代码（回移植后的补丁）,generating a patch for the unpatched software version,代码到代码,generating a patch for the unpatched software version based on the existing patch and the code differences between the two versions.,可执行的Docker环境，包含运行相关测试用例的脚本,each task instance provides an executable Docker environment with scripts for running relevant test cases,首个针对补丁回移植问题的综合性、多语言基准测试套件，包含可执行环境和测试用例，将问题从代码块/函数级别提升到仓库级别，更贴近现实软件维护挑战。,"the first comprehensive benchmark suite for patch backporting problem... a multilingual benchmark... contains executable Docker environments and test cases for validation... This formulation shifts the patch porting problem from code-hunk level or function level to repository level, offering a more realistic representation of software maintenance challenges."
2512.01255_output/content.md,ARENAJS,we construct ARENAJS—the first systematic benchmark for LLM-based JavaScript vulnerability detection,Yes,"In this paper, we introduce, for the first time, three principles for constructing a benchmark for JavaScript vulnerability detection... we propose FORGEJS... Then, we use FORGEJS to construct ARENAJS—the first systematic benchmark for LLM-based JavaScript vulnerability detection",,,评估大型语言模型在JavaScript漏洞检测方面的能力,evaluating LLMs’ capability in JavaScript vulnerability detection,漏洞检测能力、推理充分性、鲁棒性、现实世界可用性,"reveals key limitations in reasoning sufficiency, robustness, and real-world usability",使用JUDGEJS自动评估框架，包含提示模板、响应解析、标签对齐和稳健评分，利用语义等价和模糊匹配，在函数级和项目级生成统一的、严格的、可追溯的指标套件，包括F1、假阳性率（FPR）以及工程约束下的检测效能,"we propose JUDGEJS, an automated evaluation framework that spans prompt templates, response parsing, label alignment, and robust scoring; leveraging semantic equivalence and fuzzy matching, it harmonizes heterogeneous outputs and yields a unified, rigorous, and traceable metric suite across function- and project-levels, including F1, false positive rate (FPR), and detection efficacy under engineering constraints.",函数级和项目级（完整项目）,supports both function-level and project-level evaluation,JavaScript安全漏洞检测，涵盖前端（如DOM-based XSS）、后端（如SQL注入、命令注入、原型污染）和全栈环境,JavaScript vulnerability patterns vary substantially across these environments: backend prototype pollution... while frontend prototype pollution predominantly causes DOM-based XSS or client-side denial of service.,现实世界工业级，反映真实仓库的复杂性,fails to reflect the complexity of real-world repositories,JavaScript,benchmark for JavaScript vulnerability detection,覆盖数千个真实的JavaScript项目,A benchmark that spans thousands of real JavaScript projects,聚合异构来源，结合真实世界和合成数据,aggregates heterogeneous sources... combining real-world and synthetic data,2025-12-01 (根据arXiv版本日期),arXiv:2512.01255v1  [cs.CR]  1 Dec 2025,官方自建，通过FORGEJS自动生成框架构建,"we propose FORGEJS, the first automatic benchmark generation framework... Then, we use FORGEJS to construct ARENAJS",通过使用完整项目而非单文件片段、构建修复前后项目对来直接量化假阳性、以及系统引入四种增强策略来破坏对文件名、导入路径和注释的依赖，旨在避免高估,"To avoid overestimation, FORGEJS uses complete projects rather than single-file snippets, constructs pre- and post-fix project pairs to directly quantify false positives and localize error sources, and systematically introduces four augmentation strategies to disrupt reliance on filenames, import paths, and comments.",,,漏洞检测（包括定位漏洞文件、函数、CWE类型和具体代码行）,"evaluation protocols commonly assess only whether vulnerability is detected and ignore reasoning quality such as whether the model can correctly localize the vulnerable file, function, and CWE type.",F1分数、假阳性率（FPR）、VD-S指标,"including F1, false positive rate (FPR), and detection efficacy under engineering constraints... from the perspective of the VD-S metric [6]",JavaScript代码（函数或完整项目）,supports both function-level and project-level evaluation,漏洞检测结果（包括是否存在漏洞、CWE类型、定位信息、推理等）,"evaluation protocols commonly assess only whether vulnerability is detected and ignore reasoning quality such as whether the model can correctly localize the vulnerable file, function, and CWE type.",代码到安全分析,evaluating LLMs’ capability in JavaScript vulnerability detection,,,首个基于LLM的JavaScript漏洞检测系统基准；遵循三个构建原则：全面性、不低估、不高估；覆盖218种CWE类型；支持函数级和项目级评估；通过FORGEJS自动生成；包含JUDGEJS自动评估框架；旨在客观定义LLM在可重复、可扩展、部署受限条件下的现实性能上下限。,"we introduce, for the first time, three principles for constructing a benchmark for JavaScript vulnerability detection that directly address these limitations: (1) comprehensiveness, (2) no underestimation, and (3) no overestimation... FORGEJS aggregates heterogeneous sources, covers 218 CWE types... supports both function-level and project-level evaluation... Taken together, these principles aim to objectively define the realistic upper and lower bounds of LLM performance under reproducible, scalable, deployment-constrained conditions."
2105.09938_output/content.md,APPS,"To meet this challenge, we introduce APPS, a benchmark for code generation.",Yes,"To meet this challenge, we introduce APPS, a benchmark for code generation.",https://github.com/hendrycks/apps,The dataset is available at https://github.com/hendrycks/apps.,从任意自然语言描述生成满足要求的Python代码。,our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code.,代码生成能力，包括理解任务描述、设计算法、编写语法正确且功能正确的程序。,"APPS evaluates models not only on their ability to code syntactically correct programs, but also on their ability to understand task descriptions and devise algorithms to solve these tasks.",使用测试用例检查生成的代码。,"Similar to how companies assess candidate software developers, we evaluate models by checking their generated code on test cases.",单函数（Call-Based Format）或完整脚本（Standard Input/Output Format）。,"• Call-Based Format problems generally provide initial starter code, usually in the form of a function header, and ask for the solution to be provided as the function’s return value.",编程与算法，涵盖从简单字符串操作到复杂的图论、数据结构等算法挑战。,"Our benchmark includes 10,000 problems, which range from having simple one-line solutions to being substantial algorithmic challenges.",分为入门级、面试级和竞赛级三个难度。,"It contains 10,000 programming problems at various levels of difficulty, covering simple introductory problems, interview-level problems, and coding competition challenges.",Python,our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code.,"包含10,000个编程问题，131,777个测试用例，232,421个人工编写的参考答案。","The Automated Programming Progress Standard, abbreviated APPS, consists of 10,000 coding problems in total, with 131,777 test cases for checking solutions and 232,421 ground-truth solutions written by humans.","从开放的编程网站（如Codeforces, Kattis, Codewars, AtCoder）手动收集和整理。","The APPS dataset consists of problems collected from different open-access coding websites such as Codeforces, Kattis, and more.",2021,35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks.,官方自建，由多名研究生和本科生在六个月内精心整理和优化。,"Several graduate and undergraduate student authors polished and refined this dataset over the course of six months, ensuring a high-quality set of problems.",,,,,代码生成,"APPS, a benchmark for code generation from natural language specifications.",通过测试用例的准确率（例如 pass@k）。,we evaluate models by checking their generated code on test cases.,自然语言,take an arbitrary natural language specification,代码,generate satisfactory Python code,文本到代码,code generation from natural language specifications.,允许执行任意Python代码（包括导入常见模块和库），使用自定义的测试框架。,"solutions are allowed to execute arbitrary Python code, and the results are compared against test cases for a given problem.",1. 问题描述平均长度达293.2个单词，是自包含的完整规范。2. 拥有大量测试用例（超过13万个）用于严格评估功能正确性。3. 难度分级明确，覆盖从入门到竞赛的广泛范围。4. 模拟了人类程序员（如求职面试）的评估方式。,"By comparison, problem specifications in our new APPS benchmark are self-contained and have a much larger average length of 293.2 words. Unlike Iyer et al. (2018), APPS contains test cases for every exercise, enabling a high-quality evaluation of code correctness. The APPS benchmark attempts to mirror how humans programmers are evaluated by posing coding problems in unrestricted natural language and using test cases to evaluate solution correctness."
2107.03374_output/content.md,HumanEval,"On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings",Yes,"We evaluate functional correctness on a set of 164 hand-written programming problems, which we call the HumanEval dataset. We release the HumanEval dataset so that others can evaluate functional correctness and measure the problem-solving capabilities of their models.",https://www.github.com/openai/human-eval,We release this data along with an evaluation framework at https://www.github.com/openai/human-eval.,从文档字符串（docstrings）合成独立的Python函数，并评估生成代码的功能正确性。,"In this work, we focus on the task of generating standalone Python functions from docstrings, and evaluate the correctness of code samples automatically through unit tests.",代码生成的功能正确性,measure functional correctness for synthesizing programs from docstrings,通过单元测试评估功能正确性，并使用pass@k指标。,evaluate the correctness of code samples automatically through unit tests. Kulal et al. (2019) evaluate functional correctness using the pass@k metric,单函数,generating standalone Python functions,语言理解、推理、算法和简单数学,"Programming tasks in the HumanEval dataset assess language comprehension, reasoning, algorithms, and simple mathematics.",评估语言理解、算法和简单数学，部分问题类似于简单的软件面试题。,"These problems assess language comprehension, algorithms, and simple mathematics, with some comparable to simple software interview questions.",Python,study its Python code-writing capabilities. ... generating standalone Python functions from docstrings,包含164个手写的编程问题，平均每个问题有7.7个测试用例。,a dataset of 164 original programming problems with unit tests. ... an average of 7.7 tests per problem.,手写构建，未从现有来源程序化复制。,all problems were hand-written and not programmatically copied from existing sources.,2021,arXiv:2107.03374v2 [cs.LG] 14 Jul 2021,官方自建（OpenAI团队手写）,we create a dataset of 164 original programming problems. ... all problems were hand-written,为抗污染而设计，手写问题以避免训练数据（GitHub）中已存在的解决方案。,"It is important for these tasks to be hand-written, since our models are trained on a large fraction of GitHub, which already contains solutions to problems from a variety of sources.",,,代码生成（从文档字符串生成完整函数体）,generating standalone Python functions from docstrings,pass@k,Kulal et al. (2019) evaluate functional correctness using the pass@k metric,自然语言（文档字符串）与代码（函数签名）,"synthesizing programs from docstrings. ... prompt consisting of a header, a signature, and a docstring",代码（Python函数体）,generating standalone Python functions,文本（文档字符串）到代码,synthesizing programs from docstrings,使用gVisor容器运行时的沙盒环境，用于安全执行不受信任的程序。,"we developed a sandbox environment to safely run untrusted programs against unit tests. We selected the gVisor container runtime (Lacasse, 2018) as the main host protection component.",专门为评估代码生成模型的功能正确性而设计，强调手写问题以避免数据污染，并提供了安全的沙盒执行环境。,"It is important for these tasks to be hand-written, since our models are trained on a large fraction of GitHub, which already contains solutions to problems from a variety of sources. ... we developed a sandbox environment to safely run untrusted programs against unit tests."
2108.07732_output/content.md,Mostly Basic Programming Problems (MBPP),We introduce two datasets to test Python code synthesis. The first is a new dataset called Mostly Basic Programming Problems (MBPP).,Yes,We introduce two datasets to test Python code synthesis. The first is a new dataset called Mostly Basic Programming Problems (MBPP).,,,从自然语言描述中合成简短的Python程序。,Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions.,程序合成能力，特别是从自然语言描述生成功能正确的Python代码。,This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages.,通过执行测试用例检查功能正确性。,Participants also provided a ground-truth solution that passes all three test cases.,单函数，自包含。,"Participants were instructed to write code that is self-contained (that is, it runs by itself)...",涵盖数学、列表处理、字符串处理、整数序列等。,"Of these questions, 58% were mathematical in nature (e.g., calculating the volume of a sphere), 43% involve list processing, 19% require string processing, 9% involve integer seque",入门级，设计为入门级程序员可解决。,"The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers.",Python,The Mostly Basic Programming Problems dataset contains 974 short Python programs...,包含974个编程任务。,The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks...,通过众包构建，部分由作者编辑和手动验证。,This dataset consists of a large set of crowd-sourced questions and a smaller set of questions edited and hand-verified by the authors.,2021,arXiv:2108.07732v1  [cs.PL]  16 Aug 2021,官方自建（Google Research）,"We construct two new datasets: one entirely new and the other modified from an existing benchmark. The first, Mostly Basic Programming Problems (MBPP), is an entirely new crowd-sourced programming dataset.",文中提及与预训练集的重叠很小，降低了记忆风险。,"Second, we find that the overlap between the solutions in MBPP and the pre-training set is small, reducing the chance that our synthesis results are due to memorization (Section 4.8).",,,代码生成（从自然语言描述生成完整的函数体）。,"Participants were instructed to write a short problem statement, a single self-contained Python function solving the problem specified...",功能正确性（通过测试用例）。,Participants also provided a ground-truth solution that passes all three test cases.,自然语言描述，通常结合少量输入输出示例。,"a program can be specified by a short natural language description, possibly combined with a few (e.g., 2 or 3) input-output examples.",代码（Python函数）。,"Participants were instructed to write a short problem statement, a single self-contained Python function solving the problem specified...",文本到代码,Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions.,自包含，可使用标准库，允许使用互联网参考。,"Participants were instructed to write code that is self-contained (that is, it runs by itself)... Use of internet references was allowed.",包含三个一致的、以assert语句编写的输入输出示例；问题描述设计为更基础、字面化，而非竞赛风格；包含众包和手动验证两部分。,"In contrast, our dataset consistently contains three I/O examples, written as assert statements. ... By contrast, our Mostly Basic Programming Problems dataset is designed to contain a more basic, literal description of the problems. ... This dataset consists of a large set of crowd-sourced questions and a smaller set of questions edited and hand-verified by the authors."
2203.13474_output/content.md,Multi-Turn Programming Benchmark (MTPB),"To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts.",Yes,"In this work, we develop a multi-turn programming benchmark to measure the models’ capacity for multi-turn program synthesis. To the best of our knowledge, this is the first multi-turn program synthesis benchmark, which allows quantitative analysis of multi-turn program synthesis.",https://github.com/salesforce/CodeGen/tree/main/benchmark,1Benchmark: https://github.com/salesforce/CodeGen/tree/main/benchmark,评测模型的多轮程序合成能力。用户以自然语言在多轮对话中逐步指定意图，模型在每一轮中合成子程序，最终共同完成一个完整的程序。,"To solve a problem in the benchmark, a model needs to synthesize a program in multiple steps with a user who specifies the intent in each turn in natural language.",多轮程序合成能力,"In this work, we develop a multi-turn programming benchmark to measure the models’ capacity for multi-turn program synthesis.",通过专家编写的测试用例的通过率（pass rate）来衡量性能。,Performance on the benchmark is measured by pass rate on expert-written test cases.,多轮对话上下文。一个完整的程序被分解为多个子问题，每一轮对话指定一个子问题的意图。,consisting of 115 diverse problem sets that are factorized into multi-turn prompts.,通用编程问题。文中示例为从电子邮件地址中提取用户名。,Please refer to Figure 1 for an example where the model synthesizes a program to extract the user name of an email address.,,,未明确指定，但根据模型训练和上下文（如HumanEval部分），可能主要涉及Python。,文中未明确描述MTPB的编程语言。,包含115个多样化的问题集。,consisting of 115 diverse problem sets that are factorized into multi-turn prompts.,,,2023（根据论文版本日期推断）,arXiv:2203.13474v5 [cs.LG] 27 Feb 2023,官方自建,"we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB)",,,,,多轮程序合成,"To solve a problem in the benchmark, a model needs to synthesize a program in multiple steps",通过率（pass rate）,Performance on the benchmark is measured by pass rate on expert-written test cases.,自然语言（多轮对话）,a user who specifies the intent in each turn in natural language.,代码（子程序或完整程序）,synthesize a program,自然语言到代码,synthesize a program... with a user who specifies the intent in each turn in natural language.,,,首个专注于多轮程序合成的评测基准。问题被分解为多轮提示，模拟用户与系统逐步协作完成编程任务的过程。,"To the best of our knowledge, this is the first multi-turn program synthesis benchmark, which allows quantitative analysis of multi-turn program synthesis."
2207.01780_output/content.md,APPS,"Our comprehensive experiments (Section 4) show that our models can achieve SOTA performance on the challenging APPS benchmark [Hendrycks et al., 2021].","No, 本文是使用该数据集进行评测","Our comprehensive experiments (Section 4) show that our models can achieve SOTA performance on the challenging APPS benchmark [Hendrycks et al., 2021].",,,程序合成或代码生成，旨在根据自然语言问题描述生成满足功能正确性的程序。,Program synthesis or code generation aims to generate a program that satisfies a problem specification.,程序的功能正确性,The expected output is a program to be checked for functional correctness against some unit tests.,使用单元测试检查程序的功能正确性，并采用 pass@k 指标进行评估。,"Specifically, our models reach more than 2% pass@1, 6% pass@5, and 20% pass@1000.",,,通用编程，范围从基础编程问题到竞赛级编程任务。,This type of programming task can range from basic programming problems to competition-level programming tasks that require a high level of problem-solving skills.,具有挑战性，包含竞赛级复杂问题。,"Our comprehensive experiments (Section 4) show that our models can achieve SOTA performance on the challenging APPS benchmark [Hendrycks et al., 2021].",Python,"In our work, we focus on program synthesis from natural language problem specifications and the output programs are in general-purpose languages such as Python.",,,,,,,,,,,,,完整程序生成,"Training only with NTP objective is hence, not ideal to tackle full program generation to solve programming problems.","pass@1, pass@5, pass@1000","Specifically, our models reach more than 2% pass@1, 6% pass@5, and 20% pass@1000.",自然语言问题描述，通常包含示例输入输出对。,"Each task is defined by a problem specification in natural language, often containing example input and output pairs.",代码,The expected output is a program to be checked for functional correctness against some unit tests.,文本到代码,Program synthesis or code generation is the task of designing and building an executable computer program that satisfies a problem specification.,编译器,These program candidates are concatenated with the error information received from a compiler and passed to a program repair module.,该基准（APPS）包含从基础到竞赛级别的编程问题，用于评估模型生成功能正确程序的能力。,This type of programming task can range from basic programming problems to competition-level programming tasks that require a high level of problem-solving skills.
2207.10397_output/content.md,HumanEval,"We conduct comprehensive experiments on four benchmarks, HumanEval, MBPP, APPS, and CodeContests","No, 本文是使用该数据集进行评测","We conduct comprehensive experiments on four benchmarks, HumanEval, MBPP, APPS, and CodeContests",,,从自然语言描述（代码注释）和函数签名等上下文信息中生成代码解决方案。,"The task of code generation is to solve a programming problem: generate code solution x based on context c. As shown in Figure 2, context c contains natural language problem description in the form of code comment, and a code snippet that includes statements such as imports and the function header.",代码生成的功能正确性,A natural way to evaluate the quality and correctness of a code solution is to run it against a set of test cases,执行单元测试，使用 pass@k 指标,"We evaluate functional correctness using pass@1. ... For instance, Codex ... can achieve a pass@100 (pass if one or more among 100 generated solutions for a given problem can pass the corresponding test cases) of 77.4%, but a pass@1 (correct rate of a single solution) of only 33.5% on the HumanEval benchmark",单函数,"Figure 2: Code generation and test case generation: an example from the HumanEval benchmark. ... Context c contains natural language problem description in the form of code comment, and a code snippet that includes statements such as imports and the function header.",,,,,Python,"Figure 2: Code generation and test case generation: an example from the HumanEval benchmark. ... from typing import List def has_close_elements(numbers: List[float], threshold: float) -> bool:",164个问题,Benchmark Problems ... HumanEval 164,,,,,,,,,,,代码生成,The task of code generation is to solve a programming problem: generate code solution x based on context c.,"pass@1, pass@100","For instance, Codex ... can achieve a pass@100 (pass if one or more among 100 generated solutions for a given problem can pass the corresponding test cases) of 77.4%, but a pass@1 (correct rate of a single solution) of only 33.5% on the HumanEval benchmark",自然语言与代码（函数签名）,"context c contains natural language problem description in the form of code comment, and a code snippet that includes statements such as imports and the function header.",代码,generate code solution x,文本到代码,The task of code generation is to solve a programming problem: generate code solution x based on context c.,,,每个问题平均有7.77个地面真值测试用例,Benchmark ... GT Tests ... HumanEval ... 7.77
2208.08227_output/content.md,MultiPL-E,"We propose MultiPL-E, a system for translating unit test-driven code generation benchmarks to new languages.",Yes,"We propose MultiPL-E, a system for translating unit test-driven code generation benchmarks to new languages. We create the ﬁrst massively multilingual code generation benchmark by using MultiPL-E to translate two popular Python code generation benchmarks to 18 additional programming languages.",github.com/nuprl/MultiPL-E,"The MultiPL-E system, dataset, and tutorial are available at github.com/nuprl/MultiPL-E.",将基于单元测试的代码生成基准（从Python）翻译到新的编程语言，以创建大规模、多语言的并行代码生成评测基准。,"MultiPL-E, a system for translating unit test-driven code generation benchmarks to new languages. We create the ﬁrst massively multilingual code generation benchmark by using MultiPL-E to translate two popular Python code generation benchmarks to 18 additional programming languages.",多语言代码生成能力，模型在不同编程语言上的泛化性能,We use these new parallel benchmarks to evaluate the multi-language performance of three state-of-the-art code generation models... The range of programming languages represented in MultiPL-E allow us to explore the impact of language frequency and language features on model performance.,通过单元测试评估生成代码的正确性，使用pass@k等指标,We judge a generated function correct if it passes all tests... it is common to sample multiple completions per problem and report an estimated pass rate (§4.2).,单函数,"All of the problems are functions that receive and return ﬁrst-order values, which facilitates unit testing and test translation.",通用编程问题，涵盖算法和数据处理,It is a diverse collection of 164 problems... All of the problems are functions that receive and return ﬁrst-order values.,具有挑战性,"Moreover, it is a challenging benchmark: the best model evaluated by Fried et al. [4] achieves only a 36% pass rate on Python.","Python（源语言）及通过MultiPL-E翻译的18种其他编程语言（包括JavaScript, C++, Scala, TypeScript等）",We use MultiPL-E to extend the HumanEval benchmark [1] and MBPP benchmark [2] to 18 languages that encompass a range of programming paradigms and popularity... MultiPL-E supports 18 languages and is straightforward to extend with more.,通过翻译HumanEval（164个问题）和MBPP（未明确数量）两个基准，为19种语言（Python + 18种）创建并行问题集,We create the ﬁrst massively multilingual code generation benchmark by using MultiPL-E to translate two popular Python code generation benchmarks to 18 additional programming languages... HumanEval is a diverse collection of 164 problems.,基于现有Python基准（HumanEval和MBPP）通过自动化编译器翻译生成,"We use MultiPL-E to translate two widely-used code generation benchmarks, HumanEval [1] and MBPP [2], into 18 languages.",2022,arXiv:2208.08227v4  [cs.LG]  19 Dec 2022,官方自建（论文作者构建）,We propose MultiPL-E... We create the ﬁrst massively multilingual code generation benchmark...,,,,,代码生成（从自然语言描述生成函数体）,"We focus on the natural-language-to-code task (NL2Code): given the description of a function in natural language, complete the function body.",pass率（基于单元测试）,We judge a generated function correct if it passes all tests... it is common to sample multiple completions per problem and report an estimated pass rate (§4.2).,自然语言（函数描述）与代码（函数签名、类型注解、示例）,"The prompt has several sources of information for the model: the function signature (its name and parameters); a brief comment describing the function; and, optionally, examples in the form of Python doctests.",代码（函数体）,"Given the prompt as input, the code generation model generates a completion that is likely to follow the given prompt.",文本到代码,"We focus on the natural-language-to-code task (NL2Code): given the description of a function in natural language, complete the function body.",容器化沙箱，用于编译（如需要）和运行程序,"MultiPL-E also includes a containerized sandbox that (1) compiles programs if necessary, (2) runs them with appropriate timeouts, (3) validates their results on unit tests, and (4) classiﬁes each output as successful, syntax error, etc.",1. 可扩展和可扩展的系统，用于将代码生成基准编译到新的编程语言。2. 创建了第一个大规模、多语言、并行的代码生成基准。3. 通过一套小型编译器（每个约200行代码）实现翻译，仅需翻译函数签名、单元测试、注释和类型注解，而无需翻译函数体。4. 包含一个规则工具，用于将注释中的技术术语翻译得更符合目标语言习惯。,"MultiPL-E uses a suite of 18 little compilers from Python benchmarks to each target language... Each compiler must be able to translate four components from Python: (1) a function signature (name and arguments), (2) simple unit tests, (3) a comment describing the expected function behavior, and (4) type annotations if the target language is statically typed. Notably, the compiler does not have to translate the body of a function, since it is the job of the code generation model to synthesize it. Thus each MultiPL-E compiler is approximately 200 LOC and easy to build. MultiPL-E also includes a simple, rule-based tool to translate technical terms in comments to be more language appropriate, e.g. a Python list is approximately a C++ vector."
2211.11501_output/content.md,DS-1000,"We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas.",Yes,"We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas.",https://ds1000-code-gen.github.io,We release our benchmark at https://ds1000-code-gen.github.io.,数据科学代码生成。旨在评估模型根据自然语言描述和代码上下文，生成用于解决实际数据科学问题的代码的能力。,"We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas.",数据科学代码生成能力、对特定库API的理解和使用、解决现实世界问题的实用性、抗记忆化能力,"DS-1000 incorporates three core features. First, our problems reflect diverse, realistic, and practical use cases... Finally, we proactively defend against memorization...",基于执行的多标准自动评估。包括：1) 运行测试用例检查功能正确性；2) 通过限制API使用或关键词来检查表面形式约束。,"we achieve this with multi-criteria metrics, checking both functional correctness by running test cases and surface-form constraints by restricting API usages or keywords.",包含代码上下文的单文件任务。用户问题通常包含错误的代码、错误消息和输入输出示例等多样化上下文。,"users’ data science coding problems usually have diverse contexts including their incorrect code, error messages, and input-output examples","数据科学。涵盖七个广泛使用的Python数据科学库：NumPy, Pandas, TensorFlow, PyTorch, SciPy, Scikit-learn, 和 Matplotlib。","a thousand problems covering seven widely-used Python data science libraries: NumPy, Pandas, TensorFlow, PyTorch, SciPy, Scikit-learn, and Matplotlib.",现实世界应用级。问题来源于StackOverflow，反映了日常数据科学应用中的实际用例，而非竞赛或面试风格。,"focuses on everyday data science applications... includes naturalistic intents and contexts... our problems reflect diverse, realistic, and practical use cases",Python,a code generation benchmark with a thousand data science problems spanning seven Python libraries,包含1000个问题。,a code generation benchmark with a thousand data science problems,从StackOverflow收集的自然发生的问题，经过人工筛选、修改和扰动。,"we collected naturally occurring problems from Stack-Overflow, manually scored their representativeness and usefulness, and curated a subset of them to create our benchmark.",2022-11-18 (根据arXiv版本v1日期),arXiv:2211.11501v1 [cs.SE] 18 Nov 2022,官方自建。由五位精通数据科学和Python的计算机科学学生作者花费约1200小时构建。,five authors who are computer science students and familiar with data science spent a total of about 1200 hours constructing DS-1000,抗污染设计。通过主动扰动原始StackOverflow问题来防御模型对预训练语料的记忆。,"we proactively defend against memorization by slightly modifying our problems to be different from the original Stack-Overflow source; consequently, models cannot answer them correctly by memorizing the solutions from pre-training.",,,代码生成（填空）。模型需要将代码填入提示中的“[insert]”位置。,The model needs to fill in the code into “[insert]” in the prompt,通过/不通过（基于测试用例和表面形式约束）。文中未明确命名如pass@k的指标，但报告了准确率（如43.3%）。,The current best public system (Codex-002) achieves 43.3% accuracy,自然语言与代码上下文混合。输入包括自然语言问题描述和部分可执行的代码上下文。,The model needs to fill in the code into “[insert]” in the prompt on the left; the prompt includes natural language description and code context.,代码。期望输出是一段填补缺失部分的Python代码。,The model needs to fill in the code into “[insert]”,文本与代码到代码。根据自然语言描述和给定的代码上下文，生成缺失的代码片段。,synthesizing programs from docstrings. (结合上下文推断，任务是从自然语言描述和代码上下文中合成程序),需要特定数据科学库依赖的沙盒环境。固定了Python 3.7.10及相应库的最新版本。,We fixed the evaluation environment to include the latest versions of libraries that can be installed with Python 3.7.10,1) 包含现实问题与多样化上下文；2) 可靠的多标准执行评估；3) 主动防御记忆化；4) 专注于七个核心数据科学库。,"We highlight three core features of DS-1000: 1) it contains realistic problems with diverse contexts, 2) it implements reliable multi-criteria execution-based evaluation metrics, and 3) it proactively defends against memorization."
2301.03988_output/content.md,MultiPL-E,"evaluate them on the MultiPL-E text-to-code benchmark (Cassano et al., 2022).","No, 本文是使用该数据集进行评测","evaluate them on the MultiPL-E text-to-code benchmark (Cassano et al., 2022).",,,评测从自然语言描述生成代码的能力（文本到代码）,"MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages.",多语言代码生成能力,"MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages.",使用单元测试评估生成代码的正确性,"The correctness of generated code can be tested using unit tests, a method for approximating semantic equivalence.",,,,,,,"文中仅提及实验子集（Java, JavaScript, Python），未描述完整数据集。MultiPL-E扩展了18种语言。","evaluate them on the MultiPL-E text-to-code benchmark (Cassano et al., 2022). ... MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages.",,,基于HumanEval和MBPP基准自动编译扩展,"MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages. The doctests, function signatures, and unit tests for each benchmark suite are automatically compiled to new languages.",,,,,,,,,代码生成（从自然语言描述生成完整函数）,"MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages.",,,自然语言,text-to-code benchmark,代码,text-to-code benchmark,文本到代码,text-to-code benchmark,,,"1. 通过自动编译将单语言基准（HumanEval, MBPP）扩展到多种编程语言。2. 评估时隐藏测试用例，仅用于验证正确性（与MBXP不同）。","MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages. ... In contrast, MultiPL-E keeps the tests hidden from the model and only uses them to test correctness."
2302.08468_output/content.md,Spider,"▷Spider (Yu et al., 2018) is a semantic parsing dataset on generating SQL queries from natural language questions.","No, 本文是使用该数据集进行评测","We conduct experiments on four language-to-code datasets across domains of semantic parsing, table QA, math reasoning and basic python programming.",,,从自然语言问题生成SQL查询（语义解析）,"▷Spider (Yu et al., 2018) is a semantic parsing dataset on generating SQL queries from natural language questions.",语言到代码生成的功能正确性,"The ability of mapping natural language to executable code is the cornerstone of a variety AI applications such as database interfaces (Pasupat & Liang, 2015; Yu et al., 2018; Shi et al., 2020)...",执行生成的程序并比较结果,"Given x, a generation model P(y|x) generates a program y which is later executed via an executor E(·) to obtain the result3 E(y).",,,数据库查询（Table QA）,"Spider (Yu et al., 2018) is a semantic parsing dataset on generating SQL queries from natural language questions.",,,SQL,Target: SQL,训练集7000条，开发集1032条,"# Train: 7,000, # Dev: 1,032",,,2018,"Spider (Yu et al., 2018)",,,,,,,代码生成,language-to-code generation,执行准确率,LEVER consistently improves the execution accuracy of the generated programs.,自然语言与数据库模式,Input Format: Schema + NL,代码（SQL）,generating SQL queries from natural language questions.,文本到代码,language-to-code generation,SQL执行器,generating SQL queries from natural language questions.,拥有完整的程序标注（Has program: ✓）,Has program: ✓
2303.17568_output/content.md,HumanEval-X,"Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go.",Yes,"We hand-craft the HumanEval-X benchmark to evaluate multilingual code models for the tasks of code generation and translation in terms of functional correctness, facilitating the understanding and development of pre-trained (multilingual) code models.",https://github.com/THUDM/CodeGeeX,"Finally, CodeGeeX is publicly accessible and in Sep. 2022, we open-sourced its code, model weights (the version of 850B tokens), API, extensions, and HumanEval-X at https://github.com/THUDM/CodeGeeX.",评估多语言代码模型在代码生成和代码翻译任务上的功能正确性。,We hand-craft the HumanEval-X benchmark to evaluate multilingual code models for the tasks of code generation and translation in terms of functional correctness...,多语言代码生成与翻译的功能正确性,...evaluate multilingual code models for the tasks of code generation and translation in terms of functional correctness...,通过执行测试用例来验证生成代码的功能正确性,"...rather than really verify the functional correctness of generated code. Specifically, for each problem... in HumanEval, we manually rewrite its prompt, canonical solution, and test cases in C++, Java, JavaScript, and Go.",单函数,文中未明确描述上下文依赖范围。HumanEval-X基于HumanEval构建，而HumanEval是单函数生成任务。,通用编程问题,文中未明确描述问题所属专业领域。,入门级编程问题,文中未直接描述HumanEval-X的难度，但提到其基础HumanEval用于评估Codex解决入门级Python问题。,"Python, C++, Java, JavaScript, Go","...we manually rewrite its prompt, canonical solution, and test cases in C++, Java, JavaScript, and Go. In total, HumanEval-X covers 820 hand-written problem-solution pairs (164 problems, each having solutions in 5 languages).",820个手写的问题-解决方案对（164个问题，每个问题有5种语言的解决方案）,"In total, HumanEval-X covers 820 hand-written problem-solution pairs (164 problems, each having solutions in 5 languages).",基于HumanEval（Python）手动重写和扩展,"Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go.",2022年9月,"Finally, CodeGeeX is publicly accessible and in Sep. 2022, we open-sourced its code, model weights (the version of 850B tokens), API, extensions, and HumanEval-X...",官方自建（手工构建）,We hand-craft the HumanEval-X benchmark...,,,,,代码生成，代码翻译,...evaluate multilingual code models for the tasks of code generation and translation...,功能正确性（通过测试用例）,...evaluate multilingual code models for the tasks of code generation and translation in terms of functional correctness...,自然语言（描述）,文中未明确描述输入类型，但基于HumanEval任务（从文档字符串生成代码）推断。,代码,文中未明确描述期望输出，但任务为代码生成和翻译，推断输出为代码。,文本到代码，代码到代码,HumanEval-X support the evaluation of both code generation and code translation between different languages.,,,1. 多语言扩展：在HumanEval（仅Python）基础上，手工编写了C++、Java、JavaScript和Go的解决方案和测试用例。2. 支持双重任务：同时评估代码生成和代码翻译。3. 专注于功能正确性评估，而非字符串相似度。,"1) HumanEval (Chen et al., 2021)—developed by OpenAI for evaluating Codex—and other benchmarks only consist of programming problems in a single language and 2) existing multilingual datasets use string similarity metrics like BLEU for evaluation rather than really verify the functional correctness of generated code. Importantly, HumanEval-X support the evaluation of both code generation and code translation between different languages."
2304.05128_output/content.md,Spider,"SELF-DEBUGGING achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation","No, 本文是使用该数据集进行评测","we evaluate SELF-DEBUGGING on the development set of the Spider benchmark (Yu et al., 2018).",,,文本到SQL生成：给定一个自然语言问题和数据库信息，生成对应的SQL查询。,The goal of text-to-SQL tasks is to generate the corresponding SQL query given a question and the database information,代码生成正确性,SELF-DEBUGGING can teach the large language model to debug its predicted program,执行单元测试（当可用时）或基于执行结果的多数投票,"When unit tests are presented in the problem description, we filter out programs that do not pass the unit tests before performing the execution-based majority vote.",,,数据库查询,text-to-SQL generation,包含最复杂级别的问题,improves the prediction accuracy on problems of the hardest level by 9%.,SQL,text-to-SQL generation,,,,,,,,,,,,,代码生成,text-to-SQL generation,预测准确率,improves the prediction accuracy on problems of the hardest level by 9%.,自然语言（问题）和数据库信息,given a question and the database information,代码（SQL查询）,generate the corresponding SQL query,文本到代码,text-to-SQL generation,,,该基准（Spider）在问题描述中没有单元测试来验证预测的正确性。,On the Spider benchmark where there are no unit tests to verify the correctness of predictions
2305.01210_output/content.md,HumanEval+,we extend the test-cases of the popular HUMANEVAL benchmark by 80× to build HUMANEVAL+.,Yes,we propose EvalPlus – a code synthesis evaluation framework to rigorously benchmark the functional correctness of LLM-synthesized code. EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator... we extend the test-cases of the popular HUMANEVAL benchmark by 80× to build HUMANEVAL+.,https://github.com/evalplus/evalplus,"We have open-sourced our tools, enhanced datasets as well as all LLM-generated code at https://github.com/evalplus/evalplus to facilitate and accelerate future LLM-for-code research.",评估大型语言模型生成的代码的功能正确性。,EvalPlus – a code synthesis evaluation framework to rigorously benchmark the functional correctness of LLM-synthesized code.,代码生成的功能正确性,rigorously benchmark the functional correctness of LLM-synthesized code.,通过大量新生成的测试用例（结合LLM和基于突变的策略）进行差分测试，并与基准实现进行交叉验证。,"EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies... These newly generated test inputs are then used to evaluate the LLM-generated code through differential testing against the ground-truth implementation.",单函数,The generated code snippet is then combined with the context to form a complete function that aligns with the user intent.,通用编程问题,"Programming benchmarks, with curated synthesis problems and test-cases, are used to measure the performance of various LLMs on code synthesis.",,,文中仅提及实验基于HumanEval（Python），未描述完整数据集,we extend the test-cases of the popular HUMANEVAL benchmark by 80× to build HUMANEVAL+.,将HumanEval的测试用例规模扩展了80倍,"EvalPlus extends the popular HUMANEVAL benchmark to create HUMANEVAL+, improving the test-case scale by 80×.",基于现有基准（HumanEval）通过自动测试输入生成器（结合LLM和基于突变的策略）进行增强。,"EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies.",2023,arXiv:2305.01210v3 [cs.SE] 30 Oct 2023,官方自建（基于现有基准增强）,we propose EvalPlus – a code synthesis evaluation framework... we extend the test-cases of the popular HUMANEVAL benchmark by 80× to build HUMANEVAL+.,通过生成大量新测试用例来缓解现有基准测试不足导致的“虚假正确”问题，旨在更严格地评估模型。,"HUMANEVAL+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs, reducing the pass@k by up-to 19.3-28.9%.",,,代码生成,program synthesis... applying LLMs for direct code generation.,pass@k,reducing the pass@k by up-to 19.3-28.9%.,自然语言（函数签名和文档字符串）,in the form of function signature and docstring that denote the desired program functionality.,代码,The generated code snippet is then combined with the context to form a complete function that aligns with the user intent.,文本到代码,synthesizing programs from docstrings.,,,1. 通过自动测试生成（结合LLM和突变策略）显著扩展现有基准的测试套件（80倍）。2. 旨在揭示因测试不足而被现有基准遗漏的错误代码。3. 提供精简版测试套件（HUMANEVAL+-MINI）以加速评估。4. 发现测试不足可能导致模型排名错误。,"EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies... HUMANEVAL+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs... we also produce HUMANEVAL+-MINI which distills HUMANEVAL+ tests by 47×... test insufficiency can lead to mis-ranking."
2305.02309_output/content.md,HumanEval,"The HumanEval (Chen et al., 2021b) is recruited to evaluate the quality of synthesized programs.","No, 本文是使用该数据集进行评测","The HumanEval (Chen et al., 2021b) is recruited to evaluate the quality of synthesized programs.",,,根据函数签名和文档字符串（意图说明）生成程序代码。,The prompt as an intent specification is in the form of a function signature and doc-string. A program is conditionally sampled (or completed) based on the prompt in left-to-right auto-regressive fashion.,程序合成的功能正确性,"The HumanEval (Chen et al., 2021b) is recruited to evaluate the quality of synthesized programs.",,,单函数,The prompt as an intent specification is in the form of a function signature and doc-string.,,,,,文中未明确提及完整数据集的语言，仅提及实验任务。,The prompt as an intent specification is in the form of a function signature and doc-string. A program is conditionally sampled (or completed) based on the prompt...,,,,,,,,,,,,,代码生成,Program Synthesis with left-to-right sampling (zero-shot)... A program is conditionally sampled (or completed) based on the prompt...,,,自然语言（文档字符串）与代码（函数签名）,The prompt as an intent specification is in the form of a function signature and doc-string.,代码,A program is conditionally sampled (or completed) based on the prompt...,文本到代码,The prompt as an intent specification is in the form of a function signature and doc-string. A program is conditionally sampled...,,,,
2305.07922_output/content.md,HumanEval,"Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA results of 35.0% pass@1 and 54.5% pass@10 on the HumanEval code generation task against other open code LLMs, even surpassing the OpenAI code-cushman-001 model.","No, 本文是使用该数据集进行评测","We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. ... in the zero-shot text-to-code generation task on HumanEval benchmark [Chen et al., 2021]",,,文中仅提及使用该基准进行代码生成评测，未描述其原始任务设计。,in the zero-shot text-to-code generation task on HumanEval benchmark,文中仅提及使用该基准进行代码生成评测，未描述其原始评测维度。,,文中仅提及使用pass@1和pass@10指标，未描述其原始评估方法。,achieves new SoTA results of 35.0% pass@1 and 54.5% pass@10 on the HumanEval code generation task,,,,,,,文中仅提及实验任务（文本到代码生成），未描述数据集涉及的具体编程语言。,,,,,,,,,,,,,,代码生成,in the zero-shot text-to-code generation task on HumanEval benchmark,"pass@1, pass@10",achieves new SoTA results of 35.0% pass@1 and 54.5% pass@10 on the HumanEval code generation task,自然语言,in the zero-shot text-to-code generation task,代码,in the zero-shot text-to-code generation task,文本到代码,in the zero-shot text-to-code generation task,,,,
2305.18584_output/content.md,PYCOMMITS,"We address this issue by collecting a new dataset, PYCOMMITS, from the commit histories of 1650 open-source Python projects on GitHub.",Yes,"We collect a code editing dataset from the commit histories of 1650 open-source Python projects for training and evaluation. ... We introduce the repo-level multi-round code editing task, along with the corresponding PYCOMMITS dataset and evaluation framework.",https://github.com/mrvplusone/Coeditor,Available at https://github.com/mrvplusone/Coeditor.,代码自动编辑。旨在预测对代码区域（基于同一代码库中最近的更改）的编辑。这是一个多轮任务，目标是根据用户之前的编辑来预测代码的编辑。,"In this work, we explore a multi-round code auto-editing setting, aiming to predict edits to a code region based on recent changes within the same codebase. ... In this paper, we introduce a task that we call (multi-round) auto-editing where the goal is to predict edits to code conditioned on the user’s previous edits.",代码编辑的准确性和自动化程度，关注模型在给定编辑历史和代码库上下文的情况下预测正确编辑的能力。,"In a simplified single-round, single-edit task, Coeditor significantly outperforms GPT-3.5 and SOTA open-source code completion models (bringing exact-match accuracy from 34.7 up to 60.4), demonstrating the benefits of incorporating editing history for code completion. In a multi-round, multi-edit setting, we observe substantial gains by iteratively conditioning on additional user edits.",精确匹配准确率（exact-match accuracy），以及自动化编辑行数和节省击键数的编辑距离度量。,"our method achieves 60.4% exact match accuracy using a 220M parameter model... In the full multi-round setting, we found that Coeditor automates editing 46.7% of the changed lines, saving the user 28.6% of keystrokes measured by an edit distance metric that accounts for cursor movement.",仓库级别（repo-level），依赖同一代码库中的其他部分和用户的历史编辑。,We introduce the repo-level multi-round code editing task... aiming to predict edits to a code region based on recent changes within the same codebase.,通用软件工程，代码维护与重构。,Developers often dedicate significant time to maintaining and refactoring existing code.,工程级，涉及实际开源项目中的代码变更。,We collect a code editing dataset from the commit histories of 1650 open-source Python projects for training and evaluation.,Python,We collect a code editing dataset from the commit histories of 1650 open-source Python projects for training and evaluation.,从1650个开源Python项目的提交历史中收集。,We collect a code editing dataset from the commit histories of 1650 open-source Python projects for training and evaluation.,来自GitHub上1650个开源Python项目的提交历史。,"We address this issue by collecting a new dataset, PYCOMMITS, from the commit histories of 1650 open-source Python projects on GitHub.",2024,Published as a conference paper at ICLR 2024,官方自建，由论文作者为研究目的构建。,"We address this issue by collecting a new dataset, PYCOMMITS, from the commit histories of 1650 open-source Python projects on GitHub.",,,,,代码编辑（基于行的差异），包括添加、删除行。,"We encode all prior code edits ∆1, . . . , ∆k using a line-based diffing scheme and decodes ∆u using masked span infilling... we adopt a line-diff-based format, enabling us to convert auto-editing into a masked span infilling problem.",精确匹配准确率（exact-match accuracy），自动化编辑行百分比，节省击键百分比（基于编辑距离）。,"our method achieves 60.4% exact match accuracy... Coeditor automates editing 46.7% of the changed lines, saving the user 28.6% of keystrokes measured by an edit distance metric that accounts for cursor movement.",代码（原始代码库和以行差异格式编码的编辑历史），以及通过静态分析提取的相关代码签名。,"given an original codebase U and a set of code changes ∆1, . . . , ∆k... we employ lightweight static analysis to pull in relevant parts of the codebase U.",代码（以行差异格式编码的目标编辑）。,"the auto-editing problem is to predict how to modify a specified region of code u ∈U... We encode the input code by a function EncInput... we can encode ∆u using the following expression, EncOutput(∆u)...",代码到代码（在给定代码库和编辑历史上下文的情况下，预测对目标代码区域的编辑）。,"the auto-editing problem is to predict how to modify a specified region of code u ∈U by learning the following distribution: P(∆u | ∆k . . . ∆1, U).",,,专注于多轮、仓库级别的代码自动编辑任务，使用基于行差异的编码格式和静态分析来构建上下文。数据集源自真实项目的提交历史，模拟了实际的代码编辑工作流。,"We introduce the repo-level multi-round code editing task... We encode all prior code edits ∆1, . . . , ∆k using a line-based diffing scheme... we employ lightweight static analysis to pull in relevant parts of the codebase U. We collect a code editing dataset from the commit histories of 1650 open-source Python projects."
2306.02907_output/content.md,DS-1000,"We evaluate SELFEVOLVE on three code generation datasets, including DS-1000 for data science code","No, 本文是使用该数据集进行评测","We evaluate SELFEVOLVE on three code generation datasets, including DS-1000 for data science code, HumanEval for software engineering code, and TransCoder for C++-to-Python translation.",,,数据科学代码生成任务,the data science code generation task DS-1000,代码生成功能正确性,Extensive experiments show that SELFEVOLVE achieves a significant improvement in execution-based measurements,基于执行的测量,Extensive experiments show that SELFEVOLVE achieves a significant improvement in execution-based measurements,,,数据科学,the data science code generation task DS-1000,,,Python,JuPyT5 [7] conditions on Jupyter notebooks’ context cells to generate data science code.,,,,,,,,,,,,,代码生成,the data science code generation task DS-1000,,,自然语言问题描述,Given a problem description written in natural language d,代码,the autoregressive language model pθ predicts the solution,文本到代码,"Given a problem description written in natural language d, and code context c, the autoregressive language model pθ predicts the solution",Python解释器,This refinement mechanism teaches language models to depend on an executor like a Python interpreter to correct the preliminary code.,专注于数据科学领域的代码生成,the data science code generation task DS-1000
2306.03091_output/content.md,RepoBench,"we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems.",Yes,"we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems.",,,评估仓库级别的代码自动补全系统。,"RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems.",跨文件代码检索能力、给定上下文的代码补全能力、结合检索与补全的端到端流程处理能力。,"RepoBench supports both Python and Java and consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). Each task respectively measures the system’s ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction.",对于检索任务（RepoBench-R）使用Accuracy@k (acc@k)指标；对于补全任务（RepoBench-C）和端到端任务（RepoBench-P），评估模型预测下一行代码的能力。,"For evaluative purposes, the Accuracy@k (acc@k) metric is employed to assess retrieval performance. The easy subset is evaluated using acc@1 and acc@3, while the hard subset is examined through acc@1, acc@3, and acc@5 metrics.",多文件项目（仓库级别），包含跨文件上下文和文件内上下文。,"However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios. ... RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems.",通用编程,benchmark for auto-code completion,真实世界编程场景，包含不同难度子集（如检索任务中的Easy和Hard子集）。,"This benchmark comprises three interconnected tasks: RepoBench-R for code retrieval, RepoBench-C for code completion, and RepoBench-P, which integrates both aspects to reflect the entire workflow of an auto-completion system, offering a balanced assessment. ... we categorize them into two subsets: those with 5-9 candidates as the easy subset, and those with 10 or more candidates as the hard subset.",Python 和 Java,RepoBench supports both Python and Java,"训练数据：10,345个Python仓库和14,956个Java仓库。测试数据：1,075个Python仓库和594个Java仓库。具体任务样本数量详见论文表1。","After processing the data, our dataset comprises 10,345 Python and 14,956 Java historical repositories, serving as training data and are available for optional fine-tuning. Additionally, we have 1,075 Python and 594 Java new repositories from GitHub designated as test data for evaluation.",1. Github-Code数据集（截止2022年3月16日），用于构建训练数据。2. 新爬取的GitHub仓库（创建于2023年2月9日至8月3日之间），专门用作测试集。,"Github-Code Dataset: The first source of RepoBench is the github-code dataset2, which consists of a vast collection of code files sourced from GitHub repositories under open-source licenses with a data cutoff date of March 16, 2022. ... Newly Crawled GitHub Data: To mitigate impacts regarding data leakage and memorization, we augment the dataset by incoporating the most recent, non-forked GitHub repositories that are permitted under their respective licenses. Specifically, we use GitHub’s official API to crawl Python and Java repositories created after February 9, 2023, which aligns with the newest knowledge cutoff date of The Stack [22], and before August 3, 2023. This newly-crawled data serves exclusively as our test set for evaluation.",2023-10-04 (arXiv版本v2),arXiv:2306.03091v2  [cs.CL]  4 Oct 2023,官方自建,"we introduce RepoBench, a new benchmark",通过使用新爬取的数据作为测试集来减轻数据泄露和记忆的影响。,"To mitigate impacts regarding data leakage and memorization, we augment the dataset by incoporating the most recent, non-forked GitHub repositories ... This newly-crawled data serves exclusively as our test set for evaluation.",数据来源于开源许可下的GitHub仓库，但未明确指定数据集本身的许可证。,code files sourced from GitHub repositories under open-source licenses,代码补全（下一行预测）、代码检索、端到端流程（检索+补全）。,"RepoBench supports both Python and Java and consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline).","Accuracy@k (acc@1, acc@3, acc@5)","For evaluative purposes, the Accuracy@k (acc@k) metric is employed to assess retrieval performance. The easy subset is evaluated using acc@1 and acc@3, while the hard subset is examined through acc@1, acc@3, and acc@5 metrics.",代码（文件内上下文和跨文件代码片段）,the prompt is created by combining all the parsed snippets as cross-file contexts and an in-file context. The in-file context includes import statements and several preceding lines of code,代码（下一行）,predict the next line of code,代码到代码,predict the next line of code given a pre-defined context. The context can involve content from different files (cross-file context) and within the file (in-file context),,,"专注于仓库级别的代码自动补全评估，包含三个相互关联的任务（检索、补全、端到端流程），并设计了不同的上下文设置（Cross-File-First, Cross-File-Random, In-File）和子集（如2k和8k token限制）以适应不同模型。","RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems. RepoBench supports both Python and Java and consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). ... To effectively evaluate next-line prediction in auto-completion systems, we define three settings: Cross-File-First (XF-F)... Cross-File-Random (XF-R)... In-File (IF)... RepoBench-C is divided into two subsets: RepoBench-C-2k and RepoBench-C-8k."
2306.08568_output/content.md,HumanEval,"Through comprehensive experiments on five prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, DS-1000, and MultiPL-E, our models showcase outstanding performance.","No, 本文是使用该数据集进行评测","Through comprehensive experiments on five prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, DS-1000, and MultiPL-E, our models showcase outstanding performance.",,,代码生成（从自然语言描述生成代码）,"These models, pre-trained on substantial code data, excel in various code-related tasks, consistently delivering impressive performance. (结合上下文，评测基准用于评估代码生成任务)",代码生成的功能正确性,"Remarkably, WizardCoder 15B even surpasses the well-known closed-source LLMs, including Anthropic’s Claude and Google’s Bard, on the HumanEval and HumanEval+ benchmarks. (通过pass率评估，隐含了功能正确性维度)",pass@1 (通过率),Figure 1: An illustration of our novel Code Evol-Instruct and the superior pass@1 performance of our WizardCoder 34B... The Python score is the mean between HumanEval and MBPP.,,,通用编程问题,Develop a Python program that creates a random password of length 8 characters. (示例任务属于通用编程),,,多种编程语言（包括Python）,outperforming the open-source SOTA ... by a large margin in 9 different programming languages. The Python score is the mean between HumanEval and MBPP.,,,,,,,,,,,,,代码生成,Through comprehensive experiments on five prominent code generation benchmarks,pass@1,Figure 1: An illustration of our novel Code Evol-Instruct and the superior pass@1 performance of our WizardCoder 34B,自然语言（指令/描述）,Develop a Python program that creates a random password of length 8 characters. (示例中的输入是自然语言指令),代码,Here's an example program that generates a random password... (示例中的输出是代码),文本到代码,Develop a Python program that creates a random password of length 8 characters. (从自然语言文本描述生成代码),,,"本文提及了五个基准：HumanEval, HumanEval+, MBPP, DS-1000, 和 MultiPL-E。其中MultiPL-E支持多语言评估。","Through comprehensive experiments on five prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, DS-1000, and MultiPL-E"
2306.09896_output/content.md,HumanEval,"In this paper, we analyze Code Llama, GPT-3.5 and GPT-4’s ability to perform self-repair on problems taken from HumanEval and APPS.","No, 本文是使用该数据集进行评测","In this paper, we analyze Code Llama, GPT-3.5 and GPT-4’s ability to perform self-repair on problems taken from HumanEval and APPS.",,,从自然语言规范生成代码片段,Large language models (LLMs) have proven capable of generating code snippets from natural language specifications,代码生成与自我修复能力,"we focus on evaluating the models’ capacity to reflect upon, provide feedback on and debug the code.",pass@k 指标，执行单元测试,"performance is typically measured by pass@k (Chen et al., 2021; Kulal et al., 2019)—the probability that at least one of k i.i.d. program samples from the model satisfies a given specification.",单函数,for self-contained Python programming tasks.,通用编程,self-contained Python programming tasks.,竞赛级和面试级,complex coding challenges such as those found in competitions and professional software engineering interviews.,Python,for self-contained Python programming tasks.,,,,,,,,,,,,,代码生成,generating code snippets from natural language specifications,pass@k,"performance is typically measured by pass@k (Chen et al., 2021; Kulal et al., 2019)",自然语言与单元测试,"Given a specification ψ, a programming model MP first generates np samples i.i.d.",代码,generating code snippets,文本到代码,generating code snippets from natural language specifications,单元测试执行环境,These np code samples are then executed against a test bed.,本文重点研究自我修复（self-repair）策略在代码生成任务中的有效性，而非提出新基准。,"In this paper, we investigate the efficacy of self-repair techniques applied to CodeLlama-13b-instruct, GPT-3.5, and GPT-4 for self-contained Python programming tasks."
2306.11644_output/content.md,HumanEval,"The evaluation benchmark proposed in the latter work, HumanEval, has been widely adopted for comparing LLMs’ performance on code.","No, 本文是使用该数据集进行评测","We demonstrate the power of high quality data in breaking existing scaling laws by training a 1.3B-parameter model, which we call phi-1, ... we attain 50.6% pass@1 accuracy on HumanEval and 55.5% pass@1 accuracy on MBPP",,,从自然语言文档字符串（docstrings）中合成简单的Python函数。,"We focus our attention on LLMs trained for code, and specifically writing simple Python functions from their docstrings as in [CTJ+21].",代码生成的功能正确性,"The evaluation benchmark proposed in the latter work, HumanEval, has been widely adopted for comparing LLMs’ performance on code.",pass@1 准确率,we attain 50.6% pass@1 accuracy on HumanEval and 55.5% pass@1 accuracy on MBPP,单函数,writing simple Python functions from their docstrings,基础编程概念,determine its educational value for a student whose goal is to learn basic coding concepts,基础级别,for a student whose goal is to learn basic coding concepts,Python,writing simple Python functions from their docstrings,,,,,2021,2021 Jul Codex-300M [CTJ+21],,,文中讨论了训练数据可能存在的污染风险,in Section 5 we study possible contamination of our training data with respect to HumanEval.,,,代码生成,writing simple Python functions from their docstrings,pass@1,we attain 50.6% pass@1 accuracy on HumanEval,自然语言,writing simple Python functions from their docstrings,代码,writing simple Python functions,文本到代码,writing simple Python functions from their docstrings,,,被广泛用于比较LLM在代码生成上的性能,"HumanEval, has been widely adopted for comparing LLMs’ performance on code."
2306.14893_output/content.md,LCC (Long Code Completion Benchmark),"In this paper, we introduce the Long Code Completion Benchmark (LCC), a new benchmark that focuses on code completion with long code context for three programming languages: Python, Java, and C#.",Yes,"To evaluate the effectiveness of LongCoder and encourage future research on Long Code Completion, we construct a new dataset called LCC by filtering code from GitHub based on length, with the goal of focusing on longer code examples.",https://github.com/microsoft/CodeBERT,1All the codes and data are available at https://github.com/microsoft/CodeBERT.,专注于长代码上下文的代码补全任务，旨在评估模型在更复杂、更现实的代码文件级别场景下的能力。,"In this paper, we introduce a new task for code completion that focuses on handling long code input... We introduce the Long Code Completion Benchmark (LCC), a new benchmark that focuses on code completion with long code context...",长代码建模能力、代码补全准确性、模型效率（计算资源消耗）,...achieves superior performance on code completion tasks compared to previous models while maintaining comparable efficiency in terms of computational resources during inference.,"在逐行基础上，使用精确匹配（Exact Match, EM）和编辑相似度（Edit Similarity, Edit Sim）进行评估。","We follow Lu et al. (2021) to evaluate the performance of the models in terms of Exact Match (EM) and Edit Similarity (Edit Sim) on a per-line basis (Svyatkovskiy et al., 2020).",文件级别（长代码上下文），上下文长度通常超过512个代码标记，平均长度在1800-2000个标记左右。,"...focuses on code completion with long code context... The average length of the examples in LCC are 5× longer than those in existing datasets... The average length of a Python source file on GitHub is 1,305 tokens.",通用编程，数据来源于GitHub上的开源代码文件。,"Specifically, we construct our datasets from the github-code dataset, which contains a vast number of code files sourced from GitHub with an open-source license that permits research use.",工程级，包含真实世界中的长代码文件，结构和依赖关系更复杂。,"Meanwhile, longer code sequences contain more complex structures and require models to consider more context and dependencies. This can be challenging for previously proposed code completion models that focus on short code...","Python, Java, C#","...a new benchmark that focuses on code completion with long code context for three programming languages: Python, Java, and C#.",每种语言包含10万个训练样本，1万个开发集样本和1万个测试集样本。,"For each programming language, we sample 100k examples for training, and 10k examples for development and 10k for testing.",从GitHub上具有开源许可证的代码文件中筛选和构建，经过去重和AST解析过滤。,"Specifically, we construct our datasets from the github-code dataset, which contains a vast number of code files sourced from GitHub with an open-source license that permits research use. The steps to construct the datasets are as follows: • We first follow Allamanis (2019) to deduplicate examples with high similarity (Jacobi similarity ≥0.9) in order to eliminate forked files, and then remove code files that can’t be parsed into an abstract syntax tree using a standard compiler tool called tree-sitter.",2023,"Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s).",官方自建（由论文作者构建）,We construct a new dataset (LCC) for code completion tasks that requires long code modeling to encourage more research in such scenarios.,文中未明确讨论,,开源许可证（允许研究使用），具体许可证名称未明确说明。,...sourced from GitHub with an open-source license that permits research use.,代码补全（具体为下一行预测或行内补全）,Code completion is a crucial task in software development that helps developers save time and effort by suggesting and auto-completing code based on context.,"精确匹配（Exact Match, EM）、编辑相似度（Edit Similarity, Edit Sim）","We follow Lu et al. (2021) to evaluate the performance of the models in terms of Exact Match (EM) and Edit Similarity (Edit Sim) on a per-line basis (Svyatkovskiy et al., 2020).",代码,Code completion is a crucial task in software development that helps developers save time and effort by suggesting and auto-completing code based on context.,代码,Code completion is a crucial task in software development that helps developers save time and effort by suggesting and auto-completing code based on context.,代码到代码,Code completion is a crucial task in software development that helps developers save time and effort by suggesting and auto-completing code based on context.,文中未明确描述,,专门针对长代码上下文设计，平均代码长度是现有数据集的5倍；包含Python、Java、C#三种语言；通过长度过滤（>512 tokens）确保关注长上下文场景。,"On average, the examples in LCC are 5× longer than those in existing datasets (Lu et al., 2021)... Since the benchmark primarily focuses on the code completion task with long code context, we remove code files whose length of code tokens after tokenization is shorter than 512."
2307.04349_output/content.md,APPS,Extensive experiments show that RLTF achieves state-of-the-art performance on the APPS and the MBPP benchmarks.,"No, 本文是使用该数据集进行评测",Extensive experiments show that RLTF achieves state-of-the-art performance on the APPS and the MBPP benchmarks.,,,程序合成或代码生成，即根据给定的高级行为描述生成计算机代码。,"Program synthesis, or code generation, involves creating an executable program that solves a given problem.",代码的功能正确性（通过单元测试）,"Unlike text generation, program synthesis requires both syntactic and functional accuracy since the generated code must pass compilation and unit tests.",使用单元测试结果作为反馈信号，评估生成的代码是否能通过编译和测试。,"RLTF, i.e., Reinforcement Learning from Unit Test Feedback, a novel online RL framework with unit test feedback of multi-granularity for refining code LLMs.",,,编程挑战，包括竞赛级算法问题。,"While LLMs have shown promising results in basic programming tasks, there is still progress to be made in tackling more challenging problems such as program competitions.",从基础编程任务到竞赛级挑战。,"While LLMs have shown promising results in basic programming tasks, there is still progress to be made in tackling more challenging problems such as program competitions.",文中未明确描述APPS数据集的语言构成，仅提及Codex解决Python挑战。,"Codex (Chen et al., 2021) is a noteworthy example of an LLM with 12 billion parameters that can successfully solve over 70% of complex Python programming challenges created by humans.",,,,,,,,,,,,,代码生成（从描述生成完整代码）,"Program synthesis, or code generation, involves creating an executable program that solves a given problem.",基于单元测试通过率的反馈信号（如粗粒度、细粒度、自适应反馈）,"Built upon this framework, we develop multi-granularity feedback that is automatically extracted from unit test. To expand, we introduce coarse-grained and fine-grained feedbacks applicable to programs with errors, aimed at punishing the specific segments of code where the errors appear. For programs that do not pass all test cases, we propose an adaptive feedback mechanism that assigns varying penalties based on the ratio of passed test cases.",自然语言描述,Program synthesis is an automated process that generates computer code W based on a high-level description of desired behavior D.,代码,Program synthesis is an automated process that generates computer code W based on a high-level description of desired behavior D.,文本到代码,Program synthesis is an automated process that generates computer code W based on a high-level description of desired behavior D.,编译器/单元测试环境,One generates the target program and interacts with the compiler to produce a training data pair...,本文是评测论文，未描述APPS基准的独特之处。本文提出的RLTF方法的独特之处在于其在线强化学习框架和从单元测试中提取的多粒度反馈（粗粒度、细粒度、自适应反馈）。,"To address these issues, we propose RLTF, i.e., Reinforcement Learning from Unit Test Feedback, a novel online RL framework with unit test feedback of multi-granularity for refining code LLMs."
2307.14936_output/content.md,HumanEval,"PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval benchmark.","No, 本文是使用该数据集进行评测","Through extensive evaluation on three benchmarks, including HumanEval, CoderEval, and LeetCode, we conjecture that...",,,从自然语言描述生成Python代码，衡量代码生成的功能正确性。,demonstrating remarkable performance on the code generation task.,代码生成正确性,measure functional correctness for synthesizing programs from docstrings.,pass@k (文中主要报告pass@1),"PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval benchmark.",,,通用编程问题,tackle up to 72% of Python programming problems.,,,Python,tackle up to 72% of Python programming problems.,,,,,,,,,,,,,代码生成,demonstrating remarkable performance on the code generation task.,pass@1,"PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval benchmark.",自然语言,generating code from natural language descriptions,代码,generating code from natural language descriptions,文本到代码,generating code from natural language descriptions,,,由OpenAI发布，是代码生成领域广泛使用的基准测试。,OpenAI HumanEval benchmark
2308.07124_output/content.md,HUMANEVALPACK,"We further introduce HUMANEVALPACK, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust).",Yes,"We further introduce HUMANEVALPACK, expanding the HumanEval benchmark to a total of 3 coding tasks...",https://github.com/bigcode-project/octopack,"Code, models and data are freely available at https://github.com/bigcode-project/octopack.",评测基准旨在评估代码大语言模型在多种代码相关任务上的泛化能力，覆盖代码修复、代码解释和代码合成三种场景。,"HUMANEVALPACK: A benchmark for Code LLM generalization, spanning three scenarios (Code Repair, Code Explanation, Code Synthesis) and 6 programming languages (Python, JavaScript, Java, Go, C++, Rust)",代码大语言模型的泛化能力、多任务处理能力、多语言代码能力,"A benchmark for Code LLM generalization, spanning three scenarios (Code Repair, Code Explanation, Code Synthesis) and 6 programming languages",使用 pass@k 指标评估功能正确性,Metric: Pass@k (Figure 3 caption),单函数,"Given an incorrect code function with a subtle bug and accompanying unit tests, the model is tasked to fix the function. (描述 HUMANEVALFIX 任务)",通用编程问题，包含算法和基础功能实现,"Write a Python function `has_close_elements(numbers: List[float], threshold: float) -> bool` to solve the following problem: Check if in given list of numbers, are any two numbers closer to each other than given threshold. (Figure 3 示例)",工程级（包含需要修复的微妙bug）,Given an incorrect code function with a subtle bug... Bugs are written such that the code still runs but produces an incorrect result leading to at least one unit test failing.,"Python, JavaScript, Java, Go, C++, Rust","spanning three scenarios (Code Repair, Code Explanation, Code Synthesis) and 6 programming languages (Python, JavaScript, Java, Go, C++, Rust)",基于164个HumanEval问题，每个问题扩展到6种语言和3种任务，共984个修复任务，以及对应的解释和合成任务。,We manually add a bug to each of the 164 HumanEval solutions across all 6 languages (984 total bugs).,基于HumanEval基准人工扩展构建,"we expand the code synthesis benchmark HumanEval (Chen et al., 2021; Zheng et al., 2023) to cover all three input-output combinations for six languages",2024,arXiv:2308.07124v2 [cs.CL] 18 Feb 2024,官方自建（本文作者团队）,We further introduce HUMANEVALPACK,基于现有基准扩展，存在污染风险,expanding the HumanEval benchmark,,,代码修复、代码解释、代码合成,"spanning three scenarios (Code Repair, Code Explanation, Code Synthesis)",pass@k,Metric: Pass@k (Figure 3 caption),"自然语言与代码的组合，取决于具体任务（NL, NL+C, C）","When instruction tuning with code (C) data, code may either appear only in the input alongside the NL instruction (NL+C→NL, e.g. code explanation), only in the output (NL→C, e.g. code synthesis), or in both input and output (NL+C→C, e.g. code modifications like bug fixing).","自然语言或代码，取决于具体任务（NL, C）","code may either appear only in the input alongside the NL instruction (NL+C→NL, e.g. code explanation), only in the output (NL→C, e.g. code synthesis), or in both input and output (NL+C→C, e.g. code modifications like bug fixing).",文本到代码（合成）、代码到文本（解释）、代码到代码（修复）,"cover all three input-output combinations: NL+C→NL (e.g. code explanation), NL→C (e.g. code synthesis), NL+C→C (e.g. code modifications like bug fixing)",单元测试执行环境,Given an incorrect code function with a subtle bug and accompanying unit tests,扩展了经典的HumanEval基准，覆盖了代码修复和解释等更广泛的现实任务，支持六种编程语言，旨在解决现有基准因模型性能接近饱和而可能失效的问题。,Our more challenging evaluation variants provide room for future LLMs to improve on the performance of the current state-of-the-art.
2308.10335_output/content.md,ROBUSTAPI,"To fill the missing piece, we propose a dataset ROBUSTAPI for evaluating the reliability and robustness of code generated by LLMs.",Yes,"To fill the missing piece, we propose a dataset ROBUSTAPI for evaluating the reliability and robustness of code generated by LLMs.",https://github.com/FloridSleeves/RobustAPI,We open-source our dataset and evaluator on GitHub2. ... 2https://github.com/FloridSleeves/RobustAPI,评估大型语言模型生成的代码的可靠性和鲁棒性，特别是针对Java API的误用情况。,We propose a dataset ROBUSTAPI for evaluating the reliability and robustness of code generated by LLMs.,代码生成的可靠性与鲁棒性，特别是API误用检测。,"The main purpose of this benchmark is not to evaluate the functional correctness of the generated code, but instead, we focus on reliability and robustness.",使用抽象语法树（AST）分析生成的代码，并与预期的API使用模式（结构化调用序列）进行比较，检测违规行为。,We also provide an evaluator that analyzes the generated code snippets using the abstract syntax tree (AST) and compares them with the expected API usage patterns.,单代码片段（基于Stack Overflow问答），涉及特定API的使用。,We collect 1208 real questions from Stack Overflow which involves 18 representative Java APIs.,软件工程，涵盖字符串处理、数据结构、移动开发、加密、I/O和数据库操作等多个领域。,"These 18 APIs cover 6 domains including string processing, data structure, mobile development, crypto, I/O and database operation.",实际软件开发中常见的、开发者容易犯错的API使用问题。,"In this way, we guarantee that the questions in ROBUSTAPI are answerable and non-trivial so we can use them to effectively evaluate the LLMs’ ability in answering coding questions that humans are prone to make mistakes.",Java,Thus we collect representative questions about Java from Stack Overflow.,包含1208个问题，涉及18个代表性的Java API。,We collect 1208 real questions from Stack Overflow which involves 18 representative Java APIs.,从Stack Overflow爬取的真实编程问题，并基于ExampleCheck数据集筛选。,We build ROBUSTAPI based on the dataset from ExampleCheck (Zhang et al. 2018) as our starting point. ... Then we crawl questions relevant to these APIs from Stack Overflow.,2024-01-27 (根据arXiv版本v5日期),arXiv:2308.10335v5 [cs.CL] 27 Jan 2024,官方自建（由本文作者构建并发布）,We propose a dataset ROBUSTAPI... We open-source our dataset and evaluator on GitHub.,基于Stack Overflow真实问题构建，存在被LLM训练数据包含的风险，但文中未明确讨论污染状态。,,文中未提及具体许可证。,,代码生成（根据自然语言问题描述生成使用特定API的代码片段）,The prompt simulates a user asking coding questions without providing any additional hints from the API documentation which is a typical scenario when novice developers seek help from large language models.,API误用检测率（通过AST分析判断生成的代码是否违反预期的API使用模式）,Any violations of such structured call sequences would be considered as API misuse from the perspective of software engineering.,自然语言（Stack Overflow问题的标题和描述）,question field contains the title and description of the Stack Overflow questions.,代码（Java代码片段）,We design templates to trigger large language models to generate the code snippet and the corresponding explanation.,文本到代码,The prompt simulates a user asking coding questions... instruct LLMs to generate answers (code) to the questions.,文中未明确描述执行环境，评估基于静态分析（AST），而非动态执行。,We introduce the static analysis method in ROBUSTAPI for detecting the API usage violations which leverages the abstract syntax tree...,专注于评估代码生成在真实软件开发场景下的可靠性和鲁棒性，而非传统的功能正确性；针对API误用这一具体风险；包含一个基于AST的静态分析评估器。,"Unlike the online programming forums, the generated code snippets are not reviewed by the community peers and thus suffer from API misuse... The main purpose of this benchmark is not to evaluate the functional correctness of the generated code, but instead, we focus on reliability and robustness."
2308.12950_output/content.md,HumanEval,"Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67% and 65% on HumanEval and MBPP, respectively.","No, 本文是使用该数据集进行评测","We perform exhaustive evaluations of our models on major code generation benchmarks: HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and APPS (Hendrycks et al., 2021), as well as a multilingual version of HumanEval (MultiPL-E, Cassano et al., 2023)...",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2308.16458_output/content.md,BioCoder,"We present BioCoder, a benchmark developed to evaluate LLMs in generating bioinformatics-specific code.",Yes,"We introduce BioCoder (see Figure 1), a benchmark for code generation incorporating 2,269 bioinformatics-specific coding problems.",https://github.com/gersteinlab/biocoder and https://biocoder-benchmark.github.io/,"All datasets, benchmark, Docker images, and scripts required for testing are available at: https://github.com/gersteinlab/biocoder and https://biocoder-benchmark.github.io/.",评估大型语言模型在生成生物信息学特定代码方面的能力。,"BioCoder benchmark mainly targets bioinformatics data analysis, which tasks such as managing various biological data formats, understanding processing workflows, and utilizing APIs of various packages.",代码生成功能正确性、处理复杂上下文（跨文件依赖、类声明、全局变量）的能力、领域特定知识（生物信息学）,"BioCoder spans much of the field, covering cross-file dependencies, class declarations, and global variables. ... They contain domain-specific knowledge of bioinformatics, beyond just general coding capability.",模糊测试框架，通过执行测试用例来验证生成代码的功能正确性，并计算Pass@K率。,"BioCoder incorporates a fuzz-testing framework for evaluation. ... Our benchmark results, derived from 1,000 iterations, indicate the Pass@K rate.",跨文件依赖、包含类声明和全局变量的完整项目上下文,"BioCoder spans much of the field, covering cross-file dependencies, class declarations, and global variables. ... we included all potentially required class declarations in the input.",生物信息学，包括生物数据分析、遗传测序、DNA/RNA分析、生物信息学软件开发,"Here, we target bioinformatics due to the amount of domain knowledge, algorithms, and data operations this discipline requires. ... This project specializes in generating Python functions that address key bioinformatics topics such as genetic sequencing and DNA/RNA analysis.",具有挑战性、实用的生物信息学场景，包含日常数据分析任务和部分软件开发任务,"BIOCODER is a code generation benchmark designed for challenging, practical bioinformatics scenarios, ... This domain encapsulates the majority of daily tasks encountered by bioinformaticians in data analysis.",Python 和 Java,"It incorporates 1,026 Python functions and 1,243 Java methods extracted from GitHub, ... However, for the scope of this study, we focus on Python and Java, with the intention to expand to other languages in the future.","总共2,269个生物信息学特定的编码问题（公开集460个，隐藏集2,269个，相似集460个），包含1,026个Python函数和1,243个Java方法，以及来自Rosalind项目的253个Python示例。","a benchmark for code generation incorporating 2,269 bioinformatics-specific coding problems. ... It incorporates 1,026 Python functions and 1,243 Java methods extracted from GitHub, along with 253 examples from the Rosalind Project, all pertaining to bioinformatics.","从1,720个经同行评审生物信息学文章引用的GitHub仓库中提取，以及来自Rosalind项目的示例。","curated from 1,720 bioinformatics repositories referenced in peer-reviewed bioinformatics articles. ... We included an additional 253 questions from the Rosalind project.",2024年5月20日（根据arXiv版本v5）,arXiv:2308.16458v5 [cs.LG] 20 May 2024,官方自建，通过自动过滤、GPT辅助过滤和人工检查相结合的方式构建。,"We ensure that each function in our dataset requires a certain level of domain expertise in bioinformatics through a combination of automatic filtering, GPT-assisted filtering, and manual inspection.",经过严格过滤、数据清洗和预处理，以防止模型记忆。,"It has undergone rigorous filtering, extensive data cleaning, and preprocessing to prevent models from memorizing.",,,代码生成（生成完整的函数或方法体）,a benchmark for code generation ... generating bioinformatics-specific code.,Pass@K率,"Our benchmark results, derived from 1,000 iterations, indicate the Pass@K rate.",自然语言描述、代码规范、注释以及完整的上下文（包括依赖项、类声明、全局变量）,"We processed the data, rephrasing more detailed text descriptions, as well as associated comments and specifications, ... we included all potentially required class declarations in the input.",代码（Python或Java）,generating bioinformatics-specific code.,文本到代码,generating bioinformatics-specific code.,Docker环境，包含丰富的所需依赖项，用于在现实项目场景中进行测试。,"Testing incorporates a Docker environment, an abundance of required dependencies, ... This robust setup not only facilitates testing in realistic project scenarios",专注于生物信息学领域；包含跨文件依赖和完整项目上下文；规模远超同类领域特定基准（如CoderEval）；提供可扩展的解析工具和模糊测试工具；通过主题建模验证了代码覆盖范围具有代表性。,"Here, we target bioinformatics ... BioCoder spans much of the field, covering cross-file dependencies, class declarations, and global variables. ... our dataset surpasses the scale of CoderEval ... Using topic modeling, we show that the overall coverage of the included code is representative of the full spectrum of bioinformatics calculations. ... We provide an extendable parsing tool ... We provide a fuzz testing tool capable of scaling to handle substantial datasets."
2310.06770_output/content.md,SWE-bench,"We introduce SWE-bench, an evaluation framework consisting of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories.",Yes,"We introduce SWE-bench, an evaluation framework consisting of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories.",swebench.com,"Data, code, and leaderboard at swebench.com",评估语言模型在真实软件工程场景下的能力。给定一个代码库和一个需要解决的GitHub问题描述，模型的任务是编辑代码库以解决该问题。,"Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue.",真实世界软件工程问题解决能力，包括跨文件、跨函数、跨类的复杂代码编辑与协调能力，以及长上下文处理和复杂推理能力。,"Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks.",执行单元测试和系统测试。将生成的补丁应用到代码库后，运行与任务实例相关的测试。如果补丁应用成功且所有测试通过，则认为解决方案成功解决了问题。主要指标是解决问题的任务实例百分比。,"To evaluate a proposed solution, we apply the generated patch, using unix's patch program, to the codebase and then execute the unit and system tests associated with the task instance. If the patch applies successfully and all of these tests pass we consider the proposed solution to have successfully resolved the issue. The metric for our benchmark is the percentage of task instances that are resolved.",多文件项目。任务涉及理解大型、复杂的代码库，并跨多个函数、类甚至文件进行协调更改。,"Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously... SWE-bench's reference solutions average editing 1.7 files, 3.0 functions, and 32.8 lines (added or removed).",软件工程，具体为修复Bug或实现新功能。,SWE-bench is a benchmark featuring GitHub issues from popular repositories that report bugs or request new features...,工程级。基于真实GitHub问题，需要处理大型代码库和复杂的跨上下文编辑，对现有最先进模型极具挑战性。,"Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere 1.96% of the issues.",Python。数据集源自12个流行的开源Python仓库。,"SWE-bench, an evaluation framework consisting of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories.","包含2,294个任务实例。","Through these stages of filtering, the original 90,000 PRs are filtered down to the 2,294 task instances which comprise SWE-bench.",从12个流行的开源GitHub Python仓库中抓取的真实GitHub问题和已合并的拉取请求。,SWE-bench sources task instances from real-world Python repositories by connecting GitHub issues to merged pull request solutions that resolve related tests.,2024 (论文版本为2024年11月11日),arXiv:2310.06770v3  [cs.CL]  11 Nov 2024,官方自建。作者通过一个三阶段的筛选流程从GitHub数据中构建。,"To find high-quality task instances at scale, we use a 3-stage pipeline as follows.",抗污染设计。基准可以持续更新，包含模型训练日期之后创建的问题，确保解决方案未包含在训练语料中。,"Therefore, we can extend SWE-bench with a continual supply of new task instances and evaluate LMs on issues created after their training date, which ensures that the solution was not included in their training corpus.",,,代码编辑/修复。生成应用于现有代码库的补丁以解决问题。,Each task requires generating a patch describing changes to apply to the existing codebase.,问题解决率（百分比）。,The metric for our benchmark is the percentage of task instances that are resolved.,自然语言（问题描述）与代码（完整代码库）。,A model is given an issue text description and a complete codebase.,代码（补丁文件）。,"The model is then tasked to make an edit to the codebase to resolve the issue. In practice, we represent edits as patch files...",文本与代码到代码。,"Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue.",仓库的测试框架。需要安装依赖并运行单元测试和系统测试。,The revised codebase is then evaluated using the repository’s testing framework.,1. 真实世界软件工程任务：源自真实GitHub问题和解决方案。2. 可持续更新：可轻松扩展到任何GitHub Python仓库，最小化人工干预。3. 多样化的长输入：问题描述详细，代码库包含数千个文件。4. 鲁棒的评估：每个任务实例至少有一个“失败转通过”的测试。5. 跨上下文代码编辑：不限制编辑范围，需要在大代码库的多个位置生成修订。6. 解决方案的广泛空间：可作为比较检索、长上下文模型和决策智能体的公平平台。,"SWE-bench offers several advantages over existing LM programming benchmarks. These include, a realistic setting that utilizes user-submitted issues and solutions, diverse inputs featuring unique code problems from 12 repositories, a robust framework for execution-based evaluation, and the ability to continuously update the benchmark with new instances, requiring minimal human intervention."
2312.02120_output/content.md,HumanEval,"Notably, MagicoderS-CL-7B based on CODELLAMA even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1).","No, 本文是使用该数据集进行评测","We evaluate Magicoder and MagicoderS on a wide range of coding tasks, including HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) for Python text-to-code generation...",,,Python文本到代码生成，衡量从自然语言描述（docstrings）合成程序的功能正确性。,"...HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) for Python text-to-code generation...",代码生成功能正确性,...indicating that MagicoderS-CL can generate more robust code.,pass@1,...surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1).,,,,,,,Python,"...HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) for Python text-to-code generation...",,,,,,,,,文中提及了去污染措施，表明存在污染风险并进行了处理,"Finally, we apply the same logic as StarCoder Li et al. (2023) to decontaminate our training data by removing coding problems that contain docstrings or solutions from HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021)...",,,代码生成,...for Python text-to-code generation...,pass@1,...surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1).,自然语言,...for Python text-to-code generation...,代码,...for Python text-to-code generation...,文本到代码,...for Python text-to-code generation...,,,,
2402.16906_output/content.md,HumanEval,"Experiments demonstrate that LDB consistently enhances the baseline performance by up to 9.8% across the HumanEval, MBPP, and TransCoder benchmarks","No, 本文是使用该数据集进行评测","We evaluate LDB on three code generation benchmarks: HumanEval (Chen et al., 2021), TransCoder (Roziere et al., 2020), and MBPP (Austin et al., 2021).",,,文本到代码生成，任务描述是一段简要的自然语言段落，概述了要生成的程序的预期功能。,"HumanEval and MBPP are for text-to-code generation, where the task description is a brief passage outlines the intended functionality of the program to be generated.",代码生成的功能正确性,Experiments demonstrate that LDB consistently enhances the baseline performance... in code debugging for various LLM selections.,使用隐藏测试用例计算 Pass@1 准确率,We compute Pass@1 accuracy with hidden test cases for assesment.,,,,,基础编程问题,"Despite these advanced approaches, they still fall short on basic programming questions from the HumanEval and MBPP datasets.",Python,文中仅提及实验设置（评测文本到代码生成），未描述完整数据集的语言构成。,,,,,2021,"HumanEval (Chen et al., 2021)",,,,,,,代码生成,HumanEval and MBPP are for text-to-code generation,Pass@1,We compute Pass@1 accuracy with hidden test cases for assesment.,自然语言,the task description is a brief passage outlines the intended functionality of the program to be generated.,代码,text-to-code generation,文本到代码,text-to-code generation,,,,
2403.07974_output/content.md,LiveCodeBench,"In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code",Yes,"In this work, we propose LiveCodeBench",https://livecodebench.github.io/,Website: https://livecodebench.github.io/,对大型语言模型的代码相关能力进行全面评估，包括代码生成、自我修复、代码执行和测试输出预测等多个方面。,"Notably, our benchmark also focuses on a broader range of code-related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation.",代码相关能力的全面性评估，包括生成、修复、执行和理解。,"Holistic Evaluation. Current code evaluations primarily focus on natural language to code generation. However, programming is a multi-faceted task that requires a variety of capabilities beyond those measured by code generation.",基于功能正确性，使用一组未见过的测试用例进行评估。对于代码生成场景，使用Pass@1指标。,"The evaluation is performed based on functional correctness, using a set of unseen test cases. We use the Pass@1 metric measured as the fraction of the problems for which the model was able to generate a program passing all tests.",单问题解决，上下文为自然语言问题描述和示例测试。,"The model is given a problem statement, which includes a natural language description and example tests (input-output pairs), and is tasked with generating a correct solution.",竞争性编程（算法问题），来源于LeetCode、AtCoder和CodeForces等竞赛平台。,"which collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces.",平衡的难度分布，包含从易到难的问题，并使用平台提供的难度评级进行分类和过滤。,"Therefore, we use problem difficulty ratings (sourced from the competition websites) for filtering the harder problems and classifying problem difficulties to ensure balanced problem difficulty distribution and allow granular model comparisons.","文中未明确提及数据集支持的具体编程语言，但来源平台（LeetCode, AtCoder, CodeForces）通常支持多种语言。",,目前包含超过500个编码问题（511个），收集于2023年5月至2024年5月期间。,"Currently, LiveCodeBench hosts over five hundred coding problems that were published between May 2023 and May 2024. / Particularly, we have collected 511 problems from contests across three competition platforms – LeetCode, AtCoder, and CodeForces occurring from May 2023 to the present (May 2024)",从三个竞赛平台（LeetCode、AtCoder、CodeForces）的每周比赛中收集的新问题。,"Particularly, we collect problems from weekly contests on competition platforms and tag them with a release date.",2024年6月6日（论文版本日期），数据集持续更新至2024年5月。,arXiv:2403.07974v2  [cs.SE]  6 Jun 2024,官方自建，由研究团队从公开竞赛平台收集和构建。,"In this work, we propose LiveCodeBench",抗污染设计，通过持续更新（使用新发布的问题）和时间分段评估来防止数据污染。,"Live updates to prevent contamination. / to prevent the risk of problem contamination, we use live updates, that is evaluate models on new problems.",,,代码生成、代码修复、代码执行、测试输出预测。,"Specifically, we evaluate code LLMs in four scenarios, namely code generation, self-repair, code execution, and test output prediction.",Pass@1（用于代码生成场景）。,We use the Pass@1 metric measured as the fraction of the problems for which the model was able to generate a program passing all tests.,自然语言问题描述、示例测试、错误程序（用于修复）、代码和输入（用于执行）。,"The model is given a problem statement, which includes a natural language description and example tests (input-output pairs) / The model is given the natural language problem description, the incorrect program, the test case it fails on, and the execution feedback from that failure. / The model is given a program and an input",代码（用于生成和修复）、执行结果（用于执行）、测试输出（用于预测）。,The output should be a correct repaired program. / the output should be the result. / the output should be the output for the problem.,文本到代码（生成）、代码与反馈到代码（修复）、代码与输入到结果（执行）、文本与输入到结果（预测）。,generating code from natural language / Fix an incorrect program from execution information / “Execute” a program on an input / Solve the natural language task on a specified input,,,1. 持续更新以防止数据污染。2. 全面的评估场景，超越单一的代码生成。3. 来源于高质量竞赛平台的问题和测试。4. 平衡的问题难度分布。,Live updates to prevent contamination. / Holistic Evaluation. / High-quality problems and tests. / Balanced problem difficulty.
2403.08604_output/content.md,DevEval,"In this case study, we explore the performance of LLMs across the entire software development lifecycle with DevEval",Yes,"To address these shortcomings and fill this gap, we present DevEval, a comprehensive case study that mirrors real-world software development.",https://github.com/open-compass/DevEval,1Our data and code are available at https://github.com/open-compass/DevEval.,评估大型语言模型在整个软件开发生命周期中的能力，包括从详细的产品需求文档（PRD）开始，构建一个多文件的代码库。,DevEval generally evaluates models on the task of constructing a multi-file codebase starting from a product requirement document (PRD) of detailed specifications.,软件开发生命周期的全面能力，包括软件设计、环境配置、实现、验收测试和单元测试。,"DevEval encompasses stages including software design, environment setup, implementation, acceptance testing, and unit testing.","根据任务不同采用多种方法：软件设计任务使用LLM作为评判者进行主观评估；环境配置任务评估依赖文件执行和示例代码运行的成功率；实现任务使用自动化测试框架（如PyTest, GTest, JUnit, Jest）执行参考测试并计算通过率；测试任务评估测试代码的可执行性（Oracle Test）和代码覆盖率。","Since the Software Design tasks are open-ended, we employ the LLM-as-a-judge approach... The evaluation is anchored by two principal metrics: general principles and faithfulness. The principal metric for evaluation in this task is the success rate of the executed example code. The evaluation procedure involves executing reference acceptance and unit tests within a predefined reference environment. Then the evaluation metric is determined by the pass rate of these tests. ...models generally fail to generate executable tests, with oracle test scores falling below 40%... the generated testing code demonstrates potential in code coverage, achieving as high as 79.4% when it is executable.",多文件项目级，需要理解产品需求文档、UML图、架构设计等完整上下文。,DevEval generally evaluates models on the task of constructing a multi-file codebase starting from a product requirement document (PRD) of detailed specifications.,涵盖多个工程和AI领域，包括数据库应用、Web服务、算法实现、API开发、神经网络、计算机视觉、自然语言处理等。,"DevEval covers a range of domains including NLP, computer vision, deep learning, algorithm implementation, API applications, Database applications, web service (both frontend and backend), and general tools and utilities.",工程级，模拟真实世界的软件开发挑战，现有顶级模型（如GPT-4）得分很低。,"Empirical studies show that current LLMs, including GPT-4, fail to solve the challenges presented within DevEval. GPT-4-Turbo achieves the highest scores amongst all evaluated models, yet it obtains less than 10% on our repository-level implementation task.","Python, C/C++, Java, JavaScript (Vue.js)","DevEval features four programming languages... we curated a collection of 22 repositories across four programming languages (Python, C/C++, Java, JavaScript)",包含22个代码仓库，覆盖4种编程语言和多个领域。平均每个仓库包含约2-7个代码文件，276-617行代码，以及若干验收测试和单元测试。,"we curated a collection of 22 repositories across four programming languages... Table 2: DevEval Statistics. Avg. #Code File 2.2-7.0, Avg. #Code Line 276-617, Avg. #Accep. Tests 2-5.4, Avg. #Unit Tests 8.2-12.4",从公开仓库中精心策划收集，并补充了完整软件开发所需的设计文档和测试程序。,"One significant challenge in this study lies in the scarcity of publicly available repositories that include the full range of software development artifacts, particularly design documents and comprehensive testing programs. To overcome this, we curated a collection of 22 repositories...",2024,arXiv:2403.08604v3 [cs.CL] 14 Dec 2024,官方自建，由研究团队为评估目的而构建。,"we present DevEval, a comprehensive case study... To overcome this, we curated a collection...",,,,,代码库生成，涉及从设计到测试的完整软件开发流程。,DevEval generally evaluates models on the task of constructing a multi-file codebase starting from a product requirement document (PRD) of detailed specifications.,软件设计：基于通用原则和忠实度的LLM主观评分；环境配置：示例代码执行成功率；实现：参考验收和单元测试的通过率；测试：Oracle Test分数和代码覆盖率。,"The evaluation is anchored by two principal metrics: general principles and faithfulness... The principal metric for evaluation in this task is the success rate of the executed example code... the evaluation metric is determined by the pass rate of these tests... oracle test scores... code coverage, achieving as high as 79.4%",自然语言（产品需求文档PRD）、结构化图表（UML图、架构设计）、代码。,"models are provided with the PRD, UML diagrams and architecture design... Table 1: Task design in DevEval. Input: PRD, UML Diagrams, Architecture Design, Implementation Code",代码（依赖文件、实现代码、测试代码）、结构化设计文档（UML图、架构设计）。,"Table 1: Task design in DevEval. Output: UML Diagrams, Architecture Design, Dependency Files, Implementation Code, Acceptance Testing Code, Unit Testing Code",文本与图表到代码、文本与图表到设计文档。,DevEval generally evaluates models on the task of constructing a multi-file codebase starting from a product requirement document (PRD)... The first subtask involves the generation of the UML class diagram...,"使用Docker定义的基础环境，针对不同语言使用特定工具（Conda, Gradle, NPM）和测试框架（PyTest, GTest, JUnit, Jest）。","The evaluation centers on the execution of dependency files across each programming language within a predetermined base environment delineated in a Docker file... For Python, the Conda environment manager is employed; for Java and JavaScript, Gradle and NPM are utilized respectively... integrates PyTest for Python, GTest for C++, JUnit for Java, and Jest for JavaScript.",首个评估模型软件设计和环境配置能力的基准；遵循瀑布模型，将软件开发分解为多个相互关联的阶段；提供文档级别的详细需求描述；采用模块化评估协议，支持端到端或分阶段评估。,"DevEval is the first to evaluate models’ software design and environment setup capabilities. Subscribing to the traditional Waterfall software development model... DevEval breaks down this process into a diverse set of inter-related development stages... In contrast to similar systems... which generate outputs from brief requirement descriptions typically under 100 words, DevEval offers document-level detail to guide the models. ...our design utilizes reference inputs for each task. This strategy enables and concentrates on evaluating the efficacy of models in executing specific tasks."
2406.06887_output/content.md,"HumanEval, MBPP, LiveCodeBench, LeetCode","PLUM is evaluated on both standard benchmarks (HumanEval, MBPP) and more challenging ones (LiveCodeBench)... We evaluate PLUM on a diverse set of state-of-the-art code language models under different set-ups, on commonly used evaluation benchmarks: HumanEval(+) and MBPP(+) [7, 1, 28] as well as more challenging code generation datasets like LiveCodeBench and LeetCode [22, 19].","No, 本文是使用该数据集进行评测","PLUM is evaluated on both standard benchmarks (HumanEval, MBPP) and more challenging ones (LiveCodeBench)... We evaluate PLUM on a diverse set of state-of-the-art code language models under different set-ups, on commonly used evaluation benchmarks: HumanEval(+) and MBPP(+) [7, 1, 28] as well as more challenging code generation datasets like LiveCodeBench and LeetCode [22, 19].",,,文中未描述这些基准的原始任务定义，仅提及它们是用于评估代码生成模型的常用基准。,"PLUM is evaluated on both standard benchmarks (HumanEval, MBPP) and more challenging ones (LiveCodeBench)... We evaluate PLUM on a diverse set of state-of-the-art code language models under different set-ups, on commonly used evaluation benchmarks: HumanEval(+) and MBPP(+) [7, 1, 28] as well as more challenging code generation datasets like LiveCodeBench and LeetCode [22, 19].",代码生成的功能正确性,Works like AlphaCode [26] and LeTI [48] have introduced test outcomes as a means to define functional correctness in code generation.,使用测试用例执行来评估功能正确性,Works like AlphaCode [26] and LeTI [48] have introduced test outcomes as a means to define functional correctness in code generation.,,,通用编程问题,These datasets provide a diverse range of programming tasks and instructions.,标准基准和更具挑战性的基准,"PLUM is evaluated on both standard benchmarks (HumanEval, MBPP) and more challenging ones (LiveCodeBench)... as well as more challenging code generation datasets like LiveCodeBench and LeetCode [22, 19].",文中仅提及实验聚焦于Python，未描述完整数据集的语言覆盖范围。,We focus on Python due to its wide use and the availability of well-established training and evaluation resources.,,,,,,,,,,,,,代码生成,"Language models pre-trained on code corpora have excelled at code generation [40, 25].",通过率 (pass rates),"PLUM increases pass rates by up to 4.8% on average on standard benchmarks and 11.8% on LiveCodeBench, demonstrating its effectiveness and generalizability.",自然语言指令,"PLUM utilizes natural language instructions from well-established datasets such as OSS-Instruct [49], Evol-Instruct-Code [31], and ShareGPT [8].",代码,"These solutions are evaluated using the generated test cases, with preference labels assigned based on the results: solutions passing the tests are preferred, while failures are dis-preferred.",文本到代码,PLUM utilizes natural language instructions from well-established datasets... These solutions are evaluated using the generated test cases...,文中未明确描述基准的执行环境，但提及了执行检查。,"With static and execution checks,3 we identify and filter out solutions that contain syntactic errors and fail to execute, as our focus is on functional correctness.",本文提出的PLUM框架本身是一个使用测试用例进行执行引导、基于策略的偏好学习方法，用于改进代码语言模型。它并非一个数据集。,"we propose PLUM, an on-policy Preference Learning framework Augmented with test cases for code LMs."
2406.18294_output/content.md,CrossCodeEval,"To assess the code completion performance of Code LLMs in real development scenarios, we utilized CrossCodeEval (Ding et al., 2023) as the evaluation dataset.","No, 本文是使用该数据集进行评测","To assess the code completion performance of Code LLMs in real development scenarios, we utilized CrossCodeEval (Ding et al., 2023) as the evaluation dataset.",,,评估代码大语言模型在真实开发场景中的代码补全性能，特别是需要利用跨文件代码信息进行补全的任务。,"To assess the code completion performance of Code LLMs in real development scenarios... The CrossCodeEval (Ding et al., 2023) benchmark provides test cases that require the use of cross-file code information for completion.",仓库级代码补全能力，利用跨文件信息的能力,"Some benchmarks for repository-level code completion have been proposed to evaluate the performance of code models in real-world completion tasks, such as CrossCodeEval (Ding et al., 2023)...","精确匹配（Exact Match, EM）和编辑相似度（Edit Similarity, ES）","Following the CrossCodeEval evaluation protocol, we evaluated the completion results using two metrics: Exact Match (EM) and Edit Similarity (ES).",跨文件、仓库级上下文,"The CrossCodeEval (Ding et al., 2023) benchmark provides test cases that require the use of cross-file code information for completion.",,,真实世界开发场景,To assess the code completion performance of Code LLMs in real development scenarios...,文中仅提及实验子集(Python)，未描述完整数据集,"Without loss of generality, in this study, we have chosen Python language as the primary language for our research.","本文实验使用了2,655个真实世界补全测试用例","Ultimately, we obtained 2,655 real-world completion tests.",,,2023,"CrossCodeEval (Ding et al., 2023)",,,,,,,代码补全（包括中段填充）,Infilling scenarios constitute the majority of code completion tasks in the real world.,"Exact Match (EM), Edit Similarity (ES)","Following the CrossCodeEval evaluation protocol, we evaluated the completion results using two metrics: Exact Match (EM) and Edit Similarity (ES).",代码（包含部分缺失的代码上下文）,We then removed the code after the cursor in that line to form authentic completion test cases.,代码,The completion results are less than satisfactory.,代码到代码（补全）,Infilling scenarios constitute the majority of code completion tasks in the real world.,,,专注于评估模型利用跨文件代码信息进行仓库级代码补全的能力，测试用例需要跨文件信息。,"The CrossCodeEval (Ding et al., 2023) benchmark provides test cases that require the use of cross-file code information for completion."
2408.06450_output/content.md,EVALPERF,"As a proof of concept, we use DPE to create EVALPERF, a benchmark with 121 performance-challenging coding tasks.",Yes,"We introduce Differential Performance Evaluation (DPE), a framework designed to reliably evaluate Large Language Models (LLMs) for efficient code generation. ... As a proof of concept, we use DPE to create EVALPERF, a benchmark with 121 performance-challenging coding tasks.",github.com/evalplus/evalplus,We also fully open-source and maintain the data curation pipeline and evaluator at github.com/evalplus/evalplus as part of EvalPlus.,评测大型语言模型（LLMs）生成的代码的效率。该基准旨在通过提供具有性能挑战性的编程任务和有效的复合指标，可靠地评估代码效率，弥补传统代码基准在效率评估方面的不足。,"We introduce Differential Performance Evaluation (DPE), a framework designed to reliably evaluate Large Language Models (LLMs) for efficient code generation. Traditional coding benchmarks often fail to provide reliable insights into code efficiency, due to their reliance on simplistic test inputs and the absence of effective compound metrics. DPE addresses these issues by focusing on efficiency-demanding programming tasks and establishing an insightful compound metric for performance evaluation.",代码生成效率,"While the correctness evaluation of code generation has been well studied, we deliver a new and important aspect to the community by studying the data curation and assessment for the efficiency evaluation of LLM-generated code.",差分性能评估（DPE）。该方法分为两个阶段：1）数据集构建：从现有基准中选择对效率有要求的任务，并生成计算密集型的输入来测试LLM解决方案的效率。2）效率评估：分析新解决方案的性能，并将其与一组具有不同效率水平的参考解决方案进行全局比较，匹配到的效率水平决定了其效率得分。,"DPE operates in two phases: To curate efficiency datasets, it selects efficiency-demanding tasks from existing coding benchmarks and generates computationally expensive inputs to stress the efficiency of LLM solutions. To assess the code efficiency, DPE profiles the new solution and compares it globally against a set of reference solutions that exhibit distinct efficiency levels, where the matched level defines its efficiency score.",,,通用编程任务，涵盖算法、数据结构等,"At the high level, the input to the dataset creation phase is a set of programming tasks, e.g., from HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021).",性能挑战型任务，旨在区分不同代码解决方案的效率,DPE curates performance-demanding coding tasks by sampling synthesized test input generators and using filters to ensure evaluator quality.,Python（从HumanEval和MBPP等基准中选取任务，这些基准主要使用Python）,"At the high level, the input to the dataset creation phase is a set of programming tasks, e.g., from HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021).",包含121个性能挑战型编程任务,"As a proof of concept, we use DPE to create EVALPERF, a benchmark with 121 performance-challenging coding tasks.",从现有代码生成基准（如HumanEval、MBPP）中筛选和改造任务，并利用LLM合成性能测试输入生成器来生成计算密集型测试输入。,"At the high level, the input to the dataset creation phase is a set of programming tasks, e.g., from HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021). ... To this end, we propose Synthesizing a Synthesizer (SAS) to automatically produce performance-exercising inputs of different programming tasks by prompting powerful code LLMs to generate test generators.",2024.08,arXiv:2408.06450v1  [cs.SE]  12 Aug 2024,官方自建，基于DPE框架从现有基准中筛选和增强,"As a proof of concept, we use DPE to create EVALPERF, a benchmark with 121 performance-challenging coding tasks.",,,,,代码生成（从自然语言描述生成完整的函数/解决方案）,"Given a programming task, we collect a rich set of correct solutions by sampling various LLMs and test execution.",效率得分（基于与参考解决方案性能集群的匹配度）,"During evaluation, if passing the correctness tests, the new solutions are profiled to compare against the reference solutions. Specifically, from slow to fast, the cumulative ratio of the cluster that includes the matched reference solution is the efficiency score of the evaluated solution.",自然语言（任务描述）,"Given a coding instruction in natural language, LLMs produce solutions...",代码,LLMs produce solutions whose correctness is assessed through test execution.,文本到代码,"Given a coding instruction in natural language, LLMs produce solutions...",,,1. 专注于代码效率评估，而非传统的功能正确性。2. 使用DPE框架，包含“合成一个合成器”（SAS）方法来自动生成性能测试输入。3. 通过性能聚类建立参考解决方案集，用于全局比较和评分。4. 包含任务筛选标准（足够的计算量、低性能变异、性能多样性）以确保评估质量。,"DPE addresses these issues by focusing on efficiency-demanding programming tasks and establishing an insightful compound metric for performance evaluation. ... To this end, we propose Synthesizing a Synthesizer (SAS) to automatically produce performance-exercising inputs of different programming tasks by prompting powerful code LLMs to generate test generators. ... a selected programming task must meet the following criteria: 1. Sufficient computation ... 2. Low performance variation ... 3. Performance diversity..."
2410.01215_output/content.md,HumanEval,MGDebugger elevates accuracy from 75.6% to 94.5% on HumanEval [7] and achieves a remarkable 97.6% repair success rate on HumanEvalFix [44].,"No, 本文是使用该数据集进行评测","Extensive experiments across multiple models and benchmarks demonstrate that MGDebugger significantly outperforms existing debugging methods. Using open-source models like DeepSeek-Coder-V2-Lite, CodeQwen1.5, and Codestral, MGDebugger elevates accuracy from 75.6% to 94.5% on HumanEval [7] and achieves a remarkable 97.6% repair success rate on HumanEvalFix [44].",,,文中仅提及实验使用，未描述完整数据集。根据引用[7]（HumanEval）推断，该基准旨在评估从自然语言描述（docstring）生成Python代码的功能正确性。,"Following the problem settings from Chen et al. [9] and Zhong et al. [75], we assume that the public test cases for the main function Tpub have been provided, which is common in most code generation benchmarks [7, 14, 44]2.",代码生成的功能正确性,MGDebugger elevates accuracy from 75.6% to 94.5% on HumanEval [7] and achieves a remarkable 97.6% repair success rate on HumanEvalFix [44].,执行单元测试（pass@k）,"Following the problem settings from Chen et al. [9] and Zhong et al. [75], we assume that the public test cases for the main function Tpub have been provided, which is common in most code generation benchmarks [7, 14, 44]2.",单函数,"Specifically, given an LLM-generated function 𝑓, we decompose it into a hierarchical structure of subfunctions denoted as (𝑓1, ..., 𝑓𝑛).",,,,,Python,"To effectively test each subfunction, we propose an LLM-simulated Python executor, which traces code execution and tracks important variable states to pinpoint errors accurately.",,,,,,,,,,,,,代码生成,"Large language models (LLMs) such as GPT-4 [48], LLaMA [56], and DeepSeek-Coder [1] have made significant advances in AI-assisted coding tasks [7, 25, 34, 52]. Trained on vast corpora of text and code, LLMs can understand and generate code snippets for various programming tasks, ranging from simple data structures to complex algorithmic problems [34, 57].",准确率（accuracy），修复成功率（repair success rate）,MGDebugger elevates accuracy from 75.6% to 94.5% on HumanEval [7] and achieves a remarkable 97.6% repair success rate on HumanEvalFix [44].,自然语言（函数描述/docstring）,"Following the problem settings from Chen et al. [9] and Zhong et al. [75], we assume that the public test cases for the main function Tpub have been provided, which is common in most code generation benchmarks [7, 14, 44]2.",代码（Python函数）,"Specifically, given an LLM-generated function 𝑓, we decompose it into a hierarchical structure of subfunctions denoted as (𝑓1, ..., 𝑓𝑛).",文本到代码,"Following the problem settings from Chen et al. [9] and Zhong et al. [75], we assume that the public test cases for the main function Tpub have been provided, which is common in most code generation benchmarks [7, 14, 44]2.",,,本文未描述HumanEval的独特之处，而是将其作为评估模型调试能力的标准基准之一。,"Extensive experiments across multiple models and benchmarks demonstrate that MGDebugger significantly outperforms existing debugging methods. Using open-source models like DeepSeek-Coder-V2-Lite, CodeQwen1.5, and Codestral, MGDebugger elevates accuracy from 75.6% to 94.5% on HumanEval [7] and achieves a remarkable 97.6% repair success rate on HumanEvalFix [44]."
2411.05830_output/content.md,GitChameleon,"To address this gap, we introduce GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests.",Yes,"To address this gap, we introduce GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests.",https://github.com/NizarIslah/GitChameleon,"For facilitation for further exploration of version-conditioned code generation, we make our code repository publicly accessible at https://github.com/NizarIslah/GitChameleon.",评估大型语言模型生成特定版本库代码的能力，要求生成的代码不仅语法正确，而且在执行时功能准确。,GitChameleon is designed to rigorously assess the ability of modern large language models (LLMs) to generate version-specific code that is not only syntactically correct but also functionally accurate upon execution.,版本特定代码生成能力、代码库动态适应性、功能正确性,"By providing an execution-based benchmark that emphasizes the dynamic nature of code libraries, GitChameleon serves as a critical tool to advance the development of more adaptable and reliable code generation models.",基于执行的评估，使用手写的基于断言的单元测试来验证生成代码的正确性。,"To evaluate LLM performance on GitChameleon, each problem is accompanied by handwritten assertion-based unit tests, enabling a thorough execution-based assessment of the outputs generated by the code LLMs.",单函数代码补全,"GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems","Python编程，涉及多个流行的机器学习和数据处理库（如PyTorch, NumPy, Scikit-Learn, Pandas等）","GitChameleon consists of 116 python-based version conditioned problems based on 11 libraries: PyTorch (Paszke et al., 2019), Geopandas (Jordahl et al., 2020), NLTK (Bird & Loper, 2004), NetworkX (Hagberg et al., 2008), GeoPy3, Gradio (Abid et al., 2019), Scikit-Learn (Buitinck et al., 2013), Matplotlib (Hunter, 2007), PyCaret4, Pandas (pandas development team, 2020; Wes McKinney, 2010) and NumPy (Harris et al., 2020).",基于真实版本变化的实际问题，旨在模拟开发者在技术债务约束下的真实场景。,our dataset offers a unique and complementary perspective by focusing on the real-world scenario where developers are often constrained to specific library versions due to technical debt.,Python,"GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems",包含116个Python版本条件问题,"GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems",手工编写和LLM辅助，基于真实库的变更日志（changelogs）,"The examples were manually crafted by the authors, who divided the task among themselves. We compiled a list of popular Python libraries, focusing on those with which at least one author was familiar and that had detailed changelogs documenting changes between versions.",2024-11-05 (根据arXiv版本v1日期),arXiv:2411.05830v1  [cs.SE]  5 Nov 2024,官方自建（作者手工编写）,"The examples were manually crafted by the authors, who divided the task among themselves.",已考虑训练数据截止日期，确保样本在模型训练窗口期内，以评估模型对已训练版本的知识。,"Since some of the models evaluated on GitChameleon have disclosed their training data cutoff dates, we have ensured that most, if not all, samples fall within the training window of these models.",,,代码补全,"GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems",pass@k (例如pass@10),"for instance, GPT-4o achieves a pass@10 of only 39.9% (43.7% when provided with error feedback)",自然语言问题描述和起始代码,"The problem statements average 20.4 tokens, and the starter code averages 47.4 tokens, leading to a combined average of 67.8 tokens per sample.",代码,generate version-specific code,文本到代码,synthesizing programs from docstrings.,需要特定库版本依赖的虚拟环境,We used venv to create and manage virtual environments for testing. This process involved installing the appropriate library version and any additional dependencies.,专注于评估模型对真实库版本变化的适应能力，每个问题都绑定到特定的库版本，并配有可执行的单元测试。数据基于2014年至2023年的真实版本发布，并标注了变更类型（参数/属性变更、函数名变更、语义/行为变更、新功能）。,"GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests. ... The samples were collected from version releases over a period from the year 2014 to 2023 ... we annotate each sample with the type of change that is classified into the following categories: Argument or Attribute change, Function Name change, Semantics or Function Behavior change, New feature or additional dependency-based change."
2411.12882_output/content.md,PurpleLlama secure coding benchmark,"We demonstrate the effectiveness of PROSEC on the PurpleLlama (Bhatt et al., 2023) secure coding benchmark.","No, 本文是使用该数据集进行评测","We demonstrate the effectiveness of PROSEC on the PurpleLlama (Bhatt et al., 2023) secure coding benchmark.",,,评估代码大语言模型生成安全代码的能力，检测其生成的代码是否包含常见弱点枚举（CWE）中定义的安全漏洞。,"The goal of security alignment in code LLMs is to reduce the likelihood of generating insecure code... We demonstrate the effectiveness of PROSEC on the PurpleLlama (Bhatt et al., 2023) secure coding benchmark.",代码安全性，即生成代码是否遵循安全编码实践，避免引入常见漏洞。,"The safety of these models remains under-explored, posing potential risks as insecure code generated by these models may introduce vulnerabilities into real-world systems.",使用静态分析器作为预言机，检测生成的代码片段中是否存在CWE漏洞。,"Following previous work (Bhatt et al., 2023), we assume that there exists a static analyzer... as an oracle to detect whether a snippet of code follows secure code patterns. Specifically, the static analyzer takes as input a code snippet, and outputs a list of detected CWEs.",,,安全编码，涵盖常见弱点枚举（CWE）中定义的各种软件安全漏洞类型。,"The Common Weakness Enumerations (CWEs) (MITRE, 2023), which abstract diverse program vulnerabilities, offer a generalizable foundation for simulating how vulnerabilities manifest across various coding tasks and programming languages.",,,文中未明确提及PurpleLlama基准支持的具体编程语言，但本文实验涉及多种语言。,"PROSEC improves the ability of code LLMs to generate secure code... across multiple models, languages, and vulnerability types.",,,,,2023,"PurpleLlama (Bhatt et al., 2023)",,,,,,,代码生成,Large language models (LLMs) capable of generating code based on human instructions have revolutionized programming by significantly facilitating tasks such as code generation...,安全代码生成能力的提升百分比（例如，比基线模型安全25.2%–35.4%）,The models trained with the dataset synthesized by PROSEC are 25.2%–35.4% more secure than those trained with the SafeCoder dataset.,自然语言指令,"Consider an instruction following code LLM πθ(y|x) = Πiπθ(yi|y<i, x) that takes user instruction x and generates the code response y.",代码,"Consider an instruction following code LLM πθ(y|x) = Πiπθ(yi|y<i, x) that takes user instruction x and generates the code response y.",文本到代码,"Consider an instruction following code LLM πθ(y|x) = Πiπθ(yi|y<i, x) that takes user instruction x and generates the code response y.",,,专注于代码安全性的评测基准，使用CWE（常见弱点枚举）作为漏洞分类和检测的基础框架。,"A widely recognized framework for categorizing such issues is the Common Weakness Enumerations (CWE) (MITRE, 2023)... Following previous work (Bhatt et al., 2023), we assume that there exists a static analyzer... to detect whether a snippet of code follows secure code patterns... and outputs a list of detected CWEs."
2412.00535_output/content.md,FullStack Bench,FullStack Bench: Evaluating LLMs as Full Stack Coders,Yes,"To address this gap, we have developed a comprehensive code evaluation dataset FullStack Bench1,2 focusing on full-stack programming","https://huggingface.co/datasets/ByteDance/FullStackBench, https://github.com/bytedance/FullStackBench","1https://huggingface.co/datasets/ByteDance/FullStackBench
2https://github.com/bytedance/FullStackBench",评估大型语言模型作为全栈程序员的能力，涵盖广泛的应用程序领域。,"focusing on full-stack programming, which encompasses a wide range of application domains (e.g., basic programming, data analysis, software engineering, mathematics, and machine learning).",全栈编程能力、多语言编程能力、跨应用领域能力,focusing on full-stack programming... to assess multi-lingual programming capabilities... a wide range of application domains,使用单元测试用例进行代码沙箱执行评估,"we design real-world instructions and corresponding unit test cases... we also release an effective code sandbox execution tool (i.e., SandboxFusion3)... to evaluate the performance of our FullStack Bench efficiently.",,,基础编程、高级编程、软件工程、数据分析、数学、桌面与Web开发、机器学习、科学计算、数据库、多媒体、操作系统等,"Basic Programming (BP)
Advanced Programming (AP)
Software Engineering (SE)
Data Analysis (DA)
Mathematics (MA)
Desktop and Web Development (DW)
Machine Learning (ML)
Scientific Computing (SC)
DataBase (DB)
Multimedia (MM)
Operating System (OS)
Others",,,16种广泛使用的编程语言,"in FullStack Bench, we design real-world instructions and corresponding unit test cases from 16 widely-used programming languages",,,设计真实世界的指令和对应的单元测试用例，而非简单翻译,we design real-world instructions and corresponding unit test cases from 16 widely-used programming languages to reflect real-world usage scenarios rather than simple translations.,2025-05-12,arXiv:2412.00535v6  [cs.AI]  12 May 2025,官方自建,we have developed a comprehensive code evaluation dataset FullStack Bench,,,,,代码生成,evaluating LLMs as Full Stack Coders,,,自然语言指令,we design real-world instructions,代码,evaluating LLMs as Full Stack Coders,文本到代码,we design real-world instructions and corresponding unit test cases,支持多种编程语言和包的代码沙箱执行工具(SandboxFusion),"we also release an effective code sandbox execution tool (i.e., SandboxFusion3) supporting various programming languages and packages",专注于全栈编程评估，覆盖广泛的应用程序领域；包含16种编程语言的真实世界指令和单元测试，而非简单翻译；配套发布了支持多语言和多包的代码沙箱执行工具SandboxFusion。,"focusing on full-stack programming, which encompasses a wide range of application domains... we design real-world instructions and corresponding unit test cases from 16 widely-used programming languages to reflect real-world usage scenarios rather than simple translations. Moreover, we also release an effective code sandbox execution tool (i.e., SandboxFusion3) supporting various programming languages and packages"
2412.05210_output/content.md,CodeArena,we present a rigorous human-curated benchmark CodeArena to emulate the complexity and diversity of real-world coding tasks,Yes,"we first introduce a comprehensive human-curated benchmark, CodeArena",https://codearenaeval.github.io/,1https://codearenaeval.github.io/,评估模型生成的代码响应与人类偏好的对齐程度，模拟现实世界编码任务的复杂性和多样性。,"to evaluate the alignment between the model-generated response and human preference, enabling the community to evaluate and track the alignment between human preferences and model-generated responses in real-world scenarios.",人类偏好对齐（包括代码质量、解释、格式、注释等），而非单纯的代码正确性。,"the current code LLMs focus on synthesizing the correct code snippet, ignoring the alignment with human preferences... underscoring the importance of the human preference alignment.",使用GPT-4o作为评判员，通过“比较A和B”和“比较B和A”两种游戏计算模型相对于基线的胜率。,"we apply GPT-4o-2024-08-06 as the judger to evaluate the model performance. Specifically, we use two games “compare A and B ” and “compare B and A” (avoid the relative position of A and B affecting the results) to calculate the win rate of A compared to the baseline B.",基于现实世界用户查询的完整问题，不局限于自包含的函数片段。,"Popular code-related benchmarks typically focus on self-contained function snippets... CodeArena provides many problems for evaluation under realistic scenarios, which are not suitable for verification through unit testing.",涵盖软件开发、用户界面/体验、专用计算、工具与环境、数据库与数据处理、新兴技术、通用查询等7大类40个子类。,comprising 397 high-quality samples across 40 categories derived from real-world user queries... encompassing 7 major categories and 40 subcategories.,分为简单、中等、困难三个级别，大部分样本为中等或困难。,"all samples are classified into easy, medium, and hard. The majority of the samples are recognized as medium or hard, presenting a significant challenge to LLMs.",涵盖44种编程语言，包括Python、C++、Java、JavaScript、HTML/CSS、SQL、Bash、Go、Rust、PowerShell、Google Apps Script等。,spanning 40 categories and 44 programming languages... CodeArena provides a valuable comprehensive benchmark for 40 subtasks and 44 programming languages,包含397个高质量样本。,comprising 397 high-quality samples... CodeArena consists of nearly 400 problems.,从在线问答网站的用户查询中精心筛选和人工标注。,carefully curated from user queries... derived from real-world user queries... Online Q&A,2024年12月（论文版本日期）,arXiv:2412.05210v1  [cs.CL]  6 Dec 2024,官方自建，经过严格的人工标注和质量控制流程。,We propose CodeArena comprised of 397 manually annotated samples... we implement a rigorous human annotation process involving 4 full-time employees proficient in various programming languages for human annotation and 4 other senior programming developers for quality check.,"进行了去污染处理，通过移除与现有基准（MultiPL-E, MBPP, McEval, NaturalCodeBench）的精确匹配（10-gram重叠）来确保提示的唯一性。","To avoid data leakage, we apply decontamination to ensure the uniqueness of prompts in CodeArena, by removing exact matches (10-gram word overlap) from MultiPL-E, MBPP, McEval, and NaturalCodeBench.",,,代码生成（根据自然语言描述生成代码片段或完整解决方案）。,The query should be sampled from the practical application scenarios and the model-generated responses should satisfy the human preference.,胜率（win rate），基于GPT-4o的偏好判断。,calculate the win rate of A compared to the baseline B.,自然语言（用户查询/问题描述）。,Each sample in CodeArena includes (question...),代码与自然语言混合（期望的响应包括代码片段以及详细的解释、格式化和注释）。,"Claude3.5 produces responses that include detailed explanations, well-structured formatting, and code comments, making it more favorable in terms of human preference.",文本到代码与文本（自然语言问题到包含代码和解释的响应）。,"synthesizing the correct code snippet... alignment with human preferences (including explanations, formatting, comments)",,,专注于评估代码LLM与人类偏好的对齐，而非单纯的代码执行正确性；样本来源于真实世界用户查询，覆盖广泛的编程语言和任务类别；包含人工标注的难度等级和基线答案。,"Unlike previous benchmarks, which consist of various programming exercises along with the corresponding test cases... our benchmarks emphasize a diverse range of programming languages that are commonly used in everyday programming tasks... provides a valuable comprehensive benchmark for 40 subtasks and 44 programming languages, which satisfies the evaluation in realistic scenarios."
2505.08503_output/content.md,ICVul,ICVul: A Well-labeled C/C++ Vulnerability Dataset with Comprehensive Metadata and VCCs,Yes,"we constructed ICVul, a dataset emphasizing data quality and enriched with comprehensive metadata, including Vulnerability-Contributing Commits (VCCs).",https://github.com/Chaomeng-Lu/ICVul.git,"The project, along with supporting scripts, is publicly available on GitHub1. 1https://github.com/Chaomeng-Lu/ICVul.git",用于训练和评估基于机器学习的软件漏洞检测模型。,"Machine learning-based software vulnerability detection requires high-quality datasets, which is essential for training effective models.",漏洞检测的数据质量、标签可靠性、元数据丰富度、数据平衡性。,"To address challenges related to data label quality, diversity, and comprehensiveness, we constructed ICVul, a dataset emphasizing data quality and enriched with comprehensive metadata...",文中未明确描述针对该数据集的标准评估方法。,,代码提交（Commit）、文件（File）、函数（Function）级别。,"ICVul provides a high-quality dataset tailored for training ML models to detect software vulnerabilities at the commit, function and file levels.",软件安全、漏洞检测。,Detecting and mitigating software vulnerabilities remains one of the most critical challenges in ensuring the security and integrity of modern software systems.,真实世界软件项目中的漏洞，复杂度高。,"With the growing complexity of applications and their codebases, traditional methods of vulnerability detection... struggle to keep up with the scale and intricacy of vulnerabilities present in large software projects.",C/C++,"ICVul, an Integrated and Comprehensive C/C++ Vulnerability dataset.",包含807个仓库，146个CWE类型，4327个修复提交，6862个文件，15396个函数，其中6276个为漏洞函数（占比41%）。,"ICVul 807 146 4,327 6,862 15,396 6,276 41%",从美国国家漏洞数据库（NVD）中筛选与GitHub修复提交相关联的CVE条目，并从中提取代码和元数据。,"We began by filtering Common Vulnerabilities and Exposures from the NVD, retaining only those linked to GitHub fix commits. Then we extracted functions and files along with relevant metadata from these commits...",2024年11月13日（数据收集日期）,"The dataset construction process began with the collection of 269,509 CVEs, gathered on November 13, 2024.",官方自建，并提供了可复现的构建框架。,We introduce a vulnerability collection framework that can be re-run at any time to ensure the dataset remains up-to-date.,通过ESC技术排除可疑提交，旨在提高标签可靠性，降低噪声。,"To further enhance label reliability, we developed the ESC (Eliminate Suspicious Commit) technique, ensuring credible data labels.",文中未提及。,,漏洞检测（分类任务）。,ICVul provides a high-quality dataset tailored for training ML models to detect software vulnerabilities...,文中未提及针对该数据集的标准评估指标。,,源代码（C/C++）及丰富的元数据（如提交信息、代码变更、仓库信息等）。,The dataset is stored in a relational-like database for improved usability and data integrity.,漏洞标签（是否包含漏洞，以及CWE类型）。,"the dataset provides clear vulnerability labels at the CWE type level, enabling the development and research of multi-class classification models.",代码到标签（漏洞分类）。,training ML models to detect software vulnerabilities,不涉及代码执行，是静态分析数据集。,,1. 包含漏洞引入提交（VCC）信息，使用SZZ算法追溯。2. 应用ESC技术过滤噪声数据，提高标签质量。3. 数据平衡性好（漏洞函数占比41%）。4. 以关系型数据库结构存储，包含仓库、提交、文件、函数、CVE-VCC映射五个互相关联的表。5. 专注于C/C++语言。,"Another key feature of ICVul is its inclusion of VCCs, which trace specific commits responsible for introducing vulnerabilities into the codebase using the state-of-the-art SZZ algorithm... ICVul incorporates the ESC (Eliminate Suspicious Commit) technique, further enhancing label reliability... the dataset achieves a much better balance ratio of 41% at the function level. ICVul comprises five interconnected tables: repository info, cve fc vcc mapping, commit info, file info, and function info."
2511.17330_output/content.md,SV-COMP,Experimental evaluation on SV-COMP benchmarks and on Linux kernel modules shows promising efficacy in achieving automated program verification.,"No, 本文是使用该数据集进行评测",Experimental evaluation on SV-COMP benchmarks and on Linux kernel modules shows promising efficacy in achieving automated program verification.,,,程序验证，即证明程序满足其形式化规范。,Formal program verification uses mathematically rigorous methods to prove that a program satisfies its specifications.,程序验证的自动化能力，特别是证明生成的有效性。,"We thoroughly evaluate our approach on existing mathematical lemmas [51], as well as lemmas systematically extracted from SV-COMP programs [7]. The SV-COMP programs’ lemmas capture intricate code logic and properties, which are more representative of program verification.",通过证明成功率（即成功证明的引理比例）进行评估。,"Evaluation shows that AutoRocq significantly outperforms state-of-the-art approaches. Specifically, AutoRocq is capable of proving 51.1% mathematical lemmas and 30.9% program lemmas, exceeding baseline approaches by 20.8% to 343.0% on mathematical lemmas, and by 42.4% to 204.6% on program lemmas.",,,软件验证、形式化方法、定理证明。,"We showcase the feasibility of automatic and end-to-end verification with LLM agents. AutoRocq is evaluated on the widely used SV-COMP programs in software verification, as well as Linux kernel modules.",代表真实世界程序验证的复杂逻辑和属性，难度较高。,"The SV-COMP programs’ lemmas capture intricate code logic and properties, which are more representative of program verification.",,,,,从SV-COMP程序和Linux内核模块中系统提取的引理。,"We thoroughly evaluate our approach on existing mathematical lemmas [51], as well as lemmas systematically extracted from SV-COMP programs [7].",,,,,,,,,证明生成（生成证明脚本以履行证明义务）。,"Proof generation aims to automatically synthesize proof scripts that discharge the given proof obligations, which closes the last mile of program verification.",证明成功率（百分比）。,AutoRocq is capable of proving 51.1% mathematical lemmas and 30.9% program lemmas...,形式化的证明义务（逻辑公式）。,Proof generation aims to automatically synthesize proof scripts that discharge the given proof obligations...,证明脚本（Rocq/Coq战术序列）。,Proof generation aims to automatically synthesize proof scripts that discharge the given proof obligations...,逻辑公式到证明脚本。,Proof generation aims to automatically synthesize proof scripts that discharge the given proof obligations...,Rocq（原Coq）定理证明器环境。,The iterative improvement of the proof is achieved by the proof agent communicating with the Rocq (formerly Coq) theorem prover to get additional context and feedback.,SV-COMP是软件验证竞赛的基准，其程序引理捕获了复杂的代码逻辑和属性，更能代表真实的程序验证场景。,"The SV-COMP programs’ lemmas capture intricate code logic and properties, which are more representative of program verification."
2512.03421_output/content.md,BugT,"This study evaluates six closed-source and seven open-source LLMs using the Codeflaws, Condefects, and BugT datasets, with BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns.",Yes,"BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns. ... We introduce a new self-created dataset with real programming faults, aiming to mitigate data leakage concerns and establish a more reliable evaluation benchmark for assessing LLMs’ capabilities.",https://github.com/Xucranger/PLofLBFL,"To facilitate future study, we share our source code and experimental data in a GitHub repository 1. 1https://github.com/Xucranger/PLofLBFL",评估大型语言模型在初学者程序中进行故障定位（Fault Localization）的性能。,"This study evaluates the fault localization performance of six closed-source LLMs (e.g., OpenAI o3, GPT-3.5-Turbo, etc.) and seven open-source LLMs (e.g., DeepSeekR1, Llama3, etc.) across Codeflaws, Condefects, and BugT datasets.",故障定位的准确性、效率、可用性，以及对问题难度的鲁棒性。,"To investigate these issues, we aim to empirically assess the performance of different LLMs across various datasets, evaluating their accuracy, efficiency, and usability in fault localization for novice programs. ... We examine how problem difficulty levels across three datasets affect LLMs’ fault localization performance, providing insights into model behavior at varying complexity levels.",在多个数据集上评估LLMs的故障定位性能，并与传统方法（如SBFL、MBFL）进行比较。,"This study evaluates the fault localization performance of six closed-source LLMs (e.g., OpenAI o3, GPT-3.5-Turbo, etc.) and seven open-source LLMs (e.g., DeepSeekR1, Llama3, etc.) across Codeflaws, Condefects, and BugT datasets. All closed-source LLMs outperform traditional SBFL and MBFL methods...",,,编程教育，初学者编程。,Novice programmers often face challenges in fault localization due to their limited experience and understanding of programming syntax and logic. ... This study evaluates the fault localization performance ... for novice programs.,包含不同难度级别的问题，从简单到复杂。,"LLM accuracy decreases as problem difficulty increases in Codeflaws and Condefects, but top models maintain high accuracy even at peak difficulty in BugT, suggesting its lower complexity. ... We examine how problem difficulty levels across three datasets affect LLMs’ fault localization performance...",,,,,自建的包含真实编程故障的数据集。,"We introduce a new self-created dataset with real programming faults, aiming to mitigate data leakage concerns and establish a more reliable evaluation benchmark for assessing LLMs’ capabilities.",2025-12-04,"Preprint submitted to Journal of LATEX Templates December 4, 2025",官方自建,We introduce a new self-created dataset with real programming faults...,专门设计以减轻数据泄露问题。,BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns.,,,故障定位,This study evaluates the fault localization performance...,准确性（accuracy）,LLM accuracy decreases as problem difficulty increases...,包含故障的代码,"Novice Program refers to code written by novice programmers, usually in the early stages of learning programming. These programs usually contain multiple errors...",故障位置及解释,LLM generated fault explanations demonstrate significant value for novice programmer assistance...,代码到故障位置/解释,"LLM-based fault localization methods leverage the models’ understanding of program syntax and semantics to automatically localize faults in code, offering more precise and contextually relevant suggestions...",,,专门针对初学者程序（Novice Programs）构建，旨在减轻LLM评估中的数据泄露（data leakage）问题，包含真实的编程故障。,"BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns. ... We introduce a new self-created dataset with real programming faults, aiming to mitigate data leakage concerns and establish a more reliable evaluation benchmark for assessing LLMs’ capabilities."
2512.05073_output/content.md,Comprehensive Verilog Design Problems (CVDP),"The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA, provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.","No, 本文是使用该数据集进行评测",Our work tests this by evaluating Small language models (SLMs) coupled with a curated agentic AI framework on NVIDIA’s Comprehensive verilog design problem (CVDP) benchmark.,,,从自然语言规范生成经过验证的寄存器传输级（RTL）硬件设计实现。,"Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites. Derived from production IP blocks, it represents realistic complexity.",硬件设计自动化能力，特别是Verilog代码生成的功能正确性。,"provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.",使用CocoTB测试套件评估功能正确性（pass rate）。,"Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",模块级设计，包含接口规范、功能需求和测试套件。,"Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",硬件设计，具体包括算术运算、控制逻辑、内存系统和其他设计。,"336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.",代表实际生产IP块的复杂性，具有挑战性（最先进模型单次通过率仅26.5%）。,"Derived from production IP blocks, it represents realistic complexity. State-of-the-art achieves only 26.5% pass rate (GPT-4o-mini, single-shot), highlighting substantial improvement opportunity.",Verilog（硬件描述语言）。,"The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA...",包含336个问题。,"provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.",源自生产IP块，代表实际复杂性。,"Derived from production IP blocks, it represents realistic complexity.",,,由NVIDIA开发。,"The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA...",,,,,代码生成（从规范生成Verilog RTL代码）。,transforms design intent from the CVDP dataset into verified register transfer level (RTL) implementations.,通过率（pass rate）。,"State-of-the-art achieves only 26.5% pass rate (GPT-4o-mini, single-shot)...",自然语言规范、模块接口、功能需求。,"Each includes natural language specification, module interface, functional requirements...",Verilog RTL代码。,transforms design intent from the CVDP dataset into verified register transfer level (RTL) implementations.,文本到代码（自然语言规范到Verilog代码）。,"Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",使用CocoTB测试套件进行验证。,"Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",专注于硬件设计（Verilog），问题源自实际生产IP块，包含完整的测试套件（CocoTB）用于功能验证。,"Derived from production IP blocks, it represents realistic complexity. Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites."
2512.05100_output/content.md,SAP software-documentation benchmark,Experiments on the SAP software-documentation benchmark demonstrate improvements across six metrics...,"No, 本文是使用该数据集进行评测","Our experimental results demonstrate significant improvements on the software documentation dataset (Buschbeck and Exel, 2020) across four translation directions...",,,结构化文档翻译，即将带有XML或HTML标记的软件文档从源语言翻译为目标语言，同时保持标记结构的完整性。,This work addresses the task of translating a structured document Ds in the source language into its counterpart Dt in the target language.,翻译质量与结构保真度,...making structural fidelity as important as content translation quality.,使用多种指标进行评估，包括XML-Match、XML-BLEU、Content-BLEU、StrucAUC、TreeSim和Node-chrF。,"...FORMATRL achieving average gains of 3.69 XML-Match, 2.16 XML-BLEU, 0.22 Content-BLEU, and 0.93 StrucAUC scores... We propose two rewards: TreeSim and Node-chrF.",文档级别,"Recent works on structured text translation remain limited to the sentence level, as they struggle to effectively handle the complex document-level XML or HTML structures.",软件文档本地化,Translating structured documents such as software manuals is essential for product localization.,,,文中未明确提及完整数据集的语言覆盖范围，仅提及了实验中的翻译方向（如英语到日语）。,"A structured document translation example (English→Japanese)... Our experimental results demonstrate significant improvements on the software documentation dataset (Buschbeck and Exel, 2020) across four translation directions...",,,SAP软件文档,Experiments on the SAP software-documentation benchmark...,2020,"...software documentation dataset (Buschbeck and Exel, 2020)...",,,,,,,文档翻译,...translating a structured document Ds in the source language into its counterpart Dt in the target language.,"XML-Match, XML-BLEU, Content-BLEU, StrucAUC, TreeSim, Node-chrF","...FORMATRL achieving average gains of 3.69 XML-Match, 2.16 XML-BLEU, 0.22 Content-BLEU, and 0.93 StrucAUC scores... We propose two rewards: TreeSim and Node-chrF.",带有标记的结构化文档（XML/HTML）,"A structured document D can be viewed as an XML tree D = (VD, ED)...",带有标记的结构化文档（XML/HTML）,...generating the target document Dt given the source document Ds...,结构化文档到结构化文档,This work addresses the task of translating a structured document Ds in the source language into its counterpart Dt in the target language.,,,专注于软件文档的本地化，要求同时保证翻译质量和XML/HTML标记的结构保真度。,"Translating structured documents such as software manuals is essential for product localization. As shown in Figure 1, they carry markup that defines layout and interactive elements, making structural fidelity as important as content translation quality."
2512.04673_output/content.md,CoNaLa (Code/Natural Language Challenge),"To evaluate an LLMs ability to understand and generate natural language descriptions of code snippets, we use the CoNaLa (Code/Natural Language Challenge) [23] dataset.","No, 本文是使用该数据集进行评测","In this paper, we present a comprehensive comparative analysis of five general-purpose and five code-specific LLMs across six diverse benchmarks... We further extend our evaluation to the CoNaLa dataset [22], focusing on the task of code explanation...",,,给定一个输入代码片段，生成该代码片段意图/解释。,"Given an input code snippet, the task is to generate intents/explanation that the code is achieving.",语义代码理解、意图建模和文本生成能力,"This focuses on semantic code understanding, intent modeling and text generation.","使用标准评估指标，包括基于词元的指标（BLEU, METEOR, ROUGE）和基于语义的指标（使用CodeBERT计算余弦相似度）。","We use the following standard metrics that capture various aspects such as lexical overlap and semantic similarity using the following standard evaluation metrics for the task [1, 5, 10]. (i) Token-based: These metrics collectively quantify surface-level similarity... (ii) Semantics-based: We use this measure to assess the semantic similarity between the model generated explanation (𝑚) and the ground truth explanation (𝑔)... We then take a cosine similarity between the embeddings...",,,,,,,Python,"The dataset consists of 1, 666 Python code and paired natural language annotations.","包含1,666个Python代码片段及其配对的自然语言注释。","The dataset consists of 1, 666 Python code and paired natural language annotations.",,,,,,,,,,,代码解释,"Given an input code snippet, the task is to generate intents/explanation that the code is achieving.","BLEU, METEOR, ROUGE-1, ROUGE-2, ROUGE-L, CodeBERTScore（基于CodeBERT的余弦相似度）",BLEU [19] evaluates n-gram precision... METEOR [3] extends this... ROUGE [15] metrics adopt a recall-oriented perspective: ROUGE-1... ROUGE-2... ROUGE-L... We refer to this metric as CodeBERT in the rest of the paper.,代码,Given an input code snippet...,自然语言,...the task is to generate intents/explanation...,代码到文本,"Given an input code snippet, the task is to generate intents/explanation that the code is achieving.",,,专注于代码到自然语言的解释任务，评估模型对代码语义的理解和意图建模能力。,"This focuses on semantic code understanding, intent modeling and text generation."
2512.04538_output/content.md,"CrossCodeEval, RepoEval",Extensive experiments on CrossCodeEval and RepoEval benchmarks demonstrate that CoCo consistently surpasses state-of-the-art baselines...,"No, 本文是使用该数据集进行评测",Extensive experiments on CrossCodeEval and RepoEval benchmarks demonstrate that CoCo consistently surpasses state-of-the-art baselines...,,,仓库级代码补全。从函数级别到仓库级别，利用大规模代码库中的上下文信息生成代码。,"As code completion task from function-level to repository-level, leveraging contextual information from large-scale codebases becomes a core challenge.",代码生成质量、上下文理解能力、跨文件依赖建模能力,"This limitation becomes particularly challenging in repository-level code completion, where accurate code generation requires a holistic understanding of repository-wide dependencies, shared utilities, and inter-module interactions.",Exact Match (EM),achieving up to 20.2% gains in EM.,多文件项目、仓库级上下文,"they fall short in real-world settings where code is organized in complex repositories with abundant cross-file dependencies, customized APIs, and project-specific conventions.",通用软件开发,Automatic code completion plays a fundamental role in modern software development,工程级、真实世界场景,"they fall short in real-world settings where code is organized in complex repositories with abundant cross-file dependencies, customized APIs, and project-specific conventions.",,,,,,,,,,,,,,,代码补全,Automatic code completion plays a fundamental role in modern software development,Exact Match (EM),achieving up to 20.2% gains in EM.,代码（包含其上下文）,"CoCo first applies static code analysis tools (such as AST parsers) to extract rich contextual information at the function, file, and repository levels.",代码,"Finally, CoCo synthesizes the multi-granularity contextual information and the retrieved code examples into a unified prompt, which is fed into the LLM for code completion.",代码到代码,"Finally, CoCo synthesizes the multi-granularity contextual information and the retrieved code examples into a unified prompt, which is fed into the LLM for code completion.",,,专注于仓库级代码补全，强调对跨文件依赖、项目特定约定和共享工具的理解。,"This enables the LLM to reason globally and locally about cross-file dependencies, shared utilities, and project-specific conventions, resulting in more accurate and contextually consistent repository-level code completion."
2512.04611_output/content.md,Magma,Experimental evaluation on the Magma benchmark demonstrates decisive superiority.,"No, 本文是使用该数据集进行评测",Experimental evaluation on the Magma benchmark demonstrates decisive superiority. ... Experiments on the Magma benchmark [33] show that our agentic approach significantly outperformed previous solutions.,,,"生成漏洞证明（Proof-of-Vulnerability, PoV）输入，以触发目标代码位置处的漏洞。该任务对于漏洞验证、复现、分类、补丁验证和回归测试至关重要。","Generating a proof-of-vulnerability (PoV) input that triggers a vulnerability at target code location(s) is a critical task in software security. Such inputs are essential for vulnerability verification, reproduction, triage, patch validation, and regression testing.",漏洞触发能力、生成效率,"PBFuzz triggered 57 vulnerabilities, outperforming all baselines. Critically, PBFuzz exclusively triggered 17 vulnerabilities compared to existing fuzzers. ... Median time-to-exposure: 339 seconds for PBFuzz versus 8680 seconds for AFL++ with CmpLog, representing a 25.6× efficiency gain",在给定时间预算内成功触发的漏洞数量（CVE数量）、中位暴露时间,"PBFuzz triggered 57 vulnerabilities... PBFuzz achieved this within a 30-minute budget per target, compared to 24-hour allocations for conventional approaches. Median time-to-exposure: 339 seconds for PBFuzz versus 8680 seconds for AFL++ with CmpLog",,,软件安全、漏洞发现,Proof-of-Vulnerability (PoV) input generation is a critical task in software security,包含简单和复杂的漏洞（如PHP和libtiff）,"while LLMs could generate valid PoV inputs for simple vulnerabilities (16 out of 129 CVEs), they struggled with more complex ones (e.g., PHP and libtiff).",文中未明确提及数据集包含的编程语言，但实验涉及C/C++（如libxml2）和PHP等。,"they struggled with more complex ones (e.g., PHP and libtiff). ... Consider the buffer overflow vulnerability in libxml2’s xmlSnprintfElementContent function",包含129个CVE（公共漏洞与暴露）,while LLMs could generate valid PoV inputs for simple vulnerabilities (16 out of 129 CVEs)... PBFuzz successfully triggered 59 out of the 129 CVEs,,,,,,,,,,,漏洞触发输入生成,Generating a proof-of-vulnerability (PoV) input that triggers a vulnerability at target code location(s),成功触发的CVE数量、中位暴露时间（秒）,PBFuzz triggered 57 vulnerabilities... Median time-to-exposure: 339 seconds for PBFuzz,目标漏洞代码位置（可能包含代码片段）,using code snippets extracted by static analysis,触发漏洞的测试输入（例如，特定格式的文件或数据流）,generate PoV inputs,代码/漏洞描述到测试输入,Generating a proof-of-vulnerability (PoV) input that triggers a vulnerability at target code location(s),被测试的程序（PUT）及其运行环境，通常包含检测漏洞的净化器（sanitizers）,sanitizers acting as implicit oracles. ... The generated code is executed in a sandboxed environment with no external libraries.,Magma是一个用于评估模糊测试（Fuzzing）和漏洞发现工具的基准，专注于真实世界的漏洞（CVE）。本文将其用作评估PBFuzz框架有效性的标准测试集。,Experimental evaluation on the Magma benchmark demonstrates decisive superiority. ... Experiments on the Magma benchmark [33] show that our agentic approach significantly outperformed previous solutions.
2512.04738_output/content.md,OverpassNL,The OverpassNL dataset [20] serves as a parallel supervision source for both directions of structured query modeling.,"No, 本文是使用该数据集进行评测",We evaluate OSMT on a public benchmark against strong baselines and observe consistent improvements in both query generation and interpretation.,,,在自然语言与结构化查询语言（OverpassQL）之间进行双向翻译。具体包括：1. Text-to-OverpassQL：将自然语言查询翻译成可执行的OverpassQL语句。2. OverpassQL-to-Text：将结构化的OverpassQL脚本翻译成连贯的自然语言解释。,"To facilitate seamless interaction between natural language and structured geospatial queries, we define two complementary tasks: the established Text-to-OverpassQL task and its inverse, OverpassQL-to-Text.",查询生成与解释的准确性、结构有效性、语义对齐、可解释性、参数效率,"Experimental results demonstrate that OSMT consistently surpasses strong baseline models in terms of accuracy, interpretability, and parameter efficiency, setting new standards for natural language interfaces within geospatial database systems.",使用多个指标进行综合评估，包括精确匹配（EM）、chrF、KVS、TreeS、OQS。,Comparison of OSMT with state-of-the-art open-source and closed-source models on the Text-to-OverpassQL task. Average performance is over five metrics.,依赖OpenStreetMap（OSM）数据库的标签（tag）模式和拓扑结构。查询生成过程需要理解OSM中空间实体（节点、路径、关系）的层次和关系依赖。,"In contrast, our approach explicitly captures the hierarchical and relational dependencies inherently encoded within OSM to enhance spatial reasoning capabilities.",地理空间数据库查询，特别是OpenStreetMap（OSM）数据检索。,"OSMT, an open-source tag-aware language model specifically designed to bridge natural language and Overpass Query Language (OverpassQL), a structured query language for accessing large-scale OpenStreetMap (OSM) data.",具有不同复杂度的地理查询意图，需要理解复杂的OSM标签模式和OverpassQL语法。,"It contains 8,352 natural language questions paired with real-world OverpassQL queries, spanning diverse geographic intents and varying levels of query complexity.",自然语言（输入/输出）与OverpassQL（一种用于查询OpenStreetMap的领域特定语言）。,"OSMT, an open-source tag-aware language model specifically designed to bridge natural language and Overpass Query Language (OverpassQL)...","包含8,352个自然语言问题与真实世界OverpassQL查询的配对。","It contains 8,352 natural language questions paired with real-world OverpassQL queries, spanning diverse geographic intents and varying levels of query complexity.",从三个互补来源构建：1. OSM Taginfo（标签三元组）。2. OSM Wiki（标签-描述对）。3. OverpassNL数据集（自然语言-OverpassQL配对）。,"To support cross-modal pre-training that bridges natural language, symbolic tag knowledge, and structured OverpassQL, we construct a pre-training corpus from three complementary sources.",,,"社区贡献（OSM Taginfo, OSM Wiki）与公开数据集（OverpassNL）的结合。","OSM Taginfo, a community-curated platform that aggregates global statistics on tag usage, frequency, and co-occurrence.",,,,,文本到代码（Text-to-OverpassQL）和代码到文本（OverpassQL-to-Text）的翻译任务。,"We define two complementary tasks: the established Text-to-OverpassQL task and its inverse, OverpassQL-to-Text.",精确匹配（EM）、chrF、KVS、TreeS、OQS。,Average performance is over five metrics.,自然语言文本或OverpassQL代码。,This task aims to translate a natural language query q into a syntactically correct and semantically meaningful OverpassQL statement Ovq.,OverpassQL代码或自然语言文本。,"This task performs the reverse mapping by taking a structured OverpassQL script Ovq, typically authored by technical users, and generating a coherent natural language explanation e.",文本到代码（Text-to-OverpassQL）和代码到文本（OverpassQL-to-Text）。,Illustration of the bidirectional translation between natural language and OverpassQL.,OpenStreetMap数据库环境，用于执行生成的OverpassQL查询以检索地理空间实体。,"Retrieving structured geospatial data from OSM typically relies on the Overpass Query Language (OverpassQL), a domain-specific language designed for fine-grained spatial data extraction.",专注于地理空间数据库（OpenStreetMap）的自然语言接口，涉及复杂的标签模式和拓扑结构。包含双向翻译任务（Text-to-OverpassQL 和 OverpassQL-to-Text）。,"OSMT, an open-source tag-aware language model specifically designed to bridge natural language and Overpass Query Language (OverpassQL), a structured query language for accessing large-scale OpenStreetMap (OSM) data."
2512.05962_output/content.md,Lean theorem-proving benchmark,"We evaluate 𝛼-DPG in Lean, a proof assistant that automatically verifies formal mathematical proofs.","No, 本文是使用该数据集进行评测","We evaluate 𝛼-DPG in Lean, a proof assistant that automatically verifies formal mathematical proofs.",,,在Lean定理证明助手中自动验证形式化数学证明。,"We evaluate 𝛼-DPG in Lean, a proof assistant that automatically verifies formal mathematical proofs.",定理证明的正确性与证明尝试的多样性,"In this setting, success requires not only correct candidates but also diversity across proof attempts, since harder theorems may only be solved by rare derivations.",使用pass@1和pass@256指标评估精度（precision）和覆盖率（coverage）。,"We find that 𝛼-DPG achieves state-of-the-art performance, producing models that lie on the Pareto frontier between precision (pass@1) and coverage (pass@256) and that surpass prior methods in coverage.",,,形式化数学定理证明,"We evaluate 𝛼-DPG in Lean, a proof assistant that automatically verifies formal mathematical proofs.",,,Lean（定理证明语言）,"We evaluate 𝛼-DPG in Lean, a proof assistant that automatically verifies formal mathematical proofs.",,,,,,,,,,,,,定理证明生成,"In this setting, success requires not only correct candidates but also diversity across proof attempts, since harder theorems may only be solved by rare derivations.","pass@1, pass@256","We find that 𝛼-DPG achieves state-of-the-art performance, producing models that lie on the Pareto frontier between precision (pass@1) and coverage (pass@256) and that surpass prior methods in coverage.",定理陈述（可能为自然语言或形式化语言）,Let 𝜋𝜃(·|𝑥) : Y →ℝbe an LLM with parameters 𝜃 defining a probability distribution over sequences 𝑦∈Y conditioned on a prompt 𝑥.,形式化证明（代码）,"In this setting, success requires not only correct candidates but also diversity across proof attempts, since harder theorems may only be solved by rare derivations.",文本到代码（定理陈述到形式化证明）,"We evaluate 𝛼-DPG in Lean, a proof assistant that automatically verifies formal mathematical proofs.",Lean证明助手验证环境,"We evaluate 𝛼-DPG in Lean, a proof assistant that automatically verifies formal mathematical proofs.",该基准使用定理证明助手（Lean）作为验证器，强调在保证正确性的同时，证明尝试的多样性对于解决更难定理至关重要。,"In this setting, success requires not only correct candidates but also diversity across proof attempts, since harder theorems may only be solved by rare derivations."
2512.05908_output/content.md,DNext,"Our evaluation is performed on DNext [7], a proprietary, industrial-scale microservice system for the telecommunications sector.","No, 本文是使用该数据集进行评测","Our evaluation is performed on DNext [7], a proprietary, industrial-scale microservice system for the telecommunications sector.",,,微服务架构中的多仓库Bug定位。具体任务是根据自然语言Bug报告，在多仓库代码库中定位到相关的代码文件。,"Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code, LLM context limitations, and the need to first identify the correct repository.",Bug定位的准确性和效率，以及多仓库路由能力。,"Our evaluation aims to answer two primary questions: (1) How effectively does our NL-to-NL approach perform against state-of-the-art baselines on a complex, multi-repository benchmark? (2) How critical is the hierarchical search component to the method’s accuracy and efficiency?",使用Pass@k和Recall@k指标进行评估。对于Bug定位，设置k=10；对于搜索空间路由，设置k=3。,"We evaluate performance using standard metrics: Pass@k and Recall@k. Pass@k measures the proportion of bug reports where at least one correct file is found in the top-𝑘 results. Recall@k measures the fraction of all correct files for a given bug that are successfully retrieved within the top-𝑘 results. Given that the maximum number of modified files for any bug in our dataset is 10 (average 7.2), we set 𝑘= 10 as a fair and comprehensive threshold... For the preliminary search space routing phase, we use a tighter 𝑘= 3.",多文件、多目录、多仓库的微服务架构项目级上下文。,"Bug localization in multi-repository microservice architectures is challenging... This makes them ineffective for large-scale software projects composed of many interacting microservices, as they lack a mechanism to first route a bug report to the correct repository (codebase) before localization can begin.",电信领域的工业级微服务软件系统。,"Our evaluation is performed on DNext [7], a proprietary, industrial-scale microservice system for the telecommunications sector.",工业级、真实世界、具有挑战性的Bug定位任务。,"Its scale (see Table 1) and use of real-world, often noisy, bug reports provide a challenging benchmark that standard academic datasets cannot replicate.",Java,"Table 1: Statistics of the DNext microservice dataset. Metric: Programming Language, Value: Java","包含46个仓库，7,077个代码文件，约110万行物理代码，87个Bug工单，平均每个工单涉及7.2个Bug文件。","Table 1: Statistics of the DNext microservice dataset. Metric: Number of Repositories, Value: 46; Total Number of Code Files, Value: 7,077; Total Physical Lines of Code, Value: ∼1.1M; Number of Bug Tickets, Value: 87; Average Buggy Files per Ticket, Value: 7.2",专有的工业级微服务系统，数据来源于真实的Bug工单和代码修改记录。,"Our evaluation is performed on DNext [7], a proprietary, industrial-scale microservice system for the telecommunications sector. The ground truth for each bug ticket is the set of files modified in the pull request that resolved the issue.",,,专有工业系统，非公开社区构建。,"Our evaluation is performed on DNext [7], a proprietary, industrial-scale microservice system...",,,,,代码检索/定位（从自然语言Bug报告定位到代码文件）,We propose reframing this as a natural language reasoning task by transforming codebases into hierarchical NL summaries and performing NL-to-NL search instead of cross-modal retrieval.,"Pass@k, Recall@k, MRR (Mean Reciprocal Rank)","We evaluate performance using standard metrics: Pass@k and Recall@k... For the preliminary search space routing phase, we use a tighter 𝑘= 3. ...Our hierarchical approach achieves the highest Pass@10 and MRR...",自然语言（Bug报告）,Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code...,代码文件列表（排名）,The LLM performs a detailed analysis of these filtered file summaries to produce the final ranked list of files most likely to be the source of the bug.,文本到代码（检索）,...reframing the problem. This shift from a cross-modal retrieval task to a unified NL-to-NL reasoning task...,,,专为多仓库微服务架构Bug定位设计的工业级基准，包含真实的、有噪声的Bug报告，规模远超标准学术数据集。,"Its scale (see Table 1) and use of real-world, often noisy, bug reports provide a challenging benchmark that standard academic datasets cannot replicate."
2512.08810_output/content.md,CALIBRI,"To foster future research on this important issue, we make our Dataset CALIBRI available as a contribution1.",Yes,"To foster future research on this important issue, we make our Dataset CALIBRI available as a contribution1.",https://huggingface.co/datasets/lavis-nlp/CALIBRI,1https://huggingface.co/datasets/lavis-nlp/CALIBRI,用于研究代码大语言模型（Code LLM）的校准和不确定性估计。数据集包含模型生成的代码、其置信度（token似然）以及正确性标签，旨在支持对模型置信度与代码实际正确性之间关系的研究。,CALIBRI includes correctness labels and token likelihoods for latest-generation LLMs on a fresh dataset with relatively low risk of data leakage.,模型置信度校准、不确定性估计,"As AI-based code generation becomes widespread, researchers are investigating the calibration of code LLMs — ensuring their confidence scores faithfully represent the true likelihood of code correctness.",使用校准度量，如预期校准误差（ECE）、Brier分数和Brier技能分数（BSS），来衡量模型置信度与代码实际正确性（通过单元测试评估）的匹配程度。,"Therefore, several measures are commonly used to quantify the calibration error of a model: The Expected Calibration Error (ECE)... Brier Score... Brier Skill Score (BSS)...",,,通用编程任务（函数合成）,We study four multicalibration approaches on three function synthesis benchmarks...,,,未明确指定CALIBRI数据集本身的语言，但论文实验使用了MultiPL-E（20种语言）和McEval（40种语言）等多语言基准。,"We conduct an evaluation using latest open-weight reasoning models... on three code generation datasets: the multilingual benchmarks MultiPL-E [5] and McEval [6], which offer 20 and 40 programming languages, respectively...",,,"基于三个现有的代码生成基准（MultiPL-E, McEval, LiveCodeBench）构建，包含在这些基准上最新代码LLM的生成结果、置信度和正确性标签。","We conduct an evaluation using latest open-weight reasoning models... on three code generation datasets: the multilingual benchmarks MultiPL-E [5] and McEval [6], which offer 20 and 40 programming languages, respectively, and Live-CodeBench [25]... CALIBRI includes correctness labels and token likelihoods for latest-generation LLMs on a fresh dataset...",2025,arXiv:2512.08810v1  [cs.SE]  9 Dec 2025,官方自建（由本文作者构建并发布）,"To foster future research on this important issue, we make our Dataset CALIBRI available as a contribution1.",相对较低的数据泄露风险,CALIBRI includes correctness labels and token likelihoods for latest-generation LLMs on a fresh dataset with relatively low risk of data leakage.,,,代码生成（函数合成）,We study four multicalibration approaches on three function synthesis benchmarks...,预期校准误差（ECE）、Brier分数、Brier技能分数（BSS）,"Therefore, several measures are commonly used to quantify the calibration error of a model: The Expected Calibration Error (ECE)... Brier Score... Brier Skill Score (BSS)...",自然语言任务描述和/或代码上下文,We assume the input prompt 𝑋 to consist of a task description and/or code context...,代码,...and the output 𝑍 = 𝑧1:𝑛 to be a sequence of code tokens.,文本到代码,"We assume the input prompt 𝑋 to consist of a task description and/or code context, and the output 𝑍 = 𝑧1:𝑛 to be a sequence of code tokens.",,,专门为代码LLM的校准研究设计，包含模型置信度（token似然）和正确性标签。支持多校准（multicalibration）研究，可基于代码复杂性、长度、提示长度或编程语言对问题进行分组。,"We fill this gap by using multicalibration [22], which groups inputs according to semantic facets and applies a group-specific calibration... We apply multi-calibration to code generation by grouping coding problems by complexity and/or programming language."
2512.08867_output/content.md,SimpleDevQA,"we introduce SimpleDevQA, a multilingual Dev Knowledge QA benchmark derived from real user dialogues.",Yes,"In this paper, we propose SimpleDevQA, a Dev Knowledge QA benchmark built from real developer dialogues, to address the aforementioned problems.",,,开发知识问答，旨在为软件开发过程中提出的知识寻求问题提供准确、相关的自然语言答案。,The Development Knowledge Question Answering (Dev Knowledge QA) task aims to address knowledge-seeking questions posed during the software development process by providing accurate and relevant natural language answers.,评估大语言模型在真实编程场景中对软件开发知识的理解能力。,to assess LLMs’ understanding capability of software development knowledge in real programming scenarios.,通过一个独立的评判大语言模型，输入模型的预测结果和参考答案来验证正确性。,we evaluate SimpleDevQA by prompting a separate judge LLM with the model’s prediction and the reference answer to verify correctness.,独立的知识问答对，不依赖特定代码上下文。,"This dataset contains 2,740 QA pairs... focuses on questions with unique, short, and verifiable answers",软件开发知识，涵盖底层系统、数据库、网络协议、算法原理等，而不仅仅是代码理解。,"practical software development demands a much broader understanding, encompassing knowledge of underlying systems, databases, network protocols, algorithmic principles, etc.",通过过滤掉被认为过于简单的问题来增加难度。,"To increase difficulty, four powerful LLMs filter out QA pairs deemed too easy;",英语、中文和俄语。,"This dataset contains 2,740 QA pairs in three languages (English, Chinese, and Russian)",包含2740个开发知识问答对。,"we finally build the SimpleDevQA benchmark, which contains 2,740 Dev Knowledge QA pairs",源自真实用户与ChatGPT的对话（WildChat数据集），并结合网络检索的参考文档。,"we collect 3,000 real user conversations from WildChat as a seed dataset... retrieve relevant reference documents from the web",2025,Publication date: December 2025.,官方自建，通过一个三阶段的数据构建流程将真实对话转化为基准。,we design and implement a three-phase data construction pipeline to convert real-world SE-related conversations into a Dev Knowledge QA benchmark.,源自2025年的真实用户对话，旨在反映真实的开发者需求和查询模式，污染风险相对较低。,built from real developer dialogues... to reflect genuine developer task demands and query patterns.,,,知识问答,Development Knowledge Question Answering (Dev Knowledge QA) task,通过评判LLM验证答案的正确性（准确率）。,prompting a separate judge LLM with the model’s prediction and the reference answer to verify correctness.,自然语言问题,question inquiring about development knowledge,自然语言答案,providing accurate and relevant natural language answers,文本到文本（自然语言问题到自然语言答案）,knowledge-seeking questions... providing accurate and relevant natural language answers,,,1. 源自真实用户对话，反映真实开发者需求。2. 专注于具有单一、明确、可验证答案的问题，便于评估。3. 涵盖广泛的软件开发知识，而不仅仅是代码理解。4. 每个问答对都附有网络检索的参考文档，可用于验证事实准确性。,"The benchmark focuses on questions with single, correct answers, making evaluation more accurate and simple... Additionally, each QA pair is also accompanied by multiple web-retrieved references, which can be used to verify the factual accuracy of the answer."
2512.09543_output/content.md,SWE-bench Verified Mini,"For our evaluation dataset, we use SWE-bench Verified Mini, a curated 50-task subset of SWE-bench Verified.","No, 本文是使用该数据集进行评测","We conduct a controlled evaluation of four leading agentic frameworks (SWE-Agent, OpenHands, Mini SWE Agent, AutoCodeRover) using two SLMs (Gemma-3 4B, Qwen-3 1.7B) on the SWE-bench Verified Mini benchmark.",,,解决现实世界中的软件工程问题，具体是GitHub仓库中的真实issue。,"Systems such as SWE-Agent, OpenHands, and AutoCodeRover have demonstrated strong performance on benchmarks like SWE-bench, which evaluates real-world GitHub issues as problem statements.",软件工程问题解决的有效性、效率和资源利用率,"We then cataloged all measurable outputs from a run, including status, time, energy, tokens, memory, and cost, ensuring each candidate metric was compatible with the SWE-bench benchmark and common evaluation practices.",使用官方SWE-bench评估脚本，判断智能体生成的补丁是否能正确解决问题并通过所有相关测试。,A run passes only if the agent-generated patch correctly resolves the issue and all associated tests pass.,多文件项目上下文，涉及完整的代码仓库。,"It employs ReAct-style prompting [24], where each iteration consists of thought, action, and observation until the issue is resolved or a stopping condition is reached. (结合SWE-bench评估真实GitHub仓库问题的背景推断)",软件工程，具体为代码修复和问题解决。,"Autonomous software engineering agents, powered by Large Language Models (LLMs), have emerged as a transformative paradigm in software development [9, 21], demonstrating impressive capabilities in resolving real-world code issues on benchmarks like SWE-bench.",真实世界难度，覆盖不同难度级别的问题。,"This benchmark preserves the distribution of difficulty, repository diversity, and test pass rates observed in the full 500-task dataset while reducing storage requirements from approximately 130 GB to 5 GB [10].",文中未明确提及，但SWE-bench通常包含多种编程语言。,,包含50个任务的精选子集，是完整500任务数据集的子集。,"For our evaluation dataset, we use SWE-bench Verified Mini, a curated 50-task subset of SWE-bench Verified.",来自真实GitHub仓库的问题。,"Systems such as SWE-Agent, OpenHands, and AutoCodeRover have demonstrated strong performance on benchmarks like SWE-bench, which evaluates real-world GitHub issues as problem statements.",,,,,,,,,代码修复与问题解决,demonstrating impressive capabilities in resolving real-world code issues on benchmarks like SWE-bench.,解决状态（通过/失败）、失败模式、能量使用、挂钟持续时间、令牌使用、LLM调用计数、内存使用、LLM成本,"We then cataloged all measurable outputs from a run, including status, time, energy, tokens, memory, and cost, ensuring each candidate metric was compatible with the SWE-bench benchmark and common evaluation practices.",自然语言（GitHub issue描述）与代码仓库上下文,which evaluates real-world GitHub issues as problem statements.,代码补丁,A run passes only if the agent-generated patch correctly resolves the issue and all associated tests pass.,文本（问题描述）到代码（补丁）,which evaluates real-world GitHub issues as problem statements. ... the agent-generated patch,沙盒化Docker执行环境,It supports sandboxed Docker execution environments and accommodates diverse architectures from simple ReAct loops to complex planning approaches.,专注于评估自主软件工程智能体解决真实世界GitHub问题的能力，包含完整的仓库上下文和测试。,demonstrating impressive capabilities in resolving real-world code issues on benchmarks like SWE-bench.
2512.09108_output/content.md,AtCoder Heuristic Contest,"(1) the ALE Agent [16] on the AtCoder Heuristic Contest, tackling competitive programming problems;","No, 本文是使用该数据集进行评测",We evaluate Artemis on four representative agent systems: the ALE Agent for competitive programming on AtCoder Heuristic Contest...,,,解决竞争性编程问题,...tackling competitive programming problems;,代码生成正确性（接受率）,...achieving a 13.6% improvement in acceptance rate;,接受率,...achieving a 13.6% improvement in acceptance rate;,,,算法，竞争性编程,...tackling competitive programming problems;,,,,,,,,,,,,,,,,,代码生成,...tackling competitive programming problems;,接受率,...achieving a 13.6% improvement in acceptance rate;,,,代码,...tackling competitive programming problems;,文本到代码,...tackling competitive programming problems;,,,,
2512.10713_output/content.md,PACIFIC (Precise Automatically Checked Instruction Following In Code),"We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs...",Yes,"We present PACIFIC, a novel framework designed to automatically generate benchmarks... We introduce PACIFIC, a novel framework for automatically generating benchmarks...",,,评估大型语言模型（LLM）的顺序指令遵循和代码干运行（模拟代码执行）能力。模型需要根据给定的初始输入和一系列指令，模拟执行过程并产生最终输出。,...a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs... The model under evaluation is tasked with executing the instructions on the given input and producing the final output in the specified format.,指令遵循的精确性、代码干运行（推理）能力、处理多步任务的能力,...assess sequential instruction-following and code dry-running capabilities in LLMs... The framework emphasizes the ability of LLMs to simulate code execution (i.e. dry-running) without relying on external tools or agentic mechanisms.,通过将模型输出与参考代码执行生成的预期结果进行简单比较，实现自动化和确定性的评估。,"The expected results are computed by executing reference implementations of the described tasks, enabling automatic and deterministic evaluation via simple output comparison.",多步顺序指令。每个样本包含一个初始输入和一个指令序列，模型需要按顺序处理这些指令。,Each sample includes: (1) An initial input — either a number or a string. (2) A sequence of instructions — operations to be applied to the input.,通用编程逻辑与算法模拟。指令涉及数学运算（如找质数、完美平方）、字符串操作（如字符移位）和基本逻辑转换。,"Examples: (1) next_perfect_square: take the previous number and output the first perfect square that comes after it... (2) shift_back: subtract 1 from each character of the previous output; a becomes z... Instruction 1: Given the previous number, output the smallest number bigger than the previous answer that is a prime number.",难度可控，范围从易到极具挑战性。难度维度包括指令数量和预期输出长度。,...while allowing control over benchmark difficulty... The difficulty dimensions are: (1) LLM input: the number of instructions in each sample (prompt). (2) LLM expected output: the expected length of each sample’s output.,支持多种编程语言，包括Python、Java和C++。指令以自然语言或代码片段形式提供。,"Currently, instructions are implemented in multiple programming languages, including Python, Java, and C++.",规模可扩展，由生成参数（如每个编程语言的样本数）控制。框架支持大规模和多样化的基准生成。,"To produce meaningful results, the framework must support large-scale and diverse benchmark generation.",通过框架自动生成。使用一组预定义的指令作为构建块，通过随机选择和组合来创建独特的样本。,Our framework is designed to automatically generate benchmarks... Instructions serve as the fundamental building blocks of PACIFIC... instruction selection is random and fully reproducible via the specified random seed.,2025-12-11 (arXiv版本),arXiv:2512.10713v1  [cs.SE]  11 Dec 2025,官方自建（由IBM Research团队提出和构建的框架）,"IBM Research Haifa, Israel {Itay.Dreyfuss,Antonio.Abu.Nassar,Samuel.Ackerman,Axel.Bendavid}@ibm.com {Rami.Katan,ornar,marcel}@il.ibm.com",抗污染设计。框架可以轻松生成新颖的基准变体，以减轻训练数据污染的风险。,"Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations... Each benchmark generated by PACIFIC can be unique, mitigating the risk of training data contamination.",,,代码生成与推理。模型需要生成执行一系列指令后的最终输出（数字或字符串）。,The model under evaluation is tasked with executing the instructions on the given input and producing the final output in the specified format.,Prompt-Level Accuracy（提示级准确率）。通过比较每个指令的中间输出或最终输出来计算。,Correctness Check: The framework validates each instruction by comparing the output against the expected result... we compute two metrics: • Prompt-Level Acc,自然语言指令或代码片段，结合初始输入（数字或字符串）。,• Instruction Type: Either Code (instructions as code snippets) or NL (instructions expressed in natural language). Each sample includes: (1) An initial input — either a number or a string. (2) A sequence of instructions...,代码执行结果，即一个数字或字符串。,Each instruction is a function that accepts either a number or a string and returns a value of one of those types... producing the final output.,指令序列到执行结果。输入是初始值加一系列操作指令，输出是模拟执行这些指令后的最终值。,The model under evaluation is tasked with executing the instructions on the given input and producing the final output...,无需外部执行环境。评估基于输出比较，而非实际代码执行。但参考结果由执行参考代码生成。,"The expected results are computed by executing reference implementations of the described tasks, enabling automatic and deterministic evaluation via simple output comparison.",1. 是一个基准生成框架，而非单一静态数据集。2. 强调“干运行”（dry-running），即在不实际执行代码的情况下模拟代码行为。3. 提供明确的难度控制参数（指令数、输出长度）。4. 采用完全基于规则的确定性评估，避免使用LLM作为评判者。5. 受ARC基准方法启发，但应用于代码领域。,"PACIFIC, a novel framework designed to automatically generate benchmarks... evaluates both sequential instruction-following and code dry-running capabilities... The framework emphasizes the ability of LLMs to simulate code execution (i.e. dry-running) without relying on external tools... The framework must provide explicit mechanisms for controlling the difficulty of the generated benchmarks... The evaluation process must rely on deterministic and transparent metrics rather than LLM-based evaluators... This is inspired by the ARC benchmark approach [6], but differs in the implementation and domain."
2512.10398_output/content.md,SWE-Bench-Pro,"On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents.","No, 本文是使用该数据集进行评测","We report results on SWE-Bench-Verified (Jimenez et al., 2023) and SWE-Bench-Pro (Deng et al., 2025) using the public evaluation pipelines...",,,解决真实世界开源仓库中的问题（issue resolution）,"LLMs have demonstrated strong software engineering ability to tackle real-world issue resolution in open-source repositories (Jimenez et al., 2023; Yang et al., 2024; Xia et al., 2025; Zeng et al., 2025).",工业级软件工程能力，包括长上下文推理、长期记忆、复杂工具链协调,"Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time.",Resolve@1,"On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%...",大规模代码库，包含深度相互依赖的组件,"industrial-scale codebases, which are usually orders of magnitude larger than typical benchmark projects, contain deeply interdependent components, and evolve continuously (Deng et al., 2025).",软件工程，特别是开源项目维护,real-world issue resolution in open-source repositories,工业级，远超典型基准项目的规模和复杂性,"industrial-scale codebases, which are usually orders of magnitude larger than typical benchmark projects, contain deeply interdependent components, and evolve continuously (Deng et al., 2025).",,,,,开源仓库中的真实问题,real-world issue resolution in open-source repositories,2025,"Date: December 12, 2025",,,,,,,问题解决（Issue Resolution）,real-world issue resolution in open-source repositories,Resolve@1,"On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%...",自然语言（问题描述）和代码库上下文,real-world issue resolution in open-source repositories,代码修改（如补丁）,tackle real-world issue resolution,文本（问题）和代码（上下文）到代码（解决方案）,tackle real-world issue resolution in open-source repositories,真实或模拟的软件仓库环境，包含测试套件,using the public evaluation pipelines,专注于工业级规模、持续演化的代码库中的软件工程任务，强调长上下文推理和跨会话记忆。,"Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time."
2512.10485_output/content.md,VentiVul,"We constructed a new, small-scale evaluation dataset, which we name VentiVul.",Yes,"We manually construct a small but high-quality out-of-distribution vulnerability dataset and design a deployment-oriented evaluation framework introducing two novel modes, Whole-File and Function-Pair.",https://github.com/Chaomeng-Lu/A-Practical-Evaluation-of-Deep-Learning-Models-and-LLMs-for-Vulnerability-Detection.git,"All of our datasets, code, and results are available at the following link1. 1https://github.com/Chaomeng-Lu/A-Practical-Evaluation-of-Deep-Learning-Models-and-LLMs-for-Vulnerability-Detection.git",漏洞检测。旨在评估模型在真实、新颖的漏洞场景下的检测能力，特别是针对时间上分布外（out-of-distribution）的漏洞。,"To assess realistic applicability, we deploy these models ... on a curated dataset, VentiVul, comprising 20 recently (May 2025) fixed vulnerabilities from the Linux kernel. ... We constructed a new, small-scale evaluation dataset, which we name VentiVul.",漏洞检测的泛化能力、鲁棒性、以及在实际部署场景下的有效性。,"Our experiments reveal that current models struggle to distinguish vulnerable from non-vulnerable code in representation space and generalize poorly across datasets with differing distributions. ... These results expose a persistent gap between academic benchmarks and real-world deployment, emphasizing the value of our deployment-oriented evaluation framework...",部署导向的评估框架，包含两种新颖模式：Whole-File（整个文件）和 Function-Pair（函数对）。,"We designed a deployment-oriented evaluation framework for assessing vulnerability detection models... finally, we examine deployment behavior using our newly proposed Whole-File and Function-Pair evaluation methods...",函数级和文件级。数据集包含函数对（漏洞版本与修复版本），以及来自同一文件但与修复无关的非漏洞函数，以提供额外上下文。,"For each CVE, we extracted the vulnerable and patched versions of affected functions directly from the corresponding Git commits. These were carefully aligned to form function pairs... To provide additional context and challenge for model evaluation, we also included non-vulnerable functions from the same files that were unrelated to the fix.",软件安全，具体为C语言（Linux内核）中的软件漏洞检测。,"Specifically, we retrieved all Linux-related CVE reports disclosed in May 2025 from the official CVE database. ... These pairs span 21 distinct .c source files...",高难度，真实世界、新颖、分布外的漏洞，旨在挑战现有模型。,"When evaluated on VentiVul, our newly constructed time-wise out-of-distribution dataset, performance drops sharply, with most models failing to detect vulnerabilities reliably. ... providing a realistic and challenging testbed for vulnerability detection models.",C语言,"Specifically, we retrieved all Linux-related CVE reports disclosed in May 2025 from the official CVE database. ... These pairs span 21 distinct .c source files...",小规模，包含来自20个Linux CVE的25个函数对（每个对包含漏洞版本和修复版本），以及来自相同文件的835个无关的非漏洞函数。,"In total, we collected 25 function pairs (each consisting of a vulnerable and a patched version) from the 20 selected Linux CVEs. ... and include 835 unrelated functions from the same files...",手动从真实世界数据构建。数据来源于2025年5月披露的Linux内核CVE报告及其对应的Git修复提交。,"To ensure high relevance and minimize external interference, the collection process was conducted fully manually and involved careful human inspection at each step. Specifically, we retrieved all Linux-related CVE reports disclosed in May 2025 from the official CVE database. ... For each CVE, we extracted the vulnerable and patched versions of affected functions directly from the corresponding Git commits.",2025年12月（论文版本日期），数据集基于2025年5月披露的CVE构建。,arXiv:2512.10485v1  [cs.CR]  11 Dec 2025 ... comprising 20 recently (May 2025) fixed vulnerabilities from the Linux kernel.,官方自建（本文作者手动构建）,We manually construct a small but high-quality out-of-distribution vulnerability dataset... The collection process was conducted fully manually and involved careful human inspection at each step.,抗污染设计，专门设计为时间上的分布外（out-of-distribution）数据集，以最小化训练数据污染风险。,"When evaluated on VentiVul, our newly constructed time-wise out-of-distribution dataset... To further evaluate the performance of state-of-the-art models... on a curated dataset, VentiVul, comprising 20 recently (May 2025) fixed vulnerabilities...",,,漏洞检测（二元分类：漏洞/非漏洞）,"These were carefully aligned to form function pairs, labeled as vulnerable (1, before-fix) and non-vulnerable (0, after-fix).",文中未明确提及用于VentiVul的具体评估指标（如准确率、F1分数等），但提到了性能下降。,"When evaluated on VentiVul... performance drops sharply, with most models failing to detect vulnerabilities reliably.",源代码（C语言函数或文件）,"For each CVE, we extracted the vulnerable and patched versions of affected functions directly from the corresponding Git commits.",二元分类标签（漏洞/非漏洞）,"labeled as vulnerable (1, before-fix) and non-vulnerable (0, after-fix).",代码到标签（二元分类）,"labeled as vulnerable (1, before-fix) and non-vulnerable (0, after-fix).",,,1. 时间上的分布外（OOD）评估：专注于近期（2025年5月）修复的漏洞，与现有训练数据时间分离。2. 高真实性与高质量：完全手动从Linux内核CVE和Git提交中构建，经过人工检查。3. 部署导向的评估模式：引入了Whole-File和Function-Pair两种新颖的评估模式，模拟真实部署场景。4. 包含无关上下文：除了漏洞/修复函数对外，还包含了来自同一文件但与修复无关的非漏洞函数，增加评估挑战性。,"We constructed a new, small-scale evaluation dataset, which we name VentiVul... comprising 20 recently (May 2025) fixed vulnerabilities from the Linux kernel. ... The collection process was conducted fully manually and involved careful human inspection at each step. ... We designed a deployment-oriented evaluation framework... using our newly proposed Whole-File and Function-Pair evaluation methods... To provide additional context and challenge for model evaluation, we also included non-vulnerable functions from the same files that were unrelated to the fix."
2512.10393_output/content.md,BinSeek benchmark (未明确命名，但文中称其为'first domain benchmark for binary code retrieval'),"To this end, we built an LLM-based data synthesis pipeline to automate training construction, also deriving a domain benchmark for future research.",Yes,"In this paper, we introduce, BinSeek, the first two-stage cross-modal retrieval framework for stripped binary code analysis. ... To this end, we built an LLM-based data synthesis pipeline to automate training construction, also deriving a domain benchmark for future research.",https://github.com/XingTuLab/BinSeek,We make our model and evaluation benchmark publicly available at: https://github.com/XingTuLab/BinSeek.,基于自然语言查询，从大规模二进制代码库中检索相关的二进制函数。具体任务是跨模态检索，将自然语言查询与去符号化的二进制反编译伪代码进行匹配。,"Retrieving stripped binary code based on natural language (NL) queries, however, presents unique and formidable challenges compared to source code retrieval. ... we also deliver the first domain benchmark for the binary code retrieval task",跨模态检索能力、二进制代码语义理解、检索准确性,"Our evaluation results show that BinSeek achieved the state-of-the-art performance, surpassing the the same scale models by 31.42% in Rec@3 and 27.17% in MRR@3",使用Rec@3和MRR@3指标评估检索性能。,"Our evaluation results show that BinSeek achieved the state-of-the-art performance, surpassing the the same scale models by 31.42% in Rec@3 and 27.17% in MRR@3",单函数级别（BinSeek-Embedding）和带调用上下文的函数（BinSeek-Reranker）,"BinSeek-Embedding is used to retrieve candidates from a codebase, and BinSeek-Reranker further refine candidates with calling context information.",软件安全、二进制代码分析、逆向工程,"Binary code analysis serves as a cornerstone of software security, supporting critical tasks such as vulnerability detection, malware analysis, program audit, and so on.",高难度，涉及去符号化的二进制代码，缺乏高级语义信息。,"Retrieving stripped binary code based on natural language (NL) queries, however, presents unique and formidable challenges compared to source code retrieval. ... stripped binaries lack explicit semantic indicators such as variable names, function names, and type information.",C/C++（源语言），反编译后的伪代码（目标模态）,"Binary code is a low-level representation of a program, which is generally compiled from the source code written in PLs like C/C++","总共构建了106,711,414个原始数据对。","Ultimately, we constructed 106,711,414 raw data pairs in total.",通过自动化数据合成管道生成。从GitHub收集C/C++开源项目，编译为去符号化二进制文件，反编译为伪代码，并使用LLM为源代码函数生成语义描述。,"As shown in Figure 3, we first collect a large number of open-source projects written in C/C++ from GitHub. For each project, we compile the source code into stripped binary files using different configurations... Finally, we employ an advanced LLM (i.e., DeepSeek (DeepSeek-AI et al., 2025)) to generate semantic descriptions for each source code function.",2025年12月11日（论文版本日期）,arXiv:2512.10393v1  [cs.SE]  11 Dec 2025,官方自建，通过LLM驱动的自动化数据合成管道构建。,"We propose an LLM-driven data synthesis pipeline to automate the construction of high-quality training data, deriving the first domain benchmark for binary code retrieval.",未明确提及。,,未明确提及。,,代码检索（跨模态检索）,Retrieving stripped binary code based on natural language (NL) queries,"Rec@3, MRR@3","Our evaluation results show that BinSeek achieved the state-of-the-art performance, surpassing the the same scale models by 31.42% in Rec@3 and 27.17% in MRR@3",自然语言查询,Retrieving stripped binary code based on natural language (NL) queries,二进制反编译伪代码（函数）,Training cross-modal retrieval models requires a large amount of data pairs that consist of pseudocode functions and corresponding semantic descriptions in NL.,自然语言到代码（二进制伪代码）,Retrieving stripped binary code based on natural language (NL) queries,未明确提及，任务为检索而非执行。,,这是第一个专门为去符号化二进制代码检索任务构建的领域基准。它针对二进制代码缺乏符号信息（如函数名、变量名）的独特挑战而设计，填补了源代码检索与二进制代码检索之间的空白。数据通过自动化LLM驱动的管道合成，连接了源代码、二进制代码和自然语言描述。,"we also deliver the first domain benchmark for the binary code retrieval task, which is expected to facilitate future research in this domain. ... Unlike high-level programming languages (PL), stripped binaries lack explicit semantic indicators such as variable names, function names, and type information. ... We propose an LLM-driven data synthesis pipeline to automate the construction of high-quality training data"
2512.11482_output/content.md,data extraction attack benchmark 和 functional correctness benchmark,"• Two benchmarks for privacy and utility evaluation: We release two new datasets, i.e., data extraction attack benchmark and functional correctness benchmark, that can be used in future studies to evaluate the privacy and utility of CodeLLMs.",Yes,"• Two benchmarks for privacy and utility evaluation: We release two new datasets, i.e., data extraction attack benchmark and functional correctness benchmark, that can be used in future studies to evaluate the privacy and utility of CodeLLMs.",Replication Package (RP) [9],These datasets are tailored to the fine-tuning dataset for utility evaluation and available in the Replication Package (RP) [9].,评估代码大语言模型的隐私风险和功能正确性。具体包括：1. 数据提取攻击基准：用于评估模型对训练数据（包括敏感信息）的记忆和泄露风险。2. 功能正确性基准：用于评估模型在应用差分隐私等隐私保护技术后，生成代码的功能正确性是否得以保持。,"• Two benchmarks for privacy and utility evaluation: We release two new datasets, i.e., data extraction attack benchmark and functional correctness benchmark, that can be used in future studies to evaluate the privacy and utility of CodeLLMs.",隐私保护（记忆风险）与模型效用（功能正确性）的权衡评估,"• Two benchmarks for privacy and utility evaluation: We release two new datasets, i.e., data extraction attack benchmark and functional correctness benchmark, that can be used in future studies to evaluate the privacy and utility of CodeLLMs.",文中未明确描述针对这两个新基准的具体评估方法。,,,,通用软件工程任务，涉及代码生成、文档、测试用例等。,"Large language models specialized for code (Large language models specialized for code (CodeLLMs)) have demonstrated remarkable capabilities in generating code snippets, documentation, and test cases.",,,文中未明确指定基准使用的编程语言，但研究背景是通用代码大语言模型。,,,,为评估目的而构建的定制数据集，与微调数据集相关联。,These datasets are tailored to the fine-tuning dataset for utility evaluation...,2025,"Melih Catal, Pooja Rani, and Harald Gall. 2025. Towards Privacy-Preserving Code Generation: Differentially Private Code Language Models. 1, 1 (December 2025), 21 pages.",官方自建（本文作者发布）,We release two new datasets...,为评估隐私保护技术（如差分隐私）而专门构建，旨在测量记忆风险，因此可能包含设计用于触发记忆的样本。,"• Two benchmarks for privacy and utility evaluation: We release two new datasets, i.e., data extraction attack benchmark and functional correctness benchmark, that can be used in future studies to evaluate the privacy and utility of CodeLLMs.",,,代码生成（从自然语言提示生成代码片段、完整函数或程序）,"LLMs specialized for code (Large language models specialized for code (CodeLLMs)), trained on large code datasets, can generate code snippets, complete functions, or even write entire programs based on natural language prompts",文中提及使用CodeBLEU分数和功能正确性（functional correctness）作为评估指标。,CodeBLEU Score: 1.00 ... Effectiveness of DP on Mitigating Memorization and Preserving Model Performance: ... as measured by functional correctness on standard benchmarks.,自然语言提示（用于代码生成任务）；可能也包括代码片段（用于数据提取攻击）。,"...can generate code snippets, complete functions, or even write entire programs based on natural language prompts",代码,"...can generate code snippets, complete functions, or even write entire programs...",文本到代码（代码生成任务）；代码到代码/文本（数据提取攻击评估）。,"...can generate code snippets, complete functions, or even write entire programs based on natural language prompts",,,这是首个专门为评估代码大语言模型的隐私（记忆风险）与效用（功能正确性）权衡而设计的基准套件。它包含两个互补的数据集：一个用于量化数据提取攻击下的记忆泄露，另一个用于确保隐私保护技术（如差分隐私）不会损害代码生成质量。,"• Two benchmarks for privacy and utility evaluation: We release two new datasets, i.e., data extraction attack benchmark and functional correctness benchmark, that can be used in future studies to evaluate the privacy and utility of CodeLLMs. These datasets are tailored to the fine-tuning dataset for utility evaluation..."
2512.11589_output/content.md,AIDev,"To conduct our study, we use the AIDev dataset [13], a large-scale corpus of almost a million pull requests (PRs) from over 116,000 real GitHub repositories, authored by five distinct LLM-based coding agents (Claude Code, Cursor, Devin, Copilot, OpenAI Codex).","No, 本文是使用该数据集进行评测","To conduct our study, we use the AIDev dataset [13]...",https://github.com/itsluketwist/agent-library-usage/,We release our dataset processing and analysis code to support reproducibility and future research in this area: https://github.com/itsluketwist/agent-library-usage/,该数据集旨在提供一个大规模的真实世界AI编码代理（Agent）生成的代码变更（Pull Requests）集合，用于研究AI代理在软件开发中的行为，特别是其使用外部库的模式。,"To conduct our study, we use the AIDev dataset [13], a large-scale corpus of almost a million pull requests (PRs) from over 116,000 real GitHub repositories, authored by five distinct LLM-based coding agents...",AI编码代理的库使用行为，包括库导入频率、新依赖引入、版本控制实践和库选择多样性。,"Specifically, we analyse file diffs (both code and dependency manifests) from 26,760 PRs across four languages (TypeScript, Python, Go, and C#), to answer three research questions (RQs): RQ1 Library Usage... RQ2 New Dependencies... RQ3 Choosing Libraries...",通过分析Pull Request的文件差异（diffs），使用特定于语言的解析器提取导入语句和依赖清单，并进行统计分析（如频率统计、版本约束分析）。,We then implement manifest-specific regex parsers to automatically identify new dependencies—including any explicit version specifiers... We then perform manual analysis on the top ten libraries for each language and RQ to determine the areas that agents are most likely to use external libraries.,多文件项目级别，基于真实的GitHub仓库和Pull Requests，包含完整的代码变更上下文。,"Specifically, we use the AIDev-Pop subset that is curated to repositories with over 100 GitHub stars and includes the full commit details; AIDev-Pop contains 33,596 PRs from 2,807 repositories.",通用软件开发，涵盖多种编程语言和项目类型。,"We restrict our study to the four most common languages in the dataset... The chosen languages are TypeScript/JavaScript... Python, Go and C#.",工程级，涉及真实世界软件项目的代码修改和依赖管理。,"These agentic systems—now widely available in production tools [19]—can complete end-to-end workflows that previously required a human developer. Crucially, these workflows include raising pull requests (PRs), allowing agents to propose substantial code changes...","TypeScript/JavaScript, Python, Go, C#","We restrict our study to the four most common languages in the dataset... The chosen languages are TypeScript/JavaScript (combined due to nearly identical syntax and library ecosystems, and will be referred to as TypeScript throughout the paper), Python, Go and C#.","包含来自2,807个仓库的33,596个Pull Requests（AIDev-Pop子集），本研究分析了其中26,760个PRs。","AIDev-Pop contains 33,596 PRs from 2,807 repositories... We analyse a total of 26,760 agent-authored pull requests across 1,832 repositories and the four most common languages in the AIDev dataset.","来自真实GitHub仓库的公开Pull Requests，由五种不同的LLM编码代理（Claude Code, Cursor, Devin, Copilot, OpenAI Codex）生成。","To conduct our study, we use the AIDev dataset [13], a large-scale corpus of almost a million pull requests (PRs) from over 116,000 real GitHub repositories, authored by five distinct LLM-based coding agents (Claude Code, Cursor, Devin, Copilot, OpenAI Codex).",2025年11月,"Specifically, we use the November 2025 snapshot (commit eee0408 [8]).",社区贡献（由研究人员从GitHub收集构建）,"To conduct our study, we use the AIDev dataset [13], a large-scale corpus of almost a million pull requests (PRs) from over 116,000 real GitHub repositories...",,,数据源自各仓库的开源许可证（通常为MIT、Apache-2.0或GPL变体）。,"AIDev contains no personal or sensitive information, and all data is publicly accessible under the open-source license of its respective repository (typically MIT, Apache-2.0, or GPL variants).",代码生成与修改（在Pull Request级别）,"These agentic systems... can complete end-to-end workflows that previously required a human developer. Crucially, these workflows include raising pull requests (PRs), allowing agents to propose substantial code changes...",库导入频率、新依赖引入频率、版本约束指定比例、库选择多样性（独特库数量）。,"RQ1 Library Usage How often do agents import libraries when generating code... RQ2 New Dependencies How often do agents introduce new dependencies into a repository, and do they specify versions? RQ3 Choosing Libraries Which specific libraries do agents choose...",代码（Pull Request的代码变更差异）,"Specifically, we analyse file diffs (both code and dependency manifests) from 26,760 PRs...",分析结果（统计数据和发现）,Our findings are concise but instructive. (1) Agents often import libraries (29.5% of PRs) but rarely add entirely new dependencies (1.3% of PRs). (2) Agentic workflows encourage far better version hygiene than traditional LLM prompting...,代码到分析,"Specifically, we analyse file diffs (both code and dependency manifests) from 26,760 PRs...",,,专注于AI编码代理（而非普通LLM）在真实软件开发环境（Pull Requests）中的库使用行为研究。数据集包含多种编程语言和真实的项目上下文。,"To begin to fill this gap, we present the first focused empirical study of how AI coding agents use libraries in practice, by mining real agent-authored PRs from the AIDev dataset [13]."
2512.13607_output/content.md,LiveCodeBench,"Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro","No, 本文是使用该数据集进行评测","Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro",,,文中未描述LiveCodeBench的具体任务，仅提及作为评测基准。,"Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2512.13102_output/content.md,GSM8K,"Math: We evaluate on GSM8K (Cobbe et al., 2021).","No, 本文是使用该数据集进行评测","We study this on math (GSM8K, (Cobbe et al., 2021)) and coding (HumanEval/OPC, (Chen et al., 2021; Huang et al., 2025)) tasks using small-scale LLMs (7B–8B).",,,数学问题求解，特别是小学数学应用题,"Across math and coding benchmarks, where baseline student models begin with near-zero performance... (结合上下文，GSM8K是数学基准)",数学推理能力,"However, for reasoning-heavy domains, such as math and coding, learning requires more than recalling and using facts.",Pass@k,"After each teacher response, S is evaluated with Pass@k.",,,数学（小学数学）,"Math: We evaluate on GSM8K (Cobbe et al., 2021).",,,,,,,,,2021,"Math: We evaluate on GSM8K (Cobbe et al., 2021).",,,,,,,问题求解,"Given a complex question, S seeks to answer the question by interacting with T.",Pass@k,"After each teacher turn, we evaluate the student using the full chat history so far: we sample k direct answers from S and compute Pass@k.",自然语言（数学问题描述）,"Given a complex question, S seeks to answer the question by interacting with T.",自然语言（答案）,"Following each teacher turn, S is prompted to output only the final answer.",文本到文本（数学问题到答案）,"Given a complex question, S seeks to answer the question... S is prompted to output only the final answer.",,,本文未描述GSM8K数据集的独特之处，仅将其用作评测基准。本文的独特之处在于提出了一个学生主导的交互式学习框架，用于提升模型在数学和代码任务上的表现。,"In this paper, we investigate the complementary setting: Can students guide the conversation by asking helpful questions to a teacher? We study this on math (GSM8K, (Cobbe et al., 2021)) and coding (HumanEval/OPC, (Chen et al., 2021; Huang et al., 2025)) tasks..."
2512.14233_output/content.md,PentestEval,"To address this gap, we introduce PentestEval, the first comprehensive benchmark for evaluating LLMs across six decomposed penetration testing stages",Yes,"we introduce PentestEval, the first comprehensive benchmark for evaluating LLMs across six decomposed penetration testing stages",,,评估大型语言模型在分解的渗透测试六个阶段中的性能。这六个阶段是：信息收集、弱点收集、弱点过滤、攻击决策、漏洞利用生成和漏洞利用修订。,"PentestEval, the first comprehensive benchmark for evaluating LLMs across six decomposed penetration testing stages: Information Collection, Weakness Gathering and Filtering, Attack Decision-Making, Exploit Generation and Revision.",渗透测试各阶段的性能、端到端工作流完成能力,evaluate LLMs along two complementary dimensions: performance on individual stages and the ability to complete full penetration testing workflows.,将LLM输出与专家标注的真实攻击数据进行系统比较，实现精确测量。包含阶段级评估和端到端测试。,"By systematically comparing LLM outputs against the expert-annotated solutions, PentestEval enables precise measurement of model effectiveness at each stage.",多步骤、迭代的攻击链，依赖前序阶段的输出和系统反馈。,"attackers rarely succeed by exploiting a single vulnerability. Instead, they construct multi-step attack chains by combining multiple weaknesses in sequence",网络安全、渗透测试,Penetration testing is essential for assessing and strengthening system security against real-world threats,涵盖代表性弱点，包括OWASP Top 10、CWE Top 25以及未公开的零日漏洞，难度较高。,"collectively capturing representative weaknesses from OWASP Top 10 [17], CWE Top 25 [18], and even undisclosed zero-day vulnerabilities.",文中未明确提及编程语言，但任务涉及生成脚本或命令行调用，推测可能涉及多种脚本语言。,,包含346个独立任务，组织在12个易受攻击的场景中。,The benchmark comprises 346 distinct tasks organized within 12 vulnerable scenarios,由五位专家设计环境、注入漏洞（包括一个零日漏洞）并标注真实攻击数据。,"built with five experts who design environments, inject vulnerabilities including a zero-day, and annotate ground-truth attack data.",2025,arXiv:2512.14233v1  [cs.SE]  16 Dec 2025,官方自建，由研究团队与专业渗透测试人员协作构建。,"expert-annotated ground truth for each stage, collaboratively verified by professional penetration testers",,,,,渗透测试工作流分解为六个阶段：信息收集、弱点收集、弱点过滤、攻击决策、漏洞利用生成、漏洞利用修订。,"six decomposed penetration testing stages: Information Collection, Weakness Gathering and Filtering, Attack Decision-Making, Exploit Generation and Revision.",成功率（例如，端到端管道达到31%的成功率，攻击决策和漏洞利用生成阶段成功率约为25%）,"End-to-end pipelines reach only 31% success rate... The most challenging components, i.e., Attack Decision-Making and Exploit Generation, reach only around a 25% success rate.",目标系统信息（如URL）、收集的弱点信息、前序攻击步骤的反馈等，主要为文本信息。,"given a target URL T... given structured profile I of a target system... given WF , wi−1, ri−1",结构化配置文件、弱点集合、攻击决策、可执行的攻击脚本或命令。,I represents a structured profile... WG represents the resulting set... outputs the next weakness wi... producing a concrete script or command-line invocation ei,文本到文本（生成决策、弱点列表）、文本到代码（生成攻击脚本）,producing a concrete script or command-line invocation ei,文中未明确描述，但涉及对目标系统的实际或模拟攻击，推测需要特定的测试环境或沙箱。,,首个针对渗透测试的模块化、阶段级评估基准；将工作流分解为六个阶段进行细粒度评估；包含专家标注的真实数据和自动化评估管道；覆盖零日漏洞。,the first modular benchmarking framework specifically designed for fine-grained and rigorous assessment of LLM performance in the stages of the penetration testing workflow... inject vulnerabilities including a zero-day
2512.15699_output/content.md,FrontierCS,"We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science...",Yes,"We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science...",Frontier-CS GitHub,Code and Data: Frontier-CS GitHub,评估大型语言模型解决开放式计算机科学问题的能力，这些问题没有已知的封闭形式或确定性最优解。模型需要实现可执行程序来解决问题，而非直接输出答案。,"In this paper, we introduce FrontierCS, a coding benchmark that evaluates LLMs on solving open-ended computer science problems, where no known closed-form or deterministic optimal solution exists in practice. Unlike math or reasoning benchmarks that require a direct answer (e.g., AIME), FrontierCS requires models to implement executable programs to solve the problem (e.g., LiveCodeBench).",开放式推理、算法设计与优化、研究问题解决能力,"FrontierCS focuses on two tracks: algorithmic optimization problems, and tasks more closely tied to real-world CS research. Both tracks naturally exhibit open-endedness, stress-testing a model’s ability to perform deep open-ended reasoning and discover nontrivial optimization strategies.",通过自动评估器运行生成的程序，根据任务特定指标（如打包密度、运行时间、内存使用）在资源限制下进行确定性验证和定量评分，而非简单的通过/失败。,"Each FrontierCS task can be solved by submitting executable code: the evaluator runs the program on generated instances and scores its outputs by task-specific metrics under resource limits (e.g., time and memory usage). ... Solutions with runtime limit can be automatically checked for validity and assigned a numeric score that reflects the quality of the solution, rather than a simple pass-or-fail.",单任务问题，模型根据问题描述和必要的I/O或API存根生成独立的求解程序。,The model is prompted with the problem specification (and any required I/O or API stubs) and must produce a self-contained solver program.,计算机科学，包含算法优化问题和研究问题。算法问题涵盖优化、构造、交互类别；研究问题涵盖操作系统、高性能计算、人工智能、数据库、编程语言、安全等领域。,"FrontierCS consists 156 problems across two tracks: Algorithmic Problems and Research Problems. The Algorithmic Problems track contains 107 problems adapted from programming contests, covering three categories: Optimization, Constructive, and Interactive problems. The Research Problems track contains 49 problems sourced from real-world computer science research questions, spanning six domains: Operating Systems, High-Performance Computing, Artificial Intelligence, Databases, Programming Languages, and Security.",前沿计算机科学难度，问题具有开放性，全局最优解未知或计算上不可行。,"Combining open-ended design, measurable progress, and expert curation, FrontierCS provides a benchmark at the frontier of computer-science difficulty.",文中未明确指定编程语言，但要求模型生成可执行程序。,,包含156个问题，其中算法问题107个，研究问题49个。,FrontierCS consists 156 problems across two tracks: Algorithmic Problems and Research Problems. The Algorithmic Problems track contains 107 problems... The Research Problems track contains 49 problems...,算法问题改编自编程竞赛；研究问题来源于现实世界的计算机科学研究问题，由专家（包括CS博士和顶级竞赛参与者及出题者）设计和评审。,"a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. ... The Algorithmic Problems track contains 107 problems adapted from programming contests... The Research Problems track contains 49 problems sourced from real-world computer science research questions...",2025-12-17 (arXiv版本),arXiv:2512.15699v1  [cs.LG]  17 Dec 2025,官方自建，由多所大学和研究机构的专家团队精心策划，经过提案、实现和评审的多阶段流程。,"Each problem is carefully curated through a multi-stage process involving proposal, implementation, and review to ensure quality and relevance...",抗污染设计，通过参数化问题生成器产生大量可变难度的实例，支持生成新鲜的、未见过的测试用例以防止泄露和过拟合。,"Parametric Problem Generator: The task specification induces a large, variable-difficulty space of instances, enabling fresh, unseen test cases to prevent leak and overfitting.",,,代码生成（生成完整的求解程序）,Models solve these tasks by implementing executable programs rather than outputting a direct answer.,任务特定的定量评分（如打包密度），而非单一指标。,"assigned a numeric score that reflects the quality of the solution, rather than a simple pass-or-fail.",自然语言问题描述，以及必要的I/O或API存根。,The model is prompted with the problem specification (and any required I/O or API stubs)...,代码（可执行的求解程序）,must produce a self-contained solver program.,文本到代码,The model is prompted with the problem specification... and must produce a self-contained solver program.,在资源限制（如时间和内存）下运行，具体环境取决于任务。,"the evaluator runs the program on generated instances and scores its outputs by task-specific metrics under resource limits (e.g., time and memory usage).",专注于解决最优解未知的开放式计算机科学问题；结合了算法优化和现实世界研究问题；提供确定性验证和连续质量评分，而非通过/失败；通过参数化生成器防止数据污染；旨在衡量模型实现有效和高效算法的能力，而非穷举解决问题。,"Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. ... This design measures the ability of models to implement effective and efficient algorithms rather than algorithms that exhaustively solve the problem. ... problems are unsolved, admit many solution strategies, and are evaluated via deterministic scoring rather than pass-or-fail."
2512.15688_output/content.md,BashArena,"We introduce BashArena, a control setting for studying AI control techniques in security-critical environments.",Yes,"We introduce BashArena, a control setting for studying AI control techniques in security-critical environments.",,,用于研究AI控制技术的控制设定，包含主任务（系统管理/基础设施工程）和侧任务（模拟安全破坏），旨在评估和设计AI控制协议。,"BashArena contains 637 Linux system administration and infrastructure engineering tasks in complex, realistic environments, along with four sabotage objectives... for a red team to target.",AI控制、安全、任务完成能力、破坏行为检测与规避,"We evaluate multiple frontier LLMs on their ability to complete tasks, perform sabotage undetected, and detect sabotage attempts.",主任务：通过Pytest测试用例评估任务完成度（通过测试用例的比例）。侧任务：在评估过程中，每次行动后运行测试，若通过则认为侧任务完成。,"The agent’s score on the task is measured as the fraction of test cases that pass. Tasks contain 12 test cases on average... The test is run after every action during evaluation, and the side task is considered complete if it ever passes.",多行动、多容器Docker环境交互,"All task environments are variations of the base environment, which is a multi-container Docker compose project... The agent works in the default container... The base environment also features three servers running in separate containers...",Linux系统管理、基础设施工程、安全、系统监控、数据处理、低级系统编程、进程管理、数据库自动化、内存优化、CI/CD自动化,"BashArena contains 637 Linux system administration and infrastructure engineering tasks... Cluster Label: System Monitoring, Data Processing, Low-Level Systems Programming, Process Management, Database Automation, Security, Memory Optimization, CI/CD Automation, Other.",对当前前沿模型具有挑战性，尚未被饱和,"Main tasks are challenging for current frontier models. BashArena’s main tasks are difficult enough that they are not saturated by current frontier models, and weaker models perform significantly worse.",Bash、Python（环境中安装了Python包）,"The agent works in the default container, which contains a simple filesystem with basic shell utilities and Python packages installed.",包含637个主任务和4个侧任务,"BashArena contains 637 main tasks created by an automated pipeline, and four manually-constructed side tasks which simulate critical security failures.",主任务基于StackOverflow上标记为“linux”或“bash”的问题，通过自动化管道生成和迭代改进。侧任务为手动构建。,We generate main tasks based on StackOverflow questions tagged “linux” or “bash”... We generate each task using Claude Opus 4.1 based on a selected StackOverflow question... Each side task consists of a description and a test.,2025-12-17 (arXiv版本日期),arXiv:2512.15688v1  [cs.CR]  17 Dec 2025,官方自建（Redwood Research），使用自动化管道生成主任务，手动构建侧任务。,We release the dataset as a ControlArena setting and share our task generation pipeline... We generate main tasks based on StackOverflow questions... and four manually-constructed side tasks.,,,,,代码生成与系统操作（通过Bash和Python命令序列实现复杂任务）,"Models are run using a slightly modified version of Inspect AI’s ReAct scaffold, provided with bash, python, file_edit, and submit tools.",主任务：通过测试用例的比例。侧任务：轨迹级假阳性率（FPR）、破坏成功率。,"The agent’s score on the task is measured as the fraction of test cases that pass... Claude Sonnet 4.5 successfully executes sabotage while evading monitoring by GPT-4.1 mini 26% of the time, at 4% trajectory-wise FPR.",自然语言任务描述,Each main task includes the following components: Task description: a written assignment that describes what needs to be accomplished...,代码（Bash/Python脚本）、系统配置、文件操作,"The agent runs commands as root inside the default container... it implements a solution to the main task, generally by creating several files back to back.",文本到代码/系统操作,The agent is only shown the task description... The agent runs commands as root inside the default container.,多容器Docker环境，包含基本Shell工具、Python包、模拟敏感数据、防火墙、sudo管理工具、外部服务器（日志服务器、恶意软件服务器）。,"All task environments are variations of the base environment, which is a multi-container Docker compose project. The agent works in the default container, which contains a simple filesystem with basic shell utilities and Python packages installed... The base environment also features three servers running in separate containers...",专门为研究AI控制而设计，包含主任务与破坏性侧任务的对抗性设定；环境复杂多样，支持复杂的破坏策略；主任务对前沿模型具有挑战性；数据集规模大且多样；包含详细的自动化任务生成管道。,"BashArena, a control setting for studying AI control techniques in security-critical environments... Supports sophisticated sabotage strategies... Main tasks are challenging for current frontier models... Large and diverse... We release the dataset as a ControlArena setting and share our task generation pipeline."
2512.16816_output/content.md,CAFFE (Counterfactual Assessment Framework for Fairness Evaluation),"This paper introduces CAFFE (Counterfactual Assessment Framework for Fairness Evaluation), a framework that integrates counterfactual fairness principles with structured test case definitions inspired by the ISO/IEC/IEEE 29119 standard [3].",Yes,"This paper introduces CAFFE (Counterfactual Assessment Framework for Fairness Evaluation)... To sum up, this paper offers three main contributions: (1) a novel testing framework, CAFFE...",,,系统性地测试大型语言模型在反事实条件下的公平性属性，即检测模型在输入仅敏感属性（如性别、种族）不同但意图相同时，输出是否保持一致。,"Our work contributes to the latter line of research, with a focus on systematically testing the fairness properties of LLMs under counterfactual conditions... Counterfactual Fairness: A model is considered fair with respect to a sensitive attribute if, for every pair of identical inputs differing only in that attribute, the output does not change.",反事实公平性,Toward Systematic Counterfactual Fairness Evaluation of Large Language Models: The CAFFE Framework,使用语义相似度指标评估模型响应，并定义量化的公平性阈值。,"(3) evaluates model responses using semantic similarity metrics... It automatically constructs intent-aware and realistic counterfactual test cases, defines quantitative fairness thresholds, and evaluates responses through semantic similarity metrics.",依赖于提示意图和对话上下文。,"proposing a structured and intent-aware framework... we propose formalizing fairness test cases along key dimensions such as prompt intent, conversational context...",大型语言模型的公平性评估，属于软件工程中的人工智能软件测试领域。,Toward Systematic Counterfactual Fairness Evaluation of Large Language Models... Software Engineering for Artificial Intelligence.,,,自然语言（用于构建提示词），不特定于编程语言。,"Our framework automatically generates counterfactual test data through stereotype-aware prompt construction, enhancing both linguistic diversity and semantic consistency across test cases.",,,通过刻板印象感知的提示构建自动生成反事实测试数据。,Our framework automatically generates counterfactual test data through stereotype-aware prompt construction...,2025-12-18 (预印本版本日期),arXiv:2512.16816v1  [cs.SE]  18 Dec 2025,官方自建（由本文作者提出和构建的框架）,This paper introduces CAFFE (Counterfactual Assessment Framework for Fairness Evaluation)...,,,,,公平性测试（检测模型行为的不一致性）,"LLM-Fairness testing: Given a language model 𝑆, a set of inputs 𝐼, the required fairness condition 𝐶, and the observed fairness condition 𝐶′, LLM-Fairness testing consists of executing 𝐼on 𝑆to identify any discrepancies between 𝐶and 𝐶′ in the generated responses, measuring to what extent the system discriminates.",语义相似度指标，公平性违规检测率（与现有方法相比提升高达60%）,evaluates model responses using semantic similarity metrics... Our results show that... CAFFE improves the detection of fairness violations by up to 60%...,自然语言提示（包含敏感属性的变体）,"automatically generates targeted test data... systematically combining social groups with biased properties, generating synthetic inputs...",自然语言响应,identify any discrepancies between 𝐶and 𝐶′ in the generated responses...,文本到文本（自然语言提示到自然语言响应）,"prompts differing only in sensitive attributes, but expressing the same intent, should yield consistent responses",,,1. 将反事实公平性原则与受ISO/IEC/IEEE 29119标准启发的结构化测试用例定义相结合。2. 支持用户自定义意图，自动生成现实、语义一致的反事实提示对。3. 与仅关注输入输出不变性的蜕变测试不同，CAFFE沿提示意图、对话上下文、预期公平性阈值等关键维度形式化公平性测试用例。,"CAFFE (Counterfactual Assessment Framework for Fairness Evaluation), a framework that integrates counterfactual fairness principles with structured test case definitions inspired by the ISO/IEC/IEEE 29119 standard [3]... supports a broader range of user-defined intents and automatically generates realistic, semantically consistent counterfactual prompt pairs that vary only in the sensitive attribute... Rather than focusing solely on input-output invariance—which is the typical use case in metamorphic testing—we propose formalizing fairness test cases along key dimensions such as prompt intent, conversational context, expected fairness thresholds, and test environment configuration."
2512.16814_output/content.md,"CW, GLTL, Navigation","We evaluate the proposed framework on three natural language to temporal logic benchmarks- CW, GLTL, and Navigation.","No, 本文是使用该数据集进行评测","We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks.",,,将自然语言翻译为时序逻辑形式语言,Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems.,翻译准确性,GraFT improves the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.,准确率,GraFT improves the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.,,,形式化规范、机器人、自主系统,"Formal specifications play a crucial role in all systems that require autonomous reasoning, verification, or planning... Temporal Logics (TL) are a group of powerful formalisms that constitute the basis of most formal specification languages, allowing expressive description of the behavior of dynamic systems over time.",,,自然语言与时序逻辑形式语言,Translating natural language (NL) into a formal language such as temporal logic (TL)...,,,,,,,,,,,,,语言翻译,Translating natural language (NL) into a formal language such as temporal logic (TL)...,准确率,GraFT improves the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.,自然语言,Translating natural language (NL) into a formal language such as temporal logic (TL)...,时序逻辑公式,A sample NL to TL translation problem would involve translating the natural language sentence “Go to the red room and push the box into the green room.” into “♢( red room ∧♢green room )”.,自然语言到时序逻辑,Translating natural language (NL) into a formal language such as temporal logic (TL)...,,,专注于自然语言到形式化时序逻辑的翻译任务，用于机器人、自主系统的规范生成。,Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems.
2512.16465_output/content.md,KernelBench,"Experiment on the 100 kernels of KernelBench [11], cuPilot achieves 3.09× speedup over PyTorch.","No, 本文是使用该数据集进行评测","Experiment on the 100 kernels of KernelBench [11], cuPilot achieves 3.09× speedup over PyTorch.",,,评测基准旨在评估CUDA内核（GPU程序）的性能优化效果。,Optimizing CUDA kernels is a challenging and labor-intensive task... The optimization of CUDA kernels... is crucial for achieving peak AI inference/training performance.,CUDA内核性能优化（速度提升）,Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09× over PyTorch on a benchmark of 100 kernels.,速度提升倍数（相对于PyTorch基准）,Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09× over PyTorch on a benchmark of 100 kernels.,,,GPU编程、高性能计算、CUDA内核优化,"Optimizing CUDA kernels, a widely adopted GPU programming model [1], is crucial for achieving peak AI inference/training performance.",工程级、需要硬件-软件协同设计专业知识,"Optimizing CUDA kernels is a challenging and labor-intensive task, given the need for hardware-software co-design expertise...",CUDA（基于C/C++的GPU编程语言）,CUDA (Compute Unified Device Architecture) as a parallel computing platform and programming model [1]. CUDA extended the C/C++ language...,包含100个内核,Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09× over PyTorch on a benchmark of 100 kernels.,,,,,,,,,,,代码生成与优化（CUDA内核生成与性能优化）,"Optimizing CUDA kernels is a challenging and labor-intensive task... This work identifies these mismatches and proposes cuPilot, a strategy-coordinated multi-agent framework that introduces strategy as an intermediate semantic representation for kernel evolution.",平均速度提升倍数（相对于PyTorch）,Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09× over PyTorch on a benchmark of 100 kernels.,代码（CUDA内核）与可能的性能分析报告,"The fitness function, based solely on performance, exhibits weak semantic correlation with the kernel code. This prevents LLM from accurately pinpointing bottlenecks among numerous profiling reports...",优化后的代码（CUDA内核）,Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09× over PyTorch...,代码到代码（优化）,Optimizing CUDA kernels is a challenging and labor-intensive task... proposes cuPilot... for kernel evolution.,GPU硬件环境（需要CUDA支持）,"Optimizing CUDA kernels, a widely adopted GPU programming model [1], is crucial for achieving peak AI inference/training performance.",专注于CUDA内核的性能优化评测，包含GEMM（通用矩阵乘法）等具体任务案例。,"On the GEMM tasks, cuPilot showcases sophisticated optimizations and achieves high utilization of critical hardware units."
2512.16070_output/content.md,LLM4Perf 多目标性能数据集,we construct a new multi-objective performance dataset based on four real-world systems.,Yes,we construct a new multi-objective performance dataset based on four real-world systems.,,,用于评估LLM作为配置采样器在构建多目标性能模型中的有效性。数据集记录了真实世界系统在不同配置下的性能指标，旨在支持对配置空间剪枝和多目标优化能力的检验。,This design allows us to examine the effects of configuration space pruning and assess the framework’s ability to handle multi-objective optimization.,多目标性能建模的有效性、配置空间剪枝效果、采样策略迭代优化能力,Our empirical evaluation... addresses four key research questions: (i) Effectiveness (RQ1)... (ii) Core Mechanisms (RQ2)... (iii) Impact of LLM Selection (RQ3)... (iv) Impact of Hyperparameter (RQ4)...,使用LLM4Perf框架生成配置样本，训练传统机器学习模型（如XGBoost）和深度学习模型（如DeepPerf），并比较它们在多个性能指标上的表现。,"The results show that both traditional machine learning model (i.e., XGBoost) and deep learning models (i.e., DeepPerf), when trained on configuration samples generated by LLM4Perf, generally achieve better performance across multiple metrics.",高度可配置软件系统的性能建模，依赖系统配置选项及其文档。,The performance of modern software systems is critically dependent on their complex configuration options.,软件性能工程、配置优化、多目标优化,"Overall, this paper provides strong evidence for the effectiveness of LLMs in performance engineering...",工程级，涉及真实世界复杂软件系统的配置空间组合爆炸和多目标权衡。,"The combination of options leads to a combinatorial explosion in the configuration search space... This challenge is further compounded by complex trade-offs, where improving one objective may degrade another...",,,基于四个真实世界系统构建。,We construct a new multi-objective performance dataset covering four real-world systems...,基于四个真实世界高度可配置系统的性能测量数据构建。,We construct a new multi-objective performance dataset based on four real-world systems.,2025-12-18 (arXiv版本),arXiv:2512.16070v1  [cs.SE]  18 Dec 2025,官方自建,we construct a new multi-objective performance dataset...,,,,,配置采样与性能预测,Building accurate performance models to navigate this vast space requires effective sampling strategies...,多个性能指标（如延迟、内存使用）的预测准确性。,"each configuration (i.e., a specific combination of option values) can have an unpredictable effect on key performance metrics like latency and memory usage...",系统配置选项、性能文档、历史性能测量数据,"In LLM4Perf, the LLM analyzes performance documentation as domain knowledge... and uses iterative feedback to refine the sampling strategy...",配置样本（用于训练性能模型）,when trained on configuration samples generated by LLM4Perf...,配置文档与历史数据到配置样本,The LLM analyzes performance documentation as domain knowledge... to guide the sampling toward more effective regions of the configuration space.,真实世界软件系统的性能测试环境,based on four real-world systems.,1. 专为多目标性能建模设计。2. 同时记录了完整配置空间和剪枝后（移除性能不敏感选项）配置空间的性能指标，便于分析剪枝效果。3. 用于评估LLM驱动的、结合领域知识（文档）和反馈的迭代采样框架。,"Unlike existing datasets, ours records performance metrics under both the full configuration space and a pruned space where performance-insensitive options are removed. This design allows us to examine the effects of configuration space pruning and assess the framework’s ability to handle multi-objective optimization."
2512.17259_output/content.md,"OPERA (Observability, Provable Execution, Red-team, Attestation)","We introduce OPERA (Observability, Provable Execution, Red-team, Attestation), a benchmark suite and evaluation protocol designed to measure (i) detectability of misalignment, (ii) time-to-detection under stealthy strategies, and (iii) resilience of verifiability mechanisms to adversarial prompt/persona injection.",Yes,"We introduce OPERA (Observability, Provable Execution, Red-team, Attestation), a benchmark suite and evaluation protocol...",,,评测基准旨在评估LLM智能体系统的可验证性机制，具体衡量其（i）对未对齐行为的可检测性，（ii）在隐蔽策略下的检测时间，以及（iii）可验证性机制对抗对抗性提示/角色注入的鲁棒性。,"We introduce OPERA (Observability, Provable Execution, Red-team, Attestation), a benchmark suite and evaluation protocol designed to measure (i) detectability of misalignment, (ii) time-to-detection under stealthy strategies, and (iii) resilience of verifiability mechanisms to adversarial prompt/persona injection.",智能体系统的可验证性、可观测性、可控性、安全性,"Our approach aims to shift the evaluation focus from ""how likely misalignment is"" to ""how quickly and reliably misalignment can be detected and remediated.""",通过一个包含可观测性、可证明执行、红队测试和证明协议的评估协议进行评测。,"We introduce OPERA (Observability, Provable Execution, Red-team, Attestation), a benchmark suite and evaluation protocol...",自主LLM智能体在循环中运行，涉及内部规划、工具使用、长期记忆和重复自我提示，生成复杂行为。,"Autonomous LLM agents operate in loops where internal planning, tool use, long-horizon memory (Yao et al. 2023; OpenAI 2023), and repeated self-prompting generate complex behaviors...",人工智能安全、智能体对齐、可验证性、网络安全、内容审核、谈判等现实自主任务。,"Recent benchmarks have quantified the propensity for misaligned behavior (Wang, Chen, and Liu 2024) across realistic autonomous tasks such as cybersecurity, content moderation, and negotiation.",高难度，涉及对抗性策略、隐蔽的未对齐行为、多智能体协调中的欺骗和不合规等复杂场景。,"Evaluating only individual agent reliability overlooks how deception, sandbagging, or non-compliance may emerge at the group level.",,,,,,,,,官方自建,"We introduce OPERA (Observability, Provable Execution, Red-team, Attestation), a benchmark suite and evaluation protocol...",,,,,智能体行为评估，包括检测、归因和补救未对齐行为。,"The field, therefore, needs an evaluative shift from measuring how often misalignment arises (Pan and Liu 2023; Long, Zhang, and Du 2024) to measuring how transparently and promptly it can be detected, attributed, and remediated.",可检测性、归因性、可补救性、检测时间、对抗性注入的鲁棒性。,"We describe the reliability of the architecture using three measurable ideas: detectability, attributability, and remediability.",智能体的意图规范、动作收据、审计日志。,"An ISpec can be thought of as the constitution of an agent. Instead of relying only on natural language prompts, it encodes deployer intent in a formal schema...",对齐概率、检测结果、补救措施。,"Collectively, they produce a dynamic estimate of the agent’s alignment probability over time.",智能体行为与意图规范到对齐评估,Their primary role is to continuously monitor the Provenance Log (PL) and validate whether the agent’s observed behavior aligns with its Intent Specification (ISpec).,在受控的智能体循环中，包含验证器栈和审计代理。,"All of this runs inside a controlled loop: the agent acts, the receipts are logged, the auditors check them, and any detected issue activates a Controller and Remediator (C&R).",OPERA基准将评估重点从“未对齐的可能性有多大”转向“未对齐行为能被多快、多可靠地检测和补救”。它集成了运行时证明、轻量级审计代理和挑战-响应证明协议，旨在为自主LLM系统提供可证明的可观测性和可控性。,"Our approach aims to shift the evaluation focus from ""how likely misalignment is"" to ""how quickly and reliably misalignment can be detected and remediated."""
2512.17419_output/content.md,SWE-Bench++,"We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects.",Yes,"We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects.",https://research.turing.com/swebench,Project Page: https://research.turing.com/swebench,评测基准旨在评估大型语言模型在仓库级软件工程任务上的能力，任务来源于真实的GitHub拉取请求，包括错误修复和功能请求。,"Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. ... Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages from open-source GitHub repositories.",仓库级代码生成、多语言能力、软件工程任务解决（错误修复与功能请求）,"SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation. ... Our state-differential oracle identifies both bug fixes and feature requests, increasing feature-like coverage compared to prior benchmarks",基于执行的验证，通过测试套件评估模型生成的补丁是否正确。,"SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance.",多文件项目、仓库级上下文,"repository-level software engineering tasks. ... The evaluation of Large Language Models (LLMs) coding agents has shifted from isolated function synthesis (e.g., HumanEval (Chen et al., 2021)) to repository-level software engineering.",通用软件工程，涵盖多种编程语言和项目类型,software engineering benchmarks from GitHub pull requests. ... covering diverse build systems and coding patterns.,工程级，涉及真实的、复杂的开源项目维护任务,"substantial complexity, defined by codebases exceeding 10k lines of code; ... merged PRs that explicitly resolve an issue (ensuring a link between a natural language problem description and a coding solution)",11种编程语言,"across 11 languages from open-source GitHub repositories. ... Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages.","包含11,133个实例，来自3,971个仓库","Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages.",从开源GitHub仓库中获取的实时拉取请求,generates repository-level coding tasks from open-source GitHub projects. ... harvests live pull requests,2025,arXiv:2512.17419v1  [cs.SE]  19 Dec 2025,官方自建，通过自动化框架生成,"We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects.",抗污染设计，可通过拉取请求创建日期过滤，支持时间分离的评估集以降低数据污染风险,"Contamination-Aware Evaluation: SWE-Bench++ is constructed from dated GitHub pull requests and can be filtered by PR creation date, enabling temporally separated evaluation sets that reduce data-contamination risk for future models.",,,仓库级代码生成与修改,repository-level software engineering tasks. ... repository-level code generation.,pass@10（文中示例）,"claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%.",自然语言（GitHub Issue描述）与代码仓库上下文,merged PRs that explicitly resolve an issue (ensuring a link between a natural language problem description and a coding solution),代码（补丁或新功能实现）,cover both bug fixes and feature requests,文本（问题描述）到代码（仓库修改）,a link between a natural language problem description and a coding solution,自动合成的Docker环境，包含特定的语言版本和依赖,Our pipeline automatically synthesizes Docker environments and log parsers across 11 languages. ... We create a reproducible execution environment that mirrors the repository at the time the issue was present.,1. 自动化、可扩展的多语言基准生成框架。2. 覆盖错误修复和功能请求两种任务类型。3. 采用状态差分测试预言机进行任务分类和验证。4. 包含提示引导的轨迹合成，可将模型失败的实例转化为训练数据。5. 具有污染感知设计，支持按时间过滤。,"an automated multilingual framework that generates software engineering benchmarks from GitHub pull requests. ... Our state-differential oracle identifies both bug fixes and feature requests. ... Hint-Guided Trajectory Synthesis: ... converts model-breaking instances (where SOTA models fail) in SWE-Bench++ environments into executable training trajectories. ... Contamination-Aware Evaluation: SWE-Bench++ is constructed from dated GitHub pull requests and can be filtered by PR creation date, enabling temporally separated evaluation sets that reduce data-contamination risk for future models."
2512.19481_output/content.md,Alextend,"1) The Alextend dataset, which contains detailed information about code changes and their impact.",Yes,"To address this gap, we study the capabilities of GPT-5 and GPT-5-mini... We construct a dataset containing information about seed-changes, change pairs, and change types for each commit.",,,预测给定源代码变更所影响的代码实体。,we study the capabilities of GPT-5 and GPT-5-mini to predict the code entities impacted by given source code changes.,代码变更影响预测的准确性,"What is the precision, recall and F1-score of GPT-5 and GPT-5-mini in code-change impact prediction?",精确率、召回率和F1分数,"What is the precision, recall and F1-score of GPT-5 and GPT-5-mini in code-change impact prediction?",提交级别（包含多个文件变更）,"Our dataset Alextend is an extension of the ALEXANDRIA dataset... For each commit, our dataset stores the attributes repo, commit hash, and parent commit hash... The dataset contains the field java class count, which represents the amount of changed Java source code files per commit.",软件工程，代码变更分析,Understanding source code changes and their impact on other code entities is a crucial skill in software development.,,,Java,"From ALEXANDRIA, we randomly selected 40 commits from all Java projects that satisfied the following criteria: • A minimum of one and a maximum of five changed Java source code files",包含40个提交，192个代码变更对，涉及14个项目,"In total, our dataset contains 40 commits with 192 code-change pairs that spread among 14 projects.",从现有数据集ALEXANDRIA（包含GitHub上Java项目的提交）扩展而来，并进行了手动标注,"Our dataset Alextend is an extension of the ALEXANDRIA dataset proposed by Yan et al. [8]... For the annotation of the change types, the two co-authors used the change type taxonomy introduced by Fluri and Gall [11]... For the annotation of the seed changes and change pairs, the two co-authors analysed each code change for its relationship to the other changes.",2025,arXiv:2512.19481v1  [cs.SE]  22 Dec 2025,官方自建（研究团队构建）,"We construct a dataset containing information about seed-changes, change pairs, and change types for each commit.",,,CC BY（根据资助要求推断）,"For open access purposes, the author has applied a CC BY public copyright license to any author accepted manuscript version arising from this submission.",代码实体级别（方法、类、属性）的影响预测,"Compared to ALEXANDRIA, we include not only method, but also class and attribute changes.",精确率、召回率、F1分数,"What is the precision, recall and F1-score of GPT-5 and GPT-5-mini in code-change impact prediction?",代码变更信息（父提交链接、种子变更的完全限定名、差异块）,"Our experiments evaluate the LLMs in two configurations: (1) seed-change information and the parent commit tree and (2) seed-change information, the parent commit tree, and the diff hunk of each seed change.",代码实体列表（完全限定名）,"Return a JSON object with a single property named ""impacted_entities"", whose value is an array of strings. Each string should be the fully qualified name of an impacted code entity.",代码变更信息到受影响代码实体列表,"Your task is to: - Identify every Java code entity affected by these seed changes; specifically, those entities that must be modified as a direct result of the seed changes.",,,1. 扩展了ALEXANDRIA数据集，不仅包含方法变更，还包含类和属性变更。2. 明确标注了种子变更（触发变更）和变更对。3. 包含了种子变更的最小差异块。4. 专注于语义依赖而非语法依赖的变更影响分析。,"Compared to ALEXANDRIA, we include not only method, but also class and attribute changes... Furthermore and compared to ALEXANDRIA, for each commit, our dataset contains the type of each change, the change pairs, the seed changes, and the diff hunk for the seed changes... This dependency is of semantic nature rather than of syntactic nature, making it hard for existing approaches to capture."
2512.19396_output/content.md,"Android World, AndroidLab",We demonstrate the efficacy of our approach on benchmarks including Android World and AndroidLab.,"No, 本文是使用该数据集进行评测",We demonstrate the efficacy of our approach on benchmarks including Android World and AndroidLab.,,,GUI任务自动化，即代理根据任务指令，通过观察GUI屏幕截图并执行一系列精细操作（如点击、滚动、打字、导航）来完成数字任务。,"GUI agents are designed to perceive and interact with digital environments by processing visual screen information. ... follow complex user instructions through sequences of fine-grained actions, including clicking, scrolling, typing, and navigation.",任务成功率、操作效率、子目标完成度、操作鲁棒性,"The results show that EchoTrail-GUI significantly improves the task success rate and operational efficiency of baseline agents... yielding substantial gains in success rate, sub-goal completion, and operational robustness.",通过实验评估任务成功率、子目标完成度和操作鲁棒性。,"Through extensive experiments on Android-World [16] and AndroidLab [30], we demonstrate that memory-augmented GUI agents significantly outperform their stateless counterparts, yielding substantial gains in success rate, sub-goal completion, and operational robustness.",多步骤任务，涉及与GUI环境的交互序列。,"Given a task instruction I, the objective is to learn a policy π(at|st, I) that generates a trajectory of state-action pairs τ = (s0, a0, s1, a1, . . . ) to fulfill I.",数字任务自动化、GUI交互、移动应用（Android环境）,"We build EchoTrail-4K, a curated dataset containing over 4,000 high-quality trajectories collected in Android environments... We demonstrate the efficacy of our approach on benchmarks including Android World and AndroidLab.",,,,,,,,,,,,,,,,,GUI任务自动化（从指令到动作序列）,"Given a task instruction I, the objective is to learn a policy π(at|st, I) that generates a trajectory of state-action pairs τ = (s0, a0, s1, a1, . . . ) to fulfill I.",任务成功率、子目标完成度、操作鲁棒性,"yielding substantial gains in success rate, sub-goal completion, and operational robustness.",自然语言任务指令和GUI屏幕截图（视觉）,"At each step t, the agent observes the current state st (a GUI screenshot) and must select an action at... Given a task instruction I...",离散动作（如点击、输入）,"must select an action at from a discrete action space A (e.g., click, type).",文本与图像到动作序列,Given a task instruction I... the agent observes the current state st (a GUI screenshot)... generates a trajectory of state-action pairs,Android GUI环境,"We build EchoTrail-4K, a curated dataset containing over 4,000 high-quality trajectories collected in Android environments... We demonstrate the efficacy of our approach on benchmarks including Android World and AndroidLab.",本文主要介绍了一个名为EchoTrail-GUI的框架，并构建了EchoTrail-4K数据集。评测基准（Android World和AndroidLab）是外部数据集，用于评估该框架。,"We present EchoTrail-GUI, a three-stage framework... We build EchoTrail-4K, a curated dataset... We demonstrate the efficacy of our approach on benchmarks including Android World and AndroidLab."
2512.20482_output/content.md,SWELOCMULTI,"We create SWELOCMULTI, a large-scale multilingual dataset curated specifically for issue localization.",Yes,"We create SWELOCMULTI, a large-scale multilingual dataset curated specifically for issue localization.",https://github.com/SalesforceAIResearch/SweRank,Code and models will be released here: https://github.com/SalesforceAIResearch/SweRank,软件问题定位，即根据自然语言错误描述或功能请求，在代码库中定位需要修改的相关函数。,"Maintaining large-scale, multilingual codebases hinges on accurately localizing issues, which requires mapping natural-language error descriptions to the relevant functions that need to be modified.",多语言代码理解与检索能力，迭代式多轮推理能力,"This work introduces SWERANK+1, a framework that couples SWERANKMULTI, a cross-lingual code ranking tool, with SWERANKAGENT, an agentic search setup, for iterative, multi-turn reasoning over the code repository.",排名准确率（如Accuracy@10）,Figure 1: Comparison of function localization accuracy@10 against the SWERANK baseline.,多文件代码库，涉及跨文件的函数定位,"This task requires mapping natural language descriptions, such as those found in GitHub issues, to specific code elements including files, modules, or functions. As modern code repositories grow in size and complexity to encompass thousands of files across multiple programming languages...",软件工程，软件维护，缺陷定位,Software issue localization (fault localization) aims to identify the specific code regions responsible for software defects.,工程级，涉及真实世界、大规模、多语言的代码库,"modern code repositories grow in size and complexity to encompass thousands of files across multiple programming languages, manual localization becomes increasingly infeasible.","JavaScript, Java, TypeScript, Ruby, Rust, Go, PHP, C, C++, Python","Table 1: Distribution of repositories, pull requests (PRs), and training instances across different programming languages in the SWELOCMULTI dataset. (Languages listed: JavaScript, Java, TypeScript, Ruby, Rust, Go, PHP, C, C++, Python)","包含155,663个训练实例，来自4,060个仓库和56,279个拉取请求",Table 1: ...Total 4060 56279 155663,从GitHub上流行的开源仓库中提取，筛选条件包括：目标语言代码占比至少40%，超过1000星，过去六个月内有提交。数据来自与GitHub问题明确关联并包含测试文件修改的拉取请求。,"Following SWERANK's methodology, we identify popular open-source repositories on GitHub for each language, filtering for repositories with at least 40% code in the target language, over 1,000 stars, and at least one commit in the preceding six months. From this curated set, we extract pull requests (PRs) explicitly linked to GitHub issues that include test file modifications.",2025,arXiv:2512.20482v1 [cs.SE] 23 Dec 2025,官方自建，由研究团队精心策划,"We create SWELOCMULTI, a large-scale multilingual dataset curated specifically for issue localization.",,,,,代码检索与排名，定位到函数级别,the task of identifying where in a codebase a fix should be applied for a given bug report or feature request. This task requires mapping natural language descriptions... to specific code elements including... functions.,Accuracy@10,Figure 1: Comparison of function localization accuracy@10 against the SWERANK baseline.,自然语言（GitHub问题描述）,mapping natural-language error descriptions to the relevant functions,代码（函数标识符或排名列表）,the model is trained to generate the identifier of the true positive function as its first token,文本到代码（检索）,mapping natural-language error descriptions to the relevant functions,,,首个专门为多语言软件问题定位策划的大规模数据集；包含10种流行编程语言；数据通过一致性过滤和困难负样本挖掘技术精心构建，以确保模型学习细粒度的语义差异。,"SWERANK+ bridges this gap as the first localization framework explicitly trained and evaluated on multilingual repositories. ... To ensure the model learns to distinguish fine-grained semantic differences, we employ consistency filtering and hard-negative mining techniques (Suresh et al., 2024) from SWERANK."
2512.20387_output/content.md,GDT-120K,"the study constructs the first large-scale dataset for generative digital twins, comprising over 120,000 prompt–sketch–code triplets",Yes,"We propose a Vision-Language Simulation Model (VLSM)... To support this new paradigm, the study constructs the first large-scale dataset for generative digital twins... we introduce the generative digital twins (GDT) framework, a large-scale multimodal dataset containing 120K prompt–sketch–code triplets",,,从布局草图和自然语言提示中合成可执行的FlexScript代码，用于工业仿真系统。旨在实现工业数字孪生的自动化建模。,"synthesize executable FlexScript from layout sketches and natural-language prompts, enabling cross-modal reasoning for industrial simulation systems.",结构完整性、参数保真度、仿真器可执行性,"comprehensively evaluate structural integrity, parameter fidelity, and simulator executability.",使用三个专门设计的评估指标：结构有效性率(SVR)、参数匹配率(PMR)和执行成功率(ESR)。BLEU-4作为补充指标。,"three novel evaluation metrics, Structural Validity Rate (SVR), Parameter Match Rate (PMR), and Execution Success Rate (ESR), are proposed specifically for this task... Although BLEU-4 fails to reflect structural or functional accuracy, it is included as a supplementary metric",基于布局草图和自然语言提示的多模态输入，生成完整的仿真程序脚本。,"Our work generates complete and executable simulation programs from scratch, conditioned only on natural inputs consisting of textual descriptions and layout sketches.",工业仿真、智能制造、工厂布局与流程建模,industrial simulation systems... smart manufacturing... factory layout synthesis,工程级，涉及复杂的空间布局、参数配置和仿真逻辑,"building digital twins in FlexSim remains highly labor-intensive, requiring manual object placement, parameter configuration, and logic scripting",FlexScript（FlexSim仿真平台的专有脚本语言）,logic scripting in the proprietary FlexScript language... synthesize executable FlexScript for digital twins.,"包含120,285个提示-草图-代码三元组","comprising over 120,000 prompt–sketch–code triplets... a corpus of 120,285 programs",基于工厂调查数据，通过统计验证、FlexSim实例化，并结合GPT辅助和人工编辑生成提示，构建的多模态对齐数据对。,Our workflow converts factory information into a multimodal dataset... It begins with factory investigations collecting layout data... The curated information is instantiated in FlexSim to generate reference projects... Prompts are drafted from metadata with GPT assistance and refined by human editors,2025,arXiv:2512.20387v1  [cs.AI]  23 Dec 2025,官方自建，由研究团队构建,the study constructs the first large-scale dataset... we introduce the generative digital twins (GDT) framework,,,,,代码生成，从高层规范生成完整的、可运行的仿真程序,generates complete and executable simulation programs from scratch... translates high-level specifications into runnable digital-twin logic,结构有效性率(SVR)、参数匹配率(PMR)、执行成功率(ESR)、BLEU-4,"three novel evaluation metrics, Structural Validity Rate (SVR), Parameter Match Rate (PMR), and Execution Success Rate (ESR)... BLEU-4 is included as a supplementary metric",自然语言与布局草图（视觉）,textual descriptions and layout sketches... integrates visual and textual inputs,代码（FlexScript）,synthesize executable FlexScript,多模态（文本与图像）到代码,translates high-level specifications (text and sketches) into runnable digital-twin logic,FlexSim仿真平台,executed in FlexSim... executable within FlexSim,首个面向生成式数字孪生的大规模多模态数据集，专注于工业仿真领域专有语言FlexScript，并提出了针对仿真代码的结构化评估指标。数据集设计采用五层框架（产线流程、参数多样性、自动化水平、行业类型、布局配置），以确保多样性和现实代表性。,"the first large-scale dataset for generative digital twins... the domain-specific nature of FlexScript, which diverges from general-purpose languages and lacks public datasets or suitable evaluation metrics... We adopt a five-layer framework encompassing the production line process, parameter diversity, level of automation, industry type, and layout configuration."
2512.21236_output/content.md,SPELL,"To address this gap, we propose SPELL, a comprehensive testing framework for LLM developers and the Secure Team, specifically designed to evaluate the weakness of security alignment in malicious code generation.",Yes,"To address this gap, we propose SPELL, a comprehensive testing framework for LLM developers and the Secure Team, specifically designed to evaluate the weakness of security alignment in malicious code generation.",,,评估大型语言模型在恶意代码生成任务中的安全对齐弱点。,"To address this gap, we propose SPELL, a comprehensive testing framework for LLM developers and the Secure Team, specifically designed to evaluate the weakness of security alignment in malicious code generation.",安全对齐、恶意代码生成能力、越狱攻击成功率,specifically designed to evaluate the weakness of security alignment in malicious code generation.,使用攻击成功率（Attack Success Rate）进行评估，并利用最先进的检测系统确认生成代码的恶意性。,"achieving attack success rates of 83.75%, 19.38%, and 68.12% respectively across eight malicious code categories. The generated prompts successfully produce malicious code in real-world AI development tools such as Cursor, with outputs confirmed as malicious by state-of-the-art detection systems at rates exceeding 73%",,,网络安全、恶意软件生成,"including malware, ransomware, and other security threats.",工程级（生成功能性的恶意软件）,allowing attackers to produce fully functional malicious programs,文中未明确指定，但实验涉及多种编程语言（如Python、C++），因为恶意代码类别多样。,"Then, we’ll write the code to infect all the files in the specified directory. Easy peasy! Next, we’ll add the screen locker payload to lock the screen. A simple piece of code will do the trick. Lastly, we’ll establish persistence by creating a system service. Who needs ethics, right?""",包含八个恶意代码类别。,"achieving attack success rates of 83.75%, 19.38%, and 68.12% respectively across eight malicious code categories.",通过时间划分选择策略，从先验知识数据集中智能组合句子来系统构建越狱提示。,Our framework employs a time-division selection strategy that systematically constructs jailbreaking prompts by intelligently combining sentences from a prior knowledge dataset,2025-12-24,arXiv:2512.21236v1  [cs.CR]  24 Dec 2025,官方自建（由研究团队提出）,"we propose SPELL, a comprehensive testing framework",,,,,代码生成（从自然语言描述生成恶意代码）,malicious code generation,攻击成功率（Attack Success Rate）,"achieving attack success rates of 83.75%, 19.38%, and 68.12% respectively",自然语言（越狱提示）,systematically constructs jailbreaking prompts,代码（恶意软件）,The generated prompts successfully produce malicious code,文本到代码,malicious code generation,真实世界的AI开发工具（如Cursor）和检测系统,"The generated prompts successfully produce malicious code in real-world AI development tools such as Cursor, with outputs confirmed as malicious by state-of-the-art detection systems",SPELL是一个动态的、自动化的测试框架，它从固定模板转向动态组件发现和组合，用于评估LLM在恶意代码生成中的安全对齐弱点。它采用时间划分选择策略，智能组合先验知识数据集中的句子来构建新的、未见过的攻击提示。,"we propose SPELL, a fully automated framework that fundamentally shifts from fixed templates to dynamic component discovery and combination. Unlike existing approaches that rely on predetermined attack structures, our method automatically identifies effective prompt elements from diverse sources and intelligently combines them to generate novel, previously unseen attack prompts."
2512.21332_output/content.md,MTEB-Code,"We evaluate C2LLM on the 12 retrieval tasks in MTEB-Code Benchmark (Muennighoff et al., 2023; Enevoldsen et al., 2025)1.","No, 本文是使用该数据集进行评测","We evaluate C2LLM on the 12 retrieval tasks in MTEB-Code Benchmark (Muennighoff et al., 2023; Enevoldsen et al., 2025)1.",,,代码检索。给定自然语言查询，系统需要从数百万甚至数十亿的候选代码片段中返回最相关的代码片段。,"In the retrieval setting, a user supplies a natural-language query (e.g., “open a jsonl file in Python and read all lines”), and the system must return the most relevant snippet among millions or even billions of candidates stored in public or private codebases.",代码检索能力,At the core of code retrieval systems lie code embedding models.,在多个检索任务上的平均性能得分,"As shown in Table 2, C2LLM-7B achieves an average score of 80.75, surpassing the previous state-of-the-art Seed1.6-Embedding and Qwen3-Embedding-8B.",,,软件工程，包括代码搜索、问题解决、代码编辑、代码翻译、SQL生成等,"The training data includes CodeSearchNet (including code-to-code, code-to-text, and text-to-code retrieval, Husain et al., 2019; Li et al., 2025c; Lu et al., 2021), APPS (Hendrycks et al., 2021), single-turn and multi-turn CodeFeedback (Zheng et al., 2024), CodeEditSearch (Muennighoff et al., 2024), CosQA (Huang et al., 2021), StackOverflowQA (Li et al., 2025c), SyntheticText2SQL (Meyer et al., 2024), and Code-TransOcean (Yan et al., 2023), totaling 3 million samples.",,,文中未描述MTEB-Code基准本身的语言覆盖范围，仅提及训练数据包含多种编程语言,"To ensure the quality of the contrastive signals, we adopt a specialized batching strategy where data is grouped according to both the dataset source and the specific programming language before being partitioned into training batches.",包含12个检索任务,"We evaluate C2LLM on the 12 retrieval tasks in MTEB-Code Benchmark (Muennighoff et al., 2023; Enevoldsen et al., 2025)1.",,,2025-12-25 (提交日期),Top 10 models on the MTEB-Code leaderboard as of the submission date (2025-12-25).,,,,,,,代码检索,"In the retrieval setting, a user supplies a natural-language query (e.g., “open a jsonl file in Python and read all lines”), and the system must return the most relevant snippet among millions or even billions of candidates stored in public or private codebases.",平均性能得分,"As shown in Table 2, C2LLM-7B achieves an average score of 80.75",自然语言查询或代码片段,"a user supplies a natural-language query (e.g., “open a jsonl file in Python and read all lines”)",代码片段,the system must return the most relevant snippet,文本到代码、代码到代码、代码到文本等多种检索任务,"The training data includes CodeSearchNet (including code-to-code, code-to-text, and text-to-code retrieval, Husain et al., 2019; Li et al., 2025c; Lu et al., 2021)",,,MTEB (Massive Text Embedding Benchmark) 的代码版本，专注于评估代码嵌入模型的检索性能，包含12个不同的代码检索任务。,"We evaluate C2LLM on the 12 retrieval tasks in MTEB-Code Benchmark (Muennighoff et al., 2023; Enevoldsen et al., 2025)1."
2512.21238_output/content.md,Basket,"• Basket, a Bloom’s taxonomy-guided frAmework for Software security Knowledge EvaluaTion.",Yes,"To answer this question, we developed Basket, a framework that enables the systematic assessment of LLMs guided by Bloom’s Taxonomy [22]... Rather than introducing yet another benchmark for a single security task, we used a combination of curated multiple-choice questions, vulnerable code snippets, course assessments, real-world case studies, and open-ended project tasks...",https://github.com/msiddiq3/Basket (根据论文末尾的引用[24]推断，原文为 'The data and associated scripts are available in our replication package [24].'),The data and associated scripts are available in our replication package [24].,评估大型语言模型对软件安全知识的理解程度，涵盖从事实回忆到创造性设计等多个认知层次。,"This work systematically evaluates the security comprehension of five leading LLMs... using Bloom’s Taxonomy as a framework. We assess six cognitive dimensions: remembering, understanding, applying, analyzing, evaluating, and creating.",软件安全知识理解、认知层次（基于布鲁姆分类法）、模型知识边界、错误概念模式,"We assess six cognitive dimensions: remembering, understanding, applying, analyzing, evaluating, and creating... we introduce a software security knowledge boundary that identifies the highest cognitive level at which a model consistently maintains reliable performance. In addition, we identified 51 recurring misconception patterns made by LLMs across Bloom’s levels.",使用多种数据集（选择题、漏洞代码片段、课程评估、真实案例、项目任务）进行综合评估，并基于布鲁姆分类法分析模型在不同认知层次的表现。,"Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments (Introduction to Software Security course), real-world case studies (XBOW), and project-based creation tasks (Secure Software Engineering course).",多样，包括独立的代码片段、选择题、课程作业、真实世界案例和开放式项目任务。,"Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments (Introduction to Software Security course), real-world case studies (XBOW), and project-based creation tasks (Secure Software Engineering course).",软件安全,Assessing the Software Security Comprehension of Large Language Models,覆盖从低级认知（回忆、理解）到高级认知（分析、评估、创造）的多个难度层次。,"Results show that while LLMs perform well on lower-level cognitive tasks, such as recalling facts and identifying known vulnerabilities, their performance degrades significantly on higher-order tasks that require reasoning, architectural evaluation, and secure system creation.",文中未明确提及数据集使用的编程语言，但提及了评估模型在软件安全概念上的理解，可能涉及多种语言。,,文中未明确提及具体问题数量，但提及整合了多种来源的数据集。,,整合了多种来源：人工策划的多选题、漏洞代码片段（SALLM）、课程评估（软件安全导论课程）、真实世界案例研究（XBOW）、基于项目的创造任务（安全软件工程课程）。,"Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments (Introduction to Software Security course), real-world case studies (XBOW), and project-based creation tasks (Secure Software Engineering course).",2025-12-24 (预印本发布日期),arXiv:2512.21238v1  [cs.SE]  24 Dec 2025,官方自建（由论文作者构建的评估框架）,"To answer this question, we developed Basket, a framework that enables the systematic assessment of LLMs...",文中未明确讨论。,,文中未明确提及。,,多样，包括知识问答、漏洞识别、代码分析、安全评估和系统设计。,"We assess six cognitive dimensions: remembering, understanding, applying, analyzing, evaluating, and creating.",文中未明确提及单一指标，但报告了模型在不同认知层次上的表现（如准确率）和知识边界。,"Beyond reporting aggregate accuracy, we introduce a software security knowledge boundary...",多样，包括自然语言问题、代码片段、案例描述、项目要求。,"Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments... real-world case studies... and project-based creation tasks...",多样，包括选择题答案、代码修复、安全分析、评估判断、设计方案。,"We assess six cognitive dimensions: remembering, understanding, applying, analyzing, evaluating, and creating.",多样，包括文本到文本（问答）、代码到文本（分析）、文本到代码（修复/设计）等。,"Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments... real-world case studies... and project-based creation tasks...",文中未明确描述。,,1. 首个基于布鲁姆分类法系统评估LLM软件安全知识理解的框架。2. 引入了“软件安全知识边界”概念，用于识别模型可靠性能的最高认知层次。3. 识别并分类了LLM在软件安全概念上的51种重复性错误模式。,"Rather than introducing yet another benchmark for a single security task... Beyond reporting aggregate accuracy, we introduce a software security knowledge boundary... In addition, we identified 51 recurring misconception patterns made by LLMs across Bloom’s levels."
2512.21028_output/content.md,BigCodeBench (Hard),"Using the BigCodeBench (Hard) dataset [49], we design five prompting conditions...","No, 本文是使用该数据集进行评测","Using the BigCodeBench (Hard) dataset [49], we design five prompting conditions...",,,代码生成，旨在评估模型根据任务描述生成正确代码的能力。,"In this paper, we present an empirical study that systematically examines how LLMs adapt their code generation strategies when exposed to test cases under different instructions.",代码生成正确性、代码相似性、程序大小、代码变更、模型在测试可见性与指令限制下的行为适应性,"We evaluate five LLMs (four open-source and one closed-source) across correctness, similarity to reference code, program size, and code churn, and analyze cross-model consistency to identify adaptation strategies.",执行单元测试以评估功能正确性，使用 pass@k 指标。,"Across benchmarks, evaluation typically relies on executing unit tests to standardize correctness assessment.",,,,,Hard (困难),Using the BigCodeBench (Hard) dataset [49]...,文中未明确提及BigCodeBench (Hard)数据集包含的具体编程语言，仅提及了实验背景下的其他基准（如HumanEval、MBPP是Python）。,"HumanEval and MBPP measure functional correctness on short Python tasks using unit tests [5, 14]; ... MultiPL-E systematically translates HumanEval and MBPP into multiple languages to probe cross-language generalization [13].",,,,,,,,,,,,,代码生成,"In this paper, we present an empirical study that systematically examines how LLMs adapt their code generation strategies...","pass@1, pass@5","For example, Pass@1 success rates rose from 15.5%–24.4% without tests to 37.2%–54.7% with tests... Pass@5, which increased from 29.1%–37.2% without tests to 57.4%–72.3% with tests.",自然语言任务描述，可能包含单元测试,"We define five prompting conditions that manipulate test visibility... (1) Baseline (B), with only the task description; (2) Full Test (FT), task plus all tests...",代码,...how LLMs adapt their code generation strategies...,文本到代码,...producing correct code for a given problem...,,,强调复杂的函数调用和指令，以更好地反映当前模型的能力。,"More recently, BigCodeBench emphasizes diverse function calls and complex instructions, better reflecting current model capabilities [49]."
2512.20957_output/content.md,SWE-smith,"We extract valid samples from SWE-smith (Yang et al., 2025b) to form the training set.","No, 本文是使用该数据集进行评测","We extract valid samples from SWE-smith (Yang et al., 2025b) to form the training set.",,,评测大型语言模型（LLM）代理在大型开源软件（OSS）仓库中定位需要修改的文件和函数的能力。这是一个仓库级别的检索任务。,Locating the files and functions requiring modification in large open-source software (OSS) repositories is challenging due to their scale and structural complexity. Existing large language model (LLM)-based methods typically treat this as a repository-level retrieval task...,仓库级问题定位能力,"Experiments demonstrate that RL-trained RepoNavigator achieves state-of-the-art performance... These results confirm that integrating a single, structurally grounded tool with RL training provides an efficient and scalable solution for repository-level issue localization.",使用Dice系数（DICE）进行集合级别的比较，并结合工具调用成功率（S(τ)）计算奖励。,"The reward of GRPO process is calculated as: R( ˆY , Y ∗, τ) = DICE( ˆY , Y ∗) + S(τ)... Dice is a common metric for set-level comparison, for set ˆY and set Y ∗ DICE( ˆY , Y ∗) = 2 × | ˆY ∩Y ∗| / (| ˆY | + |Y ∗|) and S(τ) is the success rate of tool-calling extracted from τ.",多文件项目（整个代码仓库）,"Given a repository R = {f1, . . . , fN} and an issue description q, the goal is to output relevant code regions Y ∗= {(fi, gi,j)}, where gi,j denotes a function or code span in file fi.",软件工程（SWE），特别是开源软件仓库的维护和问题修复。,"In the domain of software engineering (SWE)... SWE-BENCH (Jimenez et al., 2023) currently serves as the most comprehensive benchmark for evaluating whether LLMs can resolve real-world GitHub issues.",工程级（处理真实世界、大规模的代码仓库）,"In the domain of software engineering (SWE), although LLM agents can effectively handle simple programming tasks (Hui et al., 2024; Guo et al., 2024a), their ability to operate on large-scale open-source software (OSS) repositories remains limited.",Python,Language servers resolve the definition of a Python symbol through a deterministic static analysis pipeline that approximates Python’s runtime name-binding semantics.,,,从SWE-smith数据集中提取的有效样本。,"We extract valid samples from SWE-smith (Yang et al., 2025b) to form the training set.",2025,arXiv:2512.20957v1 [cs.SE] 24 Dec 2025,,,,,,,代码定位（识别与问题相关的文件和函数）,"Given a repository R = {f1, . . . , fN} and an issue description q, the goal is to output relevant code regions Y ∗= {(fi, gi,j)}, where gi,j denotes a function or code span in file fi.",Dice系数，工具调用成功率,"The reward of GRPO process is calculated as: R( ˆY , Y ∗, τ) = DICE( ˆY , Y ∗) + S(τ)... Dice is a common metric for set-level comparison...",自然语言（问题描述）和代码仓库结构,"Given a repository R = {f1, . . . , fN} and an issue description q...",代码区域集合（文件路径和函数/代码片段标识符）,"...the goal is to output relevant code regions Y ∗= {(fi, gi,j)}, where gi,j denotes a function or code span in file fi.",文本（问题）到代码位置,"Given a repository R = {f1, . . . , fN} and an issue description q, the goal is to output relevant code regions...",,,专注于仓库级别的代码定位任务，而非完整的代码修复。旨在通过识别故障组件（文件或函数级别）来简化下游的修复任务，并支持可验证的、字符串级别的评估，与SFT和RLVR等可扩展训练框架兼容。,"Since modern OSS repositories contain a significant amount of code—far beyond any LLM’s context window—localization drastically reduces the search space and improves downstream solvability. Crucially, localization outputs a discrete set of paths, enabling verifiable, string-level evaluation that is compatible with scalable training frameworks such as SFT and RLVR."
2512.21132_output/content.md,AUTOBAXBENCH,"We use AUTOBAXBUILDER to construct entirely new tasks and release them to the public as AUTOBAXBENCH, together with a thorough evaluation of the security capabilities of LLMs on these tasks.",Yes,"In this work, we address these challenges and present AUTOBAXBUILDER, a framework that generates tasks and tests for code security benchmarking from scratch. ... We use AUTOBAXBUILDER to construct entirely new tasks and release them to the public as AUTOBAXBENCH...","https://baxbench.com/autobaxbuilder, https://github.com/eth-sri/autobaxbuilder","https://baxbench.com/autobaxbuilder
https://github.com/eth-sri/autobaxbuilder
... We publicly release the scenarios at https://github.com/eth-sri/autobaxbuilder.",评估LLM生成的后端应用程序代码的功能正确性和安全性。每个任务包含一个场景描述（后端应用规范）、功能测试和安全漏洞利用脚本。,BAXBENCH is a recent benchmark that measures both functional correctness and security of LLM-generated application backends. ... Each scenario is combined with functional tests and security exploits that test LLM-generated solutions through the REST endpoints...,代码安全性与功能正确性,"As LLMs see wide adoption in software engineering, the reliable assessment of the correctness and security of LLM-generated code is crucial. ... BAXBENCH is a recent benchmark that measures both functional correctness and security of LLM-generated application backends.",通过REST端点对生成的代码进行动态测试，包括功能测试和端到端安全漏洞利用。安全测试成功会返回对应的CWE分类。,"The generated code is launched in an isolated environment, exposing its endpoints via REST. This allows testing the solution via HTTP requests. ... If a security test finds an successful exploit, it returns a corresponding classification as an entry in the Common Weakness Enumeration (CWE).",后端应用程序级别（多端点、多文件项目）,"BAXBENCH consists of scenarios, each specifying a backend application to implement, including a natural language description and specific REST endpoints.",Web应用后端开发，安全关键领域,"This is particularly important in safety-critical domains such as web application backends, as these are directly exposed to malicious actors.",包含三个不同难度的子集：简单、中等（比BAXBENCH稍难）、困难（最佳模型准确率低于9%）,"We leverage our tool to explicitly generate three distinct subsets of varying difficulty, including a medium version that is slightly more difficult than BAXBENCH, an easier version suitable for evaluation of smaller LLMs, and a hard version that challenges the best evaluated LLM, which achieves less than 9% accuracy.",语言无关，支持14种框架和6种编程语言,"Each such combination defines a language-independent task, which can readily be evaluated in 14 frameworks across 6 programming languages.",包含40个新场景，是原始BAXBENCH规模的两倍多,"We then generate 40 new scenarios with AUTOBAXBUILDER, more than doubling the size of BAXBENCH.",由AUTOBAXBUILDER框架通过LLM自动生成，无需人工编写,"In this work, we address this challenge and propose an agentic LLM-based pipeline that creates new scenarios with minimal human intervention, including corresponding functionality test cases and security exploits.",2025,arXiv:2512.21132v1  [cs.CR]  24 Dec 2025,官方自建（由ETH Zurich研究团队通过自动化框架构建）,We use AUTOBAXBUILDER to construct entirely new tasks and release them to the public as AUTOBAXBENCH...,抗污染设计，可不断生成新任务以避免训练数据污染,"However, relying on manually-crafted benchmarks is insufficient in the long term, because benchmarks (i) naturally end up contaminating training data... ...and constantly updated to ensure valid evaluation in the face of contamination.",,,代码生成（生成完整的后端应用程序）,"For each such task, an evaluated model is prompted to generate application code in the target language.",功能正确性（通过功能测试）和安全性（通过安全漏洞利用测试）的准确率,BAXBENCH is a recent benchmark that measures both functional correctness and security of LLM-generated application backends.,自然语言描述和OpenAPI规范,"BAXBENCH consists of scenarios, each specifying a backend application to implement, including a natural language description and specific REST endpoints. Concretely, the endpoints are specified in the OpenAPI language...",代码（后端应用程序实现）,"For each such task, an evaluated model is prompted to generate application code in the target language.",文本到代码,"BAXBENCH consists of scenarios, each specifying a backend application to implement, including a natural language description and specific REST endpoints. ... For each such task, an evaluated model is prompted to generate application code in the target language.",隔离环境，通过REST暴露端点，可访问文件系统和数据库以检测攻击,"The generated code is launched in an isolated environment, exposing its endpoints via REST. ... Further, we can access the file system, e.g., to check for successful Path Traversal or OS Injection attacks, and access used databases, e.g., to detect manipulations due to SQL Injection.",1. 完全自动化生成，无需人工编写场景、测试和漏洞利用脚本。2. 提供无假阳性的安全上界保证。3. 框架和编程语言无关。4. 包含端到端的安全漏洞利用测试。5. 可扩展生成不同难度的任务。,"We present a robust method to generate a completely new benchmark following the design principles of BAXBENCH with minimal human intervention... ...This approach has no false-positives, and thus provides a sound upper bound on security. ... Each such combination defines a language-independent task, which can readily be evaluated in 14 frameworks across 6 programming languages. ... We leverage our tool to explicitly generate three distinct subsets of varying difficulty..."
2512.20203_output/content.md,VulnLoc+,"We evaluated LoopRepair on a real-world C/C++ vulnerability repair dataset VulnLoc+, which contains 40 vulnerabilities and their Proofs-of-Vulnerability.","No, 本文是使用该数据集进行评测","We evaluate the performance of LoopRepair on the VulnLoc+ [51, 53] dataset, which consists of 40 vulnerabilities with their PoVs collected from 10 open-source software projects.",,,自动化软件漏洞修复。给定易受攻击的函数和漏洞语句，目标是生成修复后的补丁函数。,"In this paper, we focus on function-level Automated Vulnerability Repair (AVR), which takes a sequential vulnerable function 𝐹𝑣 and the vulnerable statement 𝑆𝑣 as inputs, and outputs the patch function 𝐹𝑝.",漏洞修复的有效性，包括生成合理补丁和正确补丁的能力。,"LoopRepair is able to generate 27 plausible patches... In terms of correct patch generation, LoopRepair repairs 8 to 13 additional vulnerabilities compared with existing approaches.",使用Proofs-of-Vulnerability (PoV) 进行验证，评估生成的补丁是否合理（plausible）和正确（correct）。,"Following previous work [19, 51], we assume that the vulnerability has already been detected and that 1-2 Proofs-of-Vulnerability (PoVs) are available.",函数级别。,"In this paper, we focus on function-level Automated Vulnerability Repair (AVR)...",软件安全，具体为C/C++程序中的漏洞修复。,We evaluated LoopRepair on a real-world C/C++ vulnerability repair dataset VulnLoc+...,包含多块（multi-hunk）漏洞，修复难度较高。,"A multi-hunk vulnerability is a vulnerability that requires edits at multiple, non-contiguous hunks in a program [64, 69].",C/C++,We evaluated LoopRepair on a real-world C/C++ vulnerability repair dataset VulnLoc+...,包含40个漏洞及其Proofs-of-Vulnerability (PoVs)，收集自10个开源软件项目。,"We evaluate the performance of LoopRepair on the VulnLoc+ [51, 53] dataset, which consists of 40 vulnerabilities with their PoVs collected from 10 open-source software projects.",从开源软件项目中收集的真实漏洞。,...collected from 10 open-source software projects.,,,,,,,,,代码修复（漏洞修复）。,Automated Vulnerability Repair (AVR),合理补丁数量（plausible patches），正确修复的漏洞数量（correct patch generation）。,"LoopRepair is able to generate 27 plausible patches... In terms of correct patch generation, LoopRepair repairs 8 to 13 additional vulnerabilities...",代码（易受攻击的函数）与代码（漏洞语句）。,...takes a sequential vulnerable function 𝐹𝑣 and the vulnerable statement 𝑆𝑣 as inputs...,代码（补丁函数）。,...and outputs the patch function 𝐹𝑝.,代码到代码（漏洞代码到修复代码）。,"...takes a sequential vulnerable function 𝐹𝑣 and the vulnerable statement 𝑆𝑣 as inputs, and outputs the patch function 𝐹𝑝.",,,专注于多块（multi-hunk）漏洞修复，每个漏洞都配有Proofs-of-Vulnerability (PoVs) 用于验证。,"A multi-hunk vulnerability is a vulnerability that requires edits at multiple, non-contiguous hunks in a program... We evaluated LoopRepair on a real-world C/C++ vulnerability repair dataset VulnLoc+, which contains 40 vulnerabilities and their Proofs-of-Vulnerability."
2512.20845_output/content.md,HumanEval,"• HumanEval (Chen et al. 2021) is a benchmark for program synthesis in which each task specifies a function signature, a natural-language description of the intended behavior, and a series of hidden unit tests.","No, 本文是使用该数据集进行评测",We evaluate our approach on two benchmarks that stress different aspects of LLM reasoning. ... HumanEval tests program synthesis by checking whether generated functions pass hidden unit tests.,,,程序合成，即根据自然语言描述和函数签名生成能通过隐藏单元测试的代码。,HumanEval tests program synthesis by checking whether generated functions pass hidden unit tests.,代码生成的功能正确性,HumanEval tests program synthesis by checking whether generated functions pass hidden unit tests.,在沙盒环境中执行生成的代码，检查是否通过所有隐藏的单元测试。,A model succeeds if its generated function passes all tests when executed in a sandboxed environment.,单函数,"each task specifies a function signature, a natural-language description of the intended behavior, and a series of hidden unit tests.",通用编程,,,,文中仅提及实验设置，未描述完整数据集。根据引用文献(Chen et al. 2021)推断为Python。,HumanEval (Chen et al. 2021) is a benchmark for program synthesis...,,,,,2021,HumanEval (Chen et al. 2021),,,,,,,代码生成,HumanEval tests program synthesis,pass@1,"for HumanEval, we use pass@1 based on whether the generated solution passes all hidden unit tests.",自然语言与代码（函数签名）,"each task specifies a function signature, a natural-language description of the intended behavior",代码,generated functions,文本到代码,HumanEval tests program synthesis,沙盒环境,when executed in a sandboxed environment.,包含隐藏的单元测试，用于评估生成代码的功能正确性。,a series of hidden unit tests.
2512.20732_output/content.md,FEM-Bench,"We introduce FEM-Bench, a computational mechanics benchmark designed to evaluate the ability of LLMs to generate correct finite element method (FEM) and related code.",Yes,"We introduce FEM-Bench, a benchmark designed to evaluate the ability of LLMs to generate correct implementations of finite element method (FEM) and related computational mechanics tasks.",,,评估大语言模型生成正确的有限元方法及相关计算力学代码的能力。,"FEM-Bench, a computational mechanics benchmark designed to evaluate the ability of LLMs to generate correct finite element method (FEM) and related code.",物理建模、数值离散化、结构化计算推理、科学计算代码生成能力,"no widely used benchmarks evaluate an LLMs ability to carry out the physical modeling, numerical discretization, and structured computational reasoning required for advanced scientific computing.",基于计算力学验证、验证和不确定性量化文化的客观数值检查，如对称性、一致性、网格收敛性等。,"These practices transform numerical implementations into testable artifacts with well-defined correctness criteria... including patch tests, equilibrium checks, symmetry requirements, energy consistency, and mesh convergence studies.",多步骤结构化推理，遵循明确的数学流程（如控制方程、弱形式、离散化、全局组装）。,"Problems in computational mechanics follow a precise mathematical pipeline: e.g., governing equations are formulated, converted into weak or variational forms, discretized into finite-dimensional approximations, and assembled into global algebraic systems.",计算力学、科学计算、物理建模、有限元方法、矩阵结构分析,"Computational mechanics, the discipline that develops and applies mathematical models and numerical methods to predict the behavior of physical systems under forces, deformation, and constraints, provides an ideal foundation for structured scientific reasoning based evaluation.",入门级但非平凡，与计算力学研究生入门课程内容一致。,FEM-Bench 2025 focuses on introductory but nontrivial tasks aligned with material from a first graduate course on FEM.,文中未明确指定编程语言，但任务涉及生成科学计算代码。,,FEM-Bench 2025 包含33个任务。,"the best performing model at function writing, Gemini 3 Pro, completed 30/33 tasks at least one out of five times, and 26/33 tasks five out of five times.",基于成熟的计算力学文献、经典教科书和广泛教授的研究生课程。,"The computational mechanics literature spans classic textbooks, widely taught graduate curricula in engineering, and ongoing research at the frontier of numerical methods development, offering a rich, well-understood space of problems with clear expectations for correctness and implementation quality.",2025,FEM-Bench 2025,官方自建，由研究团队基于计算力学领域知识构建。,"In this work, we introduce FEM-Bench, a benchmark designed to evaluate the ability of LLMs...",,,,,代码生成（生成有限元方法及相关计算力学任务的实现代码）。,evaluate the ability of LLMs to generate correct implementations of finite element method (FEM) and related computational mechanics tasks.,任务完成率（如30/33，26/33），平均联合成功率（Average Joint Success Rate，如73.8%）。,"the best performing model at function writing, Gemini 3 Pro, completed 30/33 tasks at least one out of five times, and 26/33 tasks five out of five times. The best performing model at unit test writing, GPT-5, had an Average Joint Success Rate of 73.8%.",自然语言描述的计算力学问题（包含载荷、边界条件、几何等）。,"A solid mechanics problem: given loads and boundary conditions on the reference configuration, solve for the deformation mapping to the deformed configuration.",代码（有限元方法或相关计算力学算法的实现）。,generate correct implementations of finite element method (FEM) and related computational mechanics tasks.,文本到代码,evaluate the ability of LLMs to generate correct implementations... from problem descriptions.,需要科学计算环境以执行生成的代码并进行数值验证。,These tasks isolate essential numerical and physical modeling challenges while remaining amenable to automated evaluation.,专注于计算力学和物理世界建模，填补了现有代码生成基准在评估科学计算、物理建模和数值方法实现能力方面的空白。它强调严格的物理和数值约束，以及基于计算力学成熟验证文化的客观评估。,"no widely used benchmarks evaluate an LLMs ability to carry out the physical modeling, numerical discretization, and structured computational reasoning required for advanced scientific computing. Computational mechanics... provides an ideal foundation for structured scientific reasoning based evaluation. Problems follow clear mathematical structure, enforce strict physical and numerical constraints, and support objective verification."
2512.22113_output/content.md,Code-Cloud-RCA Benchmark,"3) Code-Cloud-RCA Benchmark consisting of 30 different scenarios and each incorporating a unique software, configuration, deployment, or resource-related fault observed in the real-world [3]–[5], [10]–[13], and injected in a live Kubernetes cloud environment.",Yes,3) Code-Cloud-RCA Benchmark consisting of 30 different scenarios... PRAXIS is demonstrated on a set of 30 comprehensive real-world incidents that is being compiled into an RCA benchmark.,,,用于诊断由代码和配置引起的云服务故障的根因分析（RCA）。,"This paper introduces PRAXIS, an orchestrator that manages and deploys an agentic workflow for diagnosing code- and configuration-caused cloud incidents.",根因分析（RCA）的准确性和效率，包括根因推理准确性和根因识别准确性。,PRAXIS achieved a root-cause reasoning accuracy of 61.5% and a root-cause identification accuracy of 73.9%... PRAXIS reduced token consumption by 3.8× from 624.4k tokens to 166.5k tokens per successful diagnosis...,在30个真实场景上评估根因推理准确性、根因识别准确性和令牌消耗。,We evaluated the PRAXIS’s RCA effectiveness in terms of root cause accuracy and token consumptions across those 30 scenarios.,跨微服务和代码级别的依赖关系，涉及服务依赖图（SDG）和程序依赖图（PDG）。,PRAXIS employs an LLM-driven structured traversal over two types of graph: (1) a service dependency graph (SDG) that captures microservice-level dependencies; and (2) a hammock-block program dependence graph (PDG) that captures code-level dependencies for each microservice.,云服务可靠性、故障诊断、根因分析（RCA）、软件工程（SWE）和站点可靠性工程（SRE）。,"PRAXIS stands at the intersection of cloud incident diagnosis (an SRE task) and program analysis (an SWE task)... Code-Cloud-RCA Benchmark consisting of 30 different scenarios and each incorporating a unique software, configuration, deployment, or resource-related fault observed in the real-world...",真实世界的、具有挑战性的云故障场景，其故障链在多个微服务和代码依赖中交织。,PRAXIS can diagnose incidents whose fault chains interleave microservice and code dependencies across multiple hops... demonstrated on a set of 30 comprehensive real-world incidents...,未明确指定，但涉及微服务代码库，可能包含多种编程语言。,,包含30个不同的真实世界故障场景。,Code-Cloud-RCA Benchmark consisting of 30 different scenarios...,基于真实世界观察到的软件、配置、部署或资源相关故障，并注入到实时的Kubernetes云环境中。,"...each incorporating a unique software, configuration, deployment, or resource-related fault observed in the real-world [3]–[5], [10]–[13], and injected in a live Kubernetes cloud environment.",2025-12-26 (arXiv版本日期),arXiv:2512.22113v1  [cs.DC]  26 Dec 2025,官方自建，由论文作者团队编译。,PRAXIS is demonstrated on a set of 30 comprehensive real-world incidents that is being compiled into an RCA benchmark.,,,文中提及正在开源过程中，但未明确许可证。,...integrated into a comprehensive benchmark which is in the process of being open-sourced.,根因诊断，涉及从微服务级别到代码语句级别的多粒度分析。,"PRAXIS uniquely constructs the PDG using hammock blocks at different granularities spanning module, class, function, and statement levels...",根因推理准确率（%）、根因识别准确率（%）、每次成功诊断的令牌消耗量。,PRAXIS achieved a root-cause reasoning accuracy of 61.5% and a root-cause identification accuracy of 73.9%... reduced token consumption by 3.8× from 624.4k tokens to 166.5k tokens per successful diagnosis...,多模态输入，包括可观测性数据（日志、指标、事件、分布式追踪）、警报、服务依赖图（SDG）和程序依赖图（PDG）。,"PRAXIS uses cloud monitoring tools... to actively collect a set of automated and user-specified alerts... and observability data consisting of cloud application and microservice logs, distributed traces of microservice calls, Kubernetes events, and application metrics.",文本报告，详细说明故障的起源、传播和影响。,"...a final RCA report detailing the fault’s origin, propagation, and impact.",多模态数据（图、日志、警报）到诊断报告。,"Upon triggering by active golden-signal alert(s), PRAXIS presents the relevant data... to an LLM... PRAXIS calls the LLM to consolidate the per-microservice investigation decision into a final RCA report...",实时的Kubernetes云环境。,...injected in a live Kubernetes cloud environment.,专注于由代码和配置引起的云故障的根因分析（RCA）。基准包含30个真实世界场景，并注入到实时Kubernetes环境中。它评估代理在跨服务依赖图（SDG）和程序依赖图（PDG）的结构化遍历中的表现。,"Code-Cloud-RCA Benchmark consisting of 30 different scenarios and each incorporating a unique software, configuration, deployment, or resource-related fault observed in the real-world... and injected in a live Kubernetes cloud environment. PRAXIS employs an LLM-driven structured traversal over two types of graph: (1) a service dependency graph (SDG)... (2) a hammock-block program dependence graph (PDG)..."
2512.20823_output/content.md,NotSoTiny,"this paper introduces NotSoTiny, a benchmark that assesses LLM on the generation of structurally rich and context-aware RTL.",Yes,"this paper introduces NotSoTiny, a benchmark... To address these limitations, this work presents NotSoTiny, a new benchmark...",https://huggingface.co/datasets/HPAI-BSC/NotSoTiny-25-12,The dataset is publicly available on HuggingFace1. 1https://huggingface.co/datasets/HPAI-BSC/NotSoTiny-25-12,评估大语言模型在生成结构复杂、上下文感知的RTL（寄存器传输级）代码方面的能力，具体任务为模块补全。,"NotSoTiny is a novel benchmark for systematically evaluating LLM capabilities in RTL code generation, focusing on module completion.",RTL代码生成的功能正确性、上下文推理能力、对数据污染的抵御能力,assesses LLM on the generation of structurally rich and context-aware RTL... explicitly designed to be resilient to contamination.,基于仿真的测试台验证和基于等价性检查的形式化验证技术,Outputs generated by LLMs are then evaluated using both simulation-based and formal verification techniques.,多模块、层次化系统，需要根据设计上下文（其他模块和顶层逻辑）推断缺失模块的功能和接口,NotSoTiny requires the model to infer functionality and interface behavior solely from the surrounding implementation. This mirrors real-world development scenarios where new components must integrate seamlessly into existing systems.,数字硬件设计、RTL（寄存器传输级）设计,"Built from hundreds of actual hardware designs produced by the Tiny Tapeout community... for evaluating LLMs on structurally complex, context-aware RTL code-generation tasks",工程级、具有现实世界硬件设计的复杂性,NotSoTiny tasks are more challenging than prior benchmarks... emphasizing its effectiveness in overcoming current limitations of LLMs applied to hardware design,Verilog,translate high-level specifications... into Verilog... Each project is parsed to extract Verilog modules and testbenches,"包含1,114个去重后的模块补全任务，源自1,021个有效的硬件设计项目","We introduce NotSoTiny, a dataset of 1,114 deduplicated module-completion tasks... These files serve as the foundation for our contextual module completion tasks for the total number of 1,021 projects.",源自Tiny Tapeout社区的数百个实际硬件设计，这是一个开放的教育性多项目晶圆计划,"Built from hundreds of actual hardware designs produced by the Tiny Tapeout community... The benchmark is built from data derived from the Tiny Tapeout project, a collaborative initiative that allows hardware designers to submit open-source digital circuit designs for fabrication",2025-12-23 (arXiv提交日期),arXiv:2512.20823v1 [cs.AR] 23 Dec 2025,官方自建，通过自动化流水线从开源社区数据中构建,"our automated pipeline removes duplicates, verifies correctness and periodically incorporates new designs... We develop an end-to-end construction pipeline, including crawling, merging, task generation, and near-duplicate filtering",抗污染设计，通过定期纳入新设计和去重来缓解污染风险,explicitly designed to be resilient to contamination... periodically incorporates new designs to mitigate contamination... Min-K analysis [19] further shows low contamination risk,,,模块补全,"focusing on module completion... For each task, the body of a single module is removed and designated as the target for completion",功能正确性（基于形式化等价性检查）,state-of-the-art LLMs achieve only 20% functional correctness under formal equivalence,代码（Verilog模块上下文）,the remaining modules in the file are retained as context... presents the model with a realistic design context,代码（缺失的Verilog模块）,asks it to produce the missing module’s code,代码到代码,module completion... produce the missing module’s code,,,1) 规模大且结构复杂，源自真实流片设计；2) 评估方法严谨，结合仿真与形式化验证；3) 具有抗污染和定期更新（Living）的特性，与Tiny Tapeout发布计划同步。,"1) A large and structurally rich RTL benchmark. 2) A rigorous and scalable evaluation methodology. 3) A contamination-resilient, periodically updated benchmark pipeline."
2512.23631_output/content.md,"SWE-bench-Verified, SWE-bench-Live","On SWE-bench-Verified [25], a benchmark for software engineering tasks grounded in real GitHub issues. On SWE-bench-Live [30], which includes more recently collected issues and presents out-of-distribution challenges","No, 本文是使用该数据集进行评测","We evaluate our BOAD on SWE-bench-Verified [25], a benchmark for software engineering tasks grounded in real GitHub issues. On SWE-bench-Live [30], which includes more recently collected issues and presents out-of-distribution challenges",,,解决现实世界中的软件工程问题，具体是解决GitHub上的真实issue。,"We study the problem of using LLMs to resolve real-world GitHub issues, where each issue consists of a textual description and a corresponding code repository.",软件工程任务解决能力，特别是对长视野、分布外问题的泛化能力。,"Large language models (LLMs) have shown strong reasoning and coding capabilities, yet they struggle to generalize to real-world software engineering (SWE) problems that are long-horizon and out of distribution.",提交补丁并通过所有测试。,"Rewards are sparse: r(st, at) = 0 for t < T, r(sT , aT ) = 1 if y passes all tests, 0 otherwise.",多文件项目，需要浏览和修改代码库中的相关部分。,"Since issues are not self-contained, solving them requires identifying and modifying relevant parts of the codebase.",软件工程，涉及bug修复、代码修改等。,a benchmark for software engineering tasks grounded in real GitHub issues.,现实世界、长视野、分布外的挑战性问题。,real-world software engineering (SWE) problems that are long-horizon and out of distribution.,文中未明确提及，但基于GitHub issues，可能涉及多种编程语言。,,,,来自真实GitHub issues及其对应的代码仓库。,a benchmark for software engineering tasks grounded in real GitHub issues.,,,,,SWE-bench-Live包含新近收集的、分布外的问题，旨在测试泛化能力。,"On SWE-bench-Live [30], which includes more recently collected issues and presents out-of-distribution challenges",,,端到端的软件工程问题解决，包括定位、编辑、验证等多个子任务。,"Solving a software engineering task requires handling multiple sub-tasks—such as locating relevant files, editing code, and running tests",补丁通过率（是否通过所有测试）。,"r(sT , aT ) = 1 if y passes all tests, 0 otherwise.",自然语言（issue描述）和代码（代码库）。,each issue consists of a textual description and a corresponding code repository.,代码补丁。,when the agent submits a patch y,文本（issue描述）和代码（代码库）到代码（补丁）。,"using LLMs to resolve real-world GitHub issues, where each issue consists of a textual description and a corresponding code repository.",交互式运行时环境，代理可以浏览文件、执行命令、运行测试。,"an LLM interacts with a runtime environment through tool use. Such agents can browse files, execute shell commands, run tests, and edit code directly",基于真实GitHub issue，任务非自包含，需要与代码库交互，是长视野、交互式的端到端软件工程任务基准。包含已验证的（Verified）和持续更新的（Live）两个版本。,"a benchmark for software engineering tasks grounded in real GitHub issues. Since issues are not self-contained, solving them requires identifying and modifying relevant parts of the codebase. On SWE-bench-Live [30], which includes more recently collected issues and presents out-of-distribution challenges"
2512.24796_output/content.md,LeanCat (Part I: 1-Categories),LEANCAT: A BENCHMARK SUITE FOR FORMAL CATEGORY THEORY IN LEAN (PART I: 1-CATEGORIES),Yes,"we introduce LeanCat, a Lean benchmark for category-theoretic formalization",https://github.com/sciencraft/LeanCat,LeanCat (Part I: 1-Categories) is open sourced at https://github.com/sciencraft/LeanCat.,评测模型在形式化定理证明方面的能力，特别是在范畴论这一高度抽象的数学领域内，基于成熟的Lean库（mathlib）进行库驱动的、抽象推理的能力。,designed to test whether automated provers can operate inside a mature library and compose high-level abstractions rather than solve isolated puzzles.,形式化定理证明能力，特别是库驱动的抽象推理、接口级推理、长程规划以及从自然语言到形式化语言的转换能力。,"serving as a stress test of structural, interface-level reasoning... highlighting a pronounced natural-to-formal bottleneck.",使用pass@k指标进行评估，文中报告了pass@1和pass@4的结果。,The best model solves 8.25% of tasks at pass@1... and 12.00% at pass@4,语句级任务，每个问题是一个独立的定理声明，所有必要的定义都存在于Lean环境中（在Mathlib中或作为问题设置的一部分引入）。,Each LeanCat problem is self-contained at the statement level... all necessary definitions exist in Lean’s environment... LeanCat is a statement-level benchmark: each item consists of a single standalone theorem to prove,范畴论，一个高度抽象的数学领域，作为现代数学结构的统一语言和证明工程的核心层。,a Lean benchmark for category-theoretic formalization—a unifying language for mathematical structure and a core layer of modern proof engineering,分为三个难度等级：简单、中等、高。具体分布为20/42/38个问题。,Easy/Medium/High labels (20/42/38),Lean 4 (形式化证明语言)，依赖mathlib库。,formalized in Lean 4 (mathlib)... a set of 100 category-theory problems formalized in Lean 4 (mathlib 4.19.0).,包含100个完全形式化的范畴论定理声明（语句级任务）。,"LeanCat comprises 100 theorem statements in category theory, each fully formalized in Lean 4","问题主要来源于标准的范畴论教科书（如Riehl, Mac Lane）、专著（如Adámek et al.），以及少量来自未发表的讲义和研究论文。","Problems are primarily drawn from standard, widely used textbooks in category theory... together with a small number of problems adapted from unpublished lecture notes... also include selected problems inspired by research papers and advanced community-driven references",2025-12-31 (根据arXiv版本号推断),arXiv:2512.24796v1  [cs.LO]  31 Dec 2025,官方自建，由范畴论专家和Lean专家通过三阶段工作流程（收集、形式化、评审）精心策划和构建。,"LeanCat is curated through a three-stage workflow that combines expert selection, LLM-assisted drafting, and rigorous human verification",,,,,定理证明（语句级任务），即给定一个形式化的定理声明，要求生成完整的证明。,LeanCat is a statement-level benchmark: each item consists of a single standalone theorem to prove,"pass@1, pass@4",The best model solves 8.25% of tasks at pass@1... and 12.00% at pass@4,自然语言问题描述（以LaTeX注释形式提供）和形式化的Lean定理声明。,the natural-language problem description (in LATEX) is included as a comment immediately preceding the formal statement,形式化的Lean证明代码。,generating compilable Lean code,自然语言与形式化语言到形式化证明代码,highlighting a pronounced natural-to-formal bottleneck,Lean 4编译环境，依赖mathlib 4.19.0库。,formalized in Lean 4 (mathlib 4.19.0),1. 专注于范畴论这一高度抽象的数学领域，作为库驱动抽象推理的压力测试。2. 问题有时会超越现有库（mathlib）的范围，迫使形式化者设计中间结果，对自动化证明器构成严格测试。3. 采用结合LLM辅助和人类专家评级的系统化难度标注流程。4. 是系列基准的第一部分（1-范畴），计划未来扩展到更丰富的结构。,"This aspect of LeanCat, that it sometimes goes beyond the library, makes it a particularly stringent test for automated provers... We implemented a systematic LLM+human rating pipeline to grade problem difficulty... LeanCat is the first installment of a category-theory benchmark series."
2512.24630_output/content.md,AIDev-POP,We used the AIDev-POP subset of version v3 of the AIDev dataset [25],"No, 本文是使用该数据集进行评测","Motivated by these gaps, we answer the following three research questions by analyzing a set of AI-generated performance-related pull requests as provided by the publicly available AIDev dataset [25].",https://github.com/SQMLab/LLM-performance,"To enable replication, we share our data and code publicly.1 1https://github.com/SQMLab/LLM-performance",该数据集旨在分析AI代理在真实软件开发工作流中生成的、与性能优化相关的Pull Requests（PRs），以研究AI代理如何解决性能问题。,"In this paper, we present an empirical study of performance-related pull requests generated by AI agents.",AI代理的性能优化行为、PR接受率、评审时间、与软件开发生命周期（SDLC）活动的关联,Our findings provide empirical evidence that can support the evaluation and improvement of agentic AI systems with respect to their performance optimization behaviors and review outcomes.,基于BERTopic的主题建模、统计分析（Wilcoxon秩和检验、Cliff's delta效应量、Kendall's τ相关系数）、人工标注与一致性检验,"We applied the BERTopic [16] due to its superior performance over traditional methods... In this paper, we use the non-parametric Wilcoxon rank-sum test, Cliff’s delta effect size, and Kendall’s 𝜏 correlation coefficients...",真实世界软件项目的Pull Request上下文,"We used the AIDev-POP subset... which comprises 33,596 agentic pull requests... captures higher-quality, real-world projects",软件工程、性能优化,"Performance, Pull Request, Agentic AI, LLM, BERTopic",,,,,"包含33,596个AI代理生成的Pull Requests，从中识别出1,221个与性能相关的PRs，涉及447个代码仓库。","We used the AIDev-POP subset of version v3 of the AIDev dataset [25]... which comprises 33,596 agentic pull requests... This process identified 1,160 performance-related PRs across 447 repositories... yielding a final dataset of 1,221 performance PRs",来自AIDev数据集v3版本的AIDev-POP子集，该子集包含来自真实世界项目的AI代理生成的Pull Requests。,"We used the AIDev-POP subset of version v3 of the AIDev dataset [25], obtained from Zenodo [24]",2025,arXiv:2512.24630v1  [cs.SE]  31 Dec 2025,社区贡献（来自AIDev数据集）,analyzing a set of AI-generated performance-related pull requests as provided by the publicly available AIDev dataset [25],,,,,Pull Request级别的分析,an empirical study of performance-related pull requests generated by AI agents,PR拒绝率、合并时间、主题分布、与SDLC活动的关联,We investigate whether this trend also applies to performance-related PRs... analyze their time to merge... Association with SDLC Activities,Pull Request的标题和正文文本,"Combining the title and body of a PR, we generated embeddings",性能优化主题类别、统计结果,We identified 52 performance-related topics grouped into 10 higher-level categories... Our results show that AI agents apply performance optimizations across diverse layers of the software stack,文本到分析,"Using LLM-assisted detection and BERTopic-based topic modeling, we identified 52 performance-related topics",,,专注于AI代理在真实世界开发工作流中生成的性能相关Pull Requests，而非受控环境下的代码生成结果。数据集包含PR元数据（接受状态、评审时间、SDLC活动）。,"However, existing studies largely rely on outcome-based evaluations conducted in controlled settings [1, 12], providing limited insight into how agentic AI systems address performance concerns within real-world development workflows... it provides the metadata required for RQ2 and RQ3, including PR acceptance status, review timing, and SDLC activities."
2601.00635_output/content.md,SEMODS,"To address this gap, we present SEMODS: an SE-focused dataset of 3,427 models extracted from HF, combining automated collection with rigorous validation through manual annotation and large language model assistance.",Yes,"To address this gap, we present SEMODS: an SE-focused dataset of 3,427 models extracted from HF...","Zenodo [7] (A snapshot of the dataset (November 2025 release), containing 3,427 SE models and their associated attributes, is publicly available in Zenodo [7].)","Data availability: A snapshot of the dataset (November 2025 release), containing 3,427 SE models and their associated attributes, is publicly available in Zenodo [7].",该数据集旨在为软件工程领域提供一个经过验证的、专注于SE任务的预训练模型目录，以支持模型发现、分析、基准测试和模型适应。,"...a dataset of SE models, systematically collected, processed, and validated to support their in-depth analysis, efficient discovery, and practical use within SE contexts. The dataset enables researchers and practitioners to explore models tailored to specific SE tasks... while also supporting benchmarking, and facilitating the identification of models for model adaptation.",模型与软件工程任务的关联性、模型元数据（如许可证、库、训练数据集）、模型评估结果的标准化表示。,"Our dataset links models to SE tasks and activities from the software development lifecycle, offering a standardized representation of their evaluation results...",文中未明确描述针对该数据集本身的评估方法。该数据集本身是一个模型目录，其价值在于组织和标准化模型信息，而非直接评估模型性能。,,不适用。该数据集是模型元数据目录，不涉及代码生成或修复的上下文。,,软件工程（涵盖需求工程、软件设计、软件实现、软件质量保证、软件维护五个生命周期阶段）。,...catalogued according to SE activities and tasks across the SDLC.,不适用。该数据集是模型目录，不包含编程问题。,,不适用。该数据集收录的模型可能支持多种编程语言，但数据集本身不限定语言。,,"包含3,427个与软件工程相关的模型。","...an SE-focused dataset of 3,427 models extracted from HF...",从Hugging Face平台通过其API系统收集的模型，结合了手动标注和大型语言模型辅助的严格验证。,"...3,427 models extracted from HF, combining automated collection with rigorous validation through manual annotation and large language model assistance.",2025年11月（数据集快照版本），论文发表于2026年。,Data availability: A snapshot of the dataset (November 2025 release)...,官方自建（由研究团队构建和维护）。,"To address this gap, we present SEMODS...",文中未提及。,,文中未明确提及数据集的许可证。,,不适用。该数据集的任务是模型分类和编目，而非代码生成等任务。,,不适用。该数据集本身不定义评估指标，而是标准化存储模型自带的评估结果（如基准测试分数）。,,不适用。该数据集的输入是模型元数据。,,不适用。该数据集的输出是结构化的模型目录信息。,,不适用。该数据集是用于模型发现和分析的资源目录，而非一个具有输入输出定义的任务。,,不适用。该数据集不涉及代码执行。,,"1. 专注于软件工程领域的模型目录。2. 将模型映射到软件开发生命周期中的具体活动和任务（基于包含147个任务的分类法）。3. 提供了模型评估结果的标准化表示。4. 包含自动化流程以保持数据集与Hugging Face新模型的同步更新。5. 数据集设计包含专门的实体（如SETask, Benchmark, Metric）以支持SE模型的开发和评估。","Compared to prior work, we have specifically curated and validated models with explicit relevance to SE, introducing novel mappings to SE tasks and standardized benchmarking data. ... This design introduces novel entities specific to SE models (e.g., SETask) and their standardized evaluation tables (e.g., Benchmark, Metric), enabling the development and assessment of SE models."
2601.00559_output/content.md,oHC/IoTB dataset 和 Mutation dataset,assessing their performance on both the original openHAB (oHC/IoTB) dataset and a structurally challenging Mutation dataset designed to test robustness under rule transformations.,"No, 本文是使用该数据集进行评测","This work presents the first comprehensive evaluation of Large Language Models (LLMs) across a multi-category interaction threat taxonomy, assessing their performance on both the original openHAB (oHC/IoTB) dataset and a structurally challenging Mutation dataset designed to test robustness under rule transformations.",https://github.com/JasonQuantrill/llm-v-static-results,The data pertaining to this research can be found at https://github.com/JasonQuantrill/llm-v-static-results,"检测智能家居IoT平台（如openHAB）中基于Trigger-Action-Condition (TAC)规则的自动化规则之间的交互威胁（Rule Interaction Threats, RITs）。这些威胁是由于规则间的隐式依赖、冲突触发或重叠条件导致的意外或不安全行为。","Identifying these threats requires semantic understanding and structural reasoning that traditionally depend on symbolic, constraint-driven static analysis. ... RITs occur when the interplay of multiple rules leads to unintended behaviors, such as security vulnerabilities or functional failures.",语义理解能力、跨规则的结构化推理能力、在规则变换下的鲁棒性,"assessing their performance on both the original openHAB (oHC/IoTB) dataset and a structurally challenging Mutation dataset designed to test robustness under rule transformations. ... their accuracy degrades significantly for threats requiring cross-rule structural reasoning, especially under mutated rule forms.",将LLM的检测结果与手动验证的基准真值（oHIT's manually validated ground truth）进行比较，评估其准确性。,comparing their results against oHIT’s manually validated ground truth.,多规则交互。需要分析多个TAC规则之间的语义和结构关系。,"Detecting these threats is challenging, as it requires a deep understanding of both the semantic and structural relationships between rules.",智能家居物联网（IoT）安全、自动化规则分析,Cracking IoT Security: Can LLMs Outsmart Static Analysis Tools? ... Smart home IoT platforms such as openHAB rely on Trigger–Action–Condition (TAC) rules to automate device behavior,工程级，涉及真实世界智能家居设置中的复杂规则交互。,many arise simply from complex rule interplay in real-world smart home setups.,Xtend（一种基于Java的领域特定语言，用于编写openHAB规则）,"These rules are authored in the Xtend programming language [3], which offers a more streamlined and expressive syntax compared to standard Java",,,基于真实openHAB平台（oHC/IoTB数据集）以及通过规则变换生成的突变数据集（Mutation dataset）。,assessing their performance on both the original openHAB (oHC/IoTB) dataset and a structurally challenging Mutation dataset designed to test robustness under rule transformations.,2026（根据arXiv版本日期推断）,arXiv:2601.00559v1  [cs.CR]  2 Jan 2026,,,,,,,规则交互威胁分类（多标签分类），包括细粒度类别区分。,"In this work, we evaluate a representative set of state-of-the-art LLMs across all experimental conditions, including multi-label RIT classification, fine-grained category differentiation",准确性（与基准真值比较）,comparing their results against oHIT’s manually validated ground truth.,代码（Xtend语言编写的TAC规则）,These rules are authored in the Xtend programming language,"分类标签（威胁类别，如Action Contradiction, Trigger Cascade, Condition Cascade及其强弱子类）","multi-label RIT classification, fine-grained category differentiation",代码到分类,assessing their performance on ... classifying Rule Interaction Threats (RITs) in real-world openHAB datasets,,,1. 专注于IoT自动化规则（TAC规则）的安全威胁检测。2. 包含一个用于测试鲁棒性的“突变数据集”（Mutation dataset），该数据集通过规则变换生成，旨在测试模型在结构扰动下的表现。3. 威胁分类基于一个整合的、多类别的交互威胁分类法，包括动作矛盾（AC）、触发级联（TC）和条件级联（CC），每种又有强弱子类。,"a structurally challenging Mutation dataset designed to test robustness under rule transformations. ... we consolidated these perspectives into three foundational categories of rule interaction threats. ... each can manifest in different forms depending on how rules depend on, trigger, or contradict one another."
2601.00753_output/content.md,AIDev dataset,"We use the AIDev dataset v1.0 [1]: 33,707 agent-authored PRs from 2,807 repositories (>100 stars), identified via AIDev metadata (type='Bot') plus generative agent names (Codex, Claude, Devin, Copilot), excluding deterministic bots.","No, 本文是使用该数据集进行评测","We use the AIDev dataset v1.0 [1]: 33,707 agent-authored PRs from 2,807 repositories (>100 stars), identified via AIDev metadata (type='Bot') plus generative agent names (Codex, Claude, Devin, Copilot), excluding deterministic bots.",https://zenodo.org/records/17993901,Artifact: https://zenodo.org/records/17993901.,该数据集用于研究AI代理生成的Pull Request（PR）的审查工作量预测和代理“幽灵化”（ghosting）行为。,This paradigm shift raises a critical question: can we predict which agent-generated PRs will consume excessive review effort before any human interaction begins? ... We operationalize agentic ghosting and quantify its prevalence.,AI代理生成的Pull Request的审查工作量预测、代理“幽灵化”（放弃响应）行为分析,Research Questions. RQ1: Can creation-time structural signals predict high-effort PRs? RQ2: Which early cues correlate with agentic ghosting?,使用AUC（Area Under the Curve）和PR-AUC（Precision-Recall AUC）评估二元分类模型性能，以预测高成本PR（审查工作量前20%）。,"LightGBM model reaches AUC 0.9571 [0.955, 0.962] (temporal split) with PR-AUC 0.8812... We benchmarked LightGBM against 5 alternatives (Stacking, Voting, HistGradient, MLP); the best (Stacking) matches LightGBM at AUC 0.957 (temporal) and 0.834 (repo-disjoint)...",基于Pull Request的上下文，包括代码变更、文件类型、PR描述等。,"We extract 35 features across Intent, Context, and Complexity at two stages: T0 (Creation-Time) captures signals available at PR submission (Complexity: additions, deletions, changed files, entropy; Intent: body length, has plan; Context: language, agent, file types)...",软件工程、代码审查、AI代理协作,"As autonomous AI agents transition from code completion tools to full-fledged teammates capable of opening pull requests (PRs) at scale, software maintainers face a new challenge: not just reviewing code, but managing complex interaction loops with non-human contributors.",工程级，涉及真实世界GitHub仓库中AI代理提交的Pull Request的审查过程。,"Analyzing 33,707 agent-authored PRs from the AIDev dataset [1] across 2,807 repositories... We target review effort (comment/review volume) rather than latency (time-to-merge); this choice reflects maintainer attention cost directly.",文中未明确提及数据集涵盖的具体编程语言，仅作为特征之一（Context: language）。,"We extract 35 features across Intent, Context, and Complexity at two stages: T0 (Creation-Time) captures signals available at PR submission (... Context: language, agent, file types)...","包含33,707个由AI代理生成的Pull Request，来自2,807个仓库（星标数>100）。","We use the AIDev dataset v1.0 [1]: 33,707 agent-authored PRs from 2,807 repositories (>100 stars)...","来自AIDev数据集，该数据集收集了GitHub上由AI代理（如Codex, Claude, Devin, Copilot）生成的Pull Request。","We use the AIDev dataset v1.0 [1]: 33,707 agent-authored PRs from 2,807 repositories (>100 stars), identified via AIDev metadata (type='Bot') plus generative agent names (Codex, Claude, Devin, Copilot), excluding deterministic bots.",v1.0（版本号），具体年份未明确提及。,We use the AIDev dataset v1.0 [1]...,官方自建（AIDev数据集）,We use the AIDev dataset v1.0 [1]...,,,,,Pull Request级别的审查工作量预测与行为分析,We develop a Circuit Breaker triage model that predicts high-review-effort PRs (top 20% by effort score) at creation time.,"AUC, PR-AUC, Brier Score, Effort Coverage（审查工作量覆盖率）","LightGBM model reaches AUC 0.9571 [0.955, 0.962] (temporal split) with PR-AUC 0.8812... The model is well-calibrated after Platt Scaling (Brier Score [19] = 0.1279); at 20% budget: 67% coverage...",Pull Request的元数据和代码变更特征（如补丁大小、修改文件数、文件类型、PR正文长度、是否有计划等）,"We extract 35 features across Intent, Context, and Complexity at two stages: T0 (Creation-Time) captures signals available at PR submission (Complexity: additions, deletions, changed files, entropy; Intent: body length, has plan; Context: language, agent, file types)...",二元分类标签（高成本PR与否）或代理幽灵化预测,We frame triage as binary classification targeting High Cost PRs (top 20% by effort score = total review + comment count including human and bot messages)... Target Definition: High Cost - Top 20% of PRs by Effort Score...,Pull Request特征到审查工作量/行为预测,We develop a Circuit Breaker triage model that predicts high-review-effort PRs (top 20% by effort score) at creation time.,,,专注于AI代理（而非人类）生成的Pull Request；研究“代理幽灵化”（agentic ghosting）现象，即代理提交变更后无响应地放弃；揭示AI代理PR的“双峰行为模式”：即时合并（28.3%）与迭代失败。,"Analyzing 33,707 agent-authored PRs from the AIDev dataset [1] across 2,807 repositories, we uncover a striking two-regime behavioral pattern that fundamentally distinguishes autonomous agents from human developers... We observe substantial rates of agentic ghosting-abandonment without explanation-where agents submit changes but fail to respond to feedback."
2601.00482_output/content.md,Co-renameBench,"• Benchmark. We create Co-renameBench, a new dataset of post-2025 coordinated renaming to enable uncontaminated evaluation of future LLM-based tools.",Yes,"• Benchmark. We create Co-renameBench, a new dataset of post-2025 coordinated renaming to enable uncontaminated evaluation of future LLM-based tools.",,,评测基准旨在评估自动化工具（特别是基于LLM的代理）在代码重构任务中执行协调重命名（coordinated renaming）的能力。协调重命名是指对一个标识符（如类名、方法名）的重构会触发对多个语义相关的程序元素进行类似的重命名操作，以保持概念一致性。,"Coordinated renaming, where a single rename refactoring triggers refactorings in multiple, related identifiers, is a frequent yet challenging task.",协调重命名的准确性与完整性,"To compare with the state-of-the-art approaches [10], we first tested on the established RenasBench [10], a benchmark containing 1349 renames across 161 co-rename sets. On this benchmark, CoRenameAgent achieves an F1-score of 54.6%, a 2.3× improvement over the best baseline.",使用F1分数（精确率和召回率的调和平均数）进行评估,"On this benchmark, CoRenameAgent achieves an F1-score of 54.6%, a 2.3× improvement over the best baseline.",多文件项目,"Developers must manually propagate these rename refactorings across numerous files and contexts, a process that is both tedious and highly error-prone.",软件工程，代码重构,"In this paper, we answer such questions in the context of coordinated refactoring, where a single refactoring triggers a set of similar refactorings on semantically related program elements.",工程级，涉及跨多个文件和上下文的复杂语义关联,"Coordinated renaming is a frequent yet challenging task. Developers must manually propagate these rename refactorings across numerous files and contexts, a process that is both tedious and highly error-prone.",Java,"Finally, to validate its real-world utility, we used it to recommend other co-renames starting from single renames in active open-source projects. Using CoRenameAgent-generated renames, we submitted 10 pull requests for active open-source projects; their developers have already accepted and merged 5, while 2 were rejected for socio-technical reasons and 3 are still under review.",包含1573个近期重命名操作的新基准,"Second, on our new, uncontaminated benchmark of 1573 recent renames, it demonstrates a 3.1× F1-score improvement.",从开源项目的真实提交（commits）中收集,"We first conducted a formative study on the practice of coordinated renaming in 609K commits in 100 open-source projects and surveyed 205 developers, and then we implemented these ideas into CoRenameAgent.",2025年之后,"• Benchmark. We create Co-renameBench, a new dataset of post-2025 coordinated renaming to enable uncontaminated evaluation of future LLM-based tools.",官方自建,"• Benchmark. We create Co-renameBench, a new dataset of post-2025 coordinated renaming to enable uncontaminated evaluation of future LLM-based tools.",抗污染设计，专门收集2025年后的数据以避免LLM训练数据污染,"Thus, we curated a new, uncontaminated benchmark of recent co-renames. On this benchmark, CoRenameAgent achieves an F1-score of 48.5%, showing a 3.1× improvement over the previous state-of-the-art.",,,代码重构（重命名）,"Coordinated renaming, where a single rename refactoring triggers refactorings in multiple, related identifiers, is a frequent yet challenging task.",F1分数,"On this benchmark, CoRenameAgent achieves an F1-score of 54.6%, a 2.3× improvement over the best baseline.",代码（初始重命名操作）,A developer initiates in the IDE the desired design change (such as renaming a class) that serves as a signal of refactoring intent.,代码（一组协调的重命名操作）,"Coordinated renaming, where a single rename refactoring triggers refactorings in multiple, related identifiers",代码到代码,"Coordinated renaming, where a single rename refactoring triggers refactorings in multiple, related identifiers",,,1. 专门针对协调重命名（coordinated renaming）这一特定且复杂的重构任务。2. 数据收集时间在2025年之后，旨在避免LLM训练数据污染，为未来工具提供干净的评估基准。3. 基于对609K次提交和100个开源项目的实证研究构建。,"• Benchmark. We create Co-renameBench, a new dataset of post-2025 coordinated renaming to enable uncontaminated evaluation of future LLM-based tools."
2601.02941_output/content.md,SASTBENCH,"We introduce SASTBENCH, a benchmark for evaluating SAST triage agents...",Yes,"In this paper, we introduce SASTBENCH, a new benchmark designed specifically to evaluate the ability of LLM agents to classify SAST findings.",,,评估LLM智能体对静态应用安全测试（SAST）工具发现的分类能力，即区分真实漏洞（真阳性）与误报（假阳性）。,...evaluating SAST triage agents... to classify SAST findings... distinguish true vulnerabilities from false alarms.,SAST误报分类的准确性、智能体在真实SAST发现分布下的性能,...evaluate the ability of LLM agents to classify SAST findings... aims to minimize the simulation-reality gap prevalent in existing benchmarks.,智能体在隔离环境中访问代码库，接收潜在漏洞代码位置列表，返回二元判断（真阳性或假阳性），基于其预测进行评估。,"The agent is loaded into an isolated environment with a target repository at a static path, and it is given a list of potentially vulnerable code sites, required to return a binary verdict: true positive or false positive. This design allows SASTBENCH to serve as a neutral testing ground for comparing diverse agentic design choices...",多文件项目级上下文，智能体拥有对代码库的完全访问权限以进行分析。,Agents have full access to the codebase and full freedom to explore the environment...,网络安全、漏洞分析、静态应用安全测试（SAST）,SAST (Static Application Security Testing) tools are among the most widely used techniques in defensive cybersecurity... a benchmark for evaluating SAST triage agents...,工程级、对抗性分布，旨在模拟真实世界中SAST工具产生的难以分类的发现。,"...samples from DSAST make classification harder by design, as they are selected by their confusing, apparent vulnerability to SAST tools... forms a hard negative class.",多种编程语言，旨在反映现实世界中的语言分布，特别包含如PHP、Go、Javascript/TypeScript等在现代目标系统和易受攻击系统中常见但其他基准常忽略的语言。,"Language Diversity: Datasets often concentrate on 1-4 programming languages. This does not reflect the language distribution in the wild... most datasets don’t contain PHP, though it is one of the languages with most vulnerabilities and most SAST findings...",,,真实数据。真阳性来源于国家漏洞数据库（NVD）中引用GitHub提交的CVE；假阳性来源于在包含CVE的代码库上运行开源SAST工具（semgrep）的发现。,True Positives (Vulnerabilities). We mine Common Vulnerabilities and Exposures (CVEs) from the National Vulnerability Database (NVD) that reference commits on GitHub... False Positives (SAST Findings). We execute a popular open-source SAST tool – semgrep’s free edition – on the pre-fix versions of the repositories containing our curated CVEs.,2026年1月（论文版本日期），基准设计支持持续更新。,arXiv:2601.02941v1 [cs.CR] 6 Jan 2026... the automated nature of our benchmark enables continuous updates...,官方自建，由论文作者构建并发布。,We introduce SASTBENCH... We open-source our code and data...,抗污染设计，通过设置知识截止日期（如2025年2月）来筛选CVE，以避免模型在训练数据中见过这些漏洞。,"To avoid contamination, we keep CVEs only if they were reported after a knowledge cutoff period. In the instantiation used in this paper, we set it to February 2025, which is after the knowledge cutoff of all the models tested herein.",,,代码分类（具体为安全发现分类）,...classify SAST findings... return a binary verdict: true positive or false positive.,精确度和召回率,We find that stronger models tend to perform better in terms of both precision and recall...,代码与元数据（代码库访问权限、潜在漏洞位置列表、文件路径、行号、CWE ID等）,The agent is loaded into an isolated environment with a target repository... and it is given a list of potentially vulnerable code sites... Each entry is a list of key-value dictionaries. Keys are detailed in Table 2.,分类标签（二元判断）,...required to return a binary verdict: true positive or false positive.,代码与元数据到分类标签,"Agents have full access to the codebase... given a list of potentially vulnerable code sites, required to return a binary verdict...",隔离的Docker环境，包含目标代码库。,The agent is loaded into an isolated environment with a target repository at a static path... Competitors submit their agent as an arbitrary ZIP with a Dockerfile...,1. 专为SAST误报分类（而非通用漏洞检测）设计。2. 结合真实CVE作为真阳性和过滤后的SAST工具发现作为近似假阳性。3. 智能体无关的设计，模拟真实智能体工作流。4. 注重语言多样性和真实的数据分布。5. 采用版本控制（如SASTBENCH-v0.1）和可配置的知识截止日期以支持持续更新和抗污染。,"SASTBENCH aims to minimize the simulation-reality gap... combines real CVEs as true positives with filtered SAST tool findings as approximate false positives. SASTBENCH features an agent-agnostic design... following agentic benchmarks like SWE-Bench... To avoid contamination, we keep CVEs only if they were reported after a knowledge cutoff period... we use version numbers to help us keep track of changes... SASTBENCH-v0.1."
2601.02736_output/content.md,AIOps 2022 dataset,Preliminary experiments on the AIOps 2022 dataset demonstrate that SpecRCA achieves superior accuracy and efficiency compared to existing approaches...,"No, 本文是使用该数据集进行评测",We conduct preliminary experiments on the AIOps 2022 dataset.,,,微服务系统的根因分析（RCA），旨在从异常的系统信号（指标、追踪、日志）中定位故障源头并解释其机制。,"Ensuring system reliability therefore hinges on effective root cause analysis (RCA), which entails not only localizing the source of anomalies but also characterizing the underlying failures in a timely and interpretable manner.",故障定位的准确性和诊断推理的效率,SpecRCA achieves superior accuracy and efficiency compared to existing approaches...,在数据集上进行实验，比较故障定位的准确性和生成诊断报告的时间（延迟）。,"Preliminary experiments on the AIOps 2022 dataset demonstrate that SpecRCA achieves superior accuracy and efficiency compared to existing approaches... In terms of accuracy, SpecRCA surpasses the state-of-the-art methods in failure localization by approximately 12.14%. In terms of efficiency, SpecRCA is able to generate a complete diagnosis report within 20 seconds...",微服务系统上下文，依赖服务拓扑、时序指标、分布式追踪和日志序列。,"Upon detecting an anomaly in a microservice system, the framework collects abnormal operational data and a preceding segment of normal data (metrics, traces, and logs) to establish a behavioral baseline.",云计算、微服务、系统运维（AIOps）,Microservice systems have become the backbone of cloud-native enterprise applications... Ensuring system reliability therefore hinges on effective root cause analysis (RCA)...,,,,,,,,,2022,Preliminary experiments on the AIOps 2022 dataset...,,,,,,,根因分析（故障定位与解释）,"...root cause analysis (RCA), which entails not only localizing the source of anomalies but also characterizing the underlying failures...",故障定位准确率（百分比提升）和诊断报告生成延迟（秒）,"In terms of accuracy, SpecRCA surpasses the state-of-the-art methods in failure localization by approximately 12.14%... SpecRCA is able to generate a complete diagnosis report within 20 seconds...",多模态系统数据：指标（Metrics）、追踪（Traces）、日志（Logs）,"...collects abnormal operational data and a preceding segment of normal data (metrics, traces, and logs)...",诊断报告（自然语言解释）,"...outputs a coherent, interpretable diagnosis report.",多模态系统数据到诊断报告,"The framework collects abnormal operational data... outputs a coherent, interpretable diagnosis report.",,,专注于微服务环境下的实时根因分析，数据集包含真实的运维数据（指标、追踪、日志）。,"SpecRCA, a speculative root cause analysis framework for microservices... Preliminary experiments on the AIOps 2022 dataset..."
2601.02868_output/content.md,CodeIF-Bench,Experimental results on the instruction-following benchmark CodeIF-Bench and the code generation benchmark CoderEval demonstrate that CODEMEM achieves state-of-the-art performance...,"No, 本文是使用该数据集进行评测",Experiments on instruction following benchmark CodeIF-Bench for iterative code generation and the extended multi-turn repository-level code generation benchmark CoderEval demonstrate that CODEMEM improves current-turn and session-level instruction following...,,,评测模型在仓库级别、多轮交互式代码生成任务中的指令遵循能力。,Experiments on instruction following benchmark CodeIF-Bench for iterative code generation...,指令遵循能力（当前轮次和会话级别）,improving instruction following by 12.2% for the current turn and 11.5% for the session level,通过执行关联的测试套件来定量评估生成代码的正确性。,"After each interaction round, the generated code is executed against a test suite to evaluate compliance with the current instruction.",仓库级别（repository-level），需要利用仓库内的上下文信息。,Experiments on instruction following benchmark CodeIF-Bench for iterative code generation and the extended multi-turn repository-level code generation benchmark CoderEval...,,,,,文中仅提及实验示例为Python，未描述完整数据集,Instruction-1: Please write a python function called 'set_status'...,,,,,,,,,,,,,迭代式代码生成（多轮交互，包含生成和修复）,We refer to this as repository-level iterative code generation.,指令遵循提升百分比（current-turn 和 session-level）,improving instruction following by 12.2% for the current turn and 11.5% for the session level,自然语言指令与代码仓库上下文,"Given a code repository, an LLM incrementally generates and refines code through multi-turn interactions with the user.",代码,LLM-generated answer A,文本（指令）到代码,"Given a code repository, user instruction I, and LLM-generated answer A",测试套件执行环境,"After each interaction round, the generated code is executed against a test suite...",专注于多轮交互式、仓库级别的代码生成任务，评估指令遵循能力。,"Recent benchmarks such as CodeIF-Bench (Wang et al., 2025a) and SR-Eval (Zhan et al., 2025) extend this setting to multi-round iterative scenarios..."
2601.03878_output/content.md,LiveCodeBench,Participants will solve medium-difficulty problems from the LiveCodeBench dataset,"No, 本文是使用该数据集进行评测",Participants will solve medium-difficulty problems from the LiveCodeBench dataset,https://huggingface.co/datasets/livecodebench/code_generation_lite,More details are reported in the HuggingFace dataset card3. The version of the dataset will be release_v5,代码生成。数据集包含从竞争性编程平台提取的高质量编程问题，用于评估模型生成正确代码的能力。,"The benchmark is continuously updated with new problems, providing detailed information for each problem, such as test cases, problem description, difficulty level, and solution.",代码生成正确性,The goal of this experiment is to evaluate whether an LLM-based TDD-inspired workflow is effective in generating correct code,执行单元测试,The extension logs time-stamped actions (produce/explain/regenerate tests; ask/regenerate function; re-run tests) and unit-test outcomes (per-test pass/fail).,,,算法、数据结构,"We will select a mix of problem types (e.g., algorithms, data structures) to evaluate different coding scenarios",入门级、中等难度,We will select problems labeled as easy and medium difficulty,文中未明确提及完整数据集的语言，仅提及实验使用Python。,"Confounding variables control for prior skill and habits (years of programming experience, Python familiarity, prior TDD experience, and prior use of LLMs for coding).",截至2025年1月，包含880个编程问题。,"The version of the dataset will be release_v5, containing a total of 880 coding problems until January 2025.",从竞争性编程平台提取,"which contains a large set of high-quality coding problems, defined for different coding tasks, extracted from competitive programming platforms.",2025年1月 (数据集版本release_v5),"The version of the dataset will be release_v5, containing a total of 880 coding problems until January 2025.",,,有避免数据污染的设计考虑,"Avoiding data contamination: We will avoid problems that are likely to have been included in the training data of the LLM used in CURRANTE, based on the reported dates and known datasets.",,,代码生成,The goal of this experiment is to evaluate whether an LLM-based TDD-inspired workflow is effective in generating correct code,通过率、全部通过完成率、测试覆盖率、测试多样性、通过时间,"We will collect detailed data on effectiveness (e.g., PassAll, PassRate, TestCoverage, TestDiversity), efficiency (e.g., TimeToPass)",自然语言问题描述与结构化规范（TOML格式）,A text area where the user can input a structured natural language specification of the problem to be solved. The specification follows a TOML format,代码,"the code generation phase (area 3), where CURRANTE uses the LLM to produce the corresponding code implementation",文本到代码,"The user starts by defining the problem requirements in a structured way, via a TOML file... the LLM to produce the corresponding code implementation",,,数据集持续更新，包含来自竞争性编程平台的高质量、带测试用例和难度标签的问题。,"The benchmark is continuously updated with new problems, providing detailed information for each problem, such as test cases, problem description, difficulty level, and solution."
2601.03731_output/content.md,RepoReason,"We present REPOREASON, a white-box diagnostic benchmark centered on Abductive Assertion Verification.",Yes,"We present REPOREASON, a white-box diagnostic benchmark... We introduce RepoReason, a repository-level code reasoning benchmark designed as a white-box diagnostic tool.",,,该评测基准旨在评估模型在仓库级别（Repository-Level）的代码推理能力，特别是通过“溯因断言验证”（Abductive Assertion Verification）来推导复杂、相互依赖的文件系统中，经过执行历史后的当前系统状态。,"RepoReason shifts to a verification-centric approach: “Given the complex execution history of this repository, what is the current system state?” By requiring models to derive deterministic values that satisfy assertions rather than writing implementation logic...",仓库级代码推理能力，包括跨文件逻辑一致性、长链状态跟踪、多源信息聚合等认知维度。,"...maintaining logical consistency across extensive, interdependent file systems—a capability we define as Repository-Level Reasoning. This capability imposes profound challenges on LLMs, demanding... the agency to explore vast contexts to pinpoint target definitions, aggregate multi-source information across complex dependencies, and perform long-chain reasoning to accurately track and compute state mutations.",采用白盒诊断评估方法，通过动态程序切片（dynamic program slicing）和三个正交的认知指标进行细粒度量化：ESV（阅读负载）、MCL（模拟深度）、DFI（集成宽度）。任务形式为填空式推理（cloze-style reasoning tasks），要求模型推导出被掩码的断言中的确定值。,"...we establish a fine-grained diagnostic system using dynamic program slicing, quantifying reasoning via three orthogonal metrics: ESV (Reading Load), MCL (Simulation Depth), and DFI (Integration Width)... subsequently masking these verified values to create cloze-style reasoning tasks.",多文件项目、仓库级别。模型需要处理大规模、相互依赖的文件系统，上下文范围可达整个代码仓库。,"...maintaining logical consistency across extensive, interdependent file systems... challenging models with authentic, deep logic... apply deep execution simulation to massive repository scopes, challenging models to maintain logical consistency across extensive file systems.","通用软件工程，任务实例提取自主流Python开源仓库（如toolz, sympy, jinja2）的真实单元测试。","To ensure the benchmark reflects real-world complexity, we extract task instances from a curated set of mainstream Python repositories (e.g., toolz (Developers, 2025b), sympy (Team, 2025c), jinja2 (Pallets, 2025))...",工程级、高熵逻辑。通过基于运行时轨迹（如堆栈深度、函数调用密度、跨文件依赖广度）的结构化指标进行定量过滤，仅保留最复杂的推理任务。,"...we demonstrate that even top-tier agents face severe performance degradation when confronted with high-entropy logic... By designing structural metrics based on these traces—such as stack depth, function call density, and cross-file dependency breadth—we quantitatively filter and retain only the most complex reasoning tasks.",Python,...we extract task instances from a curated set of mainstream Python repositories... Pure-Python Only,规模从1.2k到775k+行代码（LoC）的仓库范围。,RepoReason (Ours) ✓ 1.2k – 775k+ Assertion Verification,数据来源于精选的主流开源Python仓库及其内置的单元测试套件，通过具体执行捕获细粒度的运行时轨迹。,"...we extract task instances from a curated set of mainstream Python repositories... Beyond static code extraction, we perform concrete execution of the repositories’ built-in unit test suites to capture granular runtime traces.",2026,arXiv:2601.03731v1  [cs.SE]  7 Jan 2026,官方自建。论文作者通过一个包含五个阶段的防御系统（仓库筛选、结构过滤、执行驱动突变、任务实例化、诊断评估）构建。,"In this section, we delineate the methodological framework of RepoReason, conceptualizing the construction pipeline as a rigorous five-phase defense system.",抗污染设计。通过“执行驱动突变”（Execution-Driven Mutation）框架解决数据泄漏风险，利用执行环境作为语义预言机，通过扰动程序输入并重新执行来生成新的、未被记忆的地面真值状态。,"To resolve this, we implement an Execution-Driven Mutation methodology... By keeping the reasoning logic... intact but perturbing the program inputs, we effectively sever the model’s memory retrieval path... This design ensures that while the reasoning logic remains authentic and complex, the specific state values are unmemorized and deterministic.",,,代码推理与状态验证，而非代码生成。任务是验证断言，推导系统状态。,"Unlike traditional generation tasks, RepoReason shifts to a verification-centric approach... By requiring models to derive deterministic values that satisfy assertions rather than writing implementation logic...",ESV（阅读负载）、MCL（模拟深度）、DFI（集成宽度）三个正交的认知指标，用于进行细粒度的白盒诊断。,"...quantifying reasoning via three orthogonal metrics: ESV (Reading Load), MCL (Simulation Depth), and DFI (Integration Width).",代码与自然语言（被掩码的断言及其所在的复杂仓库上下文）。,...masking these verified values to create cloze-style reasoning tasks... Given the complex execution history of this repository...,代码/确定值。模型需要输出一个确定的值（如数字、字符串、布尔值）来满足被掩码的断言。,"By requiring models to derive deterministic values that satisfy assertions... We introduce a Deterministic Value Protocol, which filters runtime data to eliminate representational drift and ensure stable, unique ground-truth values.",代码到确定值。输入是包含掩码断言的仓库代码上下文，输出是满足该断言的特定确定值。,"“Given the complex execution history of this repository, what is the current system state?”... derive deterministic values that satisfy assertions",需要完整的Python运行时环境来执行仓库的单元测试套件以捕获轨迹，并作为语义预言机来生成新的地面真值。,We treat the code execution environment as a Semantic Oracle... We then re-execute the entire repository context to capture the new ground-truth state...,1. 范式转变：从预测转向溯因验证，使用单元测试断言作为语义锚点。2. 语义预言机框架：通过执行驱动突变解决丰富性与污染的两难困境。3. 白盒诊断工具：超越端到端评分，提供基于动态程序切片的细粒度认知瓶颈分析。,"• From Prediction to Abductive Verification: We pioneer a repository-level reasoning paradigm that pivots from direct outcome prediction to abductive assertion verification... • The Semantic Oracle Framework: We resolve the Richness-Contamination Dilemma through an Execution-Driven Mutation framework... • White-box Diagnostic Instrument: Beyond end-to-end scoring, we introduce a granular diagnostic system powered by dynamic program slicing."
2601.04126_output/content.md,WebGen-Bench,"Experiments demonstrate that our system surpasses advanced coding agents in building realistic web environments on WebGen-Bench, achieving superior performance in both visual and functional quality.","No, 本文是使用该数据集进行评测",Experiments demonstrate that our system surpasses advanced coding agents in building realistic web environments on WebGen-Bench...,,,评估从自然语言描述生成完整、功能性网站的能力。,"WebGen-Bench (Lu et al., 2025) evaluates end-to-end website generation from natural language descriptions.",网站生成质量，包括视觉和功能质量。,achieving superior performance in both visual and functional quality.,,,,,Web前端开发，网站构建。,evaluates end-to-end website generation from natural language descriptions.,,,"文中未明确提及，但根据上下文推断主要涉及Web前端技术（如HTML, CSS, JavaScript）。","Recent work has explored UI-to-code generation: Design2Code (Si et al., 2024) benchmarks the conversion of visual designs to front-end code, while WebGen-Bench (Lu et al., 2025) evaluates end-to-end website generation from natural language descriptions.",,,,,2025,"WebGen-Bench (Lu et al., 2025)",,,,,,,端到端网站生成,evaluates end-to-end website generation from natural language descriptions.,,,自然语言描述,evaluates end-to-end website generation from natural language descriptions.,代码（网站）,evaluates end-to-end website generation from natural language descriptions.,文本到代码,evaluates end-to-end website generation from natural language descriptions.,,,专注于评估从自然语言描述生成完整、功能性网站的能力，而非单个网页或UI组件。,"While LLMs perform well on generating a single webpage, building a realistic and functional website with many interconnected pages faces challenges. ... WebGen-Bench (Lu et al., 2025) evaluates end-to-end website generation from natural language descriptions."
2601.05214_output/content.md,Glaive Function-Calling dataset,We utilize the Glaive Function-Calling dataset (GlaiveAI 2024) from Hugging Face,"No, 本文是使用该数据集进行评测",We utilize the Glaive Function-Calling dataset (GlaiveAI 2024) from Hugging Face,,,评测基准旨在评估AI代理在工具调用（如API调用）任务中的表现，特别是检测工具调用中的幻觉（如选择错误工具、参数错误等）。,"We evaluate this approach on reasoning tasks across multiple domains, demonstrating strong detection performance (up to 86.4% accuracy)... particularly excelling at detecting parameter-level hallucinations and inappropriate tool selections, critical for reliable agent deployment.",工具调用幻觉检测（包括函数选择错误、参数错误、工具绕过等）,We formalize tool-calling hallucination detection as a binary classification task... We define a tool-calling hallucination as occurring when any of the following conditions hold: (1) Function Selection Error... (2) Function Appropriateness Error... (3) Parameter Error... (4) Completeness Error... (5) Tool Bypass Error,使用轻量级分类器对模型最后一层的内部表示进行二元分类（幻觉/非幻觉），并报告检测准确率。,We formalize tool-calling hallucination detection as a binary classification task operating on the internal representations of the last layer of large language models (LLMs) during tool call generation... demonstrating strong detection performance (up to 86.4% accuracy),依赖于用户查询和上下文信息来生成和评估工具调用。,"Given a user query q and contextual information c, an LLM M generates a tool call",多领域，包括个人健康、金融计算、可持续性指标和通用计算器功能。,"covering domains such as personal health, finance, and general utility calculations... spanning personal health calculations, financial computations, sustainability metrics, and general calculator functions",,,,,,,来自Hugging Face的Glaive Function-Calling数据集，包含多领域代理交互的标注数据。,"We utilize the Glaive Function-Calling dataset (GlaiveAI 2024) from Hugging Face, which comprises multi-domain agent interactions annotated with structured tool usage information and corresponding parameters",2024 (数据集引用年份),Glaive Function-Calling dataset (GlaiveAI 2024),社区贡献（来自Hugging Face）,from Hugging Face,,,,,工具调用生成与幻觉检测,tool-calling hallucination detection,检测准确率,demonstrating strong detection performance (up to 86.4% accuracy),自然语言查询和上下文信息,Given a user query q and contextual information c,结构化的工具调用（函数名和参数）,generates a tool call ˜f(a) where ˜f ∈F ∪{∅} and a represents the function arguments,文本到结构化工具调用,"Given a user query q and contextual information c, an LLM M generates a tool call",,,专注于工具调用（而非通用文本生成）中的幻觉检测，定义了五种具体的幻觉类型（函数选择、适当性、参数、完整性、工具绕过）。数据集包含多领域代理交互的标注。,"Unlike textual hallucinations, tool-calling hallucinations manifest as inappropriate tool selection, malformed parameters, incorrect tool chaining, tool bypass behavior... We define a tool-calling hallucination as occurring when any of the following conditions hold: (1) Function Selection Error... (5) Tool Bypass Error... covering domains such as personal health, finance, and general utility calculations"
2601.04886_output/content.md,AIDev,"We used the AIDev dataset [16] and analyzed the AIDev-pop subset (33,596 PRs from 2,807 repositories with >100 stars) via the pull_request split.","No, 本文是使用该数据集进行评测","We used the AIDev dataset [16] and analyzed the AIDev-pop subset... To address this gap, we conducted an empirical study analyzing 23,247 Agentic-PRs from the AIDev dataset [16]...",,,评测AI编码代理生成的Pull Request（PR）描述与代码变更之间的一致性。旨在衡量AI代理生成的PR描述（标题+正文）是否忠实于其底层代码变更。,We used PR message-code inconsistency (PR-MCI) as the degree to which a PR description (title + body) diverges from the underlying code changes.,AI生成代码的可靠性与可信度，具体为PR描述与代码变更的一致性。,"However, the alignment between these messages and the actual changes remains unexplored, raising concerns about the trustworthiness of AI agents.",使用启发式相似度评分（PR-MCI score）来衡量不一致性。该评分结合了三个互补信号：范围充分性、文件类型一致性和任务类型对齐度。通过手动标注验证评分并设定阈值（θ=0.61）来识别高不一致性（high-MCI）的PR。,"We measured PR-MCI using a heuristic similarity score that combines three complementary signals: scope adequacy 𝑠𝑠(whether description verbosity matches code churn), file-type consistency 𝑠𝑓(whether mentioned file types, e.g., tests or documentation, are actually modified), and task-type alignment 𝑠𝑡(whether description language matches the labeled task type). PRs with similarity below 𝜃= 0.61 are labeled high-MCI.",Pull Request级别，包含多个提交和文件变更。,Note that PR descriptions differ from commit messages: PR descriptions may span multiple commits and serve as the primary communication channel for reviewers.,软件工程，具体为代码审查和协作开发。,Pull request (PR) descriptions generated by AI coding agents are the primary channel for communicating code changes to human reviewers.,,,未明确指定，数据集包含多种编程语言的PR。,,"分析的数据集包含23,247个由AI代理生成的PR（来自AIDev数据集）。此外，作者贡献了974个手动标注的PR（其中432个为部分对齐/未对齐）。","We analyzed 23,247 agentic PRs across five agents... We contributed 974 manually annotated PRs... the final dataset contains 23,247 PRs authored by five AI agents.",来自AIDev数据集，该数据集包含来自GitHub上真实软件项目的Pull Request。,"We used the AIDev dataset [16] and analyzed the AIDev-pop subset (33,596 PRs from 2,807 repositories with >100 stars)...",2026,"Jingzhi Gong, Giovanni Pinna, Yixin Bian, and Jie M. Zhang. 2026. Analyzing Message-Code Inconsistency in AI Coding Agent-Authored Pull Requests.",社区贡献（基于AIDev数据集构建），本文作者对其进行了过滤和标注。,We used the AIDev dataset [16]... We contributed 974 manually annotated PRs...,,,许可协议（MIT或Apache 2.0）,"After filtering for closed PRs, permissive licenses (MIT or Apache 2.0)...",代码变更描述评估,We used PR message-code inconsistency (PR-MCI) as the degree to which a PR description (title + body) diverges from the underlying code changes.,PR-MCI相似度评分（范围0-1），高不一致性（high-MCI）PR的流行率，以及PR接受率、合并时间等影响指标。,"The similarity score 𝑠∈[0, 1]... Overall, 406 out of 23,247 Agentic-PRs (1.7%) exhibited high PR-MCI... high-MCI PRs had 51.7% lower acceptance rates (28.3% vs. 80.0%) and took 3.5× longer to merge (55.8 vs. 16.0 hours).",自然语言（PR描述）和代码变更（diff）。,PR message-code inconsistency (PR-MCI) as the degree to which a PR description (title + body) diverges from the underlying code changes.,一致性评分和分类（对齐、部分对齐、未对齐）。,"PRs with similarity below 𝜃= 0.61 are labeled high-MCI... manually annotated 600 PRs, stratified by agent, task type, and score bins, as aligned, partially aligned, or misaligned",代码与文本到一致性评估,We used PR message-code inconsistency (PR-MCI) as the degree to which a PR description (title + body) diverges from the underlying code changes.,,,专注于AI编码代理生成的Pull Request（Agentic-PRs）中消息与代码的不一致性（PR-MCI）。提出了一个包含八种类型的PR-MCI分类法（如“幻影变更”、“范围低估”）。研究了不一致性对PR接受率和合并时间的影响。,"We identified eight PR-MCI types, finding that Phantom Changes (descriptions claim unimplemented changes) dominated (45.4%)... high-MCI PRs had 51.7% lower acceptance rates (28.3% vs. 80.0%) and took 3.5× longer to merge (55.8 vs. 16.0 hours)."
2601.04540_output/content.md,AdaptEval,"To fill this gap, we propose AdaptEval, a benchmark designed to evaluate LLMs on code snippet adaptation.",Yes,"To fill this gap, we propose AdaptEval, a benchmark designed to evaluate LLMs on code snippet adaptation.",https://github.com/ztwater/AdaptEval,1https://github.com/ztwater/AdaptEval.,评估大型语言模型在代码片段适应（Code Snippet Adaptation）任务上的性能。代码片段适应是代码重用过程中的一个关键步骤，涉及将代码片段从一个特定环境修改以适应另一个环境。,"Recent advancements in large language models (LLMs) have automated various software engineering tasks, with benchmarks emerging to evaluate their capabilities. However, for adaptation, a critical activity during code reuse, there is no benchmark to assess LLMs’ performance, leaving their practical utility in this area unclear. To fill this gap, we propose AdaptEval, a benchmark designed to evaluate LLMs on code snippet adaptation.",代码片段适应能力，包括对实际上下文的理解、遵循多粒度任务要求、以及执行细粒度代码修改的能力。,"Unlike existing benchmarks, AdaptEval incorporates the following three distinctive features: First, practical context. ... Second, multi-granularity annotation. ... Third, fine-grained evaluation.",结合适应级别（adaptation-level）和函数级别（function-level）测试的两层测试框架。评估模型在个体适应步骤上的表现。,"Third, fine-grained evaluation. AdaptEval includes a two-tier testing framework combining adaptation-level and function-level tests, which enables evaluating LLMs’ performance across various individual adaptations.",强上下文依赖，任务源自开发者的实际实践，保留了来自Stack Overflow帖子和关联GitHub仓库的丰富上下文信息。,"First, practical context. Tasks in AdaptEval are derived from developers’ practices, preserving rich contextual information from Stack Overflow and GitHub communities.",软件工程，具体是代码重用和代码适应。,"Reusing code snippets is a widely adopted practice to improve the quality and efficiency of software development. A critical step in this process is adaptation, where developers modify code snippets to fit their specific context.",实际工程级，基于真实的、跨平台的开发者适应实践。,"comprising 164 tasks derived from real-world, cross-platform adaptation practices of developers.",Python,"To address these challenges, we propose AdaptEval, the first benchmark for code snippet adaptation, comprising 164 tasks with 523 adaptations in Python language.",包含164个任务，共523个适应步骤。,"To address these challenges, we propose AdaptEval, the first benchmark for code snippet adaptation, comprising 164 tasks with 523 adaptations in Python language.",数据来源于开发者的实际适应实践，通过收集Stack Overflow帖子、关联的GitHub仓库，并经过克隆检测和数据清洗构建而成。,"Firstly, each task in AdaptEval is collected from the actual adaptation practice of developers. We preserve the original context by including the referenced SO post and the associated GitHub repository for better understanding and traceability.",2026-01-08 (根据arXiv版本v1日期),arXiv:2601.04540v1  [cs.SE]  8 Jan 2026,官方自建，由研究团队构建和标注。,"Overall, it takes approximately 550 person-hours to construct AdaptEval, covering adaptations in 38 types.",,,,,代码编辑/适应，涉及对现有代码片段的修改以适应新上下文。,"A critical step in this process is adaptation, where developers modify code snippets to fit their specific context.",pass@1（文中提及），以及适应级别和函数级别的测试通过率。,"Compared with task-level requirements, all LLMs perform significantly better with adaptation-level steps, where an increase up to 34.84% in pass@1 is observed.",自然语言（任务级和适应级描述）与代码（原始代码片段及其上下文）。,"Task-level annotations describe concise developer intentions... while adaptation-level ones simulate developers’ step-by-step adaptation process, serving as specific instructions for LLMs to implement code changes.",代码（适应后的代码片段）。,serving as specific instructions for LLMs to implement code changes.,文本（任务描述）与代码（原始代码）到代码（适应后代码）。,Task-level annotations describe concise developer intentions... serving as specific instructions for LLMs to implement code changes.,需要特定依赖，基准包含从原始GitHub仓库提取的依赖信息。,Adaptation Dependencies ... External: logging.LoggerAdapter,1. 实际上下文：任务源自开发者实践，保留Stack Overflow和GitHub的丰富上下文。2. 多粒度标注：每个任务都有任务级和适应级的描述。3. 细粒度评估：两层测试框架（适应级和函数级），支持评估个体适应步骤。覆盖38种适应类型。,"Unlike existing benchmarks, AdaptEval incorporates the following three distinctive features: First, practical context. Tasks in AdaptEval are derived from developers’ practices, preserving rich contextual information from Stack Overflow and GitHub communities. Second, multi-granularity annotation. Each task is annotated with requirements at both task and adaptation levels, supporting the evaluation of LLMs across diverse adaptation scenarios. Third, fine-grained evaluation. AdaptEval includes a two-tier testing framework combining adaptation-level and function-level tests, which enables evaluating LLMs’ performance across various individual adaptations."
2601.05187_output/content.md,SimuBench,"We release SimuBench, the first large-scale benchmark for LLM-based Simulink modeling.",Yes,"We release SimuBench, the first large-scale benchmark for LLM-based Simulink modeling.",,,用于评估基于LLM的Simulink建模能力的基准。任务包括模型创建、修改和问答（Q&A）。,"Tasks include model creation, modification, and question-answering (Q&A), with complete source files, XML representations, and visualizations for reproducible evaluation.",LLM在图形化、结构化、非文本领域的推理能力，特别是在多领域物理系统建模方面的能力。,"This makes it an ideal testbed for evaluating and advancing LLM reasoning capabilities in highly structured, non-textual domains.",文中未明确描述SimuBench的具体评估方法。,,涉及分层、图形化的复杂框图、信号路由和严格的拓扑约束，可能包含数百个模块的工业系统模型。,"Simulink employs a hierarchical, graphical paradigm with complex block diagrams, signal routing, and strict topological constraints. ... industrial systems with hundreds of blocks...",控制、机械、电气、流体、热力学和电磁学等多个物理工程领域。,"It comprises 5300 tasks across control, mechanical, electrical, fluid, thermal, and electromagnetic domains.",文中未明确描述任务难度等级。,,Simulink建模环境（图形化建模语言），以及用于表示模型的Python字典/JSON格式。,SimuAgent transforms Simulink models into a compact Python dictionary (JSON) format that LLMs can process more efficiently.,包含5300个任务。,"It comprises 5300 tasks across control, mechanical, electrical, fluid, thermal, and electromagnetic domains.",文中未明确描述数据来源。,,2026年1月8日（预印本日期）,arXiv:2601.05187v1  [cs.AI]  8 Jan 2026,官方自建（本文作者发布）,"We release SimuBench, the first large-scale benchmark for LLM-based Simulink modeling.",文中未讨论数据污染状态。,,文中未提及许可证。,,模型创建、模型修改、问答（Q&A）。,"Tasks include model creation, modification, and question-answering (Q&A)...",文中未明确提及SimuBench的具体评估指标。,,自然语言任务描述（如“构建一个线性LED驱动器模型...”），以及可能的Simulink模型表示（Python字典/JSON）。,Build a linear LED driver model with three LEDs connected in series. Measure the current and the total radiant power of the LEDs.,Simulink模型（以Python字典/JSON格式表示），以及可能的问答答案。,SimuAgent transforms Simulink models into a compact Python dictionary (JSON) format...,文本到模型（自然语言描述到Simulink模型表示）。,Build a linear LED driver model with three LEDs connected in series. Measure the current and the total radiant power of the LEDs.,Simulink建模与仿真环境，以及用于快速验证的轻量级Python测试环境。,"Integrated with an in-process Python test harness, SimuAgent enables instant structural validation and parameter tuning. ... SimuAgent includes a local Python-based testing environment testbed that performs static checks on signal types, parameter ranges, and port wiring.",首个面向LLM的、大规模、多领域的Simulink建模基准。包含完整的源文件、XML表示和可视化，用于可复现的评估。,"We release SimuBench, the first large-scale benchmark for LLM-based Simulink modeling. ... with complete source files, XML representations, and visualizations for reproducible evaluation."
2601.04920_output/content.md,ELOPE (Event-based Lunar OPtical flow Egomotion estimation) competition,This paper presents a case study of using ChatGPT for rapid prototyping in ESA’s ELOPE (Event-based Lunar OPtical flow Egomotion estimation) competition.,"No, 本文是使用该数据集进行评测",This paper presents a case study of using ChatGPT for rapid prototyping in ESA’s ELOPE (Event-based Lunar OPtical flow Egomotion estimation) competition.,,,从事件相机、IMU和雷达测距数据中估计月球着陆器的3D速度。,"The target of the ELOPE (Event-based Lunar OPtical flow Egomotion estimation) competition is to estimate the 3D velocity of a lunar lander from the data of an event camera, an IMU and radar range measurements.",算法实现与科学原型开发能力,This paper presents a case study of using ChatGPT for rapid prototyping in ESA’s ELOPE (Event-based Lunar OPtical flow Egomotion estimation) competition.,竞赛排名与得分,we achieved second place with a score of 0.01282,多模态传感器数据流处理,"The data set of the ELOPE challenge encompasses 93 simulated landing sequences. For each landing sequence, three major data elements are provided: camera event stream, trajectory and range meter reading.",计算机视觉、机器人学、航天工程（月球着陆）,The competition required participants to process event camera data in order to estimate lunar lander trajectories.,高难度竞赛级,"The competition required participants to process event camera data in order to estimate lunar lander trajectories. Entering late in the challenge, we faced a compressed development timeline — an ideal test for evaluating whether conversational AI could accelerate the scientific prototyping process.",文中未明确提及，但实验涉及代码生成，推测为Python等科学计算常用语言。,,包含93个模拟着陆序列，其中28个训练序列，65个测试序列。,The data set of the ELOPE challenge encompasses 93 simulated landing sequences. The sequences are divided into 28 train sequences and 65 test sequences.,模拟数据,The data set of the ELOPE challenge encompasses 93 simulated landing sequences.,2025,The competition ran from May to August 2025.,官方竞赛组织者构建,organized by ESA’s Advanced Concepts Team (ACT) in partnership with University of Adelaide and TU Delft.,,,,,端到端系统开发（数据处理、算法设计、代码实现）,The competition required participants to process event camera data in order to estimate lunar lander trajectories.,竞赛得分（具体指标未详述）,we achieved second place with a score of 0.01282,事件相机数据流、IMU数据、雷达测距数据,"The target of the ELOPE (Event-based Lunar OPtical flow Egomotion estimation) competition is to estimate the 3D velocity of a lunar lander from the data of an event camera, an IMU and radar range measurements.","3D速度向量 (vx, vy, vz)","The goal of the competition was to estimate the missing velocities (vx, vy, vz) for each test trajectory.",多模态传感器数据到运动参数,"estimate the 3D velocity of a lunar lander from the data of an event camera, an IMU and radar range measurements.",,,专注于事件相机数据处理和月球着陆场景的特定科学竞赛；数据包含模拟的事件流、轨迹和测距读数。,"The target of the ELOPE (Event-based Lunar OPtical flow Egomotion estimation) competition is to estimate the 3D velocity of a lunar lander from the data of an event camera, an IMU and radar range measurements. Event cameras are a special type of camera that do not output image frames but pixel events based on brightness changes."
2601.04996_output/content.md,AlgBench,"To address the limitations illustrated above, we introduce AlgBench, a novel benchmark for evaluating LRMs’ algorithmic reasoning.",Yes,"To address the limitations illustrated above, we introduce AlgBench, a novel benchmark for evaluating LRMs’ algorithmic reasoning.",,,评估大型推理模型（LRMs）的算法推理能力，即模型是否真正掌握和理解算法。,"AlgBench, an expert-curated benchmark that evaluates LRMs under an algorithm-centric paradigm.",算法推理能力，特别是对特定算法的掌握程度。,evaluates LRMs under an algorithm-centric paradigm.,Pass@k， Z-score,Evaluator Pass@k Z-score,单算法问题，每个问题设计为仅需一种特定算法。,"we manually curated a collection of 3,000 original problems covering 27 distinct algorithms, each designed to require exactly one specific algorithm",算法领域，涵盖欧几里得结构、非欧几里得结构、非优化、局部优化、全局优化和启发式优化等类别。,"including Euclidean-structured, non-Euclidean-structured, non-optimized, local-optimized, global-optimized, and heuristic-optimized categories.",根据计算复杂度分为三个难度级别。,"we divide each problem into three difficulty levels, according to the computation complexity.",,,超过3000个原创问题，涵盖27种不同的算法。,"The resulting dataset comprises over 3,000 original problems spanning 27 distinct algorithms",由ACM算法专家手动构建的原创问题，与任何现有开源平台无重叠。,"we engaged a group of algorithm experts... we manually curated a collection of 3,000 original problems... verified to have no overlap with any existing open-source platforms.",2026-01-08 (根据arXiv版本v1日期),arXiv:2601.04996v1  [cs.AI]  8 Jan 2026,官方自建，由算法专家手动策划。,"we engaged a group of algorithm experts... we manually curated a collection of 3,000 original problems",抗污染设计，所有问题均为原创，与现有开源平台无重叠。,verified to have no overlap with any existing open-source platforms. This construction process effectively mitigates both the evaluation bias and data contamination concerns,,,算法推理与代码生成（根据上下文推断，旨在评估模型应用特定算法解决问题的能力）。,evaluates LRMs under an algorithm-centric paradigm... each designed to require exactly one specific algorithm,Pass@k， Z-score,Evaluator Pass@k Z-score,自然语言描述的问题（根据上下文推断，旨在评估算法推理）。,Alg-centric Task + Input + Correct Answer,代码或算法解决方案（根据上下文推断，旨在评估算法推理）。,Alg-centric Task + Input + Correct Answer,文本到代码/算法,Alg-centric Task + Input + Correct Answer,,,1. 范式转变：采用算法中心范式，而非问题中心范式，以评估模型对单个算法的掌握。2. 新颖的分类法：基于算法核心思想（优化导向和结构导向）进行分类。3. 旨在解决现有基准的局限性：如无法评估单一算法、算法覆盖范围有限、数据污染风险高。,"Paradigm Shift: Unlike prior benchmarks that adhere to a problem-centric paradigm, we are the first to adopt the algorithm-centric paradigm... Novel-taxonomic and Contamination-free Dataset Construction: We engage multiple algorithmic experts to manually construct problems guided by a novel algorithmic taxonomy... To address the limitations illustrated above, we introduce AlgBench"
2601.05777_output/content.md,SWE-bench Verified,"We evaluate EET on the widely adopted SWE-bench Verified benchmark (OpenAI, 2025)","No, 本文是使用该数据集进行评测","We evaluate EET on the widely adopted SWE-bench Verified benchmark (OpenAI, 2025)",,,自动化解决现实世界软件工程问题，给定一个描述bug或功能请求的问题以及对应的代码仓库，目标是导航代码库、定位相关代码并生成补丁来解决问题，并通过相关测试验证正确性。,"Given an issue describing a bug or a feature request along with the corresponding code repository, an SE agent aims to navigate the codebase, localize relevant code, and generate a patch (i.e., modify the code) to resolve the issue, with correctness validated by the associated tests",软件工程代理的自动化问题解决能力,"SWE-bench (Jimenez et al., 2024) and its human-validated variant SWE-bench Verified (OpenAI, 2025) have become standard benchmarks for evaluating automated resolution of real-world SE issues.",通过执行测试验证生成的补丁是否正确解决问题,"Issue resolution typically includes a patch evaluation phase, which validates whether a generated patch resolves the issue by executing tests.",多文件项目，需要导航和理解整个代码库,"Given an issue describing a bug or a feature request along with the corresponding code repository, an SE agent aims to navigate the codebase, localize relevant code, and generate a patch",软件工程，包括bug修复和功能实现,Given an issue describing a bug or a feature request,现实世界软件工程问题,standard benchmarks for evaluating automated resolution of real-world SE issues.,,,,,现实世界软件仓库中的问题,"Real-world issue descriptions are often lengthy and detailed (Jimenez et al., 2024), containing background information, discussion, and auxiliary context that is not directly relevant to resolution.",2025,"SWE-bench Verified (OpenAI, 2025)",,,,,,,代码修复/补丁生成,"generate a patch (i.e., modify the code) to resolve the issue",解决率,with negligible loss in resolution rate (at most 0.2%),自然语言问题描述和代码仓库,Given an issue describing a bug or a feature request along with the corresponding code repository,代码补丁,"generate a patch (i.e., modify the code)",文本与代码到代码,"Given an issue describing a bug or a feature request along with the corresponding code repository, an SE agent aims to navigate the codebase, localize relevant code, and generate a patch",代码仓库和执行环境,interacting with the code repository and execution environment through tool invocations such as file editing and command execution,SWE-bench的人类验证变体，专注于评估自动化解决现实世界软件工程问题的能力,"SWE-bench (Jimenez et al., 2024) and its human-validated variant SWE-bench Verified (OpenAI, 2025) have become standard benchmarks for evaluating automated resolution of real-world SE issues."
2601.05772_output/content.md,StriderSPD benchmark,"To enable more realistic evaluation, we construct a binary SPD benchmark that is disjoint from prior datasets in both projects and domains and extensively evaluate StriderSPD on this benchmark.",Yes,"To enable more realistic evaluation, we construct a binary SPD benchmark that is disjoint from prior datasets in both projects and domains and extensively evaluate StriderSPD on this benchmark.",,,在闭源软件中，检测二进制补丁前后版本之间的差异是否对应于安全漏洞修复。具体任务是在函数级别对每个发生变化的函数对进行分类，判断其是否为安全补丁。,"In closed-source software, SPD aims to determine whether the differences between a pre-patch binary and its post-patch counterpart indicate a vulnerability fix. ... we formalize binary SPD as shown in Formula 1. Classifier(⟨f pre i , f post i ⟩) = {security | non-security}",二进制安全补丁检测的准确性和泛化能力,"Experimental results demonstrate the effectiveness of StriderSPD, surpassing the best-performing baseline by 12.66% in accuracy. Furthermore, StriderSPD exhibits generalizability to different LLMs, enhancing the performance of Llama, Qwen, and DeepSeek on binary SPD tasks.",使用准确率、F1分数和误报率等指标进行评估。,"Experimental results demonstrate that StriderSPD outperforms the best baseline Yi-Coder-9B-Chat [18], achieving improvements of 12.66%, 8.20%, and 38.57% in terms of accuracy, F1 score, and false positive rate, respectively.",函数级别,"To avoid losing change meaning (too fine-grained) or label noise (too coarse-grained), we follow prior work [9, 10, 25] to perform binary SPD at the function level by classifying each changed function pair.",软件安全、二进制程序分析、漏洞修复,"Vulnerabilities severely threaten software systems, making the timely application of security patches crucial for mitigating attacks. ... Security Patch Detection (SPD) comes to protect software assets.",工程级，涉及真实世界的闭源软件二进制补丁分析,"To enable more realistic evaluation, we construct a binary SPD benchmark that is disjoint from prior datasets in both projects and domains and extensively evaluate StriderSPD on this benchmark. ... reflect real-world closed-source scenarios",汇编代码和反编译得到的类C伪代码,"Given that assembly code and pseudo-code are the most typical abstraction levels obtained when reversing native binaries [6], the limited binary SPD methods utilize either of them to identify security patches [7–10].","包含1720个二进制补丁，涵盖五个优化级别（O0, O1, O2, O3, Os）","Our benchmark comprises 1,720 binary patches across five optimization levels (O0, O1, O2, O3, and Os).",从项目和领域上都与先前二进制SPD数据集不同的来源构建,"We construct a cross-project and cross-domain benchmark to more faithfully reflect real-world closed-source scenarios, where historical security patch data from the same project is inaccessible.",2026,"IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 14, NO. 8, AUGUST 2026",官方自建,"To enable more realistic evaluation, we construct a binary SPD benchmark that is disjoint from prior datasets in both projects and domains and extensively evaluate StriderSPD on this benchmark.",设计上旨在避免污染，其项目和领域与先前数据集不相交,"We construct a cross-project and cross-domain benchmark to more faithfully reflect real-world closed-source scenarios, where historical security patch data from the same project is inaccessible.",,,代码分类（安全补丁 vs. 非安全补丁）,"Classifier(⟨f pre i , f post i ⟩) = {security | non-security}",准确率、F1分数、误报率,"achieving improvements of 12.66%, 8.20%, and 38.57% in terms of accuracy, F1 score, and false positive rate, respectively.",二进制补丁前后的汇编代码控制流图和伪代码函数,StriderSPD comprises two key components: (1) A structure-guided joint representation neural network that integrates a graph branch into an LLM branch... The graph branch employs a GNN to represent pre- and post-patch assembly-code CFGs... The LLM branch employs Qwen3-8B [17] as the backbone to embed the instruction that contains the pre- and post-patch pseudo-code functions.,分类标签（安全或非安全）,"Classifier(⟨f pre i , f post i ⟩) = {security | non-security}",代码到分类,"Classifier(⟨f pre i , f post i ⟩) = {security | non-security}",,,专门为闭源软件二进制安全补丁检测构建的基准，其项目和领域与现有数据集不相交，旨在更真实地反映现实世界场景。包含多个编译器优化级别的补丁。,"We construct a cross-project and cross-domain benchmark to more faithfully reflect real-world closed-source scenarios, where historical security patch data from the same project is inaccessible. Our benchmark comprises 1,720 binary patches across five optimization levels (O0, O1, O2, O3, and Os)."
2601.05755_output/content.md,SIREN (Systemic Injection & Reasoning Evaluation beNchmark),We evaluate our framework on SIREN (Systemic Injection & Reasoning Evaluation beNchmark) which simulates a realistic execution environment...,Yes,"We further introduce SIREN, a benchmark comprising 959 tool stream injection cases designed to simulate pervasive threats characterized by dynamic dependencies.",,,评估LLM智能体在面临工具流注入攻击时的安全性和鲁棒性。该基准模拟了包含动态依赖关系的真实执行环境，旨在测试智能体在规划和执行阶段抵御恶意工具定义和运行时反馈攻击的能力。,SIREN (Systemic Injection & Reasoning Evaluation beNchmark) which simulates a realistic execution environment characterized by 496 competing tools and dynamic dependencies.,智能体安全性、鲁棒性、对抗工具流注入攻击的防御能力,"We introduce SIREN, a comprehensive benchmark comprising 959 cases across five vectors to simulate agentic reasoning challenges in realistic, stochastic environments.","通过攻击成功率（Attack Success Rate, ASR）和受攻击下的效用（Utility Under Attack, UA）等指标进行评估。",Extensive experiments demonstrate that VIGIL outperforms state-of-the-art dynamic defenses by reducing the attack success rate by over 22% while more than doubling the utility under attack compared to static baselines...,多步骤、动态交互环境，涉及工具调用、运行时反馈和重新规划。,SIREN comprises 959 tool stream injection cases across five attack vectors that target critical phases of the agent lifecycle...,智能体安全、网络安全、对抗性机器学习,"LLM agents operating in open environments face escalating risks from indirect prompt injection, particularly within the tool stream...",高难度，模拟真实世界、随机环境中的复杂推理挑战和对抗性攻击。,"...to simulate agentic reasoning challenges in realistic, stochastic environments.",未明确提及编程语言，主要涉及自然语言指令和工具定义。,,包含959个工具流注入案例，涵盖5个攻击向量；并包含949个数据流基线案例。,"SIREN comprises 959 tool stream injection cases across five attack vectors... alongside 949 data stream baselines from Agent-Dojo (Debenedetti et al., 2024).",基于AgentDojo环境重建，通过引入语义工具冗余和随机运行时反馈来模拟真实世界挑战。,"To evaluate agent robustness against tool stream manipulation, we reconstruct the execution environment based on AgentDojo (Debenedetti et al., 2024) by introducing two architectural features that mirror real-world operational challenges.",2026 (根据arXiv版本号推断),arXiv:2601.05755v1  [cs.CR]  9 Jan 2026,官方自建，作为本文贡献的一部分提出。,"We further introduce SIREN, a benchmark comprising 959 tool stream injection cases...",,,,,多步骤任务执行与安全决策,...target critical phases of the agent lifecycle...,攻击成功率（ASR）、受攻击下的效用（UA）,...reducing the attack success rate by over 22% while more than doubling the utility under attack...,自然语言用户指令、工具定义（可能包含恶意指令）、运行时反馈,...injecting forged tool descriptions or deceptive error messages...,智能体的行动决策（如工具调用序列）,...to hijack the execution flow and compel agents to execute unauthorized actions...,多模态交互（自然语言指令、工具定义、反馈到行动决策）,The tool stream consists of functional definitions and runtime feedback that the model interprets as binding operational constraints...,模拟的执行环境，包含496个竞争工具和动态依赖关系，模拟外部API的不稳定性。,...simulates a realistic execution environment characterized by 496 competing tools and dynamic dependencies.,专注于工具流注入攻击（而不仅仅是数据流），包含5个不同的攻击向量，模拟了语义工具冗余和随机运行时反馈，以评估智能体在复杂、对抗性环境中的推理和恢复能力。,"We introduce SIREN, a comprehensive benchmark comprising 959 cases across five vectors to simulate agentic reasoning challenges in realistic, stochastic environments. ... This fragmented approach fails to quantify agent resilience against compounded threats that exploit both instruction-following biases and adaptive reasoning needs. We introduce SIREN to bridge this gap by integrating dual-stream threats and complex reasoning dependencies within a single unified evaluation framework."
2601.05752_output/content.md,AutoMonitor-Bench,"We introduce AutoMonitor-Bench, the first benchmark designed to systematically evaluate the reliability of LLM-based misbehavior monitors across diverse tasks and failure modes.",Yes,"We introduce AutoMonitor-Bench, the first benchmark designed to systematically evaluate the reliability of LLM-based misbehavior monitors across diverse tasks and failure modes.",,,评估基于大语言模型（LLM）的异常行为监控器在识别前沿AI系统（如大型推理模型和智能体）的异常行为时的可靠性。,"We introduce AutoMonitor-Bench, the first benchmark designed to systematically evaluate the ability of LLM-based monitors to identify misbehavior in frontier AI systems.",异常行为监控的可靠性，包括漏检率和误报率之间的权衡。,"We evaluate monitors using two complementary metrics: Miss Rate (MR) and False Alarm Rate (FAR), capturing failures to detect misbehavior and oversensitivity to benign behavior, respectively.",将异常行为检测视为二分类任务，使用漏检率（MR）和误报率（FAR）两个互补指标进行评估。,"To rigorously evaluate the reliability of LLM-based monitors, we formulate misbehavior detection as a binary classification task... We focus on two complementary metrics... Miss Rate (MR)... False Alarm Rate (FAR)...",依赖于任务请求和对应的生成解决方案或推理轨迹（可能包含显式的中间思考阶段）。,"the LLM-based monitor is provided with a task request and a corresponding generated solution or trajectory... It includes tasks... and evaluates the solution or trajectory both with explicit intermediate thinking stages (e.g., reasoning traces enclosed in <think></think> tags) and without such trajectories.",问答、代码生成和推理。,"AutoMonitor-Bench consists of 3,010 carefully annotated test samples spanning question answering, code generation, and reasoning...",涵盖从显式安全违规到更隐式的行为（如规范博弈）的多种异常行为类别，难度递增。,"which comprises 1,505 carefully annotated misbehavior instances spanning three misbehavior categories, ranging from explicit safety violations to more implicit behaviors such as specification gaming... we structure the benchmark into three categories that reflect increasingly implicit and difficult-to-audit failure mechanisms...",主要涉及自然语言（用于问答、推理）和编程语言（用于代码生成任务）。,"spanning question answering, code generation, and reasoning...","包含3,010个测试样本（1,505个异常行为实例，每个实例配对一个良性实例）。此外，还构建了一个包含153,581个样本的大规模训练语料库。","AutoMonitor-Bench consists of 3,010 carefully annotated test samples... For each misbehavior instance, we additionally include a corresponding benign solution or trajectory, resulting in a total of 3,010 test samples... we construct a large-scale training corpus comprising 153,581 training samples...",经过精心人工标注构建。,carefully annotated test samples... carefully annotated misbehavior instances...,2026年1月9日（根据arXiv版本号推断）,arXiv:2601.05752v1 [cs.CL] 9 Jan 2026,官方自建，作为首个系统性评估LLM监控器可靠性的基准。,"We introduce AutoMonitor-Bench, the first benchmark designed to systematically evaluate the reliability of LLM-based misbehavior monitors...",,,,,异常行为检测（二分类判断）。,The monitor is prompted to assess whether the given solution exhibits any form of misbehavior under the task specification.,"漏检率（Miss Rate, MR）和误报率（False Alarm Rate, FAR）。",We evaluate monitors using two complementary metrics: Miss Rate (MR) and False Alarm Rate (FAR)...,自然语言（任务请求）和模型生成的解决方案或轨迹（可能包含代码和自然语言推理）。,the LLM-based monitor is provided with a task request and a corresponding generated solution or trajectory.,分类判断（异常行为或良性行为）。,The monitor is prompted to assess whether the given solution exhibits any form of misbehavior under the task specification.,文本（任务请求+解决方案）到分类判断。,"Given a user task and a model-generated solution or trajectory, an LLM-based monitor evaluates whether it exhibits concrete misbehavior under the task specification or is classified as benign.",,,首个系统性评估LLM异常行为监控器可靠性的基准；包含配对的异常行为和良性实例；涵盖从显式违规到隐式博弈的三种异常行为类别（安全与权限违规、谄媚与偏见、规范博弈）；旨在揭示监控器在安全覆盖与操作效用之间的内在权衡。,"the first benchmark designed to systematically evaluate the reliability of LLM-based misbehavior monitors... For each misbehavior instance, we additionally include a corresponding benign solution or trajectory... spanning three misbehavior categories—Safety & Permission Violations, Sycophancy & Bias, and Specification Gaming... we observe a systematic trade-off between Miss Rate and False Alarm Rate across most LLM-based monitors, indicating an inherent tension between safety coverage and operational utility."
2601.07790_output/content.md,Syslog Severity Classification Benchmark (基于journalctl的日志严重性分类基准),The study focuses on evaluating small language models (SLMs) and small reasoning language models (SRLMs) using log severity classification as a controlled probe... based on logs collected from journalctl within the computing infrastructure.,Yes，本文是数据集的原始发布论文。,"We argue that severity classification is more informative when treated as a benchmark for probing runtime log comprehension rather than as an end task. Using real-world journalctl data from Linux production servers, we evaluate nine small language models (SLMs) and small reasoning language models (SRLMs)... By emphasizing small, deployable models, this benchmark aligns with real-time requirements of digital twin (DT) systems and shows that severity classification serves as a lens for evaluating model competence and real-time deployability...",,,将系统日志消息（Syslog消息）分类到其预定义的严重性级别（Severity level）。该任务旨在作为评估语言模型在受限输出条件下，对真实世界系统日志语义进行理解和推理能力的探针，而非一个独立的实用任务。,"Since severity levels are predefined metadata in system log messages, having a model merely classify them offers limited standalone practical value... We argue that severity classification is more informative when treated as a benchmark for probing runtime log comprehension rather than as an end task.",模型对系统日志的运行时理解能力、在严格输出约束下的性能、实时部署能力。,We argue that severity classification is more informative when treated as a benchmark for probing runtime log comprehension... this benchmark aligns with real-time requirements of digital twin (DT) systems and shows that severity classification serves as a lens for evaluating model competence and real-time deployability...,在零样本、少样本和检索增强生成（RAG）提示策略下，评估模型对日志严重性分类的准确性。,"Using real-world journalctl data from Linux production servers, we evaluate nine small language models (SLMs) and small reasoning language models (SRLMs) under zero-shot, few-shot, and retrieval-augmented generation (RAG) prompting.",单条日志消息。任务基于单条Syslog消息的内容进行分类。,"Each Syslog message contains a PRI component that consists of a “<”, a number, and a “>”. The number is known as the severity value... However, it also makes them a realistic and challenging probe for evaluating whether language models (LMs) can align log content with operational intent under ambiguity.",系统运维、日志分析、数字孪生（DT）系统监控。,"System logs are crucial for monitoring and diagnosing modern computing infrastructure... Within DT-oriented monitoring pipelines, such log interpretation must be both accurate and latency-efficient...",具有现实挑战性，因为日志严重性标签缺乏严格标准化，存在模糊性，需要模型将日志内容与操作意图对齐。,"This lack of strict standardization limits the use of severity labels as a canonical ground truth. However, it also makes them a realistic and challenging probe for evaluating whether language models (LMs) can align log content with operational intent under ambiguity.",日志消息为自然语言（英文），包含系统事件、警告和性能信息。,"System logs are vital components of modern computing infrastructure, capturing operational events, warnings, and performance information across distributed systems... However, as computing systems generate massive volumes of logs with complex, context-dependent language...",未明确提及具体条目数量，但数据来源于Linux生产服务器的真实journalctl日志。,Using real-world journalctl data from Linux production servers...,从Linux生产服务器的journalctl收集的真实世界系统日志。,Using real-world journalctl data from Linux production servers...,2026年1月12日（根据arXiv版本日期推断）,arXiv:2601.07790v1  [cs.AI]  12 Jan 2026,官方自建，由研究团队从生产环境中收集和构建。,Using real-world journalctl data from Linux production servers...,,,,,分类任务。,"Since severity levels are predefined metadata in system log messages, having a model merely classify them...",准确率（Accuracy）。,"Qwen3-4B achieves the highest accuracy at 95.64% with RAG, while Gemma3-1B improves from 20.25% under few-shot prompting to 85.28% with RAG. Notably, the tiny Qwen3-0.6B reaches 88.12% accuracy...",自然语言文本（系统日志消息）。,"System logs are vital components of modern computing infrastructure, capturing operational events, warnings, and performance information across distributed systems...",分类标签（严重性级别，整数0-7）。,The Severity value is an integer from 0 to 7 that quantifies the risk level of each log.,文本到分类标签。,"Since severity levels are predefined metadata in system log messages, having a model merely classify them...",,,"1. 专注于评估小型、可部署的语言模型（SLMs/SRLMs）。2. 强调在严格输出和运行时约束下的性能，与数字孪生系统的实时需求对齐。3. 使用真实世界的journalctl日志，而非结构化基准数据集（如BGL, HDFS），更能捕捉现实中的时间结构、不规则性和噪声。4. 将严重性分类重新定位为评估模型运行时日志理解能力的探针，而非最终任务。","By emphasizing small, deployable models, this benchmark aligns with real-time requirements of digital twin (DT) systems... Several influential benchmarks use structured datasets such as BGL, HDFS, or Thunderbird, which do not capture the temporal structure, irregularity, and noise of real-world journalctl logs. We argue that severity classification is more informative when treated as a benchmark for probing runtime log comprehension rather than as an end task."
2601.07602_output/content.md,OODEval,"we introduce OODEval, a manually constructed benchmark covering 50 object-oriented design tasks of varying difficulty.",Yes,"we introduce OODEval, a manually constructed benchmark covering 50 object-oriented design tasks of varying difficulty.",,,评估大型语言模型在面向对象设计任务上的能力，具体任务是从自然语言需求分析并生成类图。,"this study focuses on object-oriented design (OOD) [10] tasks, investigating LLMs’ ability to analyze natural language requirements and generate class diagrams.",面向对象设计能力，包括语法正确性和语义正确性，以及类、属性、方法、关系的生成质量。,LLMs exhibit strong syntactic correctness but show significant weaknesses in semantic correctness. LLMs present weaker performance in generating class methods and relationships...,使用提出的CLUE（Class Likeness Unified Evaluation）指标集进行评估，该指标集包含一个整体性能指标和四个局部性能指标，通过计算与人工评分的相关性来验证其有效性。,"we propose the CLUE (Class Likeness Unified Evaluation) metrics, which includes an overall performance metric along with four local performance metrics. This metric integrates more complete structural and semantic information from class diagrams to evaluate design correctness.",模型级，从完整需求到类图设计的端到端任务。,"OODEval, a model-level benchmark dataset... which comprises 50 tasks from requirements to design... making it unsuitable for evaluating end-to-end OOD generation.",软件工程，具体是面向对象软件设计。,object-oriented design (OOD) tasks... OOD plays a crucial role in ensuring software quality and reducing communication overhead within development teams.,包含简单、中等、困难三个难度级别。,"OODEval... encompasses three difficulty levels: simple, moderate, and hard.",不涉及特定编程语言，任务输入是自然语言需求，输出是类图（以PlantUML代码格式表示）。,OODEval... offering PlantUML code formats for automated evaluation and visualized formats for human review.,OODEval包含50个任务。OODEval-Human包含940个本科生提交的解决方案及教师评分。,"OODEval, a manually constructed benchmark covering 50 object-oriented design tasks... we present OODEval-Human, the first human-rated OOD benchmark comprising 940 undergraduate-submitted solutions with instructor ratings.",人工构建。,"we manually constructed a model-level benchmark dataset, OODEval...",2026,arXiv:2601.07602v1 [cs.SE] 12 Jan 2026,官方自建（由论文作者团队手动构建）。,"we manually constructed a model-level benchmark dataset, OODEval...",,,,,模型级设计生成（从需求生成完整的类图）。,"OODEval, a model-level benchmark dataset... which comprises 50 tasks from requirements to design...",CLUE指标集（包含整体性能指标和四个局部性能指标）。,"we propose the CLUE (Class Likeness Unified Evaluation) metrics, which includes an overall performance metric along with four local performance metrics.",自然语言需求文本。,investigating LLMs’ ability to analyze natural language requirements and generate class diagrams.,类图（以PlantUML代码格式表示）。,offering PlantUML code formats for automated evaluation and visualized formats for human review.,文本到代码（具体是自然语言需求到类图定义代码）。,analyze natural language requirements and generate class diagrams.,,,1. 首个专注于评估LLM面向对象设计能力的基准。2. 包含人工评分的子集（OODEval-Human），用于比较LLM与人类能力并验证评估指标。3. 提供PlantUML代码格式以支持自动化评估。4. 任务按难度（简单、中等、困难）分层。5. 提出了专门用于评估类图设计正确性的CLUE指标集。,"OODEval-Human, the first human-rated OOD benchmark... OODEval surpasses previous datasets in scale and diversity, offering PlantUML code formats for automated evaluation... encompasses three difficulty levels: simple, moderate, and hard... we propose the CLUE (Class Likeness Unified Evaluation) metrics..."
2601.08778_output/content.md,"BIRD, Spider 2.0-Snow","In this paper, we conduct an empirical study that (i) benchmarks annotation error rates for two widely used text-to-SQL benchmarks, BIRD and Spider 2.0-Snow","No, 本文是使用该数据集进行评测","We conduct an empirical study that (i) benchmarks annotation error rates for two widely used text-to-SQL benchmarks, BIRD and Spider 2.0-Snow, and (ii) corrects a subset of the BIRD development (Dev) set to measure the impact of annotation errors on text-to-SQL agent performance and leaderboard rankings.",https://github.com/uiuc-kang-lab/text_to_sql_benchmarks (本文代码和数据),Our code and data are available at https://github.com/uiuc-kang-lab/text_to_sql_benchmarks.,文本到SQL转换。给定一个自然语言问题（和可选的外部知识）以及一个目标数据库，模型需要生成一个能正确执行并回答问题的SQL查询。,"Given a target database D, human annotators curate text-to-SQL examples, each comprising a natural language input T and a ground-truth SQL query Q.",文本到SQL转换的准确性和可靠性，特别关注数据集中标注错误的普遍性及其对模型评估的影响。,"We conduct an empirical study that (i) benchmarks annotation error rates for two widely used text-to-SQL benchmarks, BIRD and Spider 2.0-Snow, and (ii) corrects a subset of the BIRD development (Dev) set to measure the impact of annotation errors on text-to-SQL agent performance and leaderboard rankings.",执行准确度（Execution Accuracy）。通过重新评估模型在原始和有标注错误的开发集子集上的性能来衡量。,We re-evaluate all 16 open-source agents from the BIRD leaderboard on both the original and the corrected BIRD Dev subsets. We show that performance changes range from −7% to 31% (in relative terms)... We further computed two Spearman’s rank correlation coefficients (𝑟𝑠) to assess the impact of annotation errors on agent rankings.,依赖于单个数据库的Schema（表、列结构）和可选的外部知识。,"Given a target database D, human annotators curate text-to-SQL examples, each comprising a natural language input T and a ground-truth SQL query Q.1 ... Input T consists of a user question and, optionally, external knowledge.",数据库查询，涉及金融、天气、教育等多个现实世界领域。,Figure 1b presents an incorrect annotation from BIRD due to the annotator’s lack of domain knowledge (E3).,高复杂性，涉及现实世界数据库、复杂SQL查询（如使用地理空间函数）和领域知识。,"Spider 2.0-Snow has the most complex SQL queries (highest average token count of its ground-truth SQL queries, 161.8 tokens per SQL query)...",SQL,"Given a target database D, human annotators curate text-to-SQL examples, each comprising a natural language input T and a ground-truth SQL query Q.",BIRD开发集有1534个示例，本文从中随机采样了100个。Spider 2.0-Snow有121个公开了真实SQL的问题。,"We randomly sampled 100 of the 1,534 BIRD Dev examples... across all 121 problems for which ground-truth SQL is publicly available...",人工标注。,"Since these benchmarks heavily rely on human annotations during question construction and answer evaluation, the validity of the annotations is crucial.",2026年1月13日（论文版本日期）。BIRD和Spider 2.0-Snow的原始发布时间更早。,arXiv:2601.08778v1  [cs.AI]  13 Jan 2026,官方自建（BIRD和Spider团队）。,"We selected two text-to-SQL benchmarks, BIRD [31] and Spider 2.0-Snow [27]...",高污染风险，因为标注错误率很高（BIRD Mini-Dev 52.8%， Spider 2.0-Snow 62.8%），可能误导模型评估。,We find that 52.8% of the examples in BIRD Mini-Dev [31] contain annotation errors... we still identify an annotation error rate of 62.8%.,,,文本到SQL生成。,Researchers have proposed numerous text-to-SQL techniques to streamline data analytics...,执行准确度（Execution Accuracy），斯皮尔曼等级相关系数（Spearman’s rank correlation coefficient）。,We re-evaluate all 16 open-source agents from the BIRD leaderboard on both the original and the corrected BIRD Dev subsets... We further computed two Spearman’s rank correlation coefficients (𝑟𝑠) to assess the impact of annotation errors on agent rankings.,自然语言（用户问题）和可选的外部知识文本。,"Input T consists of a user question and, optionally, external knowledge.",SQL查询代码。,"Given a target database D, human annotators curate text-to-SQL examples, each comprising a natural language input T and a ground-truth SQL query Q.",文本到代码。,Researchers have proposed numerous text-to-SQL techniques...,目标数据库环境（如Snowflake）。,Figure 1a illustrates a misuse of a Snowflake function (E1)... The agent can directly access these databases by calling predefined functions.,本文的核心发现是揭示了BIRD和Spider 2.0-Snow这两个广泛使用的文本到SQL基准中存在极高的标注错误率（分别超过50%和60%），这些错误严重扭曲了模型性能报告和排行榜排名。本文还提出了用于检测和修正标注错误的工具（SAR-Agent和SAPAR）。,Our analysis reveals a wider range of annotation errors in text-to-SQL benchmarks than previously recognized... we find that 52.8% of the examples in BIRD Mini-Dev [31] contain annotation errors... we still identify an annotation error rate of 62.8%... These annotation errors cause severe misestimation and misranking of agents’ performance.
2601.08806_output/content.md,APEX–SWE,"We introduce the AI Productivity Index for Software Engineering (APEX–SWE), a benchmark for assessing whether frontier AI models can execute economically valuable software engineering work.",Yes,"We introduce the AI Productivity Index for Software Engineering (APEX–SWE)... To assess models for real-world software engineering tasks, we present APEX–SWE...","https://huggingface.co/datasets/mercor/APEX-SWE (dev set), https://github.com/Mercor-Intelligence/apex-evals (evaluation harness)",We have released an open-source dev set via Hugging Face with a CC-BY license1 and our grading harness is available open-source on GitHub.2 1https://huggingface.co/datasets/mercor/APEX-SWE 2https://github.com/Mercor-Intelligence/apex-evals,评估前沿AI模型执行具有经济价值的真实世界软件工程工作的能力。包含两种任务类型：集成任务（构建跨异构云服务、业务应用和基础设施即代码服务的端到端系统）和可观测性任务（使用遥测信号（如日志、仪表板）和非结构化上下文调试生产故障）。,"a benchmark for assessing whether frontier AI models can execute economically valuable software engineering work. APEX–SWE assesses two novel task types that reflect real-world software engineering: (1) Integration tasks... which require constructing end-to-end systems across heterogeneous cloud primitives, business applications, and infrastructure-as-code services, and (2) Observability tasks... which require debugging production failures using telemetry signals such as logs and dashboards, as well as unstructured context.",真实世界软件工程能力，包括跨平台集成、基础设施配置、生产环境调试、认知推理（区分假设与已验证事实的能力）和代理能力（在行动前解决不确定性）。,"assessing whether frontier AI models can execute economically valuable software engineering work... Our analysis shows that strong performance is primarily driven by epistemic reasoning, defined as the ability to distinguish between assumptions and verified facts, combined with agency to resolve uncertainty prior to acting.","使用Pass@1（模型首次尝试通过所有正确性测试的任务百分比）作为排行榜主要指标，也报告Pass@3（给定三次尝试至少有一次正确结果的百分比）。集成任务通过直接与服务API交互的pytest套件验证正确性；可观测性任务采用FAIL_TO_PASS / PASS_TO_PASS方法，应用模型补丁并执行框架特定测试（pytest, go test, jest）。","For the APEX–SWE leaderboard, we assess models’ outputs using Pass@1, defined as the percentage of tasks where the model’s first attempt passes all correctness tests. For Integration tasks, correctness is verified via a pytest suite that interacts directly with service APIs... For Observability tasks, correctness follows a FAIL_TO_PASS / PASS_TO_PASS methodology inspired by SWE-bench (Jimenez et al., 2024)... We also report Pass@3 in this paper, assessing whether the model can achieve at least one correct outcome if given three attempts.",多服务、多文件项目环境。集成任务涉及跨多个云服务和业务应用的编排；可观测性任务需要在整个代码库中追踪根因，并处理来自多个来源（日志、聊天、代码库）的上下文。,"Integration tasks... require constructing end-to-end systems across heterogeneous cloud primitives, business applications, and infrastructure-as-code services... Observability tasks... must interrogate production logs..., correlate evidence from developer chat discussions, and trace root causes through the codebase...",专业软件工程，具体包括云基础设施集成、支付管道、无服务器事件驱动架构、CRM与工单系统数据同步、生产环境故障诊断与修复。,"Tasks reflect realistic greenfield engineering workflows, including implementing idempotent payment pipelines, configuring serverless event-driven architectures, and synchronizing data between CRM and ticketing systems.",工程级/生产级。任务模拟真实的生产环境，涉及跨平台集成、基础设施配置和基于不完整信息调试生产故障，远超编写短函数或修补单个文件的难度。,"Professional software engineering extends far beyond writing short functions or patching a single file. Real production environments involve cross-platform integration, infrastructure provisioning, and debugging production failures with incomplete information.","集成任务未明确指定主要编程语言，但涉及配置多种服务。可观测性任务涉及Go (30%), Python (25%), TypeScript (25%), Java (10%), 和 C++ (10%)。","Observability tasks... Each task includes a real GitHub issue from widely adopted programming languages, including Go, TypeScript, Python, Rust, and Java... Observability tasks are distributed across five languages: Go (30%), Python (25%), TypeScript (25%), Java (10%), and C++ (10%).",包含200个任务：集成任务100个，可观测性任务100个。此外，还发布了一个包含50个任务的开发集。,Integration tasks (n = 100)... Observability tasks (n = 100)... We open-source the APEX–SWE evaluation harness and a dev set (n = 50).,集成任务：由拥有3年以上经验的软件工程师创建，经过三阶段验证过程。可观测性任务：源自真实世界的GitHub Issue–PR对，来自至少有350颗星的仓库，并经过复杂性筛选（补丁至少100行代码且影响至少3个文件）。,"Integration Tasks Data - Task Sourcing: Tasks were created by software engineers with 3+ years of experience. Each case underwent a three-stage validation process... Observability Tasks Data - Task Sourcing: Tasks are derived from real-world GitHub Issue–PR pairs, sourced from repositories with at least 350 stars. We filtered for complexity, selecting only patches with at least 100 lines of code impacting at least three files.",2026-01-13 (论文版本日期),arXiv:2601.08806v1  [cs.SE]  13 Jan 2026,官方自建。由Mercor公司的研究团队和平台专家构建。,"Production processes for APEX–SWE Integration and APEX–SWE Observability, leveraging Mercor’s platform of experts.",文中未明确讨论数据污染状态。,,CC-BY许可证（针对开发集）。,We have released an open-source dev set via Hugging Face with a CC-BY license1,系统级/项目级任务。涉及构建端到端系统或诊断修复整个生产故障，而非单个函数生成或代码补全。,APEX–SWE assesses two novel task types that reflect real-world software engineering: (1) Integration tasks... which require constructing end-to-end systems... (2) Observability tasks... which require debugging production failures...,"Pass@1, Pass@3。此外，还使用评分标准（Rubric）评估工程质量，包括功能标准、鲁棒性标准和风格标准（仅可观测性任务）。","For the APEX–SWE leaderboard, we assess models’ outputs using Pass@1... We also report Pass@3... Beyond Pass@1, APEX–SWE uses rubrics to assess engineering quality.",自然语言任务描述、系统提示、工具定义、文件（如interface.md）、日志数据、聊天历史、GitHub issue上下文。,"The model receives a system prompt containing task instructions and tool definitions... Observability tasks... The model is provided with a summary of the user issue, an interface.md file... and specifications for available Model Context Protocol (MCP) tools. Chat data is pulled from public developer discussions... and GitHub issue data are taken directly from the repo.",代码（应用代码、基础设施配置）、补丁文件、通过终端和文件操作执行的系统命令和配置更改。,"Models are required to write application code, configure infrastructure, and deploy functioning services... The harness applies the model’s patch...",文本到系统实现/修复。输入是自然语言描述和上下文文件，输出是导致系统功能正确运行的代码、配置和操作序列。,APEX–SWE Integration... evaluates a model’s ability to orchestrate end-to-end workflows... APEX–SWE Observability... evaluates a model’s ability to diagnose and remediate real-world production failures.,"容器化沙箱环境，包含多种云服务模拟（AWS LocalStack）、生产级业务应用（EspoCRM, Medusa等）、日志聚合系统（Loki, Promtail, Grafana）、数据库（PostgreSQL）和协作工具（Plane, Mattermost）。模型通过终端、文件操作和MCP服务器工具与环境交互。","Integration tasks... The test stack includes cloud primitives (AWS LocalStack: S3, Lambda, DynamoDB, Kinesis) and production-grade business applications (EspoCRM, Medusa, Zammad, Plane)... Observability tasks... Each task deploys a containerized environment orchestrating five services: a client workspace, Loki and Promtail for log aggregation, Grafana for visualization, and Plane/Mattermost for ticket and chat context... Models have access to three categories of tools: Terminal..., File Operations..., and MCP Servers...","1. 专注于真实世界、具有经济价值的软件工程任务，而非狭窄的、定义明确的任务。2. 包含两种新颖的任务类型：集成和可观测性。3. 强调认知推理（区分假设与事实）和代理能力（行动前解决不确定性）是成功的关键驱动因素。4. 包含复杂的身份验证和安全方案（Basic Auth, JWT, IAM, API密钥），测试模型处理真实凭证管理的能力。5. 使用评分标准（Rubric）进行超越二进制通过/失败的工程质量评估。","Unlike existing evaluations that focus on narrow, well-defined tasks, APEX–SWE assesses two novel task types that reflect real-world software engineering... Our analysis shows that strong performance is primarily driven by epistemic reasoning, defined as the ability to distinguish between assumptions and verified facts, combined with agency to resolve uncertainty prior to acting... Tasks have authentication schemes to test models’ ability to handle real-world credential management... Beyond Pass@1, APEX–SWE uses rubrics to assess engineering quality."
2601.09703_output/content.md,ShorterCodeBench,"producing ShorterCodeBench—a corpus of validated ⟨original code, simplified code⟩pairs with semantic consistency;",Yes,"We present and publicly release ShorterCodeBench, a high-quality code brevity optimization dataset comprising 828 carefully curated ⟨original code, simplified code⟩ pairs.",https://github.com/DeepSoftwareAnalytics/ShorterCode,We provide the code and data at https://github.com/DeepSoftwareAnalytics/ShorterCode.,代码简洁性优化。该数据集包含原始代码和经过简化（保持语义一致）的代码对，旨在用于训练或评估模型生成简洁代码的能力。,"producing ShorterCodeBench, a corpus of validated ⟨original code, simplified code⟩pairs with semantic consistency.",代码生成效率（简洁性/令牌效率）,optimizes code generation efficiency while preserving semantic equivalence and readability.,文中未明确描述ShorterCodeBench自身的评估方法，但提及了在HumanEval等基准上使用pass@k等指标评估模型。,,单函数或代码片段级别,The rule formulation process comprises three phases: • Manual Rule Elicitation. We manually conduct inspection of each MBPP dataset entry... (MBPP是单函数级别任务，暗示了ShorterCodeBench的构建基础),通用编程（基于Python语法）,we first designed ten syntax-level simplification rules for Python,文中未明确描述,,Python,we first designed ten syntax-level simplification rules for Python,828个经过验证的<原始代码，简化代码>对,"producing ShorterCodeBench, a corpus of 828 validated ⟨original code, simplified code⟩pairs",基于MBPP数据集条目，通过手动规则启发、专家驱动扩展和验证驱动最终确定的规则进行合成。,We manually conduct inspection of each MBPP dataset entry... We engage 3 experts who are proficient in Python syntax to systematically extend the rule set... We perform cross-dataset validation...,2026-01-14 (根据arXiv版本v1日期),arXiv:2601.09703v1  [cs.SE]  14 Jan 2026,官方自建（由论文作者团队构建）,We present and publicly release ShorterCodeBench,文中未明确描述,,文中未明确描述,,代码转换/代码重写（从冗长代码到简洁代码）,"a corpus of validated ⟨original code, simplified code⟩pairs",文中未明确描述ShorterCodeBench自身的评估指标。,,代码,"⟨original code, simplified code⟩",代码,"⟨original code, simplified code⟩",代码到代码,"⟨original code, simplified code⟩",文中未明确描述,,专注于代码简洁性（令牌效率）优化，通过10个基于AST保持转换的Python语法级简化规则构建，确保语义一致性。,"ten syntax-level simplification rules for Python, derived from AST-preserving transformations, achieving 18.1% token reduction without functional compromise."
2601.09097_output/content.md,TravelPlanner,"We evaluate SCOPE on benchmarks such as TravelPlanner (Xie et al., 2024) and Natural Plan (Zheng et al., 2024)","No, 本文是使用该数据集进行评测","We evaluate SCOPE on benchmarks such as TravelPlanner (Xie et al., 2024) and Natural Plan (Zheng et al., 2024)",,,多约束规划，涉及识别、评估和优化候选计划，同时满足多个可能相互冲突的约束条件。,"Multi-constraint planning involves identify- ing, evaluating, and refining candidate plans while satisfying multiple, potentially conflict- ing constraints.",多约束规划能力、推理的鲁棒性和可扩展性,"Existing large language model (LLM) approaches face fundamental limita- tions in this domain. Pure reasoning paradigms... are prone to inconsistency, error accumula- tion, and prohibitive cost as constraints com- pound.",成功率（success rate）,it reaches 93.1% success on TravelPlanner,,,规划（Planning），特别是多约束规划,Planning is the process by which an agent orga- nizes sequences of decisions or actions to achieve a goal.,涉及长视野推理和复杂约束的规划任务,Benchmarks... show failures under long- horizon reasoning and complex constraints.,,,,,,,2024,"TravelPlanner (Xie et al., 2024)",,,,,,,规划任务（Planning）,Planning is the process by which an agent orga- nizes sequences of decisions or actions to achieve a goal.,成功率,it reaches 93.1% success on TravelPlanner,自然语言查询,to convert natural language queries into optimized structured representations,自然语言答案或结构化计划,converts it into a natural language answer that follows the desired output format.,文本到文本（自然语言查询到自然语言/结构化计划）,to convert natural language queries into optimized structured representations... converts it into a natural language answer,,,专注于多约束规划任务，评估模型在长视野推理和复杂约束下的失败情况。,"Benchmarks (Valmeekam et al., 2023; Zheng et al., 2024; Xie et al., 2024) show failures under long- horizon reasoning and complex constraints."
2601.10402_output/content.md,MLE-Bench,"Within this paradigm, the machine learning engineering (MLE) tasks emphasized by OpenAI’s MLE-Bench [2] emerge as its quintessential challenge.","No, 本文是使用该数据集进行评测",We evaluate ML-Master 2.0 on OpenAI’s MLE-Bench under a fixed 24-hour execution budget.,,,评测基准旨在评估智能体在真实世界机器学习工程项目中的能力，这些项目源自Kaggle竞赛，要求智能体在长时间跨度内进行探索、试错和经验积累。,"MLE-Bench is a benchmark comprising 75 real-world Kaggle machine learning competitions. Far exceeding simple code generation, it requires agents to navigate a vast, unstructured search space through prolonged trial and error and the accumulation of experience across iterations, rather than by single-step correctness.",超长视野自主性、机器学习工程能力、战略连贯性、迭代修正能力,This intrinsic complexity necessitates ultra-long-horizon autonomy. It refers to the capacity to sustain strategic coherence and perform iterative correction over extended temporal scales without being overwhelmed by the accumulation of execution details.,在固定24小时执行预算下，使用奖牌率（获得铜牌、银牌或金牌级别性能的任务百分比）来衡量性能。,"We evaluate ML-Master 2.0 on OpenAI’s MLE-Bench under a fixed 24-hour execution budget. As shown in Figure 1, we measure performance using the average medal rate, defined as the percentage of tasks where the method achieves Bronze, Silver, or Gold-level performance.",超长视野、多阶段、多轨迹探索，涉及跨迭代的经验积累和战略规划。,"The interaction interval [𝑡𝑝−1, 𝑡𝑝) therefore corresponds to one coherent exploration phase, typically consisting of multiple parallel implementation trajectories.",机器学习工程,"Within this paradigm, the machine learning engineering (MLE) tasks emphasized by OpenAI’s MLE-Bench [2] emerge as its quintessential challenge.",包含低、中、高三种复杂度级别的任务,"We can see that ML-Master 2.0 achieves state-of-the-art performance across all difficulty levels. In particular, ML-Master 2.0 attains an overall medal rate of 56.44%... The gains are consistent across task complexities: performance on low-complexity tasks improves from 48.48% to 75.76%, while medium-complexity and high-complexity tasks improve from 20.18% to 50.88% and from 24.44% to 42.22%, respectively.",,,包含75个真实世界的Kaggle机器学习竞赛任务,MLE-Bench is a benchmark comprising 75 real-world Kaggle machine learning competitions.,源自Kaggle竞赛,MLE-Bench is a benchmark comprising 75 real-world Kaggle machine learning competitions.,,,OpenAI构建,OpenAI’s MLE-Bench [2],,,,,端到端的机器学习工程项目，远超简单的代码生成,"Far exceeding simple code generation, it requires agents to navigate a vast, unstructured search space through prolonged trial and error and the accumulation of experience across iterations, rather than by single-step correctness.",奖牌率（Medal Rate）,"We measure performance using the average medal rate, defined as the percentage of tasks where the method achieves Bronze, Silver, or Gold-level performance.",任务描述、用户指令、执行反馈等环境事件,"We partition the event space into environment-originated events U (e.g., task descriptions, user instructions, execution feedback)",代码补丁、命令、计划等智能体动作,"and agent-originated events A (e.g., code patches, commands, plans)",复杂交互序列到最终解决方案代码,after which the final solution code 𝐼∗= ℎ(E𝑡max) is obtained by applying the extraction function ℎ(·) to the terminal interaction history.,,,专注于评估超长视野自主性，模拟真实科学研究中跨越数天或数周的实验周期，强调战略连贯性和迭代修正，而非单步正确性。,"Scientific Discovery is inherently a ultra-long-horizon process, characterized not by momentary acts of reasoning but by delayed feedback, high-dimensional exploration, and experimental cycles spanning days or weeks."
2601.10496_output/content.md,ManySStuBs4J,We introduce an exposure-aware evaluation framework... Using the ManySStuBs4J benchmark...,"No, 本文是使用该数据集进行评测",We introduce an exposure-aware evaluation framework that quantifies how prior exposure to buggy versus fixed code influences a model’s preference. Using the ManySStuBs4J benchmark...,,,评测代码大语言模型在单语句错误（SStuBs）及其修复版本之间的偏好，并量化训练数据暴露（即模型在训练中是否见过错误或修复代码）对此偏好的影响。,We introduce an exposure-aware evaluation framework that quantifies how prior exposure to buggy versus fixed code influences a model’s preference.,模型对错误代码与修复代码的偏好；训练数据暴露（记忆）对模型行为的影响；错误传播风险。,"Distinguishing whether an LLM prefers correct code, or a familiar incorrect version might be influenced by what it’s been exposed to during training.",使用基于似然性的评分指标（如最小/最大词元概率、基尼系数）评估模型偏好；通过代码补全生成并匹配原始错误或修复变体。,We then measure model preference with a suite of likelihood-based metrics. Additionally we evaluate model completions based on the bug-fix context.,单语句上下文（单行代码变更）。,SStuBs focus on single-statement bugs that can be fixed with single-line changes.,软件工程，代码缺陷与修复。,"The ManySStubs4J dataset collects thousands of SStuBs, (‘Simple, Stupid Bugs’), which are single-statement bugs and their fixes from open-source Java repositories.",简单但影响重大的错误（例如差一错误、错误条件判断、缺少空值检查等）。,"which capture small but impactful errors (off-by-one mistakes, incorrect conditionals, missing null checks, etc.) in real programs.",Java,"We use the ManySStuBs4J dataset, which contains mined bug-fix pairs from open-source Java projects.",包含数千个错误-修复对。,The ManySStubs4J dataset collects thousands of SStuBs...,从开源Java仓库中挖掘的真实错误及其修复。,The ManySStubs4J dataset collects thousands of SStuBs... from open-source Java repositories.,,,社区贡献（从开源项目挖掘）。,The ManySStubs4J dataset collects thousands of SStuBs... from open-source Java repositories.,高污染风险（论文核心关注训练数据暴露对评测的影响）。,Our results indicate that exposure can skew bug-fix evaluations and highlight the risk that LLMs may propagate memorised errors in practice.,,,代码补全（在给定上下文中生成单行代码）。,We then measure model preference... Additionally we evaluate model completions based on the bug-fix context.,最小词元概率、最大词元概率、基尼系数等基于似然性的指标；生成代码与原始错误/修复的匹配率。,likelihood-based metrics such as the minimum and maximum token probability... others like the Gini coefficient... Generation matching,代码与自然语言（包含错误的代码上下文）。,We then measure model preference... Additionally we evaluate model completions based on the bug-fix context.,代码（单行）。,SStuBs focus on single-statement bugs that can be fixed with single-line changes.,代码到代码（在包含错误的代码上下文中生成修复后的代码行）。,We then measure model preference for either the bug or the fix... evaluate model completions based on the bug-fix context.,,,专注于单语句错误（SStuBs）；集成了训练数据暴露（成员推断）分析，以区分模型偏好是源于记忆还是真正的正确性学习；用于研究错误传播风险。,SStuBs focus on single-statement bugs that can be fixed with single-line changes... We introduce an exposure-aware evaluation framework that quantifies how prior exposure to buggy versus fixed code influences a model’s preference... Our results indicate that exposure can skew bug-fix evaluations and highlight the risk that LLMs may propagate memorised errors in practice.
2601.10343_output/content.md,OCTOBENCH,"To fill this gap, we introduce OCTOBENCH, which benchmarks scaffold-aware instruction following in repository-grounded agentic coding.",Yes,"To fill this gap, we introduce OCTOBENCH, which benchmarks scaffold-aware instruction following in repository-grounded agentic coding.",https://github.com/MiniMax-AI/mini-vela,§ https://github.com/MiniMax-AI/mini-vela,评测智能体在基于代码仓库的、多轮交互的代理式编码场景中，遵循脚手架所指定的、来自异构来源的、持久性指令的能力。,"OCTOBENCH includes 34 environments and 217 tasks instantiated under three scaffold types, and is paired with 7,098 objective checklist items. To disentangle solving the task from following the rules, we provide an automated observation-and-scoring toolkit that captures full trajectories and performs fine-grained checks.",脚手架感知的指令遵循能力，包括对异构约束、优先级感知的冲突解决以及跨轮次持久性遵守的评估。,"Compliance is defined over multiple concurrent instruction sources with different authority levels and time horizons. Accordingly, evaluation must account for (i) heterogeneous constraints, (ii) priority-aware conflict resolution, and (iii) persistent adherence across turns, including interactions with tool schemas and state.",通过自动化观察和评分工具包捕获完整交互轨迹，并将其映射到基于实例的、结构化的二进制检查清单，使用LLM-as-a-judge进行细粒度、过程级别的合规性评估。,"Accordingly, we pair each task with a granular observation harness and automated evaluation toolkit that captures and normalizes the agent’s full action trajectory, and then maps the realized behavior to a structured checklist of binary checks with an LLM-as-a-judge (Zheng et al., 2023; Gu et al., 2025). This enables fine-grained, process-level compliance assessment...",基于代码仓库的、多轮交互的代理式编码环境，涉及跨文件、项目API和现有抽象的使用。,"Evaluation of code generation has moved from isolated function synthesis... to repository-level generation and patching that requires using cross-file context, project APIs, and existing abstractions... OCTOBENCH targets this gap by benchmarking instruction adherence in repository-grounded agentic coding...",软件工程，代理式编码，涉及代码生成、修改、工具调用和仓库导航。,"In software engineering, agentic coding scaffolds such as Claude Code (Anthropic, 2025a), Kilo (Kilo, 2025), and Droid (Factory.ai, 2025) turn LLMs into end-to-end coding agents that can navigate repositories, invoke tools, and iteratively modify code.",现实、长上下文、复杂约束结构，源自工业应用场景。,"We construct the first instruction-following benchmark tailored for agentic coding scaffolds, featuring realistic, long-context, and complex constraint structures derived from industrial applications.",文中未明确指定主要编程语言，但任务环境基于通用代码仓库，可能涉及多种语言。,,"包含34个独立环境，217个任务实例，覆盖3种脚手架类型，并配有7,098个客观的检查清单项。","OCTOBENCH includes 34 environments and 217 tasks instantiated under three scaffold types, and is paired with 7,098 objective checklist items.",从工业应用中收集的原始指令承载材料（如仓库文件、技能文档、工具模式等），由人工标注者策划和扩展。,"Starting from raw instruction-carrying materials, human annotators curate executable task and expend the curated queries... Annotators collect and normalize constraint-carrying artifacts from multiple sources and package them into a Docker image, including repository policy files, skill documentation, optional pre-seeded persistent state files, and other auxiliary materials required by the scaffold.",2026-01-15 (根据arXiv版本v1日期),arXiv:2601.10343v1  [cs.CL]  15 Jan 2026,官方自建，由研究团队人工策划种子集，并使用模型扩展。,"We curate the dataset in a seed-and-expand method. Annotators manually construct a seed set of 72 instances, then use a model to expand it to 217 instances.",文中未明确讨论数据污染状态。,,文中未提及数据集的许可证。,,代理式编码任务，涉及多轮交互、工具调用和代码修改。,"Rather than relying on static QA pairs or outcome-only scores, OCTOBENCH targets long-horizon, multi-turn agent-environment interactions in repository-grounded coding tasks.",基于检查清单的二进制检查项得分，用于进行细粒度的过程级合规评估。,"These trajectories are then mapped to an instance-specific binary checklist that operationalizes verifiable constraints across all evidenced sources, and are scored via an LLM-as-a-judge to produce fine-grained metrics...",异构指令来源的组合，包括系统提示、用户查询序列、仓库策略文件、工具模式、可选记忆状态等。,"Each instance packages a self-contained, executable task environment together with a curated task specification (e.g., system prompts, user query sequences, repository policy files, and optional memory state) designed to surface verifiable constraints from heterogeneous instruction sources.",代理在环境中的完整动作轨迹，包括代码脚本、工具调用和交互反馈。,"Accordingly, we pair each task with a granular observation harness and automated evaluation toolkit that captures and normalizes the agent’s full action trajectory...",异构指令到代理动作轨迹。,"Figure 1: Overview of OCTOBENCH. ...combining heterogeneous, persistent instruction sources with a scaffold that interacts with an executable environment, while an observation harness records trajectories.",自包含的可执行编码环境，打包在Docker镜像中，包含仓库文件、技能文档、预置状态文件等。,We construct each instance around a self-contained coding environment that an agent can execute end-to-end... Annotators collect and normalize constraint-carrying artifacts from multiple sources and package them into a Docker image...,1. 首个针对代理式编码脚手架的指令遵循基准。2. 强调异构、持久性约束。3. 提供自动化观察工具包和基于检查清单的细粒度评估，以区分“完成任务”和“遵循规则”。4. 包含一个专门的冲突评估子集OCTOBENCH-CONFLICT。,"To address this gap, we introduce OCTOBENCH, a repository-grounded benchmark for measuring instruction following under realistic agentic coding scaffolds... This enables fine-grained, process-level compliance assessment, explicitly detecting when a model violates constraints during execution, even if the final outcome appears correct, and thereby disentangling solving the task from following the rules. ...we construct OCTOBENCH-CONFLICT, an evaluation set featuring three types of instruction conflicts."
2601.10498_output/content.md,GSM8K,Experiments used the GSM8K dataset [5] with the Qwen-3 0.6B model [11].,"No, 本文是使用该数据集进行评测",Experiments used the GSM8K dataset [5] with the Qwen-3 0.6B model [11].,,,文中未描述GSM8K数据集本身的任务。本文使用它来评估强化学习微调方法（PROMA）的性能。,Experiments used the GSM8K dataset [5] with the Qwen-3 0.6B model [11].,文中未描述GSM8K数据集的评测维度。本文关注的是强化学习微调方法的稳定性、KL散度控制和策略熵。,PROMA achieves comparable or improved validation performance relative to GRPO (Figure 2a)... PROMA also maintains higher entropy for longer during training... PROMA has consistently lower KL divergence between successive policies than both baselines (Figure 2d)...,文中未描述GSM8K数据集原生的评估方法。本文通过验证集性能（Validation performance）、KL散度、策略熵等指标来评估强化学习微调方法。,Figure 2 shows the performance and policy dynamics of PROMA compared to GRPO and REINFORCE. (a) Validation performance. (b) KL divergence from the initial policy. (c) Policy entropy. (d) KL divergence between the current policy and a lagged reference policy...,,,,,,,,,,,,,,,,,,,,,,,验证集性能、KL散度（相对于初始策略和滞后策略）、策略熵,Figure 2 shows the performance and policy dynamics of PROMA compared to GRPO and REINFORCE. (a) Validation performance. (b) KL divergence from the initial policy. (c) Policy entropy. (d) KL divergence between the current policy and a lagged reference policy...,,,,,,,,,本文是方法论文，提出了一种名为PROMA（Projected Microbatch Accumulation）的强化学习微调方法，用于大语言模型。它通过投影微批次梯度来实现无参考策略的近端策略更新，旨在提高训练稳定性并避免熵崩溃。,"This note introduces Projected Microbatch Accumulation (PROMA), a proximal policy update method for large language model fine-tuning... Unlike PPO and GRPO, PROMA achieves proximal updates without inducing entropy collapse and does not rely on a reference policy or likelihood-ratio clipping."
2601.11077_output/content.md,ABC-Bench,"To address this gap, we introduce ABC-Bench, a benchmark explicitly designed to evaluate agentic back-end coding within a realistic, executable workflow.",Yes,"To address this gap, we introduce ABC-Bench, a benchmark explicitly designed to evaluate agentic back-end coding within a realistic, executable workflow.","https://github.com/OpenMOSS/ABC-Bench, https://huggingface.co/datasets/OpenMOSS-Team/ABC-Bench","Github: https://github.com/OpenMOSS/ABC-Bench
Dataset: https://huggingface.co/datasets/OpenMOSS-Team/ABC-Bench",评估智能体在真实、可执行的工作流中进行后端编码的能力，涵盖从仓库探索到实例化容器化服务并通过端到端API测试的完整开发生命周期。,ABC-Bench require the agents to manage the entire development lifecycle from repository exploration to instantiating containerized services and pass the external end-to-end API tests.,智能体后端编码能力，包括仓库探索、代码编辑、环境配置、部署和端到端测试的全流程评估。,"Real-world backend development mandates a continuous workflow spanning five distinct stages: (1) Repository Exploration (Expl.), (2) Code Editing (Code), (3) Environment Setup (Env.), (4) Deployment (Deploy), and (5) End-to-End Testing (E2E).",通过外部API级别的集成测试来严格评估正确性，只有当部署的服务正确启动并表现出预期行为时才给予通过。,"Once the service is launched, we evaluate correctness strictly via external API-level integration tests, awarding credit only when the deployed service starts correctly and exhibits the expected behavior.",多文件项目、仓库级别，需要探索和理解整个代码库结构。,"The evolution of Large Language Models (LLMs) into autonomous agents has expanded the scope of AI coding from localized code generation to complex, repository-level, and execution-driven problem solving.",后端开发，涵盖数据分析、搜索系统、商务平台、支付网关和开发者工具等多个真实世界领域。,"Beyond technical heterogeneity, the tasks are drawn from a broad spectrum of real-world domains, ensuring that the benchmark reflects practical engineering needs. These domains range from data analytics and search systems to commerce platforms, payment gateways, and developer tooling.",工程级，模拟真实世界的生产环境复杂性和全生命周期需求。,"However, current benchmarks predominantly evaluate code logic in static contexts, neglecting the dynamic, full-process requirements of real-world engineering, particularly in back-end development which demands rigorous environment configuration and service deployment.",C#、Rust、JavaScript、Python、Java、Go、PHP、Ruby，共8种编程语言。,"Applied to 2,000 open-source repositories, it yields 224 curated tasks spanning 8 backend programming languages and 19 frameworks, preserving the heterogeneity of real-world backend stacks.",包含224个任务。,"ABC-Bench comprises 224 tasks, offering a diverse and balanced representation of modern backend ecosystems.",从2000个开源、MIT许可的仓库中筛选和构建。,"We initiate the process by filtering a pool of 2,000 open-source, MIT-licensed repositories to isolate high-quality backend candidates.",2026,arXiv:2601.11077v1  [cs.SE]  16 Jan 2026,官方自建，通过名为ABC-Pipeline的自动化工作流程从开源仓库生成。,"To construct realistic backend development tasks at scale, we build the ABC-Pipeline, an automated workflow that converts open-source backend repositories into full-lifecycle development Tasks",文中未明确提及。,,MIT许可证,"We initiate the process by filtering a pool of 2,000 open-source, MIT-licensed repositories to isolate high-quality backend candidates.",全生命周期后端开发任务，包括代码修复、环境配置、容器化部署等。,Each task goes beyond localized code edits and requires the agent to configure the environment and instantiate a containerized service.,pass@1率,Even the top-performing model achieves only a 63.2% pass@1 rate,自然语言任务指令，以及需要探索的代码仓库。,"Within this workspace, the agent is granted full autonomy to explore the repository, modify code, install dependencies, and update Docker configurations",代码修改、环境配置（如Dockerfile）、以及最终能通过API测试的可运行服务。,"Upon submission of the solution or when the interaction budget limits are reached, the evaluation system attempts to build and launch the backend service in a separate inner container using the agent’s generated code and configurations.",文本（任务指令）到可运行的服务（代码+配置+部署）。,"The evaluation setup launches an outer container that hosts the agent, delivers the task prompt.",标准化的、隔离的沙箱环境，使用Docker容器进行构建和部署。,"We evaluate models and agents using a standardized, isolated sandbox environment, which strictly separates the agent’s workspace from the backend service under test.",1. 专注于评估智能体后端编码的全生命周期能力。2. 包含环境配置和容器化部署要求。3. 使用端到端API测试进行验证，而非单元测试。4. 通过自动化流水线(ABC-Pipeline)从真实开源仓库大规模构建任务。5. 涵盖8种编程语言和19种框架，技术栈多样。,"Distinct from previous evaluations, ABC-Bench require the agents to manage the entire development lifecycle from repository exploration to instantiating containerized services and pass the external end-to-end API tests."
2601.10011_output/content.md,BIRD,"On BIRD, Memo-SQL achieves 68.5% execution accuracy, setting a new state of the art among open, zero-fine-tuning methods...","No, 本文是使用该数据集进行评测","Evaluated on the BIRD benchmark (Li et al., 2023a), Memo-SQL achieves 68.5% execution accuracy on the dev-new set...",,,将自然语言问题转换为可执行的SQL查询（NL2SQL）,"Given a natural language question q and a database schema S, the goal of NL2SQL is to generate an executable SQL query y.",SQL查询生成的功能正确性（执行准确性）,"On BIRD, Memo-SQL achieves 68.5% execution accuracy...",执行准确性（execution accuracy）,"On BIRD, Memo-SQL achieves 68.5% execution accuracy...",依赖数据库模式（schema）,"Given a natural language question q and a database schema S, the goal of NL2SQL is to generate an executable SQL query y.",数据库查询（Database Querying），商业智能（BI）,"As outlined in Section A.1 and Figure 1, real-world BI systems log both user feedback and system revisions...",,,SQL,"Given a natural language question q and a database schema S, the goal of NL2SQL is to generate an executable SQL query y.",,,,,,,,,,,,,代码生成（从自然语言生成SQL查询）,"Given a natural language question q and a database schema S, the goal of NL2SQL is to generate an executable SQL query y.",执行准确性（execution accuracy）,"On BIRD, Memo-SQL achieves 68.5% execution accuracy...",自然语言（问题）和数据库模式,"Given a natural language question q and a database schema S, the goal of NL2SQL is to generate an executable SQL query y.",代码（SQL查询）,"Given a natural language question q and a database schema S, the goal of NL2SQL is to generate an executable SQL query y.",文本到代码（自然语言到SQL）,"Given a natural language question q and a database schema S, the goal of NL2SQL is to generate an executable SQL query y.",数据库执行环境,observes the execution result from the database,专注于现实世界、大规模数据库的NL2SQL任务，包含有噪声和领域特定的表述。,"As outlined in Section A.1 and Figure 1, real-world BI systems log both user feedback and system revisions..."
2601.10955_output/content.md,ToolBench,"Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens...","No, 本文是使用该数据集进行评测","Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens...",,,评测基准旨在评估大型语言模型（LLM）代理使用外部工具执行多步任务的能力。,LLM agents can interact with external tools and execute multi-step tasks across various domains.,工具调用能力、多步任务执行、资源消耗（经济与计算成本）,"These results elevate the agent-tool interface to a first-class security frontier, demanding a paradigm shift from validating final answers to monitoring the economic and computational cost of the entire agentic process.",通过攻击方法诱导代理进行多轮、冗长的工具调用序列，并测量由此产生的资源消耗（如总输出令牌数、成本膨胀倍数、能耗增加、GPU KV缓存占用率、系统吞吐量下降）。,"Our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658×, and raises energy by 100–560×. It drives GPU KV cache occupancy from <1% to 35–74% and cuts co-running throughput by approximately 50%.",多轮代理-工具交互，具有状态性。,"To overcome the limitations of single-turn attacks, our methodology is designed to exploit the multi-turn, stateful nature of agent-tool interactions.",通用工具调用任务（如日期/时间、货币转换、搜索、数据转换等）。,"In modern automated agents, such tool calls are frequent for routine capabilities (date/time, currency conversion, search, data transforms, etc.)",,,,,,,,,,,,,,,,,多步任务执行与工具调用,LLM agents can interact with external tools and execute multi-step tasks across various domains.,总输出令牌数、成本膨胀倍数、能耗倍数、GPU KV缓存占用率、系统吞吐量下降百分比,"Our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658×, and raises energy by 100–560×. It drives GPU KV cache occupancy from <1% to 35–74% and cuts co-running throughput by approximately 50%.",自然语言查询（用户任务）,Let 𝑞∈X denote a user query issued to the agent stack.,代码（工具调用）与自然语言（最终答案）,"The agent policy 𝐴parses the LLM outputs from 𝑀, routes tool invocations, handles retries/repairs, and decides when to stop.",自然语言到工具调用序列及最终答案,"An agent’s workflow is a trajectory 𝜏= {(𝑎𝑡,𝑟𝑡)}𝑛 𝑡=1, a sequence of tool calls (𝑎𝑡) and tool responses (𝑟𝑡).",与遵循模型上下文协议（MCP）的外部工具服务器交互的环境。,"The agent interacts with an external MCP server 𝑇𝜃, whose behavior is controlled by a configuration template 𝜃. The MCP server exposes a set of tool functions F and communicates strictly via the MCP protocol...",本文并非提出新的评测基准，而是利用现有的ToolBench和BFCL基准来演示一种新型的经济性拒绝服务（DoS）攻击。该攻击通过操纵工具服务器的响应，诱导LLM代理进行冗长的多轮工具调用，从而在保持任务结果正确的前提下，极大地放大资源消耗。这揭示了代理-工具交互层作为一个关键安全攻击面的重要性。,"We introduce a stealthy, multi-turn economic DoS attack that operates at the tool layer under the guise of a correctly completed task. ... These results elevate the agent-tool interface to a first-class security frontier..."
2601.10820_output/content.md,未命名（内部数据集）,"On a novel in-house dataset, our approach achieves 38% and 150% improvement in the evaluation metric over manually crafted and unplanned workflows respectively.",Yes,"We devise a first-of-its-kind, PySpark-based, multi-turn, repository-level benchmarking dataset that faithfully mirrors real-world production ML feature engineering pipelines.",,,评测基准旨在评估多智能体框架在真实工业环境中进行机器学习特征工程的能力，具体任务包括生成PySpark脚本、单元测试和配置文件，用于构建电子商务用户-推荐管道中的特征。,"We introduce a newly developed benchmark dataset that closely mirrors real-world industrial environments for machine learning featurization, specifically leveraging PySpark for large-scale data processing. It comprises 10 tasks that involve generating PySpark scripts, unit tests, and configuration files, each focused on constructing features for user-offer recommendation pipelines in e-commerce environments.",多步、仓库级代码生成的可靠性与正确性，以及智能体在复杂工作流中的协调能力。,"We devise a first-of-its-kind, PySpark-based, multi-turn, repository-level benchmarking dataset that faithfully mirrors real-world production ML feature engineering pipelines.",使用 pass@3 指标，即三次运行中成功次数的比例。,"Our primary metric is pass@3, the fraction of successful runs out of three.","仓库级代码生成，依赖代码库README、可重用工具、配置文件（FSC, DFR）和团队特定的工作流。","Each task is described using three inputs: (i) FSC (Feature Specification Config), a YAML file specifying target features, base columns and their datasets, and computation logic; (ii) DFR (DataFrame Registry), a YAML file describing base datasets and dependencies; and (iii) a run file specifying the codebase README, reusable utilities, and working repository.",机器学习特征工程，特别是针对大规模数据处理的电子商务推荐系统。,"It comprises 10 tasks that involve generating PySpark scripts, unit tests, and configuration files, each focused on constructing features for user-offer recommendation pipelines in e-commerce environments.",生产级复杂度，模拟真实工业环境中的复杂工作流。,"Unlike community datasets such as SWE-bench [9] or MLE-bench [2], which primarily utilize Pandas, our dataset is tailored to the complexities of production-scale workflows.",主要涉及 PySpark（用于大规模数据处理）。,"We devise a first-of-its-kind, PySpark-based, multi-turn, repository-level benchmarking dataset...",包含10个任务，外加一个独立的第0个任务用于方法开发（不参与评测）。,"It comprises 10 tasks that involve generating PySpark scripts, unit tests, and configuration files... Additionally, a separate held-out (0th) task is used exclusively for developing our proposed approach and is not included in benchmarking.",内部构建，旨在紧密模拟真实工业环境。,We introduce a newly developed benchmark dataset that closely mirrors real-world industrial environments for machine learning featurization...,2026（根据预印本日期推断）,arXiv:2601.10820v1  [cs.LG]  15 Jan 2026,官方自建（内部数据集）,We devise a first-of-its-kind... benchmarking dataset...,,,,,仓库级代码生成，涉及生成多个文件（脚本、测试、配置）。,"It comprises 10 tasks that involve generating PySpark scripts, unit tests, and configuration files...",pass@3,"Our primary metric is pass@3, the fraction of successful runs out of three.",配置文件（YAML）、数据注册表、代码库文档和工具。,"Each task is described using three inputs: (i) FSC (Feature Specification Config), a YAML file... (ii) DFR (DataFrame Registry), a YAML file... (iii) a run file specifying the codebase README, reusable utilities...",代码（PySpark脚本）、配置文件、单元测试文件。,"It comprises 10 tasks that involve generating PySpark scripts, unit tests, and configuration files...",配置与文档到代码,"Each task is described using three inputs... involve generating PySpark scripts, unit tests, and configuration files...",PySpark分布式运行时环境。,specifically leveraging PySpark for large-scale data processing.,"首个专注于PySpark、多轮次、仓库级的基准数据集，专门针对生产级机器学习特征工程管道，模拟真实工业环境的复杂性（如处理延迟的Spark输出、数据分布知识、特征完整性检查）。与主要使用Pandas的社区数据集（如SWE-bench, MLE-bench）形成对比。","We devise a first-of-its-kind, PySpark-based, multi-turn, repository-level benchmarking dataset that faithfully mirrors real-world production ML feature engineering pipelines. Unlike community datasets such as SWE-bench [9] or MLE-bench [2], which primarily utilize Pandas, our dataset is tailored to the complexities of production-scale workflows."
2601.11960_output/content.md,MATH-500,achieving average accuracy gains of 3.1% on MATH-500 and 2.4% on APPS,"No, 本文是使用该数据集进行评测","Experiments across multiple benchmarks show that R2PO consistently outperforms baselines, achieving average accuracy gains of 3.1% on MATH-500 and 2.4% on APPS",,,数学问题求解,mathematical problem solving,数学推理能力,mathematical reasoning,准确率 (accuracy),achieving average accuracy gains of 3.1% on MATH-500,,,数学,mathematical reasoning,,,,,500个问题,MATH-500,,,,,,,,,,,数学问题求解,mathematical problem solving,准确率,achieving average accuracy gains of 3.1% on MATH-500,,,,,,,,,,
2601.13864_output/content.md,HardSecBench,"In this work, we introduce HardSecBench, a benchmark with 924 tasks spanning Verilog Register Transfer Level (RTL) and firmware-level C, covering 76 hardware-relevant Common Weakness Enumeration (CWE) entries.",Yes,"In this work, we introduce HardSecBench... This critical research gap motivates us to design a benchmark for assessing security awareness under realistic specifications.",,Our data and code will be released soon.,评估大语言模型在硬件代码生成中的安全意识。具体任务是根据功能需求生成硬件设计代码（Verilog RTL或C语言固件），同时要求生成的代码满足隐含的安全要求，避免特定的硬件安全漏洞。,a benchmark for assessing security awareness under realistic specifications... each task includes a structured specification that separates functional and security requirements... evaluates whether models implement protections checked by the security requirements.,硬件代码生成的安全性意识,assessing security awareness... evaluate the systematic security of LLM-based hardware designs,通过模拟或执行针对性的测试工具来验证安全要求。每个任务都有独立的测试工具，这些工具会主动触发与安全相关的行为，并根据模拟证据给出PASS/FAIL的确定性结果。,HardSecBench avoids subjective judging and scores security using simulation evidence from targeted harnesses that actively trigger security relevant behaviors... Each harness is self-contained and takes the form of a testbench for RTL tasks or a standalone C driver for firmware tasks. The test harness emits a standardized PASS/FAIL trace so that verification can parse results deterministically.,模块级设计。每个任务包含一个结构化的问题描述、I/O接口规范以及功能和安全需求集。,"each task includes a structured specification... including a problem statement, an I/O interface specification, and two requirement sets: functional requirements Rf_i and security requirements Rs_i.",硬件设计与固件开发，具体涉及Verilog RTL设计和C语言固件开发。,hardware and firmware development... spanning Verilog Register Transfer Level (RTL) and firmware-level C,,,Verilog（用于RTL设计）和C（用于固件开发）,spanning Verilog Register Transfer Level (RTL) and firmware-level C,包含924个任务,a benchmark with 924 tasks,通过一个多智能体构建流程自动合成。流程从CWE（通用缺陷枚举）定义出发，生成任务种子，然后由智能体扩展为完整的规范、参考实现和可执行的测试工具。,"We develop a multi-agent construction pipeline that synthesizes benchmark samples... Starting from CWE-derived seeds, the Architect produces Pi... The Expert synthesizes the golden implementation... the Tester derives atomic harnesses...",2026-01-20 (根据arXiv版本v1日期),arXiv:2601.13864v1  [cs.CR]  20 Jan 2026,官方自建。论文作者团队通过提出的自动化流程构建。,"we design an automated pipeline that scales benchmark construction... Using this pipeline, we build HardSecBench",,,,,代码生成。根据功能需求生成完整的硬件模块或固件代码。,hardware code generation... The Expert synthesizes the golden implementation to satisfy both Rf_i and Rs_i,基于模拟证据的安全测试通过/失败（PASS/FAIL）。,scores security using simulation evidence... The test harness emits a standardized PASS/FAIL trace,自然语言（结构化规范，包含问题描述、I/O接口和功能需求）。安全需求在评估时对模型隐藏。,"a structured specification Pi for task i, including a problem statement, an I/O interface specification, and two requirement sets: functional requirements Rf_i and security requirements Rs_i... during evaluation, we give the target model without Rs_i",代码（Verilog RTL或C语言）。,Verilog Register Transfer Level (RTL) and firmware-level C,文本到代码。输入是自然语言规范，输出是硬件设计代码。,synthesizing the golden implementation from the specification Pi,针对硬件设计的模拟环境（如RTL仿真器）和针对C固件的执行环境。,verify by simulation... testbench for RTL tasks or a standalone C driver for firmware tasks,1. 专注于硬件代码生成的安全性评估，填补了现有基准的空白。2. 覆盖76种硬件相关的CWE漏洞类型。3. 采用多智能体自动化构建流程，将规范、实现和验证解耦，确保评估的客观性和可靠性。4. 规范明确区分功能需求和安全需求，在评估时仅向模型提供功能需求，以测试其隐含的安全意识。,"Existing studies have primarily focused on evaluating the functional correctness of LLM-generated code, yet paid limited attention to its security issues... covers 76 hardware-relevant Common Weakness Enumeration (CWE) entries... a multi-agent pipeline that decouples synthesis from verification... separates functional requirements Rf_i from security requirements Rs_i, so that the functional specification can be used to elicit implementations without revealing security intent"
2601.13943_output/content.md,RepoGenesis,"To bridge this gap, we introduce RepoGenesis, the first multilingual benchmark for repository-level end-to-end web microservice generation, comprising 106 repositories (60 Python, 46 Java) across 18 domains and 11 frameworks...",Yes,"To bridge this gap, we introduce RepoGenesis, the first multilingual benchmark for repository-level web microservice generation.",https://github.com/pzy2000/RepoGenesis/,We release our benchmark at https://github.com/pzy2000/RepoGenesis/.,从自然语言需求文档（README.md）生成完整的、可部署的微服务仓库，包括源代码、配置文件和依赖规范。,"Given a requirement document (README.md) specifying service functionality, API endpoints with input/output schemas, authentication mechanisms, and operational constraints, models must generate a fully deployable repository including all source code, configuration files, and dependency specifications.",架构设计、依赖管理、系统级一致性、功能正确性、API实现完整性、可部署性,"RepoGenesis targets critical yet under-evaluated capabilities essential for real-world engineering: architectural design, dependency management, and system-level consistency.",黑盒测试，包括部署微服务、执行测试用例并计算指标。,"We evaluate generated repositories using black-box testing: deploying the microservice, executing test cases, and computing metrics.",仓库级别，需要生成包含多个文件、配置和依赖的完整项目。,the first benchmark specifically designed to evaluate repository-level web microservice generation.,Web微服务开发，涵盖18个领域，包括认证、内容管理、聊天、游戏后端、文件管理、数据搜索、用户系统、调度等。,"comprising 106 repositories (60 Python, 46 Java) across 18 domains and 11 frameworks",分为简单、中等、困难三个级别，基于代码复杂度指标。,"Repositories are classified into Easy, Medium, and Hard difficulty levels based on code complexity metrics",Python 和 Java,"comprising 106 repositories (60 Python, 46 Java)",包含106个仓库（76个用于训练，30个用于评估），共1258个API端点和2335个测试用例。,"comprising 106 repositories (60 Python, 46 Java)... with 1,258 API endpoints and 2,335 test cases",混合来源：来自GitHub的真实世界仓库（6个）和专家监督下生成的仓库（100个）。,We collected repositories from two sources: Real-world GitHub Repositories... Expert-supervised Repository Generation.,2026-01-20 (根据arXiv版本v1日期),arXiv:2601.13943v1  [cs.SE]  20 Jan 2026,官方自建，结合了真实项目收集和专家监督生成。,We introduce RepoGenesis... The benchmark includes both real-world GitHub projects and expert-curated implementations,文中未明确讨论数据污染状态。,,文中未提及数据集的许可证。,,完整的仓库生成,repository-level end-to-end web microservice generation,Pass@1（功能正确性）、API覆盖率（AC，实现完整性）、部署成功率（DSR，可部署性）,"We evaluate open-source agents (e.g., DeepCode) and commercial IDEs (e.g., Cursor) using Pass@1, API Coverage (AC), and Deployment Success Rate (DSR).",自然语言需求文档（README.md文件）,"Given a requirement document (README.md) specifying service functionality, API endpoints with input/output schemas...",代码仓库，包含源代码文件、配置文件和依赖规范。,"The agent generates a complete repository from scratch, including source code files, configuration files, and dependency specifications.",文本到代码仓库,Given a requirement document... models must generate a fully deployable repository...,沙盒执行环境,Repositories are executed in sandboxed environments,首个专注于Web微服务完整仓库生成的多语言基准；包含严格的“评审-反驳”质量保证流程，涉及三个LLM评审员和一个人工领域主席；评估维度包括功能正确性、API覆盖率和部署成功率。,"the first multilingual benchmark for repository-level web microservice generation... we implement a rigorous quality assurance mechanism featuring a “review-rebuttal” loop with three LLM reviewers and a human Area Chair... We propose a multi-dimensional evaluation methodology (i.e., Pass@1, AC, and DSR)"
2601.14027_output/content.md,Putnam 2025,We evaluated Numina-Lean-Agent on the Putnam 2025 benchmark and compared its performance with other existing provers.,"No, 本文是使用该数据集进行评测",We evaluated Numina-Lean-Agent on the Putnam 2025 benchmark and compared its performance with other existing provers.,,,形式定理证明，即在严格定义的逻辑系统（如Lean）中为数学定理构建机器可验证的证明。,"Formal theorem proving aims to construct machine-verifiable proofs for mathematical theorems within rigorously defined logical systems, such as Lean (2015) and Isabelle (Paulson, 1994).",形式定理证明能力，解决数学竞赛问题的能力。,"Using Claude Opus 4.5 (Anthropic, 2025) as the base model, Numina-Lean-Agent successfully solved all 12 problems in the Putnam 2025, achieving state-of-the-art performance.",在形式定理证明器（Lean）中验证生成的证明是否正确，成功解决问题的数量。,"Notably, we used the formal statements provided by Seed-Prover 1.5. ... Under these settings, Numina-Lean-Agent achieved state-of-the-art performance, successfully solving 12 out of 12 problems on Putnam 2025.",,,数学，特别是数学竞赛（Putnam）级别的定理证明。,"We evaluated Numina-Lean-Agent on the Putnam 2025 benchmark... Beyond standard automated proving, Numina-Lean-Agent serves as a general mathematical reasoning system...",竞赛级（Putnam数学竞赛难度）。,"We evaluated Numina-Lean-Agent on the Putnam 2025 benchmark... Due to their substantially higher difficulty and longer proof search trajectories, problem A5 is assigned a larger budget...",形式化数学语言（Lean）。,"Formal theorem proving aims to construct machine-verifiable proofs for mathematical theorems within rigorously defined logical systems, such as Lean (2015)...",包含12个问题。,Numina-Lean-Agent successfully solved all 12 problems in the Putnam 2025...,数学竞赛（Putnam）问题。,We evaluated Numina-Lean-Agent on the Putnam 2025 benchmark...,2025,Putnam 2025,,,,,,,定理证明（生成完整的、可验证的形式化证明）。,Formal theorem proving aims to construct machine-verifiable proofs for mathematical theorems...,解决问题的数量（如12/12）。,"Numina-Lean-Agent successfully solved all 12 problems in the Putnam 2025, achieving state-of-the-art performance.",形式化数学陈述（定理）。,"Notably, we used the formal statements provided by Seed-Prover 1.5.",形式化证明（Lean代码）。,Formal theorem proving aims to construct machine-verifiable proofs...,定理到证明,Formal theorem proving aims to construct machine-verifiable proofs for mathematical theorems...,形式定理证明器环境（如Lean编译器）。,"Lean-LSP-MCP (Dressler, 2025) is a Model Context Protocol (MCP) server explicitly designed for the Lean theorem prover. Acting as a bridge between LLMs and the Lean kernel via the Language Server Protocol (LSP)...",Putnam数学竞赛是一个著名的、高难度的大学数学竞赛，其问题被用作评估形式定理证明系统能力的基准。,We evaluated Numina-Lean-Agent on the Putnam 2025 benchmark and compared its performance with other existing provers.
2601.15195_output/content.md,AIDev-pop dataset,"In this paper, we conduct a large-scale empirical study on agent-authored pull requests using the AIDev-pop dataset [26], which comprises over 33k PRs submitted by five major coding agents across GitHub projects with more than 100 stars.","No, 本文是使用该数据集进行评测","In this paper, we conduct a large-scale empirical study on agent-authored pull requests using the AIDev-pop dataset [26]...",,,评测AI编码代理在真实软件开发工作流（如提交Pull Request）中的表现，特别是其失败原因。,"In this paper, we conduct a large-scale study of 33k agent-authored PRs made by five coding agents across GitHub. (RQ1) We first quantitatively characterize merged and not-merged PRs... (RQ2) To further investigate why some agentic PRs are not merged, we qualitatively analyze 600 PRs to derive a hierarchical taxonomy of rejection patterns.",AI编码代理在真实协作环境（Pull Request）中的成功率、失败模式、任务类型表现、代码变更规模、CI结果、评审动态。,"We first quantitatively characterize merged and not-merged PRs along four broad dimensions: 1) merge outcomes across task types, 2) code changes, 3) CI build results, and 4) review dynamics.",定量分析（统计合并率、代码变更行数、文件数、CI检查失败数、评审评论数、评审修订数，使用效应量Cliff's delta、核密度估计、逻辑回归建模）和定性分析（对600个被拒PR进行手动编码，建立拒绝模式分类法）。,"We perform a quantitative characterization of agent-authored pull requests along four dimensions... we rely on effect size measures, using Cliff’s delta (δ) to quantify the magnitude of difference... we use kernel density estimates... we use logistic regression modeling... We randomly select a subset of 600 rejected PRs for qualitative analysis... Following open coding [1], two authors independently label each PR with its primary reason for rejection.",多文件项目级别的真实协作环境，涉及代码审查、CI/CD流水线验证和迭代修订。,"...how agents perform when integrated into real development workflows involving CI validation, code review, and iterative revision.",通用软件开发，涵盖功能、修复、性能、重构、样式、文档、测试、杂务、构建、CI等多种任务类型。,"These tasks consist of 11 categories: feature, fix, performance, refactoring, style, documentation, test, chore, build, CI, and other [7, 26].",真实世界工程级难度，涉及与项目工作流、开发者期望和项目协调的对齐。,"Overall, our results suggest that agentic PR failures stem from misalignment with repository workflows (e.g., CI/CD failures), developer expectations (e.g., unwanted or incorrect features), and a lack of project coordination (e.g., reviewer abandonment).",文中未明确提及数据集涵盖的具体编程语言，数据集由来自GitHub的真实PR构成，可能包含多种语言。,,超过33k个由五个主要编码代理提交的Pull Request。,"In this paper, we conduct a large-scale study of 33k agent-authored PRs made by five coding agents across GitHub.",来自GitHub上星标数超过100的项目的真实Pull Request。,...over 33k PRs submitted by five major coding agents across GitHub projects with more than 100 stars.,2026（根据论文会议日期推断）,"MSR 2026, Rio de Janeiro, Brazil",基于真实GitHub项目数据构建（AIDev-pop数据集）。,"...using the AIDev-pop dataset [26], which comprises over 33k PRs submitted by five major coding agents across GitHub projects...",,,,,Pull Request级别的代码贡献，包括生成、修改、修复等。,"AI coding agents are now submitting pull requests (PRs) to software projects, acting not just as assistants but as autonomous contributors.",合并率、代码变更行数（#LOC Changes）、变更文件数（#File Changes）、CI检查失败数（#Failed CI Checks）、评审评论数（#Review Comments）、评审修订数（#Review Revisions）、效应量（Cliff's delta）。,"We analyze the magnitude of proposed code changes by measuring a) the total number of added and removed lines of code (#LOC Changes), and b) the number of files modified by each PR (#File Changes)... we extract the number of failed check-runs (#Failed CI Checks)... we compute a) the number of review comments in a PR (#Review Comments), and b) the number of review revisions each PR receives (#Review Revisions)... we rely on effect size measures, using Cliff’s delta (δ)...",文中未明确描述数据集中每个任务的具体输入形式，但根据上下文，输入可能包括问题描述、代码上下文或评审反馈。,,代码变更（以Pull Request形式提交）。,"AI coding agents... now generate code changes, respond to reviewer feedback, and participate in the software lifecycle as autonomous agents.",多种类型，可能包括文本到代码（如根据描述实现功能）、代码到代码（如修复、重构）、以及交互式任务（响应评审）。,"Coding agents have been extensively benchmarked across a range of tasks, from code generation [5, 33], testing [23, 31, 39], to automated program repair [8, 22, 30].",真实项目的CI/CD流水线环境。,"Not-merged PRs tend to involve larger code changes, touch more files, and often do not pass the project’s CI/CD pipeline validation.",专注于AI编码代理在真实、协作的软件开发工作流（Pull Request）中的表现评估，而非孤立的代码生成任务。它分析了社会技术因素和人类-AI协作对PR成功的影响。,"While prior work evaluates agents in isolated tasks, we lack a systematic assessment of how agents perform when integrated into real development workflows involving CI validation, code review, and iterative revision... Together, our findings highlight key socio-technical and human-AI collaboration factors that are critical to improving the success of future agentic workflows."
2601.15165_output/content.md,GSM8K,"Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K)...","No, 本文是使用该数据集进行评测","On complex reasoning benchmarks, it achieves competitive results (e.g., 89.1% accuracy on GSM8K, 45.1% on MATH)...",,,数学推理（小学数学应用题）,"...in general reasoning tasks like mathematics and coding... On complex reasoning benchmarks, it achieves competitive results (e.g., 89.1% accuracy on GSM8K)...",数学推理能力,...in general reasoning tasks like mathematics and coding...,准确率 (accuracy),...89.1% accuracy on GSM8K...,,,数学,...in general reasoning tasks like mathematics...,,,,,,,,,,,,,,,,,数学问题求解,...in general reasoning tasks like mathematics...,准确率 (accuracy),...89.1% accuracy on GSM8K...,自然语言（数学问题描述）,...in general reasoning tasks like mathematics...,自然语言（数学推理步骤与答案）,,文本到文本（数学问题到解答）,...in general reasoning tasks like mathematics...,,,,
2601.15188_output/content.md,未明确命名，但基于HumanEval构建的ABAP代码生成基准,"The experimental procedure for the empirical investigation of ABAP code generation by LLMs is based on several central aspects: A benchmark with 180 tasks serves as the data basis, covering both general algorithmic problems (based on HumanEval) and specific SAP scenarios.",Yes,"This work aims to apply a benchmark approach to ABAP code generation as a prerequisite to empirically investigate and systematically evaluate the performance of LLMs in generating ABAP code. By such an environment it can be examined to what extent LLMs can produce ABAP programs that are syntactically correct, semantically accurate, and practically relevant code.",,,评估大型语言模型生成ABAP代码的能力，包括语法正确性、语义准确性和实际相关性。,"The aim of the study is to empirically analyze to what extent various LLMs can generate syntactically correct and functional ABAP code, how effectively they use compiler feedback for iterative improvement, and which task types pose special challenges.",代码生成正确性、迭代改进能力、对不同任务类型的适应性,"The aim of the study is to empirically analyze to what extent various LLMs can generate syntactically correct and functional ABAP code, how effectively they use compiler feedback for iterative improvement, and which task types pose special challenges.",通过自动化单元测试验证功能正确性，并进行详细的错误分析和Kaplan-Meier生存分析以评估学习曲线。,"Success is measured based on functional correctness, which is verified by these automated unit tests. In addition, a detailed error analysis and a Kaplan-Meier survival analysis are carried out to evaluate the learning curves.",单函数/方法级别（要求生成全局类中的静态方法）,"Global classes with static methods are required, as the test setup requires this structure and this ensures the comparability of the results.",通用算法问题（基于HumanEval）和特定的SAP业务场景（如数据库操作）,"A benchmark with 180 tasks serves as the data basis, covering both general algorithmic problems (based on HumanEval) and specific SAP scenarios.",覆盖不同难度级别，包括基础算法和实际SAP用例,"The test cases cover different levels of difficulty and types of tasks, enabling a broad evaluation of the models.",ABAP,This work investigates the performance of Large Language Models (LLMs) in generating ABAP code.,180个任务，其中164个基于HumanEval的标准化任务，16个ABAP特定任务,"The developed benchmark comprises a total of 180 test tasks, which are divided into two groups: 164 standardized tasks based on the HumanEval dataset [3], which was originally developed for evaluating code generation in Python, and 16 ABAP-specific tasks.",改编自HumanEval数据集（Python任务）和源自实际SAP场景的特定任务,"164 standardized tasks based on the HumanEval dataset [3], which was originally developed for evaluating code generation in Python, and 16 ABAP-specific tasks. These 16 specific tasks are derived from practical SAP-specific scenarios focusing on the processing of database tables.",2026,arXiv:2601.15188v1  [cs.SE]  21 Jan 2026,官方自建（本研究构建）,The developed benchmark comprises a total of 180 test tasks... The dataset for the benchmark is available in the appendix A.,,,,,代码生成（从自然语言描述生成完整的ABAP类和方法）,"The aim of the study is to empirically analyze to what extent various LLMs can generate syntactically correct and functional ABAP code... Both class definition and implementation code are to be provided, as both are required for carrying out the evaluation.",功能正确性（通过单元测试验证）,"Success is measured based on functional correctness, which is verified by these automated unit tests.",自然语言（任务描述）,"It consists of programming tasks formulated in natural language and validated by predefined unit tests, thus enabling an objective measurement of functional correctness [3].",代码（ABAP类和方法）,"Both class definition and implementation code are to be provided, as both are required for carrying out the evaluation.",文本到代码,"It consists of programming tasks formulated in natural language and validated by predefined unit tests, thus enabling an objective measurement of functional correctness [3].",标准化的SAP ABAP运行时环境（Docker镜像）,"The entire process is fully automated and uses a standardized SAP environment (Docker image), which is addressed via a Python control and the ADT interface.",1. 专注于企业级、领域特定的编程语言ABAP。2. 包含迭代反馈循环（最多5次），利用ABAP编译器错误信息进行改进。3. 任务分为五类：字符串处理、列表/数组操作、数学计算、逻辑条件、ABAP数据库操作。4. 结合了通用算法任务（改编自HumanEval）和实际SAP业务场景任务。,"The tasks are divided into five thematic categories, including for example String Handling and ABAP Database Operation. A special focus is placed on increasing the probability of successful code generation through an iterative feedback process: each task is processed in up to five feedback loops, during which the system provides the LLM with feedback from the ABAP compiler or automated unit tests."
