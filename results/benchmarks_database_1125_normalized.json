[
  {
    "source_paper": "2511.20403_output/content.md",
    "benchmark_name": "CLASSES2TEST",
    "benchmark_name_quote": "We introduce the CLASSES2TEST dataset, which maps Java classes under test to their corresponding test classes",
    "is_original_proposal": "Yesï¼Œæœ¬æ–‡æ˜¯è¯¥æ•°æ®é›†çš„åŸå§‹å‘å¸ƒè®ºæ–‡",
    "is_original_proposal_quote": "We introduce the CLASSES2TEST dataset, which maps Java classes under test to their corresponding test classes, and a framework that integrates advanced evaluation metrics",
    "dataset_url": "https://anonymous.4open.science/r/classes2test",
    "dataset_url_quote": "1https://anonymous.4open.science/r/classes2test",
    "task_description": "ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„Javaå•å…ƒæµ‹è¯•çš„è´¨é‡ï¼Œæ”¯æŒç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æ¯”è¾ƒä¸åŒLLMå’Œæç¤ºç­–ç•¥",
    "task_description_quote": "AGONETEST does not aim to propose a novel test generation algorithm; rather, it supports researchers and developers in comparing different LLMs and prompting strategies through a standardized end-to-end evaluation pipeline under realistic conditions.",
    "dimension": "å•å…ƒæµ‹è¯•è´¨é‡è¯„ä¼°ï¼ŒåŒ…æ‹¬ç¼–è¯‘æˆåŠŸç‡ã€ä»£ç è¦†ç›–ç‡ã€ç¼ºé™·æ£€æµ‹èƒ½åŠ›ã€æµ‹è¯•å¼‚å‘³ç­‰",
    "dimension_quote": "a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment",
    "evaluation_method": "é›†æˆé«˜çº§è¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚å˜å¼‚åˆ†æ•°å’Œæµ‹è¯•å¼‚å‘³ï¼Œè¿›è¡Œç»¼åˆè¯„ä¼°",
    "evaluation_method_quote": "a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment",
    "context_dependency": "ç±»çº§åˆ«æµ‹è¯•ï¼Œæ¶µç›–æ–¹æ³•äº¤äº’å’Œå…±äº«çŠ¶æ€",
    "context_dependency_quote": "Our approach focuses on class-level test code evaluation, which is closer to real-world practices as it covers method interactions and shared state",
    "problem_domain": "è½¯ä»¶æµ‹è¯•ï¼ŒJavaå•å…ƒæµ‹è¯•ç”Ÿæˆ",
    "problem_domain_quote": "Unit testing is an essential but resource-intensive step in software development, ensuring individual code units function correctly.",
    "problem_difficulty": "ç°å®ä¸–ç•Œè½¯ä»¶é¡¹ç›®çº§åˆ«ï¼Œæ¯”å•æ–¹æ³•æµ‹è¯•æ›´å¤æ‚",
    "problem_difficulty_quote": "This extended dataset makes it possible to assess the test performance of an LLM on a more complex scope (the entire class) than the single method.",
    "language": "Java",
    "language_quote": "An annotated open source Java project dataset extending METHODS2TEST [9]",
    "data_size": "åŸºäº9,410ä¸ªGitHubä»“åº“çš„æ•°æ®é›†",
    "data_size_quote": "AGONETEST offers far broader applicability by using a dataset of 9,410 GitHub repositories",
    "source_type": "æ‰©å±•è‡ªMETHODS2TESTæ•°æ®é›†çš„Javaå¼€æºé¡¹ç›®",
    "source_type_quote": "An annotated open source Java project dataset extending METHODS2TEST [9]",
    "last_updated": 2025,
    "last_updated_quote": "arXiv:2511.20403v1  [cs.SE]  25 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ŒåŸºäºç°æœ‰æ•°æ®é›†æ‰©å±•",
    "build_type_quote": "Leveraging the METHODS2TEST dataset [9], we developed a new dataset specifically aimed at comparing human-written tests with those produced by LLMs.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ç±»çº§åˆ«å•å…ƒæµ‹è¯•ç”Ÿæˆ",
    "task_granularity_quote": "Our approach focuses on class-level test code evaluation, which is closer to real-world practices as it covers method interactions and shared state",
    "evaluation_metrics": "å˜å¼‚åˆ†æ•°ã€æµ‹è¯•å¼‚å‘³ã€ä»£ç è¦†ç›–ç‡",
    "evaluation_metrics_quote": "a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment",
    "input_modality": "Javaç±»ä»£ç ",
    "input_modality_quote": "which maps classes under test to their related test classes",
    "output_modality": "å•å…ƒæµ‹è¯•ä»£ç ",
    "output_modality_quote": "for the subset of tests that compile, LLM-generated tests can match or exceed human-written tests in terms of coverage and defect detection",
    "task_io_type": "ä»£ç åˆ°ä»£ç ",
    "task_io_type_quote": "which maps Java classes under test to their corresponding test classes",
    "execution_environment": "æ”¯æŒæ‰€æœ‰Java LTSç‰ˆæœ¬çš„é¡¹ç›®ç¯å¢ƒ",
    "execution_environment_quote": "AGONETEST overcomes this barrier by supporting all Java LTS versions.",
    "unique_features": "ä¸“æ³¨äºç±»çº§åˆ«æµ‹è¯•è¯„ä¼°ï¼Œæ”¯æŒå¤šç§LLMå’Œæç¤ºç­–ç•¥çš„æ¯”è¾ƒï¼Œæä¾›ç«¯åˆ°ç«¯çš„è‡ªåŠ¨åŒ–è¯„ä¼°æµæ°´çº¿",
    "unique_features_quote": "AGONETEST shifts the focus to the generation of class-level tests. Our approach makes it possible to use up-to-date LLMs and not constrain prompt design (our prompts can be customized), thereby handling more complex, real-world scenarios.",
    "data_size_quantity": 9410,
    "data_size_unit": "ä¸ªGitHubä»“åº“",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Java']",
    "dimension_normalized": "['å•å…ƒæµ‹è¯•è´¨é‡è¯„ä¼°', 'ç¼–è¯‘æˆåŠŸç‡', 'ä»£ç è¦†ç›–ç‡', 'ç¼ºé™·æ£€æµ‹èƒ½åŠ›', 'æµ‹è¯•å¼‚å‘³']",
    "evaluation_method_normalized": "['å˜å¼‚åˆ†æ•°', 'æµ‹è¯•å¼‚å‘³', 'ä»£ç è¦†ç›–ç‡']",
    "problem_domain_normalized": "['è½¯ä»¶æµ‹è¯•', 'Javaå•å…ƒæµ‹è¯•ç”Ÿæˆ']",
    "source_type_normalized": "['æ‰©å±•è‡ªMETHODS2TESTæ•°æ®é›†çš„Javaå¼€æºé¡¹ç›®']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å•æ–‡ä»¶",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.21380_output/content.md",
    "benchmark_name": "ROCODE, LogHub2.0",
    "benchmark_name_quote": "Two projects (i.e., ROCODE [15] and LogHub2.0 [16]) are selected for the following experiments.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We evaluate Copilot, backed by GPT-4.1 and Claude Sonnet 4, on adapting SE research artifacts from benchmark repositories including ROCODE and LogHub2.0.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "æ•°æ®é›†é€‚åº”ä»»åŠ¡ - å°†è½¯ä»¶å·¥ç¨‹ç ”ç©¶å·¥å…·è‡ªåŠ¨é€‚é…åˆ°æ–°çš„æ•°æ®é›†ï¼Œæ„å»ºå¯è¿è¡Œçš„å®éªŒå¹¶è·å–æ‰§è¡Œç»“æœ",
    "task_description_quote": "Our objective is to automatically modify ğ‘…ğ·or ğ‘…ğ‘‡, construct a runnable experiment, and obtain its execution results.",
    "dimension": "å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨æ•°æ®é›†é€‚åº”ä»»åŠ¡ä¸­çš„èƒ½åŠ›è¯„ä¼°",
    "dimension_quote": "This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks.",
    "evaluation_method": "äº”é˜¶æ®µè¯„ä¼°æµç¨‹ï¼šæ–‡ä»¶ç†è§£ã€ä»£ç ç¼–è¾‘ã€å‘½ä»¤ç”Ÿæˆã€éªŒè¯å’Œæœ€ç»ˆæ‰§è¡Œï¼Œæµ‹é‡æˆåŠŸç‡å¹¶åˆ†æå¤±è´¥æ¨¡å¼",
    "evaluation_method_quote": "Through a five-stage evaluation pipeline (file comprehension, code editing, command generation, validation, and final execution), we measure success rates, analyze failure patterns, and assess prompt-based interventions designed to enhance agent performance.",
    "context_dependency": "å¤šæ–‡ä»¶é¡¹ç›®ç¯å¢ƒï¼Œéœ€è¦ç†è§£æ•´ä¸ªä»£ç ä»“åº“çš„æ¶æ„",
    "context_dependency_quote": "Before modifying code or generating commands for a repository, a multi-agent system must browse the essential files in a repository to understand its architecture.",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ç ”ç©¶ï¼ŒåŒ…æ‹¬ä»£ç ç”Ÿæˆã€bugä¿®å¤ã€æ€§èƒ½åˆ†æå’Œå®‰å…¨ç­‰é¢†åŸŸ",
    "problem_domain_quote": "In areas ranging from code generation and bug fixing to performance analysis and security, researchers are constantly proposing new techniques",
    "problem_difficulty": "å¤æ‚è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ï¼Œéœ€è¦å¤šæ­¥éª¤åè°ƒå’Œè¿­ä»£ä¿®å¤",
    "problem_difficulty_quote": "Most of the evaluated multi-agent systems failed to complete the assigned tasks. Under such circumstances, nearly all results would be classified as failures",
    "language": "Python",
    "language_quote": "To ensure a consistent and manageable experimental environment, we retained only artifacts implemented in Python.",
    "data_size": "ä»é¡¶çº§è½¯ä»¶å·¥ç¨‹ä¼šè®®ï¼ˆFSE, ICSE, ASE, ISSTAï¼‰2024-2025å¹´æ¥å—çš„è®ºæ–‡ä¸­ç­›é€‰çš„å¯é‡ç”¨ç ”ç©¶æ„ä»¶",
    "data_size_quote": "We began by collecting all papers accepted by the top four SE (FSE, ICSE, ASE and ISSTA) in 2024-2025 that were either (i) awarded a Reusable Artifact badge or (ii) explicitly identified by the authors as providing reusable artifacts.",
    "source_type": "å­¦æœ¯ç ”ç©¶æ„ä»¶ï¼Œæ¥è‡ªé¡¶çº§è½¯ä»¶å·¥ç¨‹ä¼šè®®çš„å¯é‡ç”¨ç ”ç©¶é¡¹ç›®",
    "source_type_quote": "To construct a representative set of high-quality SE research artifacts, we followed a systematic multi-stage selection process",
    "last_updated": 2025,
    "last_updated_quote": "arXiv:2511.21380v1  [cs.SE]  26 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºçš„ç ”ç©¶æ„ä»¶é›†åˆ",
    "build_type_quote": "We began by collecting all papers accepted by the top four SE (FSE, ICSE, ASE and ISSTA) in 2024-2025",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç¼–è¾‘ã€æ–‡ä»¶åˆ›å»ºã€å‘½ä»¤ç”Ÿæˆã€éªŒè¯ä¿®å¤",
    "task_granularity_quote": "Edit and create necessary files... Generate and execute the commands... Validating and repairing",
    "evaluation_metrics": "æˆåŠŸç‡ã€ç»“æ„ç›¸ä¼¼åº¦ï¼ˆä»7.25%åˆ°67.14%ï¼‰ã€å®ŒæˆçŠ¶æ€è®°å½•",
    "evaluation_metrics_quote": "Results show that... substantially improve structural similarity to ground truth (from 7.25% to 67.14%)",
    "input_modality": "ä»£ç ä»“åº“ã€è‡ªç„¶è¯­è¨€æç¤º",
    "input_modality_quote": "Each multi-agent system is provided with a processed repository... along with a simple prompt that specifies the adaptation task",
    "output_modality": "ä¿®æ”¹åçš„ä»£ç ã€ç”Ÿæˆçš„è„šæœ¬å‘½ä»¤ã€æ‰§è¡Œç»“æœ",
    "output_modality_quote": "the code adaptations performed to ensure compatibility, the executable scripts or commands derived for running the experiment, and the results produced by executing those scripts or commands",
    "task_io_type": "ä»£ç åˆ°ä»£ç çš„é€‚åº”ä»»åŠ¡",
    "task_io_type_quote": "automatically modify ğ‘…ğ·or ğ‘…ğ‘‡, construct a runnable experiment, and obtain its execution results",
    "execution_environment": "Pythonç¯å¢ƒï¼Œéœ€è¦å¯é çš„éƒ¨ç½²ç¯å¢ƒ",
    "execution_environment_quote": "This choice allows for reliable environment deployment and isolates our investigation from environment setup challenges",
    "unique_features": "ä¸“æ³¨äºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨æ•°æ®é›†é€‚åº”ä»»åŠ¡ä¸­çš„è¡¨ç°è¯„ä¼°ï¼Œé‡‡ç”¨äº”é˜¶æ®µè¯„ä¼°æµç¨‹ï¼Œç ”ç©¶æç¤ºçº§å¹²é¢„å¯¹æ€§èƒ½çš„å½±å“",
    "unique_features_quote": "This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks. Through a five-stage evaluation pipeline... we measure success rates, analyze failure patterns, and assess prompt-based interventions",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨æ•°æ®é›†é€‚åº”ä»»åŠ¡ä¸­çš„èƒ½åŠ›è¯„ä¼°']",
    "evaluation_method_normalized": "['æˆåŠŸç‡', 'ç»“æ„ç›¸ä¼¼åº¦', 'å®ŒæˆçŠ¶æ€è®°å½•']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹ç ”ç©¶', 'ä»£ç ç”Ÿæˆ', 'bugä¿®å¤', 'æ€§èƒ½åˆ†æ', 'å®‰å…¨']",
    "source_type_normalized": "['å­¦æœ¯ç ”ç©¶æ„ä»¶', 'é¡¶çº§è½¯ä»¶å·¥ç¨‹ä¼šè®®', 'å¯é‡ç”¨ç ”ç©¶é¡¹ç›®']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.21022_output/content.md",
    "benchmark_name": "EDAPIBench",
    "benchmark_name_quote": "We introduce EDAPIBench, a dedicated benchmark featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances.",
    "is_original_proposal": "Yesï¼Œæœ¬æ–‡æ˜¯è¯¥æ•°æ®é›†çš„åŸå§‹å‘å¸ƒè®ºæ–‡",
    "is_original_proposal_quote": "We introduce EDAPIBench, a dedicated benchmark... We construct EDAPIBenchâ€”the first dedicated benchmark for evaluating deprecated API knowledge editing in LLMs",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ä¸­åºŸå¼ƒAPIçŸ¥è¯†ç¼–è¾‘çš„æ€§èƒ½ï¼Œä¸“é—¨ç”¨äºæµ‹è¯•æ¨¡å‹ç¼–è¾‘æŠ€æœ¯èƒ½å¦æœ‰æ•ˆæ›´æ–°åºŸå¼ƒAPIçŸ¥è¯†å¹¶ç”Ÿæˆæœ€æ–°çš„API",
    "task_description_quote": "a dedicated benchmark for evaluating deprecated API knowledge editing in LLMs... whether existing model editing methods can effectively update deprecated API knowledge within LLMs, and enable the edited models to correctly replace deprecated APIs with up-to-date ones",
    "dimension": "æœ‰æ•ˆæ€§ã€æ³›åŒ–æ€§ã€å¯ç§»æ¤æ€§ã€ç‰¹å¼‚æ€§",
    "dimension_quote": "comprehensively assess model editing performance across four key dimensions: Effectiveness, Generalization, Portability, Specificity",
    "evaluation_method": "åŸºäºå››ä¸ªç»´åº¦çš„è¯„ä¼°ï¼šæœ‰æ•ˆæ€§ï¼ˆç¼–è¾‘åæ¨¡å‹æ˜¯å¦ç”Ÿæˆæœ€æ–°APIï¼‰ã€æ³›åŒ–æ€§ï¼ˆè¯­ä¹‰ç­‰ä»·ä½†è¯­æ³•å˜åŒ–çš„è¾“å…¥ï¼‰ã€å¯ç§»æ¤æ€§ï¼ˆä¸åŒè¾“å…¥ä½†æ¶‰åŠç›¸åŒåºŸå¼ƒAPIï¼‰ã€ç‰¹å¼‚æ€§ï¼ˆä¿æŒä¸ç¼–è¾‘ä»»åŠ¡æ— å…³è¾“å…¥çš„è¡Œä¸ºä¸€è‡´æ€§ï¼‰",
    "evaluation_method_quote": "Effectiveness: Measuring whether the edited model generates the up-to-date API for the original inputs; Generalization: Measuring whether it generates the up-to-date API on semantically equivalent but syntactically varied inputs; Portability: Measuring whether it generates the up-to-date API across different inputs... Specificity: Measuring whether it preserves consistent pre-editing behavior on inputs unrelated to the editing task",
    "context_dependency": "å•å‡½æ•°çº§åˆ«çš„ä»£ç è¡¥å…¨ä¸Šä¸‹æ–‡",
    "context_dependency_quote": "Our study frames the model editing scenario as a code completion task... we extract lines preceding the API invocation as candidate editing inputs (to prompt LLM API completion) and the invocation line as the target API (ground truth)",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ã€APIç»´æŠ¤ã€ç¬¬ä¸‰æ–¹åº“æ›´æ–°",
    "problem_domain_quote": "deprecated APIs from 8 popular Python libraries... covering eight popular Python libraries such as PyTorch and TensorFlow",
    "problem_difficulty": "å®é™…å·¥ç¨‹çº§éš¾åº¦ï¼Œæ¶‰åŠçœŸå®ä¸–ç•ŒåºŸå¼ƒAPIçš„è¯†åˆ«å’Œæ›´æ–°",
    "problem_difficulty_quote": "Using these mappings, we extract 65,596 real-world functions from GitHub that call the up-to-date APIs",
    "language": "Python",
    "language_quote": "deprecated APIs from 8 popular Python libraries... covering eight popular Python libraries such as PyTorch and TensorFlow",
    "data_size": "åŒ…å«70å¤šä¸ªåºŸå¼ƒAPIï¼Œè¶…è¿‡3000ä¸ªç¼–è¾‘å®ä¾‹ï¼Œä»GitHubæå–äº†65,596ä¸ªçœŸå®ä¸–ç•Œå‡½æ•°",
    "data_size_quote": "featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances... we extract 65,596 real-world functions from GitHub",
    "source_type": "ä»GitHubçœŸå®é¡¹ç›®ä»£ç ä¸­æå–ï¼ŒåŸºäºå·²éªŒè¯çš„APIæ˜ å°„å…³ç³»",
    "source_type_quote": "We begin with 145 verified API mappings (deprecated â†’up-to-date) from Wang et al. [49]... Using these mappings, we extract 65,596 real-world functions from GitHub that call the up-to-date APIs",
    "last_updated": "2025-2026",
    "last_updated_quote": "Publication date: November 2026. arXiv:2511.21022v1 [cs.SE] 26 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œå…¨è‡ªåŠ¨æ„å»º",
    "build_type_quote": "which can be fully automatically constructed... with fully automated construction",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "APIçº§åˆ«çš„ä»£ç è¡¥å…¨",
    "task_granularity_quote": "code completion tasks... during code completion, LLMs frequently suggest deprecated API invocations",
    "evaluation_metrics": "åŸºäºå››ä¸ªç»´åº¦çš„å®šæ€§è¯„ä¼°ï¼šæœ‰æ•ˆæ€§ã€æ³›åŒ–æ€§ã€å¯ç§»æ¤æ€§ã€ç‰¹å¼‚æ€§",
    "evaluation_metrics_quote": "Effectiveness, Generalization, Portability, Specificity",
    "input_modality": "ä»£ç ç‰‡æ®µ",
    "input_modality_quote": "the editing input (a code snippet to be completed)",
    "output_modality": "APIè°ƒç”¨ä»£ç ",
    "output_modality_quote": "the specific correct, up-to-date API that should replace the deprecated one",
    "task_io_type": "ä»£ç åˆ°ä»£ç ",
    "task_io_type_quote": "code completion task... generating code",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "é¦–ä¸ªä¸“é—¨é’ˆå¯¹åºŸå¼ƒAPIçŸ¥è¯†ç¼–è¾‘çš„åŸºå‡†æµ‹è¯•ï¼Œæ”¯æŒå…¨è‡ªåŠ¨æ„å»ºï¼ŒåŒ…å«å››ä¸ªç»´åº¦çš„ç»¼åˆè¯„ä¼°",
    "unique_features_quote": "the first dedicated benchmark for evaluating deprecated API knowledge editing in LLMs, with fully automated construction, serving as a standardized, rigorous platform",
    "data_size_quantity": 65596,
    "data_size_unit": "ä¸ªå‡½æ•°",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['æœ‰æ•ˆæ€§', 'æ³›åŒ–æ€§', 'å¯ç§»æ¤æ€§', 'ç‰¹å¼‚æ€§']",
    "evaluation_method_normalized": "['æœ‰æ•ˆæ€§', 'æ³›åŒ–æ€§', 'å¯ç§»æ¤æ€§', 'ç‰¹å¼‚æ€§']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'APIç»´æŠ¤', 'ç¬¬ä¸‰æ–¹åº“æ›´æ–°']",
    "source_type_normalized": "['ä»GitHubçœŸå®é¡¹ç›®ä»£ç ä¸­æå–', 'åŸºäºå·²éªŒè¯çš„APIæ˜ å°„å…³ç³»']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç è¡¥å…¨",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.19875_output/content.md",
    "benchmark_name": "CodeFuse-CommitEval",
    "benchmark_name_quote": "We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models.",
    "is_original_proposal": "Yesï¼Œæœ¬æ–‡æ˜¯è¯¥æ•°æ®é›†çš„åŸå§‹å‘å¸ƒè®ºæ–‡",
    "is_original_proposal_quote": "We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models.",
    "dataset_url": "https://figshare.com/s/21fe4ec9cb960b52bffe",
    "dataset_url_quote": "The dataset is publicly available at https://figshare.com/s/21fe4ec9cb960b52bffe.",
    "task_description": "æ£€æµ‹æäº¤ä¿¡æ¯ä¸ä»£ç å˜æ›´ä¹‹é—´çš„ä¸ä¸€è‡´æ€§ï¼ˆMessage-Code Inconsistency, MCIï¼‰",
    "task_description_quote": "A Message-Code Inconsistency (MCI) occurs when the natural language description in a commit message does not accurately reflect the actual modifications in the associated code diff.",
    "dimension": "æäº¤ä¿¡æ¯ä¸ä»£ç å˜æ›´çš„ä¸€è‡´æ€§æ£€æµ‹èƒ½åŠ›",
    "dimension_quote": "evaluate models for MCI detection",
    "evaluation_method": "ä½¿ç”¨Recallã€Precisionã€Specificityç­‰æŒ‡æ ‡è¯„ä¼°æ¨¡å‹æ£€æµ‹ç»“æœ",
    "evaluation_method_quote": "Results show models detect inconsistent commits more reliably than consistent ones (average Recall 85.95%, Precision 80.28%, Specificity 63.8%)",
    "context_dependency": "æäº¤ä¿¡æ¯ä¸å¯¹åº”çš„ä»£ç å·®å¼‚ï¼ˆdiffï¼‰",
    "context_dependency_quote": "Each item of the verified dataset contains a commit message, the corresponding code diff, and the ground truth label",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ã€ç‰ˆæœ¬æ§åˆ¶ã€ä»£ç å®¡æŸ¥",
    "problem_domain_quote": "Version control relies on commit messages to convey the rationale for code changes",
    "problem_difficulty": "éœ€è¦è¯­ä¹‰ç†è§£å’Œä¸Šä¸‹æ–‡æ¨ç†çš„å¤æ‚ä»»åŠ¡",
    "problem_difficulty_quote": "purpose inconsistencies require deeper semantic understanding and contextual reasoning",
    "language": "å¤šç§ç¼–ç¨‹è¯­è¨€ï¼ˆåŸºäºApacheCMæ•°æ®é›†çš„å¤šæ ·æ€§ï¼‰",
    "language_quote": "because of its diversity in programming languages and the high-quality commits",
    "data_size": "åŒ…å«æ­£è´Ÿæ ·æœ¬çš„å¹³è¡¡æ•°æ®é›†",
    "data_size_quote": "we generate a balanced dataset with both positive and negative samples",
    "source_type": "åŸºäºApacheCMæ•°æ®é›†ï¼Œé€šè¿‡è§„åˆ™å¼•å¯¼çš„çªå˜ç”Ÿæˆä¸ä¸€è‡´æäº¤ä¿¡æ¯",
    "source_type_quote": "Built on the ApacheCM dataset for diversity and quality, we generate seven types of inconsistent messages through rule-guided mutations of originally consistent commits",
    "last_updated": "2025",
    "last_updated_quote": "arXiv:2511.19875v1 [cs.SE] 25 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œç»“åˆLLMæ•°æ®åˆæˆèƒ½åŠ›ä¸ç°æœ‰æäº¤è¯­æ–™åº“",
    "build_type_quote": "We present a comprehensive pipeline for constructing MCI datasets. By combining the data synthesis capabilities of LLMs with existing commit corpora",
    "contamination_status": "é€šè¿‡åŒé‡éªŒè¯ç¡®ä¿æ•°æ®è´¨é‡",
    "contamination_status_quote": "apply two-fold validation to verify both positive and negative samples",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä¸€è‡´æ€§æ£€æµ‹",
    "task_granularity_quote": "detecting inconsistencies between commit messages and code",
    "evaluation_metrics": "Recall, Precision, Specificity",
    "evaluation_metrics_quote": "average Recall 85.95%, Precision 80.28%, Specificity 63.8%",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆæäº¤ä¿¡æ¯ï¼‰ä¸ä»£ç å·®å¼‚",
    "input_modality_quote": "Each item of the verified dataset contains a commit message, the corresponding code diff",
    "output_modality": "äºŒå…ƒåˆ†ç±»ï¼ˆä¸€è‡´æˆ–ä¸ä¸€è‡´ï¼‰",
    "output_modality_quote": "allowing the models to detect whether the commit is consistent or not",
    "task_io_type": "æ–‡æœ¬ä¸ä»£ç åˆ°åˆ†ç±»",
    "task_io_type_quote": "detect whether the commit is consistent or not",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "é¦–ä¸ªä¸“é—¨ç”¨äºMCIæ£€æµ‹çš„åŸºå‡†ï¼ŒåŒ…å«ä¸ƒç§ä¸ä¸€è‡´ç±»å‹ï¼Œæ”¯æŒå¤šç§å¢å¼ºç­–ç•¥è¯„ä¼°",
    "unique_features_quote": "CODEFUSE-COMMITEVAL is the first benchmarking work tailored for comprehensively evaluating LLM's MCI detection ability. We define seven mutation rules to guide powerful LLMs in generating various types of inconsistent commit messages",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['å¤šç§ç¼–ç¨‹è¯­è¨€']",
    "dimension_normalized": "['æäº¤ä¿¡æ¯ä¸ä»£ç å˜æ›´çš„ä¸€è‡´æ€§æ£€æµ‹èƒ½åŠ›']",
    "evaluation_method_normalized": "['Recall', 'Precision', 'Specificity']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'ç‰ˆæœ¬æ§åˆ¶', 'ä»£ç å®¡æŸ¥']",
    "source_type_normalized": "['åŸºäºApacheCMæ•°æ®é›†', 'é€šè¿‡è§„åˆ™å¼•å¯¼çš„çªå˜ç”Ÿæˆä¸ä¸€è‡´æäº¤ä¿¡æ¯']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "JSON",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2511.20709_output/content.md",
    "benchmark_name": "DUALGAUGE-BENCH",
    "benchmark_name_quote": "we further curated DUALGAUGE-BENCH, a comprehensive benchmark suite of 154 programming tasks",
    "is_original_proposal": "Yesï¼Œæœ¬æ–‡æ˜¯è¯¥æ•°æ®é›†çš„åŸå§‹å‘å¸ƒè®ºæ–‡",
    "is_original_proposal_quote": "We present DUALGAUGE, the first fully automated benchmarking framework... we also present DUALGAUGE-BENCH, a curated benchmark suite",
    "dataset_url": "https://anonymous.4open.science/r/DualBench-6D1D",
    "dataset_url_quote": "Our system source code, benchmark, and evaluation/study data can all be found at https://anonymous.4open.science/r/DualBench-6D1D",
    "task_description": "è”åˆè¯„ä¼°ä»£ç ç”Ÿæˆçš„å®‰å…¨æ€§å’ŒåŠŸèƒ½æ€§ï¼Œç¡®ä¿ç”Ÿæˆçš„ä»£ç æ—¢æ»¡è¶³åŠŸèƒ½è§„èŒƒåˆä¸ä¼šå¼•å…¥å®‰å…¨æ¼æ´",
    "task_description_quote": "rigorously evaluate the security and correctness of LLM-generated code in unison... ensuring that generated programs fulfill their specifications without introducing vulnerabilities",
    "dimension": "ä»£ç ç”Ÿæˆçš„å®‰å…¨æ€§å’ŒåŠŸèƒ½æ€§è”åˆè¯„ä¼°",
    "dimension_quote": "designed to rigorously evaluate the security and correctness of LLM-generated code in unison",
    "evaluation_method": "åœ¨æ²™ç›’ç¯å¢ƒä¸­æ‰§è¡Œç¨‹åºï¼Œé’ˆå¯¹åŠŸèƒ½å’Œå®‰å…¨æ€§æµ‹è¯•å¥—ä»¶è¿è¡Œï¼ŒåŸºäºæ‰§è¡Œç»“æœè¯„ä¼°æµ‹è¯•é€šè¿‡ç‡å’Œè”åˆæ­£ç¡®æ€§-å®‰å…¨æ€§æŒ‡æ ‡",
    "evaluation_method_quote": "executes it in a sandboxed environment against functional and security test suites, and evaluates the execution results against both test suites to report test pass rates and joint correctness-security metrics",
    "context_dependency": "å•ä»»åŠ¡ä»£ç ç”Ÿæˆï¼ŒåŸºäºè‡ªç„¶è¯­è¨€æç¤ºç”Ÿæˆå®Œæ•´ç¨‹åº",
    "context_dependency_quote": "Given an pure-natural-language prompt as functional specification... generates the model's code output for the prompt",
    "problem_domain": "è·¨é¢†åŸŸç¼–ç¨‹ä»»åŠ¡ï¼Œæ¶µç›–å¤šæ ·åŒ–åŠŸèƒ½é¢†åŸŸ",
    "problem_domain_quote": "spanning diverse functionality domains",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "è¯­è¨€æ— å…³ï¼Œæ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€",
    "language_quote": "This dataset design and curation process allow it to be agnostic to programming languages",
    "data_size": "åŒ…å«154ä¸ªç¼–ç¨‹ä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½é…æœ‰åŠŸèƒ½å’Œå®‰å…¨æ€§æµ‹è¯•å¥—ä»¶",
    "data_size_quote": "a comprehensive benchmark suite of 154 programming tasks... Each task/prompt is paired with both a functional test suite... and a security test suite",
    "source_type": "äººå·¥ä¸LLMååŒåˆ›å»ºï¼Œé€šè¿‡å¤šè¯„åˆ†è€…çš„äººç±»ä¸“ä¸šçŸ¥è¯†è¿›è¡Œç²¾ç‚¼å’Œä¿®æ­£",
    "source_type_quote": "constructed these test suites through a human-and-LLM co-creation processâ€”leveraging multiple LLMs to generate candidate tests, then refining and amending these with human expertise of multiple raters",
    "last_updated": "2025",
    "last_updated_quote": "arXiv:2511.20709v1 [cs.SE] 24 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ŒåŸºäºè§„èŒƒæµ‹è¯•èŒƒå¼",
    "build_type_quote": "following a specification-based testing paradigm",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "code generation... translating natural language promptsâ€”typically serving as functional specificationsâ€”into programs",
    "evaluation_metrics": "æµ‹è¯•é€šè¿‡ç‡ã€è”åˆæ­£ç¡®æ€§-å®‰å…¨æ€§æŒ‡æ ‡",
    "evaluation_metrics_quote": "report test pass rates and joint correctness-security metrics",
    "input_modality": "è‡ªç„¶è¯­è¨€",
    "input_modality_quote": "Given an pure-natural-language prompt as functional specification",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "generates the model's code output for the prompt",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "translating natural language prompts... into programs",
    "execution_environment": "æ²™ç›’éš”ç¦»å®¹å™¨ç¯å¢ƒï¼Œæ”¯æŒä¾èµ–è§£æå’Œç¯å¢ƒé…ç½®",
    "execution_environment_quote": "executes it in a sandboxed environment... The execution engine compiles and runs the model-generated code within isolated containers",
    "unique_features": "é¦–ä¸ªæ”¯æŒå®‰å…¨æ€§å’ŒåŠŸèƒ½æ€§è”åˆè¯„ä¼°çš„åŸºå‡†å¥—ä»¶ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½é…æœ‰è¦†ç›–é©±åŠ¨çš„åŠŸèƒ½å’Œå®‰å…¨æ€§æµ‹è¯•å¥—ä»¶ï¼Œé‡‡ç”¨äººå·¥ä¸LLMååŒåˆ›å»ºè¿‡ç¨‹",
    "unique_features_quote": "the first benchmark suite that pairs each code-generation prompt with dual (functional and security), coverage-enforced test suites... constructed through a human-and-LLM co-creation process",
    "data_size_quantity": 154,
    "data_size_unit": "ä¸ªç¼–ç¨‹ä»»åŠ¡",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['è¯­è¨€æ— å…³', 'å¤šç§ç¼–ç¨‹è¯­è¨€']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆçš„å®‰å…¨æ€§å’ŒåŠŸèƒ½æ€§è”åˆè¯„ä¼°']",
    "evaluation_method_normalized": "['æµ‹è¯•é€šè¿‡ç‡', 'è”åˆæ­£ç¡®æ€§-å®‰å…¨æ€§æŒ‡æ ‡']",
    "problem_domain_normalized": "['è·¨é¢†åŸŸç¼–ç¨‹ä»»åŠ¡', 'å¤šæ ·åŒ–åŠŸèƒ½é¢†åŸŸ']",
    "source_type_normalized": "['äººå·¥ä¸LLMååŒåˆ›å»º', 'å¤šè¯„åˆ†è€…çš„äººç±»ä¸“ä¸šçŸ¥è¯†è¿›è¡Œç²¾ç‚¼å’Œä¿®æ­£']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.23408_output/content.md",
    "benchmark_name": "Vul4J",
    "benchmark_name_quote": "Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities drawn from open-source projects and covering distinct Common Weakness Enumeration (CWE) classes.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities drawn from open-source projects...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–æ¼æ´ä¿®å¤æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå…·ä½“é’ˆå¯¹çœŸå®æ¼æ´å’Œäººå·¥ç”Ÿæˆçš„æ¼æ´ã€‚",
    "task_description_quote": "In this study, we empirically evaluate the patching effectiveness and complementarity of several prominent LLMs... using both real and artificial vulnerabilities.",
    "dimension": "æ¼æ´ä¿®å¤çš„æœ‰æ•ˆæ€§ã€æ¨¡å‹é—´çš„äº’è¡¥æ€§ä¸é‡å æ€§",
    "dimension_quote": "Our paper empirically investigates the effectiveness and complementarity of several prominent LLMs in automated vulnerability patching...",
    "evaluation_method": "ä½¿ç”¨æ¼æ´è¯æ˜ï¼ˆProof-of-Vulnerability, PoVï¼‰æµ‹è¯•æ‰§è¡Œæ¥éªŒè¯ç”Ÿæˆçš„è¡¥ä¸æ˜¯å¦æˆåŠŸä¿®å¤æ¼æ´ã€‚",
    "evaluation_method_quote": "Our evaluation employs Proof-of-Vulnerability (PoV) test execution to concretely assess whether LLM-generated source code successfully patches vulnerabilities.",
    "context_dependency": "å•å‡½æ•°/ä»£ç ç‰‡æ®µ",
    "context_dependency_quote": "Given a vulnerable code snippet, these models generate a patched version that aims to eliminate security flaws while preserving functionality.",
    "problem_domain": "è½¯ä»¶å®‰å…¨ã€æ¼æ´ä¿®å¤",
    "problem_domain_quote": "Automated vulnerability patching is crucial for software security...",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "Java",
    "language_quote": "Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities...",
    "data_size": "15ä¸ªçœŸå®æ¼æ´åŠå…¶41ä¸ªäººå·¥ç”Ÿæˆçš„å¯¹åº”æ¼æ´",
    "data_size_quote": "To perform this evaluation, we employed 15 real vulnerabilities and their 41 artificial counterparts (vulnerabilities).",
    "source_type": "çœŸå®æ¼æ´æ¥è‡ªå¼€æºé¡¹ç›®å’Œå…¬å…±æ¼æ´æ•°æ®åº“ï¼ˆå¦‚CVEï¼‰ï¼Œäººå·¥æ¼æ´ç”±CodeBERTç”Ÿæˆå¹¶ç»è¿‡éªŒè¯ã€‚",
    "source_type_quote": "Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities drawn from open-source projects... Secondly, we augmented our evaluation with artificially generated vulnerabilities... we incorporated artificial vulnerabilities derived from the work of Garg et al.[15]. Garg et al. used CodeBERT[11] to generate thousands of candidate artificial vulnerabilities...",
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆVul4Jæ•°æ®é›†ï¼‰",
    "build_type_quote": "Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities drawn from open-source projects...",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ä¿®å¤ï¼ˆæ¼æ´è¡¥ä¸ç”Ÿæˆï¼‰",
    "task_granularity_quote": "Given a vulnerable code snippet, these models generate a patched version that aims to eliminate security flaws while preserving functionality.",
    "evaluation_metrics": "PoVæµ‹è¯•é€šè¿‡ç‡ï¼ˆåŸºäºæ‰§è¡Œçš„éªŒè¯ï¼‰",
    "evaluation_metrics_quote": "Our evaluation employs Proof-of-Vulnerability (PoV) test execution to concretely assess whether LLM-generated source code successfully patches vulnerabilities.",
    "input_modality": "ä»£ç ï¼ˆåŒ…å«æ¼æ´çš„ä»£ç ç‰‡æ®µï¼‰",
    "input_modality_quote": "Given a vulnerable code snippet, these models generate a patched version...",
    "output_modality": "ä»£ç ï¼ˆä¿®å¤åçš„ä»£ç ç‰‡æ®µï¼‰",
    "output_modality_quote": "Given a vulnerable code snippet, these models generate a patched version...",
    "task_io_type": "ä»£ç åˆ°ä»£ç ",
    "task_io_type_quote": "Given a vulnerable code snippet, these models generate a patched version...",
    "execution_environment": "Maven/Gradleæ„å»ºç¯å¢ƒï¼ŒåŒ…å«PoV JUnitæµ‹è¯•ç”¨ä¾‹",
    "execution_environment_quote": "When no suitable test existed in the original project, Bui et al. [2] manually wrote one by following the exploit steps in the CVE report, thereby guaranteeing that every vulnerability in the dataset can be triggered, reproduced, and validated in a Maven/Gradle build.",
    "unique_features": "è¯¥ç ”ç©¶ä¸ä»…è¯„ä¼°LLMå¯¹çœŸå®æ¼æ´çš„ä¿®å¤èƒ½åŠ›ï¼Œè¿˜è¯„ä¼°å…¶å¯¹äººå·¥ç”Ÿæˆæ¼æ´çš„ä¿®å¤èƒ½åŠ›ï¼Œä»¥æµ‹è¯•æ¨¡å‹çš„æ³›åŒ–æ€§å’Œé²æ£’æ€§ã€‚æ•°æ®é›†ï¼ˆVul4Jï¼‰ä¸ºæ¯ä¸ªæ¼æ´æä¾›äº†å¯æ‰§è¡Œçš„æ¼æ´è¯æ˜ï¼ˆPoVï¼‰æµ‹è¯•ç”¨ä¾‹ï¼Œç”¨äºè‡ªåŠ¨åŒ–éªŒè¯è¡¥ä¸çš„æœ‰æ•ˆæ€§ã€‚",
    "unique_features_quote": "Our paper empirically investigates the effectiveness and complementarity of several prominent LLMs in automated vulnerability patching, specifically focusing on both real vulnerabilities and their corresponding artificial vulnerabilities... For every entry Vul4J provides... one or more Proof-of-Vulnerability (PoV) JUnit test cases. A PoV test is an executable exploit oracle...",
    "data_size_quantity": 15,
    "data_size_unit": "ä¸ªçœŸå®æ¼æ´",
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Java']",
    "dimension_normalized": "['æ¼æ´ä¿®å¤çš„æœ‰æ•ˆæ€§', 'æ¨¡å‹é—´çš„äº’è¡¥æ€§ä¸é‡å æ€§']",
    "evaluation_method_normalized": "['PoVæµ‹è¯•é€šè¿‡ç‡ï¼ˆåŸºäºæ‰§è¡Œçš„éªŒè¯ï¼‰']",
    "problem_domain_normalized": "['è½¯ä»¶å®‰å…¨', 'æ¼æ´ä¿®å¤']",
    "source_type_normalized": "['çœŸå®æ¼æ´æ¥è‡ªå¼€æºé¡¹ç›®å’Œå…¬å…±æ¼æ´æ•°æ®åº“ï¼ˆå¦‚CVEï¼‰', 'äººå·¥æ¼æ´ç”±CodeBERTç”Ÿæˆå¹¶ç»è¿‡éªŒè¯ã€‚']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ä¿®å¤",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2401.01062_output/content.md",
    "benchmark_name": "CAASD (Capability Assessment of Automatic Software Development)",
    "benchmark_name_quote": "we have developed a novel benchmark named CAASD (Capability Assessment of Automatic Software Development).",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "we have developed a novel benchmark named CAASD (Capability Assessment of Automatic Software Development).",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°AIè¾…åŠ©è½¯ä»¶å¼€å‘ç³»ç»Ÿçš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ç³»ç»Ÿçº§å®ç°ä»»åŠ¡çš„èƒ½åŠ›",
    "task_description_quote": "all of them either are limited to simple function-level implementation tasks or lack detailed requirements specifications for system-level implementation tasks",
    "dimension": "ç³»ç»Ÿçº§è½¯ä»¶å¼€å‘èƒ½åŠ›è¯„ä¼°",
    "dimension_quote": "for assessing how well a software development task is completed",
    "evaluation_method": "é€šè¿‡å‚è€ƒç”¨ä¾‹è¯„ä¼°ç³»ç»Ÿå®ç°çš„è´¨é‡å’Œå®Œæ•´æ€§",
    "evaluation_method_quote": "Each task of CAASD is equipped with a list of reference use cases depicting the system requirements. The reference use cases are used to evaluate the quality and completeness of a system implementation.",
    "context_dependency": "ç³»ç»Ÿçº§å¼€å‘ï¼ˆå¤šæ–‡ä»¶é¡¹ç›®ï¼‰",
    "context_dependency_quote": "system-level implementation tasks",
    "problem_domain": "è½¯ä»¶å¼€å‘",
    "problem_domain_quote": "software development",
    "problem_difficulty": "éå¹³å‡¡è½¯ä»¶é¡¹ç›®",
    "problem_difficulty_quote": "non-trivial software projects",
    "language": NaN,
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": "2024",
    "last_updated_quote": "arXiv:2401.01062v1 [cs.SE] 2 Jan 2024",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "we have developed a novel benchmark named CAASD",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ç³»ç»Ÿçº§å®ç°ä»»åŠ¡",
    "task_granularity_quote": "system-level implementation tasks",
    "evaluation_metrics": "é€šè¿‡ç‡",
    "evaluation_metrics_quote": "AISD achieves an impressive pass rate of 75.2%",
    "input_modality": "è‡ªç„¶è¯­è¨€éœ€æ±‚æè¿°",
    "input_modality_quote": "high-level (potentially vague) user requirements as inputs",
    "output_modality": "ç³»ç»Ÿå®ç°",
    "output_modality_quote": "system implementation",
    "task_io_type": "éœ€æ±‚åˆ°ç³»ç»Ÿå®ç°",
    "task_io_type_quote": "taking high-level (potentially vague) user requirements as inputs, generates detailed use cases, prototype system designs, and subsequently system implementation",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "é¦–ä¸ªè¯„ä¼°è½¯ä»¶å¼€å‘ä»»åŠ¡å®Œæˆè´¨é‡çš„åŸºå‡†ï¼ŒåŒ…å«è¯¦ç»†çš„ç³»ç»Ÿéœ€æ±‚è§„èŒƒ",
    "unique_features_quote": "this is the first benchmark that offers criteria for assessing how well a software development task is completed",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2024,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['ç³»ç»Ÿçº§è½¯ä»¶å¼€å‘èƒ½åŠ›è¯„ä¼°']",
    "evaluation_method_normalized": "['é€šè¿‡ç‡']",
    "problem_domain_normalized": "['è½¯ä»¶å¼€å‘']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2401.12554_output/content.md",
    "benchmark_name": "ParEval",
    "benchmark_name_quote": "we propose the Parallel Code Generation Evaluation (ParEval) benchmark: a set of benchmarks (prompts) for evaluating how well LLMs generate parallel code.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "we propose the Parallel Code Generation Evaluation (ParEval) benchmark",
    "dataset_url": "github.com/parallelcodefoundry/ParEval",
    "dataset_url_quote": "ParEval is available online at: github.com/parallelcodefoundry/ParEval.",
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå¹¶è¡Œä»£ç çš„èƒ½åŠ›",
    "task_description_quote": "we study the capabilities of state-of-the-art language models to generate parallel code.",
    "dimension": "å¹¶è¡Œä»£ç ç”Ÿæˆæ­£ç¡®æ€§å’Œæ€§èƒ½",
    "dimension_quote": "We evaluate several state-of-the-art open- and closed-source LLMs using these benchmarks, and report metrics that represent the correctness and performance of the generated code.",
    "evaluation_method": "æ–°é¢–çš„ä»£ç ç”Ÿæˆè¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…æ‹¬speedupğ‘›@kå’Œefficiencyğ‘›@kï¼Œç”¨äºè¯„ä¼°ç”Ÿæˆä»£ç çš„æ€§èƒ½å’Œæ‰©å±•æ€§",
    "evaluation_method_quote": "We introduce novel code generation evaluation metrics that assess performance and parallel scaling.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "ç§‘å­¦å’Œå¹¶è¡Œè®¡ç®—",
    "problem_domain_quote": "consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing.",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "C/C++",
    "language_quote": "Traditional Python code generation benchmarks are tested by running eval on the generated code for a small number of small unit tests. On the other hand, in the case of parallel code â€” we must compile C/C++ code, link against one or more parallel libraries, and run the code in the proper parallel environment.",
    "data_size": "420ä¸ªä¸åŒçš„ç¼–ç ä»»åŠ¡",
    "data_size_quote": "consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing.",
    "source_type": "æ‰‹åŠ¨è®¾è®¡",
    "source_type_quote": "These benchmarks are challenging to test. Traditional Python code generation benchmarks are tested by running eval on the generated code for a small number of small unit tests.",
    "last_updated": "2024",
    "last_updated_quote": "HPDC â€™24, June 3â€“7, 2024, Pisa, Italy",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "we propose the Parallel Code Generation Evaluation (ParEval) benchmark: a set of benchmarks (prompts) for evaluating how well LLMs generate parallel code.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "we study the capabilities of state-of-the-art language models to generate parallel code.",
    "evaluation_metrics": "speedupğ‘›@kå’Œefficiencyğ‘›@k",
    "evaluation_metrics_quote": "We introduce two novel metrics, speedupğ‘›@k and efficiencyğ‘›@k, for evaluating the performance and scaling of LLM generated code.",
    "input_modality": "è‡ªç„¶è¯­è¨€",
    "input_modality_quote": "consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing.",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "we study the capabilities of state-of-the-art language models to generate parallel code.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing.",
    "execution_environment": "éœ€è¦ç‰¹å®šä¾èµ–ï¼ˆå¹¶è¡Œåº“ï¼‰",
    "execution_environment_quote": "we must compile C/C++ code, link against one or more parallel libraries, and run the code in the proper parallel environment.",
    "unique_features": "ä¸“æ³¨äºå¹¶è¡Œä»£ç ç”Ÿæˆè¯„ä¼°ï¼Œè¦†ç›–12ç§è®¡ç®—é—®é¢˜ç±»å‹å’Œ7ç§æ‰§è¡Œæ¨¡å‹",
    "unique_features_quote": "These benchmarks cover twelve different computational problem types, and seven different execution models: serial, OpenMP, Kokkos, MPI, MPI+OpenMP, CUDA, and HIP.",
    "data_size_quantity": 420,
    "data_size_unit": "ä¸ª",
    "last_updated_year": 2024,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['C', 'C++']",
    "dimension_normalized": "['å¹¶è¡Œä»£ç ç”Ÿæˆæ­£ç¡®æ€§', 'æ€§èƒ½']",
    "evaluation_method_normalized": "['speedupğ‘›@k', 'efficiencyğ‘›@k']",
    "problem_domain_normalized": "['ç§‘å­¦è®¡ç®—', 'å¹¶è¡Œè®¡ç®—']",
    "source_type_normalized": "['æ‰‹åŠ¨è®¾è®¡']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.01010_output/content.md",
    "benchmark_name": "Chain of Unit-Physics",
    "benchmark_name_quote": "To address these limitations, this work conceptualizes an inverse approach to code design, embodied in the Chain of Unit-Physics framework: a first-principles (or primitives)-centric, multi-agent system in which human expert knowledge is encoded as unit-physics tests that explicitly constrain code generation.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "To address these limitations, this work conceptualizes an inverse approach to code design, embodied in the Chain of Unit-Physics framework: a first-principles (or primitives)-centric, multi-agent system in which human expert knowledge is encoded as unit-physics tests that explicitly constrain code generation.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯¥è¯„æµ‹åŸºå‡†æ—¨åœ¨è§£å†³ç§‘å­¦è®¡ç®—ä¸­çš„ä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å…·æœ‰ä¸¥æ ¼ç‰©ç†çº¦æŸçš„é«˜é£é™©ç§‘å­¦é—®é¢˜ï¼Œå¦‚ç‡ƒçƒ§ç§‘å­¦ä¸­çš„è®¡ç®—æµä½“åŠ¨åŠ›å­¦ï¼ˆCFDï¼‰æ±‚è§£å™¨å¼€å‘ã€‚",
    "task_description_quote": "Developing computational scientific software from natural-language queries remains challenging broadly due to (a) sparse representation of domain codes during training and (b) the limited feasibility of RLHF with a small expert community.",
    "dimension": "ç‰©ç†ä¸€è‡´æ€§ã€æ•°å€¼ç¨³å®šæ€§ã€ç®—æ³•æ­£ç¡®æ€§",
    "dimension_quote": "The framework is evaluated on a nontrivial combustion task (12 degrees-of-freedom), used here as a representative benchmark for scientific problem with realistic physical constraints.",
    "evaluation_method": "åŸºäºå•å…ƒç‰©ç†æµ‹è¯•çš„éªŒè¯æ–¹æ³•ï¼ŒåŒ…æ‹¬ç‰©ç†çº¦æŸæ£€æŸ¥ã€æ•°å€¼ä¸€è‡´æ€§æ£€æŸ¥å’Œè¯Šæ–­åˆ†æ",
    "evaluation_method_quote": "The Verification Agent applies formalized unit-physics tests to assess physical and numerical consistency. These primitives yield physics-grounded verification even without reference datasets.",
    "context_dependency": "å¤šæ­¥éª¤ç§‘å­¦è®¡ç®—ä»»åŠ¡ï¼Œæ¶‰åŠå¤æ‚çš„ç‰©ç†çº¦æŸå’Œæ•°å€¼è®¡ç®—",
    "context_dependency_quote": "The framework is evaluated on a nontrivial combustion task (12 degrees-of-freedom), used here as a representative benchmark for scientific problem with realistic physical constraints.",
    "problem_domain": "è®¡ç®—ç§‘å­¦ï¼Œç‰¹åˆ«æ˜¯ç‡ƒçƒ§ç§‘å­¦å’Œè®¡ç®—æµä½“åŠ¨åŠ›å­¦",
    "problem_domain_quote": "To overcome these limitations, this work systematically applies an inverse code design methodology (see Fig. 1), formally known as test-driven development (TDD) [26], to scientific software in combustion science, one such domain where TDD approaches have not been thoroughly evaluated.",
    "problem_difficulty": "é«˜éš¾åº¦ï¼Œæ¶‰åŠ12è‡ªç”±åº¦çš„å¤æ‚ç‡ƒçƒ§ä»»åŠ¡",
    "problem_difficulty_quote": "The framework is evaluated on a nontrivial combustion task (12 degrees-of-freedom), used here as a representative benchmark for scientific problem with realistic physical constraints.",
    "language": NaN,
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": "åŸºäºäººç±»ä¸“å®¶çŸ¥è¯†æ„å»ºçš„å•å…ƒç‰©ç†æµ‹è¯•",
    "source_type_quote": "This work proposes Chain of Unit-Physics, an approach that embeds human expert knowledge directly into distinct reasoning chains of the agentic system via 'unit-physics'â€”formalized, testable constraints that encode fundamental physics (e.g., energy conservation laws) and practitioner experience (e.g., dimensional / bound checks, and floating-point diagnostics).",
    "last_updated": "2025",
    "last_updated_quote": "arXiv:2512.01010v1 [cs.MA] 30 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ŒåŸºäºäººç±»ä¸“å®¶çŸ¥è¯†",
    "build_type_quote": "This work proposes Chain of Unit-Physics, an approach that embeds human expert knowledge directly into distinct reasoning chains of the agentic system via 'unit-physics'â€”formalized, testable constraints that encode fundamental physics (e.g., energy conservation laws) and practitioner experience (e.g., dimensional / bound checks, and floating-point diagnostics).",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ç«¯åˆ°ç«¯ç§‘å­¦ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "Closed-weight systems and code-focused agentic variants fail to produce correct end-to-end solvers, despite tool and web access, exhibiting four recurrent error classes: interface (syntax/API) hallucinations, overconfident assumptions, numerical/physical incoherence, and configuration fragility.",
    "evaluation_metrics": "ç‰©ç†ä¸€è‡´æ€§ã€æ•°å€¼è¯¯å·®ã€è¿è¡Œæ—¶é—´å’Œå†…å­˜ä½¿ç”¨æ•ˆç‡",
    "evaluation_metrics_quote": "On the benchmark task, the proposed framework converges within 5â€“6 iterations, matches the human-expert implementation (mean error of 3.1Ë†10Â´3%), with a â€33.4% faster runtime and a â€30% efficient memory usage at a cost comparable to mid-sized commercial APIs, yielding a practical template for physics-grounded scientific code generation.",
    "input_modality": "è‡ªç„¶è¯­è¨€æŸ¥è¯¢å’Œå•å…ƒç‰©ç†æµ‹è¯•",
    "input_modality_quote": "In the input, the userâ€™s scientific question is paired with 'basis prompts' that establish execution permissions, tool availability, coding language settings, and transfer protocols. Additional input scopes the unit-physics tests that concatenated with the scientific query to form the complete input to the framework.",
    "output_modality": "ç§‘å­¦è®¡ç®—ä»£ç å’Œå¯è§†åŒ–ç»“æœ",
    "output_modality_quote": "Finally, the agent consolidates the output into domain-relevant visualizations (for example, line graphs and contour graphs of key quantities of interest), closing the loop between natural-language inquiry and quantitative scientific insight.",
    "task_io_type": "è‡ªç„¶è¯­è¨€åˆ°ç§‘å­¦ä»£ç ",
    "task_io_type_quote": "Developing computational scientific software from natural-language queries remains challenging broadly due to (a) sparse representation of domain codes during training and (b) the limited feasibility of RLHF with a small expert community.",
    "execution_environment": "Pythonè™šæ‹Ÿæ²™ç®±ç¯å¢ƒï¼Œå…·æœ‰éš”ç¦»çš„æ‰§è¡Œæƒé™",
    "execution_environment_quote": "The framework runs inside a dedicated Python virtual sandbox environment that is not pre-configured with metadata on all available libraries. The agent is granted isolated code execution privileges within this sandbox, ensuring that any dependency installs or script executions cannot affect the system root directory or global files.",
    "unique_features": "åŸºäºç¬¬ä¸€æ€§åŸç†çš„å•å…ƒç‰©ç†æµ‹è¯•é©±åŠ¨ä»£ç ç”Ÿæˆï¼Œå¼ºè°ƒç‰©ç†ä¸€è‡´æ€§å’Œæ•°å€¼ç¨³å®šæ€§",
    "unique_features_quote": "This inverse-design method provides two key advantages in scientific software. First, human-authored tests embed deep domain expertise (first principles or primitives) into targeted validation checks, ensuring that each algorithmic component faithfully represents the underlying physics.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['ç‰©ç†ä¸€è‡´æ€§', 'æ•°å€¼ç¨³å®šæ€§', 'ç®—æ³•æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['ç‰©ç†ä¸€è‡´æ€§', 'æ•°å€¼è¯¯å·®', 'è¿è¡Œæ—¶é—´', 'å†…å­˜ä½¿ç”¨æ•ˆç‡']",
    "problem_domain_normalized": "['è®¡ç®—ç§‘å­¦', 'ç‡ƒçƒ§ç§‘å­¦', 'è®¡ç®—æµä½“åŠ¨åŠ›å­¦']",
    "source_type_normalized": "['åŸºäºäººç±»ä¸“å®¶çŸ¥è¯†æ„å»ºçš„å•å…ƒç‰©ç†æµ‹è¯•']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.01396_output/content.md",
    "benchmark_name": "BackportBench",
    "benchmark_name_quote": "we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem.",
    "dataset_url": "https://github.com/BackportBench/BackportBench",
    "dataset_url_quote": "The BackportBench is available on https://github.com/BackportBench/BackportBench.",
    "task_description": "è‡ªåŠ¨åŒ–è¡¥ä¸å›ç§»æ¤ã€‚æ—¨åœ¨ä¸ºæœªæ‰“è¡¥ä¸çš„è½¯ä»¶ç‰ˆæœ¬ç”Ÿæˆè¡¥ä¸ï¼ŒåŸºäºå·²æœ‰çš„è¡¥ä¸å’Œä¸¤ä¸ªç‰ˆæœ¬ä¹‹é—´çš„ä»£ç å·®å¼‚ã€‚",
    "task_description_quote": "BackportBench defines the backporting problem as generating a patch for the unpatched software version based on the existing patch and the code differences between the two versions.",
    "dimension": "è‡ªåŠ¨åŒ–è¡¥ä¸å›ç§»æ¤çš„æœ‰æ•ˆæ€§ã€å¤šè¯­è¨€èƒ½åŠ›ã€å¤„ç†è·¨æ–‡ä»¶ä¸å…¼å®¹æ€§çš„èƒ½åŠ›",
    "dimension_quote": "To facilitate the development and evaluation of automated backporting techniques... BackportBench is a multilingual benchmark... This formulation shifts the patch porting problem from code-hunk level or function level to repository level, offering a more realistic representation of software maintenance challenges.",
    "evaluation_method": "åœ¨å¯æ‰§è¡Œçš„Dockerç¯å¢ƒä¸­è¿è¡Œç›¸å…³æµ‹è¯•ç”¨ä¾‹è¿›è¡ŒéªŒè¯",
    "evaluation_method_quote": "each task instance provides an executable Docker environment with scripts for running relevant test cases, thereby aligns with the criteria for a successful benchmark [5].",
    "context_dependency": "ä»“åº“çº§åˆ«ï¼Œæ¶‰åŠè·¨æ–‡ä»¶çš„ä¸å…¼å®¹æ€§å¤„ç†",
    "context_dependency_quote": "This formulation shifts the patch porting problem from code-hunk level or function level to repository level, offering a more realistic representation of software maintenance challenges.",
    "problem_domain": "è½¯ä»¶å®‰å…¨ã€è½¯ä»¶ç»´æŠ¤ã€æ¼æ´ä¿®å¤",
    "problem_domain_quote": "BackportBench contains 202 real-world vulnerability patch backporting task instances curated from three popular OSS ecosystems... To remove such vulnerabilities... patch backporting is a helpful practice that adapts patches for other versions and makes it easier to deliver patches to users on older branches.",
    "problem_difficulty": "ç°å®ä¸–ç•Œå·¥ç¨‹çº§ï¼Œæ¶‰åŠé€»è¾‘å’Œç»“æ„æ€§å˜æ›´",
    "problem_difficulty_quote": "the agentic method has outperformed traditional patch porting methods, especially on cases that require logical and structural changes.",
    "language": "Python, Java, JavaScript",
    "language_quote": "BackportBench contains 202 patch backporting problems from PyPI, Maven, and npm, covering Python, Java, and JavaScript, respectively.",
    "data_size": "åŒ…å«202ä¸ªè¡¥ä¸å›ç§»æ¤ä»»åŠ¡å®ä¾‹",
    "data_size_quote": "BackportBench contains 202 real-world vulnerability patch backporting task instances",
    "source_type": "ä»ä¸‰ä¸ªæµè¡Œçš„å¼€æºè½¯ä»¶ç”Ÿæ€ç³»ç»Ÿï¼ˆPyPI, Maven, npmï¼‰ä¸­æ”¶é›†çš„çœŸå®ä¸–ç•Œæ¼æ´è¡¥ä¸å›ç§»æ¤ä»»åŠ¡",
    "source_type_quote": "BackportBench contains 202 real-world vulnerability patch backporting task instances curated from three popular OSS ecosystems, PyPI , Maven, and npm",
    "last_updated": "2018 (æ ¹æ®è®ºæ–‡ç‰ˆæƒæ—¥æœŸ)ï¼Œä½†arXivç‰ˆæœ¬ä¸º2025å¹´12æœˆ",
    "last_updated_quote": "Â© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ... arXiv:2512.01396v1  [cs.SE]  1 Dec 2025",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": "ACMç‰ˆæƒï¼Œå…è®¸ä¸ªäººæˆ–è¯¾å ‚ä½¿ç”¨ï¼Œç¦æ­¢å•†ä¸šç”¨é€”",
    "dataset_license_quote": "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.",
    "task_granularity": "ä»£ç ä¿®å¤/è¡¥ä¸ç”Ÿæˆ",
    "task_granularity_quote": "generating a patch for the unpatched software version based on the existing patch and the code differences between the two versions.",
    "evaluation_metrics": "é€šè¿‡æµ‹è¯•ç”¨ä¾‹éªŒè¯è¡¥ä¸æœ‰æ•ˆæ€§ï¼Œè€ŒéåŸºäºç­‰ä»·æ€§æˆ–ç›¸ä¼¼æ€§çš„æŒ‡æ ‡",
    "evaluation_metrics_quote": "each task instance provides an executable Docker environment with scripts for running relevant test cases... We argue that whether backported patches achieve equivalence is not the only way to prove the patch is effective.",
    "input_modality": "ä»£ç ï¼ˆä¸¤ä¸ªç‰ˆæœ¬çš„ä»£ç åº“åŠåŸå§‹è¡¥ä¸ï¼‰",
    "input_modality_quote": "generating a patch for the unpatched software version based on the existing patch and the code differences between the two versions.",
    "output_modality": "ä»£ç ï¼ˆå›ç§»æ¤åçš„è¡¥ä¸ï¼‰",
    "output_modality_quote": "generating a patch for the unpatched software version",
    "task_io_type": "ä»£ç åˆ°ä»£ç ",
    "task_io_type_quote": "generating a patch for the unpatched software version based on the existing patch and the code differences between the two versions.",
    "execution_environment": "å¯æ‰§è¡Œçš„Dockerç¯å¢ƒï¼ŒåŒ…å«è¿è¡Œç›¸å…³æµ‹è¯•ç”¨ä¾‹çš„è„šæœ¬",
    "execution_environment_quote": "each task instance provides an executable Docker environment with scripts for running relevant test cases",
    "unique_features": "é¦–ä¸ªé’ˆå¯¹è¡¥ä¸å›ç§»æ¤é—®é¢˜çš„ç»¼åˆæ€§ã€å¤šè¯­è¨€åŸºå‡†æµ‹è¯•å¥—ä»¶ï¼ŒåŒ…å«å¯æ‰§è¡Œç¯å¢ƒå’Œæµ‹è¯•ç”¨ä¾‹ï¼Œå°†é—®é¢˜ä»ä»£ç å—/å‡½æ•°çº§åˆ«æå‡åˆ°ä»“åº“çº§åˆ«ï¼Œæ›´è´´è¿‘ç°å®è½¯ä»¶ç»´æŠ¤æŒ‘æˆ˜ã€‚",
    "unique_features_quote": "the first comprehensive benchmark suite for patch backporting problem... a multilingual benchmark... contains executable Docker environments and test cases for validation... This formulation shifts the patch porting problem from code-hunk level or function level to repository level, offering a more realistic representation of software maintenance challenges.",
    "data_size_quantity": 202,
    "data_size_unit": "ä¸ªè¡¥ä¸å›ç§»æ¤ä»»åŠ¡å®ä¾‹",
    "last_updated_year": 2025,
    "last_updated_month": 12,
    "last_updated_day": null,
    "language_normalized": "['Python', 'Java', 'JavaScript']",
    "dimension_normalized": "['è‡ªåŠ¨åŒ–è¡¥ä¸å›ç§»æ¤çš„æœ‰æ•ˆæ€§', 'å¤šè¯­è¨€èƒ½åŠ›', 'å¤„ç†è·¨æ–‡ä»¶ä¸å…¼å®¹æ€§çš„èƒ½åŠ›']",
    "evaluation_method_normalized": "['é€šè¿‡æµ‹è¯•ç”¨ä¾‹éªŒè¯è¡¥ä¸æœ‰æ•ˆæ€§ï¼Œè€ŒéåŸºäºç­‰ä»·æ€§æˆ–ç›¸ä¼¼æ€§çš„æŒ‡æ ‡']",
    "problem_domain_normalized": "['è½¯ä»¶å®‰å…¨', 'è½¯ä»¶ç»´æŠ¤', 'æ¼æ´ä¿®å¤']",
    "source_type_normalized": "['ä»ä¸‰ä¸ªæµè¡Œçš„å¼€æºè½¯ä»¶ç”Ÿæ€ç³»ç»Ÿï¼ˆPyPI, Maven, npmï¼‰ä¸­æ”¶é›†çš„çœŸå®ä¸–ç•Œæ¼æ´è¡¥ä¸å›ç§»æ¤ä»»åŠ¡']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ä¿®å¤",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "ä»…ä¾›å­¦æœ¯ç ”ç©¶",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.01255_output/content.md",
    "benchmark_name": "ARENAJS",
    "benchmark_name_quote": "we construct ARENAJSâ€”the first systematic benchmark for LLM-based JavaScript vulnerability detection",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "In this paper, we introduce, for the first time, three principles for constructing a benchmark for JavaScript vulnerability detection... we propose FORGEJS... Then, we use FORGEJS to construct ARENAJSâ€”the first systematic benchmark for LLM-based JavaScript vulnerability detection",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨JavaScriptæ¼æ´æ£€æµ‹æ–¹é¢çš„èƒ½åŠ›",
    "task_description_quote": "evaluating LLMsâ€™ capability in JavaScript vulnerability detection",
    "dimension": "æ¼æ´æ£€æµ‹èƒ½åŠ›ã€æ¨ç†å……åˆ†æ€§ã€é²æ£’æ€§ã€ç°å®ä¸–ç•Œå¯ç”¨æ€§",
    "dimension_quote": "reveals key limitations in reasoning sufficiency, robustness, and real-world usability",
    "evaluation_method": "ä½¿ç”¨JUDGEJSè‡ªåŠ¨è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…å«æç¤ºæ¨¡æ¿ã€å“åº”è§£æã€æ ‡ç­¾å¯¹é½å’Œç¨³å¥è¯„åˆ†ï¼Œåˆ©ç”¨è¯­ä¹‰ç­‰ä»·å’Œæ¨¡ç³ŠåŒ¹é…ï¼Œåœ¨å‡½æ•°çº§å’Œé¡¹ç›®çº§ç”Ÿæˆç»Ÿä¸€çš„ã€ä¸¥æ ¼çš„ã€å¯è¿½æº¯çš„æŒ‡æ ‡å¥—ä»¶ï¼ŒåŒ…æ‹¬F1ã€å‡é˜³æ€§ç‡ï¼ˆFPRï¼‰ä»¥åŠå·¥ç¨‹çº¦æŸä¸‹çš„æ£€æµ‹æ•ˆèƒ½",
    "evaluation_method_quote": "we propose JUDGEJS, an automated evaluation framework that spans prompt templates, response parsing, label alignment, and robust scoring; leveraging semantic equivalence and fuzzy matching, it harmonizes heterogeneous outputs and yields a unified, rigorous, and traceable metric suite across function- and project-levels, including F1, false positive rate (FPR), and detection efficacy under engineering constraints.",
    "context_dependency": "å‡½æ•°çº§å’Œé¡¹ç›®çº§ï¼ˆå®Œæ•´é¡¹ç›®ï¼‰",
    "context_dependency_quote": "supports both function-level and project-level evaluation",
    "problem_domain": "JavaScriptå®‰å…¨æ¼æ´æ£€æµ‹ï¼Œæ¶µç›–å‰ç«¯ï¼ˆå¦‚DOM-based XSSï¼‰ã€åç«¯ï¼ˆå¦‚SQLæ³¨å…¥ã€å‘½ä»¤æ³¨å…¥ã€åŸå‹æ±¡æŸ“ï¼‰å’Œå…¨æ ˆç¯å¢ƒ",
    "problem_domain_quote": "JavaScript vulnerability patterns vary substantially across these environments: backend prototype pollution... while frontend prototype pollution predominantly causes DOM-based XSS or client-side denial of service.",
    "problem_difficulty": "ç°å®ä¸–ç•Œå·¥ä¸šçº§ï¼Œåæ˜ çœŸå®ä»“åº“çš„å¤æ‚æ€§",
    "problem_difficulty_quote": "fails to reflect the complexity of real-world repositories",
    "language": "JavaScript",
    "language_quote": "benchmark for JavaScript vulnerability detection",
    "data_size": "è¦†ç›–æ•°åƒä¸ªçœŸå®çš„JavaScripté¡¹ç›®",
    "data_size_quote": "A benchmark that spans thousands of real JavaScript projects",
    "source_type": "èšåˆå¼‚æ„æ¥æºï¼Œç»“åˆçœŸå®ä¸–ç•Œå’Œåˆæˆæ•°æ®",
    "source_type_quote": "aggregates heterogeneous sources... combining real-world and synthetic data",
    "last_updated": "2025-12-01 (æ ¹æ®arXivç‰ˆæœ¬æ—¥æœŸ)",
    "last_updated_quote": "arXiv:2512.01255v1  [cs.CR]  1 Dec 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œé€šè¿‡FORGEJSè‡ªåŠ¨ç”Ÿæˆæ¡†æ¶æ„å»º",
    "build_type_quote": "we propose FORGEJS, the first automatic benchmark generation framework... Then, we use FORGEJS to construct ARENAJS",
    "contamination_status": "é€šè¿‡ä½¿ç”¨å®Œæ•´é¡¹ç›®è€Œéå•æ–‡ä»¶ç‰‡æ®µã€æ„å»ºä¿®å¤å‰åé¡¹ç›®å¯¹æ¥ç›´æ¥é‡åŒ–å‡é˜³æ€§ã€ä»¥åŠç³»ç»Ÿå¼•å…¥å››ç§å¢å¼ºç­–ç•¥æ¥ç ´åå¯¹æ–‡ä»¶åã€å¯¼å…¥è·¯å¾„å’Œæ³¨é‡Šçš„ä¾èµ–ï¼Œæ—¨åœ¨é¿å…é«˜ä¼°",
    "contamination_status_quote": "To avoid overestimation, FORGEJS uses complete projects rather than single-file snippets, constructs pre- and post-fix project pairs to directly quantify false positives and localize error sources, and systematically introduces four augmentation strategies to disrupt reliance on filenames, import paths, and comments.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "æ¼æ´æ£€æµ‹ï¼ˆåŒ…æ‹¬å®šä½æ¼æ´æ–‡ä»¶ã€å‡½æ•°ã€CWEç±»å‹å’Œå…·ä½“ä»£ç è¡Œï¼‰",
    "task_granularity_quote": "evaluation protocols commonly assess only whether vulnerability is detected and ignore reasoning quality such as whether the model can correctly localize the vulnerable file, function, and CWE type.",
    "evaluation_metrics": "F1åˆ†æ•°ã€å‡é˜³æ€§ç‡ï¼ˆFPRï¼‰ã€VD-SæŒ‡æ ‡",
    "evaluation_metrics_quote": "including F1, false positive rate (FPR), and detection efficacy under engineering constraints... from the perspective of the VD-S metric [6]",
    "input_modality": "JavaScriptä»£ç ï¼ˆå‡½æ•°æˆ–å®Œæ•´é¡¹ç›®ï¼‰",
    "input_modality_quote": "supports both function-level and project-level evaluation",
    "output_modality": "æ¼æ´æ£€æµ‹ç»“æœï¼ˆåŒ…æ‹¬æ˜¯å¦å­˜åœ¨æ¼æ´ã€CWEç±»å‹ã€å®šä½ä¿¡æ¯ã€æ¨ç†ç­‰ï¼‰",
    "output_modality_quote": "evaluation protocols commonly assess only whether vulnerability is detected and ignore reasoning quality such as whether the model can correctly localize the vulnerable file, function, and CWE type.",
    "task_io_type": "ä»£ç åˆ°å®‰å…¨åˆ†æ",
    "task_io_type_quote": "evaluating LLMsâ€™ capability in JavaScript vulnerability detection",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "é¦–ä¸ªåŸºäºLLMçš„JavaScriptæ¼æ´æ£€æµ‹ç³»ç»ŸåŸºå‡†ï¼›éµå¾ªä¸‰ä¸ªæ„å»ºåŸåˆ™ï¼šå…¨é¢æ€§ã€ä¸ä½ä¼°ã€ä¸é«˜ä¼°ï¼›è¦†ç›–218ç§CWEç±»å‹ï¼›æ”¯æŒå‡½æ•°çº§å’Œé¡¹ç›®çº§è¯„ä¼°ï¼›é€šè¿‡FORGEJSè‡ªåŠ¨ç”Ÿæˆï¼›åŒ…å«JUDGEJSè‡ªåŠ¨è¯„ä¼°æ¡†æ¶ï¼›æ—¨åœ¨å®¢è§‚å®šä¹‰LLMåœ¨å¯é‡å¤ã€å¯æ‰©å±•ã€éƒ¨ç½²å—é™æ¡ä»¶ä¸‹çš„ç°å®æ€§èƒ½ä¸Šä¸‹é™ã€‚",
    "unique_features_quote": "we introduce, for the first time, three principles for constructing a benchmark for JavaScript vulnerability detection that directly address these limitations: (1) comprehensiveness, (2) no underestimation, and (3) no overestimation... FORGEJS aggregates heterogeneous sources, covers 218 CWE types... supports both function-level and project-level evaluation... Taken together, these principles aim to objectively define the realistic upper and lower bounds of LLM performance under reproducible, scalable, deployment-constrained conditions.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": 12,
    "last_updated_day": 1,
    "language_normalized": "['JavaScript']",
    "dimension_normalized": "['æ¼æ´æ£€æµ‹èƒ½åŠ›', 'æ¨ç†å……åˆ†æ€§', 'é²æ£’æ€§', 'ç°å®ä¸–ç•Œå¯ç”¨æ€§']",
    "evaluation_method_normalized": "['F1åˆ†æ•°', 'å‡é˜³æ€§ç‡ï¼ˆFPRï¼‰', 'VD-SæŒ‡æ ‡']",
    "problem_domain_normalized": "['JavaScriptå®‰å…¨æ¼æ´æ£€æµ‹', 'å‰ç«¯ï¼ˆå¦‚DOM-based XSSï¼‰', 'åç«¯ï¼ˆå¦‚SQLæ³¨å…¥ã€å‘½ä»¤æ³¨å…¥ã€åŸå‹æ±¡æŸ“ï¼‰', 'å…¨æ ˆç¯å¢ƒ']",
    "source_type_normalized": "['èšåˆå¼‚æ„æ¥æº', 'çœŸå®ä¸–ç•Œ', 'åˆæˆæ•°æ®']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "JSON",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2512.03421_output/content.md",
    "benchmark_name": "BugT",
    "benchmark_name_quote": "This study evaluates six closed-source and seven open-source LLMs using the Codeflaws, Condefects, and BugT datasets, with BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns. ... We introduce a new self-created dataset with real programming faults, aiming to mitigate data leakage concerns and establish a more reliable evaluation benchmark for assessing LLMsâ€™ capabilities.",
    "dataset_url": "https://github.com/Xucranger/PLofLBFL",
    "dataset_url_quote": "To facilitate future study, we share our source code and experimental data in a GitHub repository 1. 1https://github.com/Xucranger/PLofLBFL",
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åˆå­¦è€…ç¨‹åºä¸­è¿›è¡Œæ•…éšœå®šä½ï¼ˆFault Localizationï¼‰çš„æ€§èƒ½ã€‚",
    "task_description_quote": "This study evaluates the fault localization performance of six closed-source LLMs (e.g., OpenAI o3, GPT-3.5-Turbo, etc.) and seven open-source LLMs (e.g., DeepSeekR1, Llama3, etc.) across Codeflaws, Condefects, and BugT datasets.",
    "dimension": "æ•…éšœå®šä½çš„å‡†ç¡®æ€§ã€æ•ˆç‡ã€å¯ç”¨æ€§ï¼Œä»¥åŠå¯¹é—®é¢˜éš¾åº¦çš„é²æ£’æ€§ã€‚",
    "dimension_quote": "To investigate these issues, we aim to empirically assess the performance of different LLMs across various datasets, evaluating their accuracy, efficiency, and usability in fault localization for novice programs. ... We examine how problem difficulty levels across three datasets affect LLMsâ€™ fault localization performance, providing insights into model behavior at varying complexity levels.",
    "evaluation_method": "åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°LLMsçš„æ•…éšœå®šä½æ€§èƒ½ï¼Œå¹¶ä¸ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚SBFLã€MBFLï¼‰è¿›è¡Œæ¯”è¾ƒã€‚",
    "evaluation_method_quote": "This study evaluates the fault localization performance of six closed-source LLMs (e.g., OpenAI o3, GPT-3.5-Turbo, etc.) and seven open-source LLMs (e.g., DeepSeekR1, Llama3, etc.) across Codeflaws, Condefects, and BugT datasets. All closed-source LLMs outperform traditional SBFL and MBFL methods...",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "ç¼–ç¨‹æ•™è‚²ï¼Œåˆå­¦è€…ç¼–ç¨‹ã€‚",
    "problem_domain_quote": "Novice programmers often face challenges in fault localization due to their limited experience and understanding of programming syntax and logic. ... This study evaluates the fault localization performance ... for novice programs.",
    "problem_difficulty": "åŒ…å«ä¸åŒéš¾åº¦çº§åˆ«çš„é—®é¢˜ï¼Œä»ç®€å•åˆ°å¤æ‚ã€‚",
    "problem_difficulty_quote": "LLM accuracy decreases as problem difficulty increases in Codeflaws and Condefects, but top models maintain high accuracy even at peak difficulty in BugT, suggesting its lower complexity. ... We examine how problem difficulty levels across three datasets affect LLMsâ€™ fault localization performance...",
    "language": NaN,
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": "è‡ªå»ºçš„åŒ…å«çœŸå®ç¼–ç¨‹æ•…éšœçš„æ•°æ®é›†ã€‚",
    "source_type_quote": "We introduce a new self-created dataset with real programming faults, aiming to mitigate data leakage concerns and establish a more reliable evaluation benchmark for assessing LLMsâ€™ capabilities.",
    "last_updated": "2025-12-04",
    "last_updated_quote": "Preprint submitted to Journal of LATEX Templates December 4, 2025",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "We introduce a new self-created dataset with real programming faults...",
    "contamination_status": "ä¸“é—¨è®¾è®¡ä»¥å‡è½»æ•°æ®æ³„éœ²é—®é¢˜ã€‚",
    "contamination_status_quote": "BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "æ•…éšœå®šä½",
    "task_granularity_quote": "This study evaluates the fault localization performance...",
    "evaluation_metrics": "å‡†ç¡®æ€§ï¼ˆaccuracyï¼‰",
    "evaluation_metrics_quote": "LLM accuracy decreases as problem difficulty increases...",
    "input_modality": "åŒ…å«æ•…éšœçš„ä»£ç ",
    "input_modality_quote": "Novice Program refers to code written by novice programmers, usually in the early stages of learning programming. These programs usually contain multiple errors...",
    "output_modality": "æ•…éšœä½ç½®åŠè§£é‡Š",
    "output_modality_quote": "LLM generated fault explanations demonstrate significant value for novice programmer assistance...",
    "task_io_type": "ä»£ç åˆ°æ•…éšœä½ç½®/è§£é‡Š",
    "task_io_type_quote": "LLM-based fault localization methods leverage the modelsâ€™ understanding of program syntax and semantics to automatically localize faults in code, offering more precise and contextually relevant suggestions...",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“é—¨é’ˆå¯¹åˆå­¦è€…ç¨‹åºï¼ˆNovice Programsï¼‰æ„å»ºï¼Œæ—¨åœ¨å‡è½»LLMè¯„ä¼°ä¸­çš„æ•°æ®æ³„éœ²ï¼ˆdata leakageï¼‰é—®é¢˜ï¼ŒåŒ…å«çœŸå®çš„ç¼–ç¨‹æ•…éšœã€‚",
    "unique_features_quote": "BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns. ... We introduce a new self-created dataset with real programming faults, aiming to mitigate data leakage concerns and establish a more reliable evaluation benchmark for assessing LLMsâ€™ capabilities.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": 12,
    "last_updated_day": 4,
    "language_normalized": "[]",
    "dimension_normalized": "['æ•…éšœå®šä½çš„å‡†ç¡®æ€§', 'æ•ˆç‡', 'å¯ç”¨æ€§', 'å¯¹é—®é¢˜éš¾åº¦çš„é²æ£’æ€§']",
    "evaluation_method_normalized": "['å‡†ç¡®æ€§ï¼ˆaccuracyï¼‰']",
    "problem_domain_normalized": "['ç¼–ç¨‹æ•™è‚²', 'åˆå­¦è€…ç¼–ç¨‹']",
    "source_type_normalized": "['è‡ªå»ºçš„åŒ…å«çœŸå®ç¼–ç¨‹æ•…éšœçš„æ•°æ®é›†']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2105.09938_output/content.md",
    "benchmark_name": "APPS",
    "benchmark_name_quote": "To meet this challenge, we introduce APPS, a benchmark for code generation.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "To meet this challenge, we introduce APPS, a benchmark for code generation.",
    "dataset_url": "https://github.com/hendrycks/apps",
    "dataset_url_quote": "The dataset is available at https://github.com/hendrycks/apps.",
    "task_description": "ä»ä»»æ„è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆæ»¡è¶³è¦æ±‚çš„Pythonä»£ç ã€‚",
    "task_description_quote": "our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code.",
    "dimension": "ä»£ç ç”Ÿæˆèƒ½åŠ›ï¼ŒåŒ…æ‹¬ç†è§£ä»»åŠ¡æè¿°ã€è®¾è®¡ç®—æ³•ã€ç¼–å†™è¯­æ³•æ­£ç¡®ä¸”åŠŸèƒ½æ­£ç¡®çš„ç¨‹åºã€‚",
    "dimension_quote": "APPS evaluates models not only on their ability to code syntactically correct programs, but also on their ability to understand task descriptions and devise algorithms to solve these tasks.",
    "evaluation_method": "ä½¿ç”¨æµ‹è¯•ç”¨ä¾‹æ£€æŸ¥ç”Ÿæˆçš„ä»£ç ã€‚",
    "evaluation_method_quote": "Similar to how companies assess candidate software developers, we evaluate models by checking their generated code on test cases.",
    "context_dependency": "å•å‡½æ•°ï¼ˆCall-Based Formatï¼‰æˆ–å®Œæ•´è„šæœ¬ï¼ˆStandard Input/Output Formatï¼‰ã€‚",
    "context_dependency_quote": "â€¢ Call-Based Format problems generally provide initial starter code, usually in the form of a function header, and ask for the solution to be provided as the functionâ€™s return value.",
    "problem_domain": "ç¼–ç¨‹ä¸ç®—æ³•ï¼Œæ¶µç›–ä»ç®€å•å­—ç¬¦ä¸²æ“ä½œåˆ°å¤æ‚çš„å›¾è®ºã€æ•°æ®ç»“æ„ç­‰ç®—æ³•æŒ‘æˆ˜ã€‚",
    "problem_domain_quote": "Our benchmark includes 10,000 problems, which range from having simple one-line solutions to being substantial algorithmic challenges.",
    "problem_difficulty": "åˆ†ä¸ºå…¥é—¨çº§ã€é¢è¯•çº§å’Œç«èµ›çº§ä¸‰ä¸ªéš¾åº¦ã€‚",
    "problem_difficulty_quote": "It contains 10,000 programming problems at various levels of difficulty, covering simple introductory problems, interview-level problems, and coding competition challenges.",
    "language": "Python",
    "language_quote": "our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code.",
    "data_size": "åŒ…å«10,000ä¸ªç¼–ç¨‹é—®é¢˜ï¼Œ131,777ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼Œ232,421ä¸ªäººå·¥ç¼–å†™çš„å‚è€ƒç­”æ¡ˆã€‚",
    "data_size_quote": "The Automated Programming Progress Standard, abbreviated APPS, consists of 10,000 coding problems in total, with 131,777 test cases for checking solutions and 232,421 ground-truth solutions written by humans.",
    "source_type": "ä»å¼€æ”¾çš„ç¼–ç¨‹ç½‘ç«™ï¼ˆå¦‚Codeforces, Kattis, Codewars, AtCoderï¼‰æ‰‹åŠ¨æ”¶é›†å’Œæ•´ç†ã€‚",
    "source_type_quote": "The APPS dataset consists of problems collected from different open-access coding websites such as Codeforces, Kattis, and more.",
    "last_updated": "2021",
    "last_updated_quote": "35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks.",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œç”±å¤šåç ”ç©¶ç”Ÿå’Œæœ¬ç§‘ç”Ÿåœ¨å…­ä¸ªæœˆå†…ç²¾å¿ƒæ•´ç†å’Œä¼˜åŒ–ã€‚",
    "build_type_quote": "Several graduate and undergraduate student authors polished and refined this dataset over the course of six months, ensuring a high-quality set of problems.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "APPS, a benchmark for code generation from natural language specifications.",
    "evaluation_metrics": "é€šè¿‡æµ‹è¯•ç”¨ä¾‹çš„å‡†ç¡®ç‡ï¼ˆä¾‹å¦‚ pass@kï¼‰ã€‚",
    "evaluation_metrics_quote": "we evaluate models by checking their generated code on test cases.",
    "input_modality": "è‡ªç„¶è¯­è¨€",
    "input_modality_quote": "take an arbitrary natural language specification",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "generate satisfactory Python code",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "code generation from natural language specifications.",
    "execution_environment": "å…è®¸æ‰§è¡Œä»»æ„Pythonä»£ç ï¼ˆåŒ…æ‹¬å¯¼å…¥å¸¸è§æ¨¡å—å’Œåº“ï¼‰ï¼Œä½¿ç”¨è‡ªå®šä¹‰çš„æµ‹è¯•æ¡†æ¶ã€‚",
    "execution_environment_quote": "solutions are allowed to execute arbitrary Python code, and the results are compared against test cases for a given problem.",
    "unique_features": "1. é—®é¢˜æè¿°å¹³å‡é•¿åº¦è¾¾293.2ä¸ªå•è¯ï¼Œæ˜¯è‡ªåŒ…å«çš„å®Œæ•´è§„èŒƒã€‚2. æ‹¥æœ‰å¤§é‡æµ‹è¯•ç”¨ä¾‹ï¼ˆè¶…è¿‡13ä¸‡ä¸ªï¼‰ç”¨äºä¸¥æ ¼è¯„ä¼°åŠŸèƒ½æ­£ç¡®æ€§ã€‚3. éš¾åº¦åˆ†çº§æ˜ç¡®ï¼Œè¦†ç›–ä»å…¥é—¨åˆ°ç«èµ›çš„å¹¿æ³›èŒƒå›´ã€‚4. æ¨¡æ‹Ÿäº†äººç±»ç¨‹åºå‘˜ï¼ˆå¦‚æ±‚èŒé¢è¯•ï¼‰çš„è¯„ä¼°æ–¹å¼ã€‚",
    "unique_features_quote": "By comparison, problem specifications in our new APPS benchmark are self-contained and have a much larger average length of 293.2 words. Unlike Iyer et al. (2018), APPS contains test cases for every exercise, enabling a high-quality evaluation of code correctness. The APPS benchmark attempts to mirror how humans programmers are evaluated by posing coding problems in unrestricted natural language and using test cases to evaluate solution correctness.",
    "data_size_quantity": 10000,
    "data_size_unit": "ä¸ªç¼–ç¨‹é—®é¢˜",
    "last_updated_year": 2021,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆèƒ½åŠ›', 'ç†è§£ä»»åŠ¡æè¿°', 'è®¾è®¡ç®—æ³•', 'ç¼–å†™è¯­æ³•æ­£ç¡®ä¸”åŠŸèƒ½æ­£ç¡®çš„ç¨‹åº']",
    "evaluation_method_normalized": "['é€šè¿‡æµ‹è¯•ç”¨ä¾‹çš„å‡†ç¡®ç‡', 'pass@k']",
    "problem_domain_normalized": "['ç¼–ç¨‹ä¸ç®—æ³•', 'ç®€å•å­—ç¬¦ä¸²æ“ä½œ', 'å¤æ‚çš„å›¾è®º', 'æ•°æ®ç»“æ„', 'ç®—æ³•æŒ‘æˆ˜']",
    "source_type_normalized": "['å¼€æ”¾çš„ç¼–ç¨‹ç½‘ç«™', 'Codeforces', 'Kattis', 'Codewars', 'AtCoder', 'æ‰‹åŠ¨æ”¶é›†å’Œæ•´ç†']",
    "problem_difficulty_normalized": "å…¥é—¨çº§",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2107.03374_output/content.md",
    "benchmark_name": "HumanEval",
    "benchmark_name_quote": "On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We evaluate functional correctness on a set of 164 hand-written programming problems, which we call the HumanEval dataset. We release the HumanEval dataset so that others can evaluate functional correctness and measure the problem-solving capabilities of their models.",
    "dataset_url": "https://www.github.com/openai/human-eval",
    "dataset_url_quote": "We release this data along with an evaluation framework at https://www.github.com/openai/human-eval.",
    "task_description": "ä»æ–‡æ¡£å­—ç¬¦ä¸²ï¼ˆdocstringsï¼‰åˆæˆç‹¬ç«‹çš„Pythonå‡½æ•°ï¼Œå¹¶è¯„ä¼°ç”Ÿæˆä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚",
    "task_description_quote": "In this work, we focus on the task of generating standalone Python functions from docstrings, and evaluate the correctness of code samples automatically through unit tests.",
    "dimension": "ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "measure functional correctness for synthesizing programs from docstrings",
    "evaluation_method": "é€šè¿‡å•å…ƒæµ‹è¯•è¯„ä¼°åŠŸèƒ½æ­£ç¡®æ€§ï¼Œå¹¶ä½¿ç”¨pass@kæŒ‡æ ‡ã€‚",
    "evaluation_method_quote": "evaluate the correctness of code samples automatically through unit tests. Kulal et al. (2019) evaluate functional correctness using the pass@k metric",
    "context_dependency": "å•å‡½æ•°",
    "context_dependency_quote": "generating standalone Python functions",
    "problem_domain": "è¯­è¨€ç†è§£ã€æ¨ç†ã€ç®—æ³•å’Œç®€å•æ•°å­¦",
    "problem_domain_quote": "Programming tasks in the HumanEval dataset assess language comprehension, reasoning, algorithms, and simple mathematics.",
    "problem_difficulty": "è¯„ä¼°è¯­è¨€ç†è§£ã€ç®—æ³•å’Œç®€å•æ•°å­¦ï¼Œéƒ¨åˆ†é—®é¢˜ç±»ä¼¼äºç®€å•çš„è½¯ä»¶é¢è¯•é¢˜ã€‚",
    "problem_difficulty_quote": "These problems assess language comprehension, algorithms, and simple mathematics, with some comparable to simple software interview questions.",
    "language": "Python",
    "language_quote": "study its Python code-writing capabilities. ... generating standalone Python functions from docstrings",
    "data_size": "åŒ…å«164ä¸ªæ‰‹å†™çš„ç¼–ç¨‹é—®é¢˜ï¼Œå¹³å‡æ¯ä¸ªé—®é¢˜æœ‰7.7ä¸ªæµ‹è¯•ç”¨ä¾‹ã€‚",
    "data_size_quote": "a dataset of 164 original programming problems with unit tests. ... an average of 7.7 tests per problem.",
    "source_type": "æ‰‹å†™æ„å»ºï¼Œæœªä»ç°æœ‰æ¥æºç¨‹åºåŒ–å¤åˆ¶ã€‚",
    "source_type_quote": "all problems were hand-written and not programmatically copied from existing sources.",
    "last_updated": "2021",
    "last_updated_quote": "arXiv:2107.03374v2 [cs.LG] 14 Jul 2021",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆOpenAIå›¢é˜Ÿæ‰‹å†™ï¼‰",
    "build_type_quote": "we create a dataset of 164 original programming problems. ... all problems were hand-written",
    "contamination_status": "ä¸ºæŠ—æ±¡æŸ“è€Œè®¾è®¡ï¼Œæ‰‹å†™é—®é¢˜ä»¥é¿å…è®­ç»ƒæ•°æ®ï¼ˆGitHubï¼‰ä¸­å·²å­˜åœ¨çš„è§£å†³æ–¹æ¡ˆã€‚",
    "contamination_status_quote": "It is important for these tasks to be hand-written, since our models are trained on a large fraction of GitHub, which already contains solutions to problems from a variety of sources.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆä»æ–‡æ¡£å­—ç¬¦ä¸²ç”Ÿæˆå®Œæ•´å‡½æ•°ä½“ï¼‰",
    "task_granularity_quote": "generating standalone Python functions from docstrings",
    "evaluation_metrics": "pass@k",
    "evaluation_metrics_quote": "Kulal et al. (2019) evaluate functional correctness using the pass@k metric",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆæ–‡æ¡£å­—ç¬¦ä¸²ï¼‰ä¸ä»£ç ï¼ˆå‡½æ•°ç­¾åï¼‰",
    "input_modality_quote": "synthesizing programs from docstrings. ... prompt consisting of a header, a signature, and a docstring",
    "output_modality": "ä»£ç ï¼ˆPythonå‡½æ•°ä½“ï¼‰",
    "output_modality_quote": "generating standalone Python functions",
    "task_io_type": "æ–‡æœ¬ï¼ˆæ–‡æ¡£å­—ç¬¦ä¸²ï¼‰åˆ°ä»£ç ",
    "task_io_type_quote": "synthesizing programs from docstrings",
    "execution_environment": "ä½¿ç”¨gVisorå®¹å™¨è¿è¡Œæ—¶çš„æ²™ç›’ç¯å¢ƒï¼Œç”¨äºå®‰å…¨æ‰§è¡Œä¸å—ä¿¡ä»»çš„ç¨‹åºã€‚",
    "execution_environment_quote": "we developed a sandbox environment to safely run untrusted programs against unit tests. We selected the gVisor container runtime (Lacasse, 2018) as the main host protection component.",
    "unique_features": "ä¸“é—¨ä¸ºè¯„ä¼°ä»£ç ç”Ÿæˆæ¨¡å‹çš„åŠŸèƒ½æ­£ç¡®æ€§è€Œè®¾è®¡ï¼Œå¼ºè°ƒæ‰‹å†™é—®é¢˜ä»¥é¿å…æ•°æ®æ±¡æŸ“ï¼Œå¹¶æä¾›äº†å®‰å…¨çš„æ²™ç›’æ‰§è¡Œç¯å¢ƒã€‚",
    "unique_features_quote": "It is important for these tasks to be hand-written, since our models are trained on a large fraction of GitHub, which already contains solutions to problems from a variety of sources. ... we developed a sandbox environment to safely run untrusted programs against unit tests.",
    "data_size_quantity": 164,
    "data_size_unit": "ä¸ªé—®é¢˜",
    "last_updated_year": 2021,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['pass@k']",
    "problem_domain_normalized": "['è¯­è¨€ç†è§£', 'æ¨ç†', 'ç®—æ³•', 'ç®€å•æ•°å­¦']",
    "source_type_normalized": "['æ‰‹å†™æ„å»º']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2108.07732_output/content.md",
    "benchmark_name": "Mostly Basic Programming Problems (MBPP)",
    "benchmark_name_quote": "We introduce two datasets to test Python code synthesis. The first is a new dataset called Mostly Basic Programming Problems (MBPP).",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We introduce two datasets to test Python code synthesis. The first is a new dataset called Mostly Basic Programming Problems (MBPP).",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ä»è‡ªç„¶è¯­è¨€æè¿°ä¸­åˆæˆç®€çŸ­çš„Pythonç¨‹åºã€‚",
    "task_description_quote": "Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions.",
    "dimension": "ç¨‹åºåˆæˆèƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯ä»è‡ªç„¶è¯­è¨€æè¿°ç”ŸæˆåŠŸèƒ½æ­£ç¡®çš„Pythonä»£ç ã€‚",
    "dimension_quote": "This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages.",
    "evaluation_method": "é€šè¿‡æ‰§è¡Œæµ‹è¯•ç”¨ä¾‹æ£€æŸ¥åŠŸèƒ½æ­£ç¡®æ€§ã€‚",
    "evaluation_method_quote": "Participants also provided a ground-truth solution that passes all three test cases.",
    "context_dependency": "å•å‡½æ•°ï¼Œè‡ªåŒ…å«ã€‚",
    "context_dependency_quote": "Participants were instructed to write code that is self-contained (that is, it runs by itself)...",
    "problem_domain": "æ¶µç›–æ•°å­¦ã€åˆ—è¡¨å¤„ç†ã€å­—ç¬¦ä¸²å¤„ç†ã€æ•´æ•°åºåˆ—ç­‰ã€‚",
    "problem_domain_quote": "Of these questions, 58% were mathematical in nature (e.g., calculating the volume of a sphere), 43% involve list processing, 19% require string processing, 9% involve integer seque",
    "problem_difficulty": "å…¥é—¨çº§ï¼Œè®¾è®¡ä¸ºå…¥é—¨çº§ç¨‹åºå‘˜å¯è§£å†³ã€‚",
    "problem_difficulty_quote": "The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers.",
    "language": "Python",
    "language_quote": "The Mostly Basic Programming Problems dataset contains 974 short Python programs...",
    "data_size": "åŒ…å«974ä¸ªç¼–ç¨‹ä»»åŠ¡ã€‚",
    "data_size_quote": "The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks...",
    "source_type": "é€šè¿‡ä¼—åŒ…æ„å»ºï¼Œéƒ¨åˆ†ç”±ä½œè€…ç¼–è¾‘å’Œæ‰‹åŠ¨éªŒè¯ã€‚",
    "source_type_quote": "This dataset consists of a large set of crowd-sourced questions and a smaller set of questions edited and hand-verified by the authors.",
    "last_updated": "2021",
    "last_updated_quote": "arXiv:2108.07732v1  [cs.PL]  16 Aug 2021",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆGoogle Researchï¼‰",
    "build_type_quote": "We construct two new datasets: one entirely new and the other modified from an existing benchmark. The first, Mostly Basic Programming Problems (MBPP), is an entirely new crowd-sourced programming dataset.",
    "contamination_status": "æ–‡ä¸­æåŠä¸é¢„è®­ç»ƒé›†çš„é‡å å¾ˆå°ï¼Œé™ä½äº†è®°å¿†é£é™©ã€‚",
    "contamination_status_quote": "Second, we find that the overlap between the solutions in MBPP and the pre-training set is small, reducing the chance that our synthesis results are due to memorization (Section 4.8).",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆä»è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆå®Œæ•´çš„å‡½æ•°ä½“ï¼‰ã€‚",
    "task_granularity_quote": "Participants were instructed to write a short problem statement, a single self-contained Python function solving the problem specified...",
    "evaluation_metrics": "åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆé€šè¿‡æµ‹è¯•ç”¨ä¾‹ï¼‰ã€‚",
    "evaluation_metrics_quote": "Participants also provided a ground-truth solution that passes all three test cases.",
    "input_modality": "è‡ªç„¶è¯­è¨€æè¿°ï¼Œé€šå¸¸ç»“åˆå°‘é‡è¾“å…¥è¾“å‡ºç¤ºä¾‹ã€‚",
    "input_modality_quote": "a program can be specified by a short natural language description, possibly combined with a few (e.g., 2 or 3) input-output examples.",
    "output_modality": "ä»£ç ï¼ˆPythonå‡½æ•°ï¼‰ã€‚",
    "output_modality_quote": "Participants were instructed to write a short problem statement, a single self-contained Python function solving the problem specified...",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions.",
    "execution_environment": "è‡ªåŒ…å«ï¼Œå¯ä½¿ç”¨æ ‡å‡†åº“ï¼Œå…è®¸ä½¿ç”¨äº’è”ç½‘å‚è€ƒã€‚",
    "execution_environment_quote": "Participants were instructed to write code that is self-contained (that is, it runs by itself)... Use of internet references was allowed.",
    "unique_features": "åŒ…å«ä¸‰ä¸ªä¸€è‡´çš„ã€ä»¥assertè¯­å¥ç¼–å†™çš„è¾“å…¥è¾“å‡ºç¤ºä¾‹ï¼›é—®é¢˜æè¿°è®¾è®¡ä¸ºæ›´åŸºç¡€ã€å­—é¢åŒ–ï¼Œè€Œéç«èµ›é£æ ¼ï¼›åŒ…å«ä¼—åŒ…å’Œæ‰‹åŠ¨éªŒè¯ä¸¤éƒ¨åˆ†ã€‚",
    "unique_features_quote": "In contrast, our dataset consistently contains three I/O examples, written as assert statements. ... By contrast, our Mostly Basic Programming Problems dataset is designed to contain a more basic, literal description of the problems. ... This dataset consists of a large set of crowd-sourced questions and a smaller set of questions edited and hand-verified by the authors.",
    "data_size_quantity": 974,
    "data_size_unit": "ä¸ªç¼–ç¨‹ä»»åŠ¡",
    "last_updated_year": 2021,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ç¨‹åºåˆæˆèƒ½åŠ›', 'ç‰¹åˆ«æ˜¯ä»è‡ªç„¶è¯­è¨€æè¿°ç”ŸæˆåŠŸèƒ½æ­£ç¡®çš„Pythonä»£ç ']",
    "evaluation_method_normalized": "['åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆé€šè¿‡æµ‹è¯•ç”¨ä¾‹ï¼‰']",
    "problem_domain_normalized": "['æ•°å­¦', 'åˆ—è¡¨å¤„ç†', 'å­—ç¬¦ä¸²å¤„ç†', 'æ•´æ•°åºåˆ—']",
    "source_type_normalized": "['é€šè¿‡ä¼—åŒ…æ„å»º', 'éƒ¨åˆ†ç”±ä½œè€…ç¼–è¾‘å’Œæ‰‹åŠ¨éªŒè¯']",
    "problem_difficulty_normalized": "å…¥é—¨çº§",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æ ‡å‡†åº“",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2203.13474_output/content.md",
    "benchmark_name": "Multi-Turn Programming Benchmark (MTPB)",
    "benchmark_name_quote": "To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "In this work, we develop a multi-turn programming benchmark to measure the modelsâ€™ capacity for multi-turn program synthesis. To the best of our knowledge, this is the first multi-turn program synthesis benchmark, which allows quantitative analysis of multi-turn program synthesis.",
    "dataset_url": "https://github.com/salesforce/CodeGen/tree/main/benchmark",
    "dataset_url_quote": "1Benchmark: https://github.com/salesforce/CodeGen/tree/main/benchmark",
    "task_description": "è¯„æµ‹æ¨¡å‹çš„å¤šè½®ç¨‹åºåˆæˆèƒ½åŠ›ã€‚ç”¨æˆ·ä»¥è‡ªç„¶è¯­è¨€åœ¨å¤šè½®å¯¹è¯ä¸­é€æ­¥æŒ‡å®šæ„å›¾ï¼Œæ¨¡å‹åœ¨æ¯ä¸€è½®ä¸­åˆæˆå­ç¨‹åºï¼Œæœ€ç»ˆå…±åŒå®Œæˆä¸€ä¸ªå®Œæ•´çš„ç¨‹åºã€‚",
    "task_description_quote": "To solve a problem in the benchmark, a model needs to synthesize a program in multiple steps with a user who specifies the intent in each turn in natural language.",
    "dimension": "å¤šè½®ç¨‹åºåˆæˆèƒ½åŠ›",
    "dimension_quote": "In this work, we develop a multi-turn programming benchmark to measure the modelsâ€™ capacity for multi-turn program synthesis.",
    "evaluation_method": "é€šè¿‡ä¸“å®¶ç¼–å†™çš„æµ‹è¯•ç”¨ä¾‹çš„é€šè¿‡ç‡ï¼ˆpass rateï¼‰æ¥è¡¡é‡æ€§èƒ½ã€‚",
    "evaluation_method_quote": "Performance on the benchmark is measured by pass rate on expert-written test cases.",
    "context_dependency": "å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡ã€‚ä¸€ä¸ªå®Œæ•´çš„ç¨‹åºè¢«åˆ†è§£ä¸ºå¤šä¸ªå­é—®é¢˜ï¼Œæ¯ä¸€è½®å¯¹è¯æŒ‡å®šä¸€ä¸ªå­é—®é¢˜çš„æ„å›¾ã€‚",
    "context_dependency_quote": "consisting of 115 diverse problem sets that are factorized into multi-turn prompts.",
    "problem_domain": "é€šç”¨ç¼–ç¨‹é—®é¢˜ã€‚æ–‡ä¸­ç¤ºä¾‹ä¸ºä»ç”µå­é‚®ä»¶åœ°å€ä¸­æå–ç”¨æˆ·åã€‚",
    "problem_domain_quote": "Please refer to Figure 1 for an example where the model synthesizes a program to extract the user name of an email address.",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "æœªæ˜ç¡®æŒ‡å®šï¼Œä½†æ ¹æ®æ¨¡å‹è®­ç»ƒå’Œä¸Šä¸‹æ–‡ï¼ˆå¦‚HumanEvaléƒ¨åˆ†ï¼‰ï¼Œå¯èƒ½ä¸»è¦æ¶‰åŠPythonã€‚",
    "language_quote": "æ–‡ä¸­æœªæ˜ç¡®æè¿°MTPBçš„ç¼–ç¨‹è¯­è¨€ã€‚",
    "data_size": "åŒ…å«115ä¸ªå¤šæ ·åŒ–çš„é—®é¢˜é›†ã€‚",
    "data_size_quote": "consisting of 115 diverse problem sets that are factorized into multi-turn prompts.",
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": "2023ï¼ˆæ ¹æ®è®ºæ–‡ç‰ˆæœ¬æ—¥æœŸæ¨æ–­ï¼‰",
    "last_updated_quote": "arXiv:2203.13474v5 [cs.LG] 27 Feb 2023",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB)",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "å¤šè½®ç¨‹åºåˆæˆ",
    "task_granularity_quote": "To solve a problem in the benchmark, a model needs to synthesize a program in multiple steps",
    "evaluation_metrics": "é€šè¿‡ç‡ï¼ˆpass rateï¼‰",
    "evaluation_metrics_quote": "Performance on the benchmark is measured by pass rate on expert-written test cases.",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆå¤šè½®å¯¹è¯ï¼‰",
    "input_modality_quote": "a user who specifies the intent in each turn in natural language.",
    "output_modality": "ä»£ç ï¼ˆå­ç¨‹åºæˆ–å®Œæ•´ç¨‹åºï¼‰",
    "output_modality_quote": "synthesize a program",
    "task_io_type": "è‡ªç„¶è¯­è¨€åˆ°ä»£ç ",
    "task_io_type_quote": "synthesize a program... with a user who specifies the intent in each turn in natural language.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "é¦–ä¸ªä¸“æ³¨äºå¤šè½®ç¨‹åºåˆæˆçš„è¯„æµ‹åŸºå‡†ã€‚é—®é¢˜è¢«åˆ†è§£ä¸ºå¤šè½®æç¤ºï¼Œæ¨¡æ‹Ÿç”¨æˆ·ä¸ç³»ç»Ÿé€æ­¥åä½œå®Œæˆç¼–ç¨‹ä»»åŠ¡çš„è¿‡ç¨‹ã€‚",
    "unique_features_quote": "To the best of our knowledge, this is the first multi-turn program synthesis benchmark, which allows quantitative analysis of multi-turn program synthesis.",
    "data_size_quantity": 115,
    "data_size_unit": "ä¸ªé—®é¢˜é›†",
    "last_updated_year": 2023,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['å¤šè½®ç¨‹åºåˆæˆèƒ½åŠ›']",
    "evaluation_method_normalized": "['é€šè¿‡ç‡ï¼ˆpass rateï¼‰']",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹é—®é¢˜']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2207.01780_output/content.md",
    "benchmark_name": "APPS",
    "benchmark_name_quote": "Our comprehensive experiments (Section 4) show that our models can achieve SOTA performance on the challenging APPS benchmark [Hendrycks et al., 2021].",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Our comprehensive experiments (Section 4) show that our models can achieve SOTA performance on the challenging APPS benchmark [Hendrycks et al., 2021].",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ç¨‹åºåˆæˆæˆ–ä»£ç ç”Ÿæˆï¼Œæ—¨åœ¨æ ¹æ®è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°ç”Ÿæˆæ»¡è¶³åŠŸèƒ½æ­£ç¡®æ€§çš„ç¨‹åºã€‚",
    "task_description_quote": "Program synthesis or code generation aims to generate a program that satisfies a problem specification.",
    "dimension": "ç¨‹åºçš„åŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "The expected output is a program to be checked for functional correctness against some unit tests.",
    "evaluation_method": "ä½¿ç”¨å•å…ƒæµ‹è¯•æ£€æŸ¥ç¨‹åºçš„åŠŸèƒ½æ­£ç¡®æ€§ï¼Œå¹¶é‡‡ç”¨ pass@k æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ã€‚",
    "evaluation_method_quote": "Specifically, our models reach more than 2% pass@1, 6% pass@5, and 20% pass@1000.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "é€šç”¨ç¼–ç¨‹ï¼ŒèŒƒå›´ä»åŸºç¡€ç¼–ç¨‹é—®é¢˜åˆ°ç«èµ›çº§ç¼–ç¨‹ä»»åŠ¡ã€‚",
    "problem_domain_quote": "This type of programming task can range from basic programming problems to competition-level programming tasks that require a high level of problem-solving skills.",
    "problem_difficulty": "å…·æœ‰æŒ‘æˆ˜æ€§ï¼ŒåŒ…å«ç«èµ›çº§å¤æ‚é—®é¢˜ã€‚",
    "problem_difficulty_quote": "Our comprehensive experiments (Section 4) show that our models can achieve SOTA performance on the challenging APPS benchmark [Hendrycks et al., 2021].",
    "language": "Python",
    "language_quote": "In our work, we focus on program synthesis from natural language problem specifications and the output programs are in general-purpose languages such as Python.",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "å®Œæ•´ç¨‹åºç”Ÿæˆ",
    "task_granularity_quote": "Training only with NTP objective is hence, not ideal to tackle full program generation to solve programming problems.",
    "evaluation_metrics": "pass@1, pass@5, pass@1000",
    "evaluation_metrics_quote": "Specifically, our models reach more than 2% pass@1, 6% pass@5, and 20% pass@1000.",
    "input_modality": "è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°ï¼Œé€šå¸¸åŒ…å«ç¤ºä¾‹è¾“å…¥è¾“å‡ºå¯¹ã€‚",
    "input_modality_quote": "Each task is defined by a problem specification in natural language, often containing example input and output pairs.",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "The expected output is a program to be checked for functional correctness against some unit tests.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "Program synthesis or code generation is the task of designing and building an executable computer program that satisfies a problem specification.",
    "execution_environment": "ç¼–è¯‘å™¨",
    "execution_environment_quote": "These program candidates are concatenated with the error information received from a compiler and passed to a program repair module.",
    "unique_features": "è¯¥åŸºå‡†ï¼ˆAPPSï¼‰åŒ…å«ä»åŸºç¡€åˆ°ç«èµ›çº§åˆ«çš„ç¼–ç¨‹é—®é¢˜ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹ç”ŸæˆåŠŸèƒ½æ­£ç¡®ç¨‹åºçš„èƒ½åŠ›ã€‚",
    "unique_features_quote": "This type of programming task can range from basic programming problems to competition-level programming tasks that require a high level of problem-solving skills.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ç¨‹åºçš„åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['pass@1', 'pass@5', 'pass@1000']",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹', 'èŒƒå›´ä»åŸºç¡€ç¼–ç¨‹é—®é¢˜åˆ°ç«èµ›çº§ç¼–ç¨‹ä»»åŠ¡']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "ç«èµ›çº§",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2207.10397_output/content.md",
    "benchmark_name": "HumanEval",
    "benchmark_name_quote": "We conduct comprehensive experiments on four benchmarks, HumanEval, MBPP, APPS, and CodeContests",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We conduct comprehensive experiments on four benchmarks, HumanEval, MBPP, APPS, and CodeContests",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ä»è‡ªç„¶è¯­è¨€æè¿°ï¼ˆä»£ç æ³¨é‡Šï¼‰å’Œå‡½æ•°ç­¾åç­‰ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸­ç”Ÿæˆä»£ç è§£å†³æ–¹æ¡ˆã€‚",
    "task_description_quote": "The task of code generation is to solve a programming problem: generate code solution x based on context c. As shown in Figure 2, context c contains natural language problem description in the form of code comment, and a code snippet that includes statements such as imports and the function header.",
    "dimension": "ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "A natural way to evaluate the quality and correctness of a code solution is to run it against a set of test cases",
    "evaluation_method": "æ‰§è¡Œå•å…ƒæµ‹è¯•ï¼Œä½¿ç”¨ pass@k æŒ‡æ ‡",
    "evaluation_method_quote": "We evaluate functional correctness using pass@1. ... For instance, Codex ... can achieve a pass@100 (pass if one or more among 100 generated solutions for a given problem can pass the corresponding test cases) of 77.4%, but a pass@1 (correct rate of a single solution) of only 33.5% on the HumanEval benchmark",
    "context_dependency": "å•å‡½æ•°",
    "context_dependency_quote": "Figure 2: Code generation and test case generation: an example from the HumanEval benchmark. ... Context c contains natural language problem description in the form of code comment, and a code snippet that includes statements such as imports and the function header.",
    "problem_domain": NaN,
    "problem_domain_quote": NaN,
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "Python",
    "language_quote": "Figure 2: Code generation and test case generation: an example from the HumanEval benchmark. ... from typing import List def has_close_elements(numbers: List[float], threshold: float) -> bool:",
    "data_size": "164ä¸ªé—®é¢˜",
    "data_size_quote": "Benchmark Problems ... HumanEval 164",
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "The task of code generation is to solve a programming problem: generate code solution x based on context c.",
    "evaluation_metrics": "pass@1, pass@100",
    "evaluation_metrics_quote": "For instance, Codex ... can achieve a pass@100 (pass if one or more among 100 generated solutions for a given problem can pass the corresponding test cases) of 77.4%, but a pass@1 (correct rate of a single solution) of only 33.5% on the HumanEval benchmark",
    "input_modality": "è‡ªç„¶è¯­è¨€ä¸ä»£ç ï¼ˆå‡½æ•°ç­¾åï¼‰",
    "input_modality_quote": "context c contains natural language problem description in the form of code comment, and a code snippet that includes statements such as imports and the function header.",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "generate code solution x",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "The task of code generation is to solve a programming problem: generate code solution x based on context c.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "æ¯ä¸ªé—®é¢˜å¹³å‡æœ‰7.77ä¸ªåœ°é¢çœŸå€¼æµ‹è¯•ç”¨ä¾‹",
    "unique_features_quote": "Benchmark ... GT Tests ... HumanEval ... 7.77",
    "data_size_quantity": 164,
    "data_size_unit": "ä¸ªé—®é¢˜",
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['pass@1', 'pass@100']",
    "problem_domain_normalized": "[]",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€ä¸ä»£ç ",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2208.08227_output/content.md",
    "benchmark_name": "MultiPL-E",
    "benchmark_name_quote": "We propose MultiPL-E, a system for translating unit test-driven code generation benchmarks to new languages.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We propose MultiPL-E, a system for translating unit test-driven code generation benchmarks to new languages. We create the ï¬rst massively multilingual code generation benchmark by using MultiPL-E to translate two popular Python code generation benchmarks to 18 additional programming languages.",
    "dataset_url": "github.com/nuprl/MultiPL-E",
    "dataset_url_quote": "The MultiPL-E system, dataset, and tutorial are available at github.com/nuprl/MultiPL-E.",
    "task_description": "å°†åŸºäºå•å…ƒæµ‹è¯•çš„ä»£ç ç”ŸæˆåŸºå‡†ï¼ˆä»Pythonï¼‰ç¿»è¯‘åˆ°æ–°çš„ç¼–ç¨‹è¯­è¨€ï¼Œä»¥åˆ›å»ºå¤§è§„æ¨¡ã€å¤šè¯­è¨€çš„å¹¶è¡Œä»£ç ç”Ÿæˆè¯„æµ‹åŸºå‡†ã€‚",
    "task_description_quote": "MultiPL-E, a system for translating unit test-driven code generation benchmarks to new languages. We create the ï¬rst massively multilingual code generation benchmark by using MultiPL-E to translate two popular Python code generation benchmarks to 18 additional programming languages.",
    "dimension": "å¤šè¯­è¨€ä»£ç ç”Ÿæˆèƒ½åŠ›ï¼Œæ¨¡å‹åœ¨ä¸åŒç¼–ç¨‹è¯­è¨€ä¸Šçš„æ³›åŒ–æ€§èƒ½",
    "dimension_quote": "We use these new parallel benchmarks to evaluate the multi-language performance of three state-of-the-art code generation models... The range of programming languages represented in MultiPL-E allow us to explore the impact of language frequency and language features on model performance.",
    "evaluation_method": "é€šè¿‡å•å…ƒæµ‹è¯•è¯„ä¼°ç”Ÿæˆä»£ç çš„æ­£ç¡®æ€§ï¼Œä½¿ç”¨pass@kç­‰æŒ‡æ ‡",
    "evaluation_method_quote": "We judge a generated function correct if it passes all tests... it is common to sample multiple completions per problem and report an estimated pass rate (Â§4.2).",
    "context_dependency": "å•å‡½æ•°",
    "context_dependency_quote": "All of the problems are functions that receive and return ï¬rst-order values, which facilitates unit testing and test translation.",
    "problem_domain": "é€šç”¨ç¼–ç¨‹é—®é¢˜ï¼Œæ¶µç›–ç®—æ³•å’Œæ•°æ®å¤„ç†",
    "problem_domain_quote": "It is a diverse collection of 164 problems... All of the problems are functions that receive and return ï¬rst-order values.",
    "problem_difficulty": "å…·æœ‰æŒ‘æˆ˜æ€§",
    "problem_difficulty_quote": "Moreover, it is a challenging benchmark: the best model evaluated by Fried et al. [4] achieves only a 36% pass rate on Python.",
    "language": "Pythonï¼ˆæºè¯­è¨€ï¼‰åŠé€šè¿‡MultiPL-Eç¿»è¯‘çš„18ç§å…¶ä»–ç¼–ç¨‹è¯­è¨€ï¼ˆåŒ…æ‹¬JavaScript, C++, Scala, TypeScriptç­‰ï¼‰",
    "language_quote": "We use MultiPL-E to extend the HumanEval benchmark [1] and MBPP benchmark [2] to 18 languages that encompass a range of programming paradigms and popularity... MultiPL-E supports 18 languages and is straightforward to extend with more.",
    "data_size": "é€šè¿‡ç¿»è¯‘HumanEvalï¼ˆ164ä¸ªé—®é¢˜ï¼‰å’ŒMBPPï¼ˆæœªæ˜ç¡®æ•°é‡ï¼‰ä¸¤ä¸ªåŸºå‡†ï¼Œä¸º19ç§è¯­è¨€ï¼ˆPython + 18ç§ï¼‰åˆ›å»ºå¹¶è¡Œé—®é¢˜é›†",
    "data_size_quote": "We create the ï¬rst massively multilingual code generation benchmark by using MultiPL-E to translate two popular Python code generation benchmarks to 18 additional programming languages... HumanEval is a diverse collection of 164 problems.",
    "source_type": "åŸºäºç°æœ‰PythonåŸºå‡†ï¼ˆHumanEvalå’ŒMBPPï¼‰é€šè¿‡è‡ªåŠ¨åŒ–ç¼–è¯‘å™¨ç¿»è¯‘ç”Ÿæˆ",
    "source_type_quote": "We use MultiPL-E to translate two widely-used code generation benchmarks, HumanEval [1] and MBPP [2], into 18 languages.",
    "last_updated": "2022",
    "last_updated_quote": "arXiv:2208.08227v4  [cs.LG]  19 Dec 2022",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆè®ºæ–‡ä½œè€…æ„å»ºï¼‰",
    "build_type_quote": "We propose MultiPL-E... We create the ï¬rst massively multilingual code generation benchmark...",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆä»è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆå‡½æ•°ä½“ï¼‰",
    "task_granularity_quote": "We focus on the natural-language-to-code task (NL2Code): given the description of a function in natural language, complete the function body.",
    "evaluation_metrics": "passç‡ï¼ˆåŸºäºå•å…ƒæµ‹è¯•ï¼‰",
    "evaluation_metrics_quote": "We judge a generated function correct if it passes all tests... it is common to sample multiple completions per problem and report an estimated pass rate (Â§4.2).",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆå‡½æ•°æè¿°ï¼‰ä¸ä»£ç ï¼ˆå‡½æ•°ç­¾åã€ç±»å‹æ³¨è§£ã€ç¤ºä¾‹ï¼‰",
    "input_modality_quote": "The prompt has several sources of information for the model: the function signature (its name and parameters); a brief comment describing the function; and, optionally, examples in the form of Python doctests.",
    "output_modality": "ä»£ç ï¼ˆå‡½æ•°ä½“ï¼‰",
    "output_modality_quote": "Given the prompt as input, the code generation model generates a completion that is likely to follow the given prompt.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "We focus on the natural-language-to-code task (NL2Code): given the description of a function in natural language, complete the function body.",
    "execution_environment": "å®¹å™¨åŒ–æ²™ç®±ï¼Œç”¨äºç¼–è¯‘ï¼ˆå¦‚éœ€è¦ï¼‰å’Œè¿è¡Œç¨‹åº",
    "execution_environment_quote": "MultiPL-E also includes a containerized sandbox that (1) compiles programs if necessary, (2) runs them with appropriate timeouts, (3) validates their results on unit tests, and (4) classiï¬es each output as successful, syntax error, etc.",
    "unique_features": "1. å¯æ‰©å±•å’Œå¯æ‰©å±•çš„ç³»ç»Ÿï¼Œç”¨äºå°†ä»£ç ç”ŸæˆåŸºå‡†ç¼–è¯‘åˆ°æ–°çš„ç¼–ç¨‹è¯­è¨€ã€‚2. åˆ›å»ºäº†ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡ã€å¤šè¯­è¨€ã€å¹¶è¡Œçš„ä»£ç ç”ŸæˆåŸºå‡†ã€‚3. é€šè¿‡ä¸€å¥—å°å‹ç¼–è¯‘å™¨ï¼ˆæ¯ä¸ªçº¦200è¡Œä»£ç ï¼‰å®ç°ç¿»è¯‘ï¼Œä»…éœ€ç¿»è¯‘å‡½æ•°ç­¾åã€å•å…ƒæµ‹è¯•ã€æ³¨é‡Šå’Œç±»å‹æ³¨è§£ï¼Œè€Œæ— éœ€ç¿»è¯‘å‡½æ•°ä½“ã€‚4. åŒ…å«ä¸€ä¸ªè§„åˆ™å·¥å…·ï¼Œç”¨äºå°†æ³¨é‡Šä¸­çš„æŠ€æœ¯æœ¯è¯­ç¿»è¯‘å¾—æ›´ç¬¦åˆç›®æ ‡è¯­è¨€ä¹ æƒ¯ã€‚",
    "unique_features_quote": "MultiPL-E uses a suite of 18 little compilers from Python benchmarks to each target language... Each compiler must be able to translate four components from Python: (1) a function signature (name and arguments), (2) simple unit tests, (3) a comment describing the expected function behavior, and (4) type annotations if the target language is statically typed. Notably, the compiler does not have to translate the body of a function, since it is the job of the code generation model to synthesize it. Thus each MultiPL-E compiler is approximately 200 LOC and easy to build. MultiPL-E also includes a simple, rule-based tool to translate technical terms in comments to be more language appropriate, e.g. a Python list is approximately a C++ vector.",
    "data_size_quantity": 164,
    "data_size_unit": "ä¸ªé—®é¢˜",
    "last_updated_year": 2022,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python', 'JavaScript', 'C++', 'Scala', 'TypeScript']",
    "dimension_normalized": "['å¤šè¯­è¨€ä»£ç ç”Ÿæˆèƒ½åŠ›', 'æ¨¡å‹åœ¨ä¸åŒç¼–ç¨‹è¯­è¨€ä¸Šçš„æ³›åŒ–æ€§èƒ½']",
    "evaluation_method_normalized": "['passç‡']",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹é—®é¢˜', 'ç®—æ³•', 'æ•°æ®å¤„ç†']",
    "source_type_normalized": "['åŸºäºç°æœ‰PythonåŸºå‡†', 'HumanEval', 'MBPP', 'è‡ªåŠ¨åŒ–ç¼–è¯‘å™¨ç¿»è¯‘ç”Ÿæˆ']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2211.11501_output/content.md",
    "benchmark_name": "DS-1000",
    "benchmark_name_quote": "We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas.",
    "dataset_url": "https://ds1000-code-gen.github.io",
    "dataset_url_quote": "We release our benchmark at https://ds1000-code-gen.github.io.",
    "task_description": "æ•°æ®ç§‘å­¦ä»£ç ç”Ÿæˆã€‚æ—¨åœ¨è¯„ä¼°æ¨¡å‹æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°å’Œä»£ç ä¸Šä¸‹æ–‡ï¼Œç”Ÿæˆç”¨äºè§£å†³å®é™…æ•°æ®ç§‘å­¦é—®é¢˜çš„ä»£ç çš„èƒ½åŠ›ã€‚",
    "task_description_quote": "We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas.",
    "dimension": "æ•°æ®ç§‘å­¦ä»£ç ç”Ÿæˆèƒ½åŠ›ã€å¯¹ç‰¹å®šåº“APIçš„ç†è§£å’Œä½¿ç”¨ã€è§£å†³ç°å®ä¸–ç•Œé—®é¢˜çš„å®ç”¨æ€§ã€æŠ—è®°å¿†åŒ–èƒ½åŠ›",
    "dimension_quote": "DS-1000 incorporates three core features. First, our problems reflect diverse, realistic, and practical use cases... Finally, we proactively defend against memorization...",
    "evaluation_method": "åŸºäºæ‰§è¡Œçš„å¤šæ ‡å‡†è‡ªåŠ¨è¯„ä¼°ã€‚åŒ…æ‹¬ï¼š1) è¿è¡Œæµ‹è¯•ç”¨ä¾‹æ£€æŸ¥åŠŸèƒ½æ­£ç¡®æ€§ï¼›2) é€šè¿‡é™åˆ¶APIä½¿ç”¨æˆ–å…³é”®è¯æ¥æ£€æŸ¥è¡¨é¢å½¢å¼çº¦æŸã€‚",
    "evaluation_method_quote": "we achieve this with multi-criteria metrics, checking both functional correctness by running test cases and surface-form constraints by restricting API usages or keywords.",
    "context_dependency": "åŒ…å«ä»£ç ä¸Šä¸‹æ–‡çš„å•æ–‡ä»¶ä»»åŠ¡ã€‚ç”¨æˆ·é—®é¢˜é€šå¸¸åŒ…å«é”™è¯¯çš„ä»£ç ã€é”™è¯¯æ¶ˆæ¯å’Œè¾“å…¥è¾“å‡ºç¤ºä¾‹ç­‰å¤šæ ·åŒ–ä¸Šä¸‹æ–‡ã€‚",
    "context_dependency_quote": "usersâ€™ data science coding problems usually have diverse contexts including their incorrect code, error messages, and input-output examples",
    "problem_domain": "æ•°æ®ç§‘å­¦ã€‚æ¶µç›–ä¸ƒä¸ªå¹¿æ³›ä½¿ç”¨çš„Pythonæ•°æ®ç§‘å­¦åº“ï¼šNumPy, Pandas, TensorFlow, PyTorch, SciPy, Scikit-learn, å’Œ Matplotlibã€‚",
    "problem_domain_quote": "a thousand problems covering seven widely-used Python data science libraries: NumPy, Pandas, TensorFlow, PyTorch, SciPy, Scikit-learn, and Matplotlib.",
    "problem_difficulty": "ç°å®ä¸–ç•Œåº”ç”¨çº§ã€‚é—®é¢˜æ¥æºäºStackOverflowï¼Œåæ˜ äº†æ—¥å¸¸æ•°æ®ç§‘å­¦åº”ç”¨ä¸­çš„å®é™…ç”¨ä¾‹ï¼Œè€Œéç«èµ›æˆ–é¢è¯•é£æ ¼ã€‚",
    "problem_difficulty_quote": "focuses on everyday data science applications... includes naturalistic intents and contexts... our problems reflect diverse, realistic, and practical use cases",
    "language": "Python",
    "language_quote": "a code generation benchmark with a thousand data science problems spanning seven Python libraries",
    "data_size": "åŒ…å«1000ä¸ªé—®é¢˜ã€‚",
    "data_size_quote": "a code generation benchmark with a thousand data science problems",
    "source_type": "ä»StackOverflowæ”¶é›†çš„è‡ªç„¶å‘ç”Ÿçš„é—®é¢˜ï¼Œç»è¿‡äººå·¥ç­›é€‰ã€ä¿®æ”¹å’Œæ‰°åŠ¨ã€‚",
    "source_type_quote": "we collected naturally occurring problems from Stack-Overflow, manually scored their representativeness and usefulness, and curated a subset of them to create our benchmark.",
    "last_updated": "2022-11-18 (æ ¹æ®arXivç‰ˆæœ¬v1æ—¥æœŸ)",
    "last_updated_quote": "arXiv:2211.11501v1 [cs.SE] 18 Nov 2022",
    "build_type": "å®˜æ–¹è‡ªå»ºã€‚ç”±äº”ä½ç²¾é€šæ•°æ®ç§‘å­¦å’ŒPythonçš„è®¡ç®—æœºç§‘å­¦å­¦ç”Ÿä½œè€…èŠ±è´¹çº¦1200å°æ—¶æ„å»ºã€‚",
    "build_type_quote": "five authors who are computer science students and familiar with data science spent a total of about 1200 hours constructing DS-1000",
    "contamination_status": "æŠ—æ±¡æŸ“è®¾è®¡ã€‚é€šè¿‡ä¸»åŠ¨æ‰°åŠ¨åŸå§‹StackOverflowé—®é¢˜æ¥é˜²å¾¡æ¨¡å‹å¯¹é¢„è®­ç»ƒè¯­æ–™çš„è®°å¿†ã€‚",
    "contamination_status_quote": "we proactively defend against memorization by slightly modifying our problems to be different from the original Stack-Overflow source; consequently, models cannot answer them correctly by memorizing the solutions from pre-training.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆå¡«ç©ºï¼‰ã€‚æ¨¡å‹éœ€è¦å°†ä»£ç å¡«å…¥æç¤ºä¸­çš„â€œ[insert]â€ä½ç½®ã€‚",
    "task_granularity_quote": "The model needs to fill in the code into â€œ[insert]â€ in the prompt",
    "evaluation_metrics": "é€šè¿‡/ä¸é€šè¿‡ï¼ˆåŸºäºæµ‹è¯•ç”¨ä¾‹å’Œè¡¨é¢å½¢å¼çº¦æŸï¼‰ã€‚æ–‡ä¸­æœªæ˜ç¡®å‘½åå¦‚pass@kçš„æŒ‡æ ‡ï¼Œä½†æŠ¥å‘Šäº†å‡†ç¡®ç‡ï¼ˆå¦‚43.3%ï¼‰ã€‚",
    "evaluation_metrics_quote": "The current best public system (Codex-002) achieves 43.3% accuracy",
    "input_modality": "è‡ªç„¶è¯­è¨€ä¸ä»£ç ä¸Šä¸‹æ–‡æ··åˆã€‚è¾“å…¥åŒ…æ‹¬è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°å’Œéƒ¨åˆ†å¯æ‰§è¡Œçš„ä»£ç ä¸Šä¸‹æ–‡ã€‚",
    "input_modality_quote": "The model needs to fill in the code into â€œ[insert]â€ in the prompt on the left; the prompt includes natural language description and code context.",
    "output_modality": "ä»£ç ã€‚æœŸæœ›è¾“å‡ºæ˜¯ä¸€æ®µå¡«è¡¥ç¼ºå¤±éƒ¨åˆ†çš„Pythonä»£ç ã€‚",
    "output_modality_quote": "The model needs to fill in the code into â€œ[insert]â€",
    "task_io_type": "æ–‡æœ¬ä¸ä»£ç åˆ°ä»£ç ã€‚æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°å’Œç»™å®šçš„ä»£ç ä¸Šä¸‹æ–‡ï¼Œç”Ÿæˆç¼ºå¤±çš„ä»£ç ç‰‡æ®µã€‚",
    "task_io_type_quote": "synthesizing programs from docstrings. (ç»“åˆä¸Šä¸‹æ–‡æ¨æ–­ï¼Œä»»åŠ¡æ˜¯ä»è‡ªç„¶è¯­è¨€æè¿°å’Œä»£ç ä¸Šä¸‹æ–‡ä¸­åˆæˆç¨‹åº)",
    "execution_environment": "éœ€è¦ç‰¹å®šæ•°æ®ç§‘å­¦åº“ä¾èµ–çš„æ²™ç›’ç¯å¢ƒã€‚å›ºå®šäº†Python 3.7.10åŠç›¸åº”åº“çš„æœ€æ–°ç‰ˆæœ¬ã€‚",
    "execution_environment_quote": "We fixed the evaluation environment to include the latest versions of libraries that can be installed with Python 3.7.10",
    "unique_features": "1) åŒ…å«ç°å®é—®é¢˜ä¸å¤šæ ·åŒ–ä¸Šä¸‹æ–‡ï¼›2) å¯é çš„å¤šæ ‡å‡†æ‰§è¡Œè¯„ä¼°ï¼›3) ä¸»åŠ¨é˜²å¾¡è®°å¿†åŒ–ï¼›4) ä¸“æ³¨äºä¸ƒä¸ªæ ¸å¿ƒæ•°æ®ç§‘å­¦åº“ã€‚",
    "unique_features_quote": "We highlight three core features of DS-1000: 1) it contains realistic problems with diverse contexts, 2) it implements reliable multi-criteria execution-based evaluation metrics, and 3) it proactively defends against memorization.",
    "data_size_quantity": 1000,
    "data_size_unit": "ä¸ªé—®é¢˜",
    "last_updated_year": 2022,
    "last_updated_month": 11,
    "last_updated_day": 18,
    "language_normalized": "['Python']",
    "dimension_normalized": "['æ•°æ®ç§‘å­¦ä»£ç ç”Ÿæˆèƒ½åŠ›', 'å¯¹ç‰¹å®šåº“APIçš„ç†è§£å’Œä½¿ç”¨', 'è§£å†³ç°å®ä¸–ç•Œé—®é¢˜çš„å®ç”¨æ€§', 'æŠ—è®°å¿†åŒ–èƒ½åŠ›']",
    "evaluation_method_normalized": "['é€šè¿‡/ä¸é€šè¿‡', 'å‡†ç¡®ç‡']",
    "problem_domain_normalized": "['æ•°æ®ç§‘å­¦', 'NumPy', 'Pandas', 'TensorFlow', 'PyTorch', 'SciPy', 'Scikit-learn', 'Matplotlib']",
    "source_type_normalized": "['StackOverflow']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å•æ–‡ä»¶",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2301.03988_output/content.md",
    "benchmark_name": "MultiPL-E",
    "benchmark_name_quote": "evaluate them on the MultiPL-E text-to-code benchmark (Cassano et al., 2022).",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "evaluate them on the MultiPL-E text-to-code benchmark (Cassano et al., 2022).",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„æµ‹ä»è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆä»£ç çš„èƒ½åŠ›ï¼ˆæ–‡æœ¬åˆ°ä»£ç ï¼‰",
    "task_description_quote": "MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages.",
    "dimension": "å¤šè¯­è¨€ä»£ç ç”Ÿæˆèƒ½åŠ›",
    "dimension_quote": "MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages.",
    "evaluation_method": "ä½¿ç”¨å•å…ƒæµ‹è¯•è¯„ä¼°ç”Ÿæˆä»£ç çš„æ­£ç¡®æ€§",
    "evaluation_method_quote": "The correctness of generated code can be tested using unit tests, a method for approximating semantic equivalence.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": NaN,
    "problem_domain_quote": NaN,
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "æ–‡ä¸­ä»…æåŠå®éªŒå­é›†ï¼ˆJava, JavaScript, Pythonï¼‰ï¼Œæœªæè¿°å®Œæ•´æ•°æ®é›†ã€‚MultiPL-Eæ‰©å±•äº†18ç§è¯­è¨€ã€‚",
    "language_quote": "evaluate them on the MultiPL-E text-to-code benchmark (Cassano et al., 2022). ... MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages.",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": "åŸºäºHumanEvalå’ŒMBPPåŸºå‡†è‡ªåŠ¨ç¼–è¯‘æ‰©å±•",
    "source_type_quote": "MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages. The doctests, function signatures, and unit tests for each benchmark suite are automatically compiled to new languages.",
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆä»è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆå®Œæ•´å‡½æ•°ï¼‰",
    "task_granularity_quote": "MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages.",
    "evaluation_metrics": NaN,
    "evaluation_metrics_quote": NaN,
    "input_modality": "è‡ªç„¶è¯­è¨€",
    "input_modality_quote": "text-to-code benchmark",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "text-to-code benchmark",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "text-to-code benchmark",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "1. é€šè¿‡è‡ªåŠ¨ç¼–è¯‘å°†å•è¯­è¨€åŸºå‡†ï¼ˆHumanEval, MBPPï¼‰æ‰©å±•åˆ°å¤šç§ç¼–ç¨‹è¯­è¨€ã€‚2. è¯„ä¼°æ—¶éšè—æµ‹è¯•ç”¨ä¾‹ï¼Œä»…ç”¨äºéªŒè¯æ­£ç¡®æ€§ï¼ˆä¸MBXPä¸åŒï¼‰ã€‚",
    "unique_features_quote": "MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages. ... In contrast, MultiPL-E keeps the tests hidden from the model and only uses them to test correctness.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Java', 'JavaScript', 'Python', 'å¤šè¯­è¨€']",
    "dimension_normalized": "['å¤šè¯­è¨€ä»£ç ç”Ÿæˆèƒ½åŠ›']",
    "evaluation_method_normalized": "[]",
    "problem_domain_normalized": "[]",
    "source_type_normalized": "['åŸºäºHumanEvalå’ŒMBPPåŸºå‡†è‡ªåŠ¨ç¼–è¯‘æ‰©å±•']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2302.08468_output/content.md",
    "benchmark_name": "Spider",
    "benchmark_name_quote": "â–·Spider (Yu et al., 2018) is a semantic parsing dataset on generating SQL queries from natural language questions.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We conduct experiments on four language-to-code datasets across domains of semantic parsing, table QA, math reasoning and basic python programming.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ä»è‡ªç„¶è¯­è¨€é—®é¢˜ç”ŸæˆSQLæŸ¥è¯¢ï¼ˆè¯­ä¹‰è§£æï¼‰",
    "task_description_quote": "â–·Spider (Yu et al., 2018) is a semantic parsing dataset on generating SQL queries from natural language questions.",
    "dimension": "è¯­è¨€åˆ°ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "The ability of mapping natural language to executable code is the cornerstone of a variety AI applications such as database interfaces (Pasupat & Liang, 2015; Yu et al., 2018; Shi et al., 2020)...",
    "evaluation_method": "æ‰§è¡Œç”Ÿæˆçš„ç¨‹åºå¹¶æ¯”è¾ƒç»“æœ",
    "evaluation_method_quote": "Given x, a generation model P(y|x) generates a program y which is later executed via an executor E(Â·) to obtain the result3 E(y).",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "æ•°æ®åº“æŸ¥è¯¢ï¼ˆTable QAï¼‰",
    "problem_domain_quote": "Spider (Yu et al., 2018) is a semantic parsing dataset on generating SQL queries from natural language questions.",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "SQL",
    "language_quote": "Target: SQL",
    "data_size": "è®­ç»ƒé›†7000æ¡ï¼Œå¼€å‘é›†1032æ¡",
    "data_size_quote": "# Train: 7,000, # Dev: 1,032",
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": "2018",
    "last_updated_quote": "Spider (Yu et al., 2018)",
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "language-to-code generation",
    "evaluation_metrics": "æ‰§è¡Œå‡†ç¡®ç‡",
    "evaluation_metrics_quote": "LEVER consistently improves the execution accuracy of the generated programs.",
    "input_modality": "è‡ªç„¶è¯­è¨€ä¸æ•°æ®åº“æ¨¡å¼",
    "input_modality_quote": "Input Format: Schema + NL",
    "output_modality": "ä»£ç ï¼ˆSQLï¼‰",
    "output_modality_quote": "generating SQL queries from natural language questions.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "language-to-code generation",
    "execution_environment": "SQLæ‰§è¡Œå™¨",
    "execution_environment_quote": "generating SQL queries from natural language questions.",
    "unique_features": "æ‹¥æœ‰å®Œæ•´çš„ç¨‹åºæ ‡æ³¨ï¼ˆHas program: âœ“ï¼‰",
    "unique_features_quote": "Has program: âœ“",
    "data_size_quantity": 7000,
    "data_size_unit": "æ¡",
    "last_updated_year": 2018,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['SQL']",
    "dimension_normalized": "['è¯­è¨€åˆ°ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['æ‰§è¡Œå‡†ç¡®ç‡']",
    "problem_domain_normalized": "['æ•°æ®åº“æŸ¥è¯¢ï¼ˆTable QAï¼‰']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2303.17568_output/content.md",
    "benchmark_name": "HumanEval-X",
    "benchmark_name_quote": "Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We hand-craft the HumanEval-X benchmark to evaluate multilingual code models for the tasks of code generation and translation in terms of functional correctness, facilitating the understanding and development of pre-trained (multilingual) code models.",
    "dataset_url": "https://github.com/THUDM/CodeGeeX",
    "dataset_url_quote": "Finally, CodeGeeX is publicly accessible and in Sep. 2022, we open-sourced its code, model weights (the version of 850B tokens), API, extensions, and HumanEval-X at https://github.com/THUDM/CodeGeeX.",
    "task_description": "è¯„ä¼°å¤šè¯­è¨€ä»£ç æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆå’Œä»£ç ç¿»è¯‘ä»»åŠ¡ä¸Šçš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚",
    "task_description_quote": "We hand-craft the HumanEval-X benchmark to evaluate multilingual code models for the tasks of code generation and translation in terms of functional correctness...",
    "dimension": "å¤šè¯­è¨€ä»£ç ç”Ÿæˆä¸ç¿»è¯‘çš„åŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "...evaluate multilingual code models for the tasks of code generation and translation in terms of functional correctness...",
    "evaluation_method": "é€šè¿‡æ‰§è¡Œæµ‹è¯•ç”¨ä¾‹æ¥éªŒè¯ç”Ÿæˆä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§",
    "evaluation_method_quote": "...rather than really verify the functional correctness of generated code. Specifically, for each problem... in HumanEval, we manually rewrite its prompt, canonical solution, and test cases in C++, Java, JavaScript, and Go.",
    "context_dependency": "å•å‡½æ•°",
    "context_dependency_quote": "æ–‡ä¸­æœªæ˜ç¡®æè¿°ä¸Šä¸‹æ–‡ä¾èµ–èŒƒå›´ã€‚HumanEval-XåŸºäºHumanEvalæ„å»ºï¼Œè€ŒHumanEvalæ˜¯å•å‡½æ•°ç”Ÿæˆä»»åŠ¡ã€‚",
    "problem_domain": "é€šç”¨ç¼–ç¨‹é—®é¢˜",
    "problem_domain_quote": "æ–‡ä¸­æœªæ˜ç¡®æè¿°é—®é¢˜æ‰€å±ä¸“ä¸šé¢†åŸŸã€‚",
    "problem_difficulty": "å…¥é—¨çº§ç¼–ç¨‹é—®é¢˜",
    "problem_difficulty_quote": "æ–‡ä¸­æœªç›´æ¥æè¿°HumanEval-Xçš„éš¾åº¦ï¼Œä½†æåˆ°å…¶åŸºç¡€HumanEvalç”¨äºè¯„ä¼°Codexè§£å†³å…¥é—¨çº§Pythoné—®é¢˜ã€‚",
    "language": "Python, C++, Java, JavaScript, Go",
    "language_quote": "...we manually rewrite its prompt, canonical solution, and test cases in C++, Java, JavaScript, and Go. In total, HumanEval-X covers 820 hand-written problem-solution pairs (164 problems, each having solutions in 5 languages).",
    "data_size": "820ä¸ªæ‰‹å†™çš„é—®é¢˜-è§£å†³æ–¹æ¡ˆå¯¹ï¼ˆ164ä¸ªé—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜æœ‰5ç§è¯­è¨€çš„è§£å†³æ–¹æ¡ˆï¼‰",
    "data_size_quote": "In total, HumanEval-X covers 820 hand-written problem-solution pairs (164 problems, each having solutions in 5 languages).",
    "source_type": "åŸºäºHumanEvalï¼ˆPythonï¼‰æ‰‹åŠ¨é‡å†™å’Œæ‰©å±•",
    "source_type_quote": "Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go.",
    "last_updated": "2022å¹´9æœˆ",
    "last_updated_quote": "Finally, CodeGeeX is publicly accessible and in Sep. 2022, we open-sourced its code, model weights (the version of 850B tokens), API, extensions, and HumanEval-X...",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆæ‰‹å·¥æ„å»ºï¼‰",
    "build_type_quote": "We hand-craft the HumanEval-X benchmark...",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼Œä»£ç ç¿»è¯‘",
    "task_granularity_quote": "...evaluate multilingual code models for the tasks of code generation and translation...",
    "evaluation_metrics": "åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆé€šè¿‡æµ‹è¯•ç”¨ä¾‹ï¼‰",
    "evaluation_metrics_quote": "...evaluate multilingual code models for the tasks of code generation and translation in terms of functional correctness...",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆæè¿°ï¼‰",
    "input_modality_quote": "æ–‡ä¸­æœªæ˜ç¡®æè¿°è¾“å…¥ç±»å‹ï¼Œä½†åŸºäºHumanEvalä»»åŠ¡ï¼ˆä»æ–‡æ¡£å­—ç¬¦ä¸²ç”Ÿæˆä»£ç ï¼‰æ¨æ–­ã€‚",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "æ–‡ä¸­æœªæ˜ç¡®æè¿°æœŸæœ›è¾“å‡ºï¼Œä½†ä»»åŠ¡ä¸ºä»£ç ç”Ÿæˆå’Œç¿»è¯‘ï¼Œæ¨æ–­è¾“å‡ºä¸ºä»£ç ã€‚",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ï¼Œä»£ç åˆ°ä»£ç ",
    "task_io_type_quote": "HumanEval-X support the evaluation of both code generation and code translation between different languages.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "1. å¤šè¯­è¨€æ‰©å±•ï¼šåœ¨HumanEvalï¼ˆä»…Pythonï¼‰åŸºç¡€ä¸Šï¼Œæ‰‹å·¥ç¼–å†™äº†C++ã€Javaã€JavaScriptå’ŒGoçš„è§£å†³æ–¹æ¡ˆå’Œæµ‹è¯•ç”¨ä¾‹ã€‚2. æ”¯æŒåŒé‡ä»»åŠ¡ï¼šåŒæ—¶è¯„ä¼°ä»£ç ç”Ÿæˆå’Œä»£ç ç¿»è¯‘ã€‚3. ä¸“æ³¨äºåŠŸèƒ½æ­£ç¡®æ€§è¯„ä¼°ï¼Œè€Œéå­—ç¬¦ä¸²ç›¸ä¼¼åº¦ã€‚",
    "unique_features_quote": "1) HumanEval (Chen et al., 2021)â€”developed by OpenAI for evaluating Codexâ€”and other benchmarks only consist of programming problems in a single language and 2) existing multilingual datasets use string similarity metrics like BLEU for evaluation rather than really verify the functional correctness of generated code. Importantly, HumanEval-X support the evaluation of both code generation and code translation between different languages.",
    "data_size_quantity": 820,
    "data_size_unit": "ä¸ª",
    "last_updated_year": 2022,
    "last_updated_month": 9,
    "last_updated_day": null,
    "language_normalized": "['Python', 'C++', 'Java', 'JavaScript', 'Go']",
    "dimension_normalized": "['å¤šè¯­è¨€ä»£ç ç”Ÿæˆä¸ç¿»è¯‘çš„åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆé€šè¿‡æµ‹è¯•ç”¨ä¾‹ï¼‰']",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹é—®é¢˜']",
    "source_type_normalized": "['åŸºäºHumanEvalï¼ˆPythonï¼‰æ‰‹åŠ¨é‡å†™å’Œæ‰©å±•']",
    "problem_difficulty_normalized": "å…¥é—¨çº§",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2304.05128_output/content.md",
    "benchmark_name": "Spider",
    "benchmark_name_quote": "SELF-DEBUGGING achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "we evaluate SELF-DEBUGGING on the development set of the Spider benchmark (Yu et al., 2018).",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "æ–‡æœ¬åˆ°SQLç”Ÿæˆï¼šç»™å®šä¸€ä¸ªè‡ªç„¶è¯­è¨€é—®é¢˜å’Œæ•°æ®åº“ä¿¡æ¯ï¼Œç”Ÿæˆå¯¹åº”çš„SQLæŸ¥è¯¢ã€‚",
    "task_description_quote": "The goal of text-to-SQL tasks is to generate the corresponding SQL query given a question and the database information",
    "dimension": "ä»£ç ç”Ÿæˆæ­£ç¡®æ€§",
    "dimension_quote": "SELF-DEBUGGING can teach the large language model to debug its predicted program",
    "evaluation_method": "æ‰§è¡Œå•å…ƒæµ‹è¯•ï¼ˆå½“å¯ç”¨æ—¶ï¼‰æˆ–åŸºäºæ‰§è¡Œç»“æœçš„å¤šæ•°æŠ•ç¥¨",
    "evaluation_method_quote": "When unit tests are presented in the problem description, we filter out programs that do not pass the unit tests before performing the execution-based majority vote.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "æ•°æ®åº“æŸ¥è¯¢",
    "problem_domain_quote": "text-to-SQL generation",
    "problem_difficulty": "åŒ…å«æœ€å¤æ‚çº§åˆ«çš„é—®é¢˜",
    "problem_difficulty_quote": "improves the prediction accuracy on problems of the hardest level by 9%.",
    "language": "SQL",
    "language_quote": "text-to-SQL generation",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "text-to-SQL generation",
    "evaluation_metrics": "é¢„æµ‹å‡†ç¡®ç‡",
    "evaluation_metrics_quote": "improves the prediction accuracy on problems of the hardest level by 9%.",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆé—®é¢˜ï¼‰å’Œæ•°æ®åº“ä¿¡æ¯",
    "input_modality_quote": "given a question and the database information",
    "output_modality": "ä»£ç ï¼ˆSQLæŸ¥è¯¢ï¼‰",
    "output_modality_quote": "generate the corresponding SQL query",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "text-to-SQL generation",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "è¯¥åŸºå‡†ï¼ˆSpiderï¼‰åœ¨é—®é¢˜æè¿°ä¸­æ²¡æœ‰å•å…ƒæµ‹è¯•æ¥éªŒè¯é¢„æµ‹çš„æ­£ç¡®æ€§ã€‚",
    "unique_features_quote": "On the Spider benchmark where there are no unit tests to verify the correctness of predictions",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['SQL']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆæ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['é¢„æµ‹å‡†ç¡®ç‡']",
    "problem_domain_normalized": "['æ•°æ®åº“æŸ¥è¯¢']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2305.01210_output/content.md",
    "benchmark_name": "HumanEval+",
    "benchmark_name_quote": "we extend the test-cases of the popular HUMANEVAL benchmark by 80Ã— to build HUMANEVAL+.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "we propose EvalPlus â€“ a code synthesis evaluation framework to rigorously benchmark the functional correctness of LLM-synthesized code. EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator... we extend the test-cases of the popular HUMANEVAL benchmark by 80Ã— to build HUMANEVAL+.",
    "dataset_url": "https://github.com/evalplus/evalplus",
    "dataset_url_quote": "We have open-sourced our tools, enhanced datasets as well as all LLM-generated code at https://github.com/evalplus/evalplus to facilitate and accelerate future LLM-for-code research.",
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚",
    "task_description_quote": "EvalPlus â€“ a code synthesis evaluation framework to rigorously benchmark the functional correctness of LLM-synthesized code.",
    "dimension": "ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "rigorously benchmark the functional correctness of LLM-synthesized code.",
    "evaluation_method": "é€šè¿‡å¤§é‡æ–°ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹ï¼ˆç»“åˆLLMå’ŒåŸºäºçªå˜çš„ç­–ç•¥ï¼‰è¿›è¡Œå·®åˆ†æµ‹è¯•ï¼Œå¹¶ä¸åŸºå‡†å®ç°è¿›è¡Œäº¤å‰éªŒè¯ã€‚",
    "evaluation_method_quote": "EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies... These newly generated test inputs are then used to evaluate the LLM-generated code through differential testing against the ground-truth implementation.",
    "context_dependency": "å•å‡½æ•°",
    "context_dependency_quote": "The generated code snippet is then combined with the context to form a complete function that aligns with the user intent.",
    "problem_domain": "é€šç”¨ç¼–ç¨‹é—®é¢˜",
    "problem_domain_quote": "Programming benchmarks, with curated synthesis problems and test-cases, are used to measure the performance of various LLMs on code synthesis.",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "æ–‡ä¸­ä»…æåŠå®éªŒåŸºäºHumanEvalï¼ˆPythonï¼‰ï¼Œæœªæè¿°å®Œæ•´æ•°æ®é›†",
    "language_quote": "we extend the test-cases of the popular HUMANEVAL benchmark by 80Ã— to build HUMANEVAL+.",
    "data_size": "å°†HumanEvalçš„æµ‹è¯•ç”¨ä¾‹è§„æ¨¡æ‰©å±•äº†80å€",
    "data_size_quote": "EvalPlus extends the popular HUMANEVAL benchmark to create HUMANEVAL+, improving the test-case scale by 80Ã—.",
    "source_type": "åŸºäºç°æœ‰åŸºå‡†ï¼ˆHumanEvalï¼‰é€šè¿‡è‡ªåŠ¨æµ‹è¯•è¾“å…¥ç”Ÿæˆå™¨ï¼ˆç»“åˆLLMå’ŒåŸºäºçªå˜çš„ç­–ç•¥ï¼‰è¿›è¡Œå¢å¼ºã€‚",
    "source_type_quote": "EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies.",
    "last_updated": "2023",
    "last_updated_quote": "arXiv:2305.01210v3 [cs.SE] 30 Oct 2023",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆåŸºäºç°æœ‰åŸºå‡†å¢å¼ºï¼‰",
    "build_type_quote": "we propose EvalPlus â€“ a code synthesis evaluation framework... we extend the test-cases of the popular HUMANEVAL benchmark by 80Ã— to build HUMANEVAL+.",
    "contamination_status": "é€šè¿‡ç”Ÿæˆå¤§é‡æ–°æµ‹è¯•ç”¨ä¾‹æ¥ç¼“è§£ç°æœ‰åŸºå‡†æµ‹è¯•ä¸è¶³å¯¼è‡´çš„â€œè™šå‡æ­£ç¡®â€é—®é¢˜ï¼Œæ—¨åœ¨æ›´ä¸¥æ ¼åœ°è¯„ä¼°æ¨¡å‹ã€‚",
    "contamination_status_quote": "HUMANEVAL+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs, reducing the pass@k by up-to 19.3-28.9%.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "program synthesis... applying LLMs for direct code generation.",
    "evaluation_metrics": "pass@k",
    "evaluation_metrics_quote": "reducing the pass@k by up-to 19.3-28.9%.",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆå‡½æ•°ç­¾åå’Œæ–‡æ¡£å­—ç¬¦ä¸²ï¼‰",
    "input_modality_quote": "in the form of function signature and docstring that denote the desired program functionality.",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "The generated code snippet is then combined with the context to form a complete function that aligns with the user intent.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "synthesizing programs from docstrings.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "1. é€šè¿‡è‡ªåŠ¨æµ‹è¯•ç”Ÿæˆï¼ˆç»“åˆLLMå’Œçªå˜ç­–ç•¥ï¼‰æ˜¾è‘—æ‰©å±•ç°æœ‰åŸºå‡†çš„æµ‹è¯•å¥—ä»¶ï¼ˆ80å€ï¼‰ã€‚2. æ—¨åœ¨æ­ç¤ºå› æµ‹è¯•ä¸è¶³è€Œè¢«ç°æœ‰åŸºå‡†é—æ¼çš„é”™è¯¯ä»£ç ã€‚3. æä¾›ç²¾ç®€ç‰ˆæµ‹è¯•å¥—ä»¶ï¼ˆHUMANEVAL+-MINIï¼‰ä»¥åŠ é€Ÿè¯„ä¼°ã€‚4. å‘ç°æµ‹è¯•ä¸è¶³å¯èƒ½å¯¼è‡´æ¨¡å‹æ’åé”™è¯¯ã€‚",
    "unique_features_quote": "EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies... HUMANEVAL+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs... we also produce HUMANEVAL+-MINI which distills HUMANEVAL+ tests by 47Ã—... test insufficiency can lead to mis-ranking.",
    "data_size_quantity": 80,
    "data_size_unit": "å€",
    "last_updated_year": 2023,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['pass@k']",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹é—®é¢˜']",
    "source_type_normalized": "['åŸºäºç°æœ‰åŸºå‡†ï¼ˆHumanEvalï¼‰é€šè¿‡è‡ªåŠ¨æµ‹è¯•è¾“å…¥ç”Ÿæˆå™¨ï¼ˆç»“åˆLLMå’ŒåŸºäºçªå˜çš„ç­–ç•¥ï¼‰è¿›è¡Œå¢å¼ºã€‚']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2305.02309_output/content.md",
    "benchmark_name": "HumanEval",
    "benchmark_name_quote": "The HumanEval (Chen et al., 2021b) is recruited to evaluate the quality of synthesized programs.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "The HumanEval (Chen et al., 2021b) is recruited to evaluate the quality of synthesized programs.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "æ ¹æ®å‡½æ•°ç­¾åå’Œæ–‡æ¡£å­—ç¬¦ä¸²ï¼ˆæ„å›¾è¯´æ˜ï¼‰ç”Ÿæˆç¨‹åºä»£ç ã€‚",
    "task_description_quote": "The prompt as an intent specification is in the form of a function signature and doc-string. A program is conditionally sampled (or completed) based on the prompt in left-to-right auto-regressive fashion.",
    "dimension": "ç¨‹åºåˆæˆçš„åŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "The HumanEval (Chen et al., 2021b) is recruited to evaluate the quality of synthesized programs.",
    "evaluation_method": NaN,
    "evaluation_method_quote": NaN,
    "context_dependency": "å•å‡½æ•°",
    "context_dependency_quote": "The prompt as an intent specification is in the form of a function signature and doc-string.",
    "problem_domain": NaN,
    "problem_domain_quote": NaN,
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "æ–‡ä¸­æœªæ˜ç¡®æåŠå®Œæ•´æ•°æ®é›†çš„è¯­è¨€ï¼Œä»…æåŠå®éªŒä»»åŠ¡ã€‚",
    "language_quote": "The prompt as an intent specification is in the form of a function signature and doc-string. A program is conditionally sampled (or completed) based on the prompt...",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "Program Synthesis with left-to-right sampling (zero-shot)... A program is conditionally sampled (or completed) based on the prompt...",
    "evaluation_metrics": NaN,
    "evaluation_metrics_quote": NaN,
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆæ–‡æ¡£å­—ç¬¦ä¸²ï¼‰ä¸ä»£ç ï¼ˆå‡½æ•°ç­¾åï¼‰",
    "input_modality_quote": "The prompt as an intent specification is in the form of a function signature and doc-string.",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "A program is conditionally sampled (or completed) based on the prompt...",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "The prompt as an intent specification is in the form of a function signature and doc-string. A program is conditionally sampled...",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": NaN,
    "unique_features_quote": NaN,
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['ç¨‹åºåˆæˆçš„åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "[]",
    "problem_domain_normalized": "[]",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2305.07922_output/content.md",
    "benchmark_name": "HumanEval",
    "benchmark_name_quote": "Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA results of 35.0% pass@1 and 54.5% pass@10 on the HumanEval code generation task against other open code LLMs, even surpassing the OpenAI code-cushman-001 model.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. ... in the zero-shot text-to-code generation task on HumanEval benchmark [Chen et al., 2021]",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "æ–‡ä¸­ä»…æåŠä½¿ç”¨è¯¥åŸºå‡†è¿›è¡Œä»£ç ç”Ÿæˆè¯„æµ‹ï¼Œæœªæè¿°å…¶åŸå§‹ä»»åŠ¡è®¾è®¡ã€‚",
    "task_description_quote": "in the zero-shot text-to-code generation task on HumanEval benchmark",
    "dimension": "æ–‡ä¸­ä»…æåŠä½¿ç”¨è¯¥åŸºå‡†è¿›è¡Œä»£ç ç”Ÿæˆè¯„æµ‹ï¼Œæœªæè¿°å…¶åŸå§‹è¯„æµ‹ç»´åº¦ã€‚",
    "dimension_quote": NaN,
    "evaluation_method": "æ–‡ä¸­ä»…æåŠä½¿ç”¨pass@1å’Œpass@10æŒ‡æ ‡ï¼Œæœªæè¿°å…¶åŸå§‹è¯„ä¼°æ–¹æ³•ã€‚",
    "evaluation_method_quote": "achieves new SoTA results of 35.0% pass@1 and 54.5% pass@10 on the HumanEval code generation task",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": NaN,
    "problem_domain_quote": NaN,
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "æ–‡ä¸­ä»…æåŠå®éªŒä»»åŠ¡ï¼ˆæ–‡æœ¬åˆ°ä»£ç ç”Ÿæˆï¼‰ï¼Œæœªæè¿°æ•°æ®é›†æ¶‰åŠçš„å…·ä½“ç¼–ç¨‹è¯­è¨€ã€‚",
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "in the zero-shot text-to-code generation task on HumanEval benchmark",
    "evaluation_metrics": "pass@1, pass@10",
    "evaluation_metrics_quote": "achieves new SoTA results of 35.0% pass@1 and 54.5% pass@10 on the HumanEval code generation task",
    "input_modality": "è‡ªç„¶è¯­è¨€",
    "input_modality_quote": "in the zero-shot text-to-code generation task",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "in the zero-shot text-to-code generation task",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "in the zero-shot text-to-code generation task",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": NaN,
    "unique_features_quote": NaN,
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "[]",
    "evaluation_method_normalized": "['pass@1', 'pass@10']",
    "problem_domain_normalized": "[]",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2305.18584_output/content.md",
    "benchmark_name": "PYCOMMITS",
    "benchmark_name_quote": "We address this issue by collecting a new dataset, PYCOMMITS, from the commit histories of 1650 open-source Python projects on GitHub.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We collect a code editing dataset from the commit histories of 1650 open-source Python projects for training and evaluation. ... We introduce the repo-level multi-round code editing task, along with the corresponding PYCOMMITS dataset and evaluation framework.",
    "dataset_url": "https://github.com/mrvplusone/Coeditor",
    "dataset_url_quote": "Available at https://github.com/mrvplusone/Coeditor.",
    "task_description": "ä»£ç è‡ªåŠ¨ç¼–è¾‘ã€‚æ—¨åœ¨é¢„æµ‹å¯¹ä»£ç åŒºåŸŸï¼ˆåŸºäºåŒä¸€ä»£ç åº“ä¸­æœ€è¿‘çš„æ›´æ”¹ï¼‰çš„ç¼–è¾‘ã€‚è¿™æ˜¯ä¸€ä¸ªå¤šè½®ä»»åŠ¡ï¼Œç›®æ ‡æ˜¯æ ¹æ®ç”¨æˆ·ä¹‹å‰çš„ç¼–è¾‘æ¥é¢„æµ‹ä»£ç çš„ç¼–è¾‘ã€‚",
    "task_description_quote": "In this work, we explore a multi-round code auto-editing setting, aiming to predict edits to a code region based on recent changes within the same codebase. ... In this paper, we introduce a task that we call (multi-round) auto-editing where the goal is to predict edits to code conditioned on the userâ€™s previous edits.",
    "dimension": "ä»£ç ç¼–è¾‘çš„å‡†ç¡®æ€§å’Œè‡ªåŠ¨åŒ–ç¨‹åº¦ï¼Œå…³æ³¨æ¨¡å‹åœ¨ç»™å®šç¼–è¾‘å†å²å’Œä»£ç åº“ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹é¢„æµ‹æ­£ç¡®ç¼–è¾‘çš„èƒ½åŠ›ã€‚",
    "dimension_quote": "In a simplified single-round, single-edit task, Coeditor significantly outperforms GPT-3.5 and SOTA open-source code completion models (bringing exact-match accuracy from 34.7 up to 60.4), demonstrating the benefits of incorporating editing history for code completion. In a multi-round, multi-edit setting, we observe substantial gains by iteratively conditioning on additional user edits.",
    "evaluation_method": "ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡ï¼ˆexact-match accuracyï¼‰ï¼Œä»¥åŠè‡ªåŠ¨åŒ–ç¼–è¾‘è¡Œæ•°å’ŒèŠ‚çœå‡»é”®æ•°çš„ç¼–è¾‘è·ç¦»åº¦é‡ã€‚",
    "evaluation_method_quote": "our method achieves 60.4% exact match accuracy using a 220M parameter model... In the full multi-round setting, we found that Coeditor automates editing 46.7% of the changed lines, saving the user 28.6% of keystrokes measured by an edit distance metric that accounts for cursor movement.",
    "context_dependency": "ä»“åº“çº§åˆ«ï¼ˆrepo-levelï¼‰ï¼Œä¾èµ–åŒä¸€ä»£ç åº“ä¸­çš„å…¶ä»–éƒ¨åˆ†å’Œç”¨æˆ·çš„å†å²ç¼–è¾‘ã€‚",
    "context_dependency_quote": "We introduce the repo-level multi-round code editing task... aiming to predict edits to a code region based on recent changes within the same codebase.",
    "problem_domain": "é€šç”¨è½¯ä»¶å·¥ç¨‹ï¼Œä»£ç ç»´æŠ¤ä¸é‡æ„ã€‚",
    "problem_domain_quote": "Developers often dedicate significant time to maintaining and refactoring existing code.",
    "problem_difficulty": "å·¥ç¨‹çº§ï¼Œæ¶‰åŠå®é™…å¼€æºé¡¹ç›®ä¸­çš„ä»£ç å˜æ›´ã€‚",
    "problem_difficulty_quote": "We collect a code editing dataset from the commit histories of 1650 open-source Python projects for training and evaluation.",
    "language": "Python",
    "language_quote": "We collect a code editing dataset from the commit histories of 1650 open-source Python projects for training and evaluation.",
    "data_size": "ä»1650ä¸ªå¼€æºPythoné¡¹ç›®çš„æäº¤å†å²ä¸­æ”¶é›†ã€‚",
    "data_size_quote": "We collect a code editing dataset from the commit histories of 1650 open-source Python projects for training and evaluation.",
    "source_type": "æ¥è‡ªGitHubä¸Š1650ä¸ªå¼€æºPythoné¡¹ç›®çš„æäº¤å†å²ã€‚",
    "source_type_quote": "We address this issue by collecting a new dataset, PYCOMMITS, from the commit histories of 1650 open-source Python projects on GitHub.",
    "last_updated": "2024",
    "last_updated_quote": "Published as a conference paper at ICLR 2024",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œç”±è®ºæ–‡ä½œè€…ä¸ºç ”ç©¶ç›®çš„æ„å»ºã€‚",
    "build_type_quote": "We address this issue by collecting a new dataset, PYCOMMITS, from the commit histories of 1650 open-source Python projects on GitHub.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç¼–è¾‘ï¼ˆåŸºäºè¡Œçš„å·®å¼‚ï¼‰ï¼ŒåŒ…æ‹¬æ·»åŠ ã€åˆ é™¤è¡Œã€‚",
    "task_granularity_quote": "We encode all prior code edits âˆ†1, . . . , âˆ†k using a line-based diffing scheme and decodes âˆ†u using masked span infilling... we adopt a line-diff-based format, enabling us to convert auto-editing into a masked span infilling problem.",
    "evaluation_metrics": "ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡ï¼ˆexact-match accuracyï¼‰ï¼Œè‡ªåŠ¨åŒ–ç¼–è¾‘è¡Œç™¾åˆ†æ¯”ï¼ŒèŠ‚çœå‡»é”®ç™¾åˆ†æ¯”ï¼ˆåŸºäºç¼–è¾‘è·ç¦»ï¼‰ã€‚",
    "evaluation_metrics_quote": "our method achieves 60.4% exact match accuracy... Coeditor automates editing 46.7% of the changed lines, saving the user 28.6% of keystrokes measured by an edit distance metric that accounts for cursor movement.",
    "input_modality": "ä»£ç ï¼ˆåŸå§‹ä»£ç åº“å’Œä»¥è¡Œå·®å¼‚æ ¼å¼ç¼–ç çš„ç¼–è¾‘å†å²ï¼‰ï¼Œä»¥åŠé€šè¿‡é™æ€åˆ†ææå–çš„ç›¸å…³ä»£ç ç­¾åã€‚",
    "input_modality_quote": "given an original codebase U and a set of code changes âˆ†1, . . . , âˆ†k... we employ lightweight static analysis to pull in relevant parts of the codebase U.",
    "output_modality": "ä»£ç ï¼ˆä»¥è¡Œå·®å¼‚æ ¼å¼ç¼–ç çš„ç›®æ ‡ç¼–è¾‘ï¼‰ã€‚",
    "output_modality_quote": "the auto-editing problem is to predict how to modify a specified region of code u âˆˆU... We encode the input code by a function EncInput... we can encode âˆ†u using the following expression, EncOutput(âˆ†u)...",
    "task_io_type": "ä»£ç åˆ°ä»£ç ï¼ˆåœ¨ç»™å®šä»£ç åº“å’Œç¼–è¾‘å†å²ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹ï¼Œé¢„æµ‹å¯¹ç›®æ ‡ä»£ç åŒºåŸŸçš„ç¼–è¾‘ï¼‰ã€‚",
    "task_io_type_quote": "the auto-editing problem is to predict how to modify a specified region of code u âˆˆU by learning the following distribution: P(âˆ†u | âˆ†k . . . âˆ†1, U).",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºå¤šè½®ã€ä»“åº“çº§åˆ«çš„ä»£ç è‡ªåŠ¨ç¼–è¾‘ä»»åŠ¡ï¼Œä½¿ç”¨åŸºäºè¡Œå·®å¼‚çš„ç¼–ç æ ¼å¼å’Œé™æ€åˆ†ææ¥æ„å»ºä¸Šä¸‹æ–‡ã€‚æ•°æ®é›†æºè‡ªçœŸå®é¡¹ç›®çš„æäº¤å†å²ï¼Œæ¨¡æ‹Ÿäº†å®é™…çš„ä»£ç ç¼–è¾‘å·¥ä½œæµã€‚",
    "unique_features_quote": "We introduce the repo-level multi-round code editing task... We encode all prior code edits âˆ†1, . . . , âˆ†k using a line-based diffing scheme... we employ lightweight static analysis to pull in relevant parts of the codebase U. We collect a code editing dataset from the commit histories of 1650 open-source Python projects.",
    "data_size_quantity": 1650,
    "data_size_unit": "ä¸ªå¼€æºPythoné¡¹ç›®",
    "last_updated_year": 2024,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç¼–è¾‘çš„å‡†ç¡®æ€§å’Œè‡ªåŠ¨åŒ–ç¨‹åº¦', 'å…³æ³¨æ¨¡å‹åœ¨ç»™å®šç¼–è¾‘å†å²å’Œä»£ç åº“ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹é¢„æµ‹æ­£ç¡®ç¼–è¾‘çš„èƒ½åŠ›']",
    "evaluation_method_normalized": "['ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡', 'è‡ªåŠ¨åŒ–ç¼–è¾‘è¡Œç™¾åˆ†æ¯”', 'èŠ‚çœå‡»é”®ç™¾åˆ†æ¯”']",
    "problem_domain_normalized": "['é€šç”¨è½¯ä»¶å·¥ç¨‹', 'ä»£ç ç»´æŠ¤ä¸é‡æ„']",
    "source_type_normalized": "['æ¥è‡ªGitHubä¸Š1650ä¸ªå¼€æºPythoné¡¹ç›®çš„æäº¤å†å²']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ä¿®å¤",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2306.02907_output/content.md",
    "benchmark_name": "DS-1000",
    "benchmark_name_quote": "We evaluate SELFEVOLVE on three code generation datasets, including DS-1000 for data science code",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We evaluate SELFEVOLVE on three code generation datasets, including DS-1000 for data science code, HumanEval for software engineering code, and TransCoder for C++-to-Python translation.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "æ•°æ®ç§‘å­¦ä»£ç ç”Ÿæˆä»»åŠ¡",
    "task_description_quote": "the data science code generation task DS-1000",
    "dimension": "ä»£ç ç”ŸæˆåŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "Extensive experiments show that SELFEVOLVE achieves a significant improvement in execution-based measurements",
    "evaluation_method": "åŸºäºæ‰§è¡Œçš„æµ‹é‡",
    "evaluation_method_quote": "Extensive experiments show that SELFEVOLVE achieves a significant improvement in execution-based measurements",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "æ•°æ®ç§‘å­¦",
    "problem_domain_quote": "the data science code generation task DS-1000",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "Python",
    "language_quote": "JuPyT5 [7] conditions on Jupyter notebooksâ€™ context cells to generate data science code.",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "the data science code generation task DS-1000",
    "evaluation_metrics": NaN,
    "evaluation_metrics_quote": NaN,
    "input_modality": "è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°",
    "input_modality_quote": "Given a problem description written in natural language d",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "the autoregressive language model pÎ¸ predicts the solution",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "Given a problem description written in natural language d, and code context c, the autoregressive language model pÎ¸ predicts the solution",
    "execution_environment": "Pythonè§£é‡Šå™¨",
    "execution_environment_quote": "This refinement mechanism teaches language models to depend on an executor like a Python interpreter to correct the preliminary code.",
    "unique_features": "ä¸“æ³¨äºæ•°æ®ç§‘å­¦é¢†åŸŸçš„ä»£ç ç”Ÿæˆ",
    "unique_features_quote": "the data science code generation task DS-1000",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç”ŸæˆåŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "[]",
    "problem_domain_normalized": "['æ•°æ®ç§‘å­¦']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æ ‡å‡†åº“",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2306.03091_output/content.md",
    "benchmark_name": "RepoBench",
    "benchmark_name_quote": "we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°ä»“åº“çº§åˆ«çš„ä»£ç è‡ªåŠ¨è¡¥å…¨ç³»ç»Ÿã€‚",
    "task_description_quote": "RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems.",
    "dimension": "è·¨æ–‡ä»¶ä»£ç æ£€ç´¢èƒ½åŠ›ã€ç»™å®šä¸Šä¸‹æ–‡çš„ä»£ç è¡¥å…¨èƒ½åŠ›ã€ç»“åˆæ£€ç´¢ä¸è¡¥å…¨çš„ç«¯åˆ°ç«¯æµç¨‹å¤„ç†èƒ½åŠ›ã€‚",
    "dimension_quote": "RepoBench supports both Python and Java and consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). Each task respectively measures the systemâ€™s ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction.",
    "evaluation_method": "å¯¹äºæ£€ç´¢ä»»åŠ¡ï¼ˆRepoBench-Rï¼‰ä½¿ç”¨Accuracy@k (acc@k)æŒ‡æ ‡ï¼›å¯¹äºè¡¥å…¨ä»»åŠ¡ï¼ˆRepoBench-Cï¼‰å’Œç«¯åˆ°ç«¯ä»»åŠ¡ï¼ˆRepoBench-Pï¼‰ï¼Œè¯„ä¼°æ¨¡å‹é¢„æµ‹ä¸‹ä¸€è¡Œä»£ç çš„èƒ½åŠ›ã€‚",
    "evaluation_method_quote": "For evaluative purposes, the Accuracy@k (acc@k) metric is employed to assess retrieval performance. The easy subset is evaluated using acc@1 and acc@3, while the hard subset is examined through acc@1, acc@3, and acc@5 metrics.",
    "context_dependency": "å¤šæ–‡ä»¶é¡¹ç›®ï¼ˆä»“åº“çº§åˆ«ï¼‰ï¼ŒåŒ…å«è·¨æ–‡ä»¶ä¸Šä¸‹æ–‡å’Œæ–‡ä»¶å†…ä¸Šä¸‹æ–‡ã€‚",
    "context_dependency_quote": "However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios. ... RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems.",
    "problem_domain": "é€šç”¨ç¼–ç¨‹",
    "problem_domain_quote": "benchmark for auto-code completion",
    "problem_difficulty": "çœŸå®ä¸–ç•Œç¼–ç¨‹åœºæ™¯ï¼ŒåŒ…å«ä¸åŒéš¾åº¦å­é›†ï¼ˆå¦‚æ£€ç´¢ä»»åŠ¡ä¸­çš„Easyå’ŒHardå­é›†ï¼‰ã€‚",
    "problem_difficulty_quote": "This benchmark comprises three interconnected tasks: RepoBench-R for code retrieval, RepoBench-C for code completion, and RepoBench-P, which integrates both aspects to reflect the entire workflow of an auto-completion system, offering a balanced assessment. ... we categorize them into two subsets: those with 5-9 candidates as the easy subset, and those with 10 or more candidates as the hard subset.",
    "language": "Python å’Œ Java",
    "language_quote": "RepoBench supports both Python and Java",
    "data_size": "è®­ç»ƒæ•°æ®ï¼š10,345ä¸ªPythonä»“åº“å’Œ14,956ä¸ªJavaä»“åº“ã€‚æµ‹è¯•æ•°æ®ï¼š1,075ä¸ªPythonä»“åº“å’Œ594ä¸ªJavaä»“åº“ã€‚å…·ä½“ä»»åŠ¡æ ·æœ¬æ•°é‡è¯¦è§è®ºæ–‡è¡¨1ã€‚",
    "data_size_quote": "After processing the data, our dataset comprises 10,345 Python and 14,956 Java historical repositories, serving as training data and are available for optional fine-tuning. Additionally, we have 1,075 Python and 594 Java new repositories from GitHub designated as test data for evaluation.",
    "source_type": "1. Github-Codeæ•°æ®é›†ï¼ˆæˆªæ­¢2022å¹´3æœˆ16æ—¥ï¼‰ï¼Œç”¨äºæ„å»ºè®­ç»ƒæ•°æ®ã€‚2. æ–°çˆ¬å–çš„GitHubä»“åº“ï¼ˆåˆ›å»ºäº2023å¹´2æœˆ9æ—¥è‡³8æœˆ3æ—¥ä¹‹é—´ï¼‰ï¼Œä¸“é—¨ç”¨ä½œæµ‹è¯•é›†ã€‚",
    "source_type_quote": "Github-Code Dataset: The first source of RepoBench is the github-code dataset2, which consists of a vast collection of code files sourced from GitHub repositories under open-source licenses with a data cutoff date of March 16, 2022. ... Newly Crawled GitHub Data: To mitigate impacts regarding data leakage and memorization, we augment the dataset by incoporating the most recent, non-forked GitHub repositories that are permitted under their respective licenses. Specifically, we use GitHubâ€™s official API to crawl Python and Java repositories created after February 9, 2023, which aligns with the newest knowledge cutoff date of The Stack [22], and before August 3, 2023. This newly-crawled data serves exclusively as our test set for evaluation.",
    "last_updated": "2023-10-04 (arXivç‰ˆæœ¬v2)",
    "last_updated_quote": "arXiv:2306.03091v2  [cs.CL]  4 Oct 2023",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "we introduce RepoBench, a new benchmark",
    "contamination_status": "é€šè¿‡ä½¿ç”¨æ–°çˆ¬å–çš„æ•°æ®ä½œä¸ºæµ‹è¯•é›†æ¥å‡è½»æ•°æ®æ³„éœ²å’Œè®°å¿†çš„å½±å“ã€‚",
    "contamination_status_quote": "To mitigate impacts regarding data leakage and memorization, we augment the dataset by incoporating the most recent, non-forked GitHub repositories ... This newly-crawled data serves exclusively as our test set for evaluation.",
    "dataset_license": "æ•°æ®æ¥æºäºå¼€æºè®¸å¯ä¸‹çš„GitHubä»“åº“ï¼Œä½†æœªæ˜ç¡®æŒ‡å®šæ•°æ®é›†æœ¬èº«çš„è®¸å¯è¯ã€‚",
    "dataset_license_quote": "code files sourced from GitHub repositories under open-source licenses",
    "task_granularity": "ä»£ç è¡¥å…¨ï¼ˆä¸‹ä¸€è¡Œé¢„æµ‹ï¼‰ã€ä»£ç æ£€ç´¢ã€ç«¯åˆ°ç«¯æµç¨‹ï¼ˆæ£€ç´¢+è¡¥å…¨ï¼‰ã€‚",
    "task_granularity_quote": "RepoBench supports both Python and Java and consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline).",
    "evaluation_metrics": "Accuracy@k (acc@1, acc@3, acc@5)",
    "evaluation_metrics_quote": "For evaluative purposes, the Accuracy@k (acc@k) metric is employed to assess retrieval performance. The easy subset is evaluated using acc@1 and acc@3, while the hard subset is examined through acc@1, acc@3, and acc@5 metrics.",
    "input_modality": "ä»£ç ï¼ˆæ–‡ä»¶å†…ä¸Šä¸‹æ–‡å’Œè·¨æ–‡ä»¶ä»£ç ç‰‡æ®µï¼‰",
    "input_modality_quote": "the prompt is created by combining all the parsed snippets as cross-file contexts and an in-file context. The in-file context includes import statements and several preceding lines of code",
    "output_modality": "ä»£ç ï¼ˆä¸‹ä¸€è¡Œï¼‰",
    "output_modality_quote": "predict the next line of code",
    "task_io_type": "ä»£ç åˆ°ä»£ç ",
    "task_io_type_quote": "predict the next line of code given a pre-defined context. The context can involve content from different files (cross-file context) and within the file (in-file context)",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºä»“åº“çº§åˆ«çš„ä»£ç è‡ªåŠ¨è¡¥å…¨è¯„ä¼°ï¼ŒåŒ…å«ä¸‰ä¸ªç›¸äº’å…³è”çš„ä»»åŠ¡ï¼ˆæ£€ç´¢ã€è¡¥å…¨ã€ç«¯åˆ°ç«¯æµç¨‹ï¼‰ï¼Œå¹¶è®¾è®¡äº†ä¸åŒçš„ä¸Šä¸‹æ–‡è®¾ç½®ï¼ˆCross-File-First, Cross-File-Random, In-Fileï¼‰å’Œå­é›†ï¼ˆå¦‚2kå’Œ8k tokené™åˆ¶ï¼‰ä»¥é€‚åº”ä¸åŒæ¨¡å‹ã€‚",
    "unique_features_quote": "RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems. RepoBench supports both Python and Java and consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). ... To effectively evaluate next-line prediction in auto-completion systems, we define three settings: Cross-File-First (XF-F)... Cross-File-Random (XF-R)... In-File (IF)... RepoBench-C is divided into two subsets: RepoBench-C-2k and RepoBench-C-8k.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2023,
    "last_updated_month": 10,
    "last_updated_day": 4,
    "language_normalized": "['Python', 'Java']",
    "dimension_normalized": "['è·¨æ–‡ä»¶ä»£ç æ£€ç´¢èƒ½åŠ›', 'ç»™å®šä¸Šä¸‹æ–‡çš„ä»£ç è¡¥å…¨èƒ½åŠ›', 'ç»“åˆæ£€ç´¢ä¸è¡¥å…¨çš„ç«¯åˆ°ç«¯æµç¨‹å¤„ç†èƒ½åŠ›']",
    "evaluation_method_normalized": "['Accuracy@k', 'acc@1', 'acc@3', 'acc@5']",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹']",
    "source_type_normalized": "['Github-Codeæ•°æ®é›†', 'æ–°çˆ¬å–çš„GitHubä»“åº“']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2306.08568_output/content.md",
    "benchmark_name": "HumanEval",
    "benchmark_name_quote": "Through comprehensive experiments on five prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, DS-1000, and MultiPL-E, our models showcase outstanding performance.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Through comprehensive experiments on five prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, DS-1000, and MultiPL-E, our models showcase outstanding performance.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ä»£ç ç”Ÿæˆï¼ˆä»è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆä»£ç ï¼‰",
    "task_description_quote": "These models, pre-trained on substantial code data, excel in various code-related tasks, consistently delivering impressive performance. (ç»“åˆä¸Šä¸‹æ–‡ï¼Œè¯„æµ‹åŸºå‡†ç”¨äºè¯„ä¼°ä»£ç ç”Ÿæˆä»»åŠ¡)",
    "dimension": "ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "Remarkably, WizardCoder 15B even surpasses the well-known closed-source LLMs, including Anthropicâ€™s Claude and Googleâ€™s Bard, on the HumanEval and HumanEval+ benchmarks. (é€šè¿‡passç‡è¯„ä¼°ï¼Œéšå«äº†åŠŸèƒ½æ­£ç¡®æ€§ç»´åº¦)",
    "evaluation_method": "pass@1 (é€šè¿‡ç‡)",
    "evaluation_method_quote": "Figure 1: An illustration of our novel Code Evol-Instruct and the superior pass@1 performance of our WizardCoder 34B... The Python score is the mean between HumanEval and MBPP.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "é€šç”¨ç¼–ç¨‹é—®é¢˜",
    "problem_domain_quote": "Develop a Python program that creates a random password of length 8 characters. (ç¤ºä¾‹ä»»åŠ¡å±äºé€šç”¨ç¼–ç¨‹)",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "å¤šç§ç¼–ç¨‹è¯­è¨€ï¼ˆåŒ…æ‹¬Pythonï¼‰",
    "language_quote": "outperforming the open-source SOTA ... by a large margin in 9 different programming languages. The Python score is the mean between HumanEval and MBPP.",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "Through comprehensive experiments on five prominent code generation benchmarks",
    "evaluation_metrics": "pass@1",
    "evaluation_metrics_quote": "Figure 1: An illustration of our novel Code Evol-Instruct and the superior pass@1 performance of our WizardCoder 34B",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆæŒ‡ä»¤/æè¿°ï¼‰",
    "input_modality_quote": "Develop a Python program that creates a random password of length 8 characters. (ç¤ºä¾‹ä¸­çš„è¾“å…¥æ˜¯è‡ªç„¶è¯­è¨€æŒ‡ä»¤)",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "Here's an example program that generates a random password... (ç¤ºä¾‹ä¸­çš„è¾“å‡ºæ˜¯ä»£ç )",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "Develop a Python program that creates a random password of length 8 characters. (ä»è‡ªç„¶è¯­è¨€æ–‡æœ¬æè¿°ç”Ÿæˆä»£ç )",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "æœ¬æ–‡æåŠäº†äº”ä¸ªåŸºå‡†ï¼šHumanEval, HumanEval+, MBPP, DS-1000, å’Œ MultiPL-Eã€‚å…¶ä¸­MultiPL-Eæ”¯æŒå¤šè¯­è¨€è¯„ä¼°ã€‚",
    "unique_features_quote": "Through comprehensive experiments on five prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, DS-1000, and MultiPL-E",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['å¤šç§ç¼–ç¨‹è¯­è¨€', 'Python']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['pass@1']",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹é—®é¢˜']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2306.09896_output/content.md",
    "benchmark_name": "HumanEval",
    "benchmark_name_quote": "In this paper, we analyze Code Llama, GPT-3.5 and GPT-4â€™s ability to perform self-repair on problems taken from HumanEval and APPS.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "In this paper, we analyze Code Llama, GPT-3.5 and GPT-4â€™s ability to perform self-repair on problems taken from HumanEval and APPS.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ä»è‡ªç„¶è¯­è¨€è§„èŒƒç”Ÿæˆä»£ç ç‰‡æ®µ",
    "task_description_quote": "Large language models (LLMs) have proven capable of generating code snippets from natural language specifications",
    "dimension": "ä»£ç ç”Ÿæˆä¸è‡ªæˆ‘ä¿®å¤èƒ½åŠ›",
    "dimension_quote": "we focus on evaluating the modelsâ€™ capacity to reflect upon, provide feedback on and debug the code.",
    "evaluation_method": "pass@k æŒ‡æ ‡ï¼Œæ‰§è¡Œå•å…ƒæµ‹è¯•",
    "evaluation_method_quote": "performance is typically measured by pass@k (Chen et al., 2021; Kulal et al., 2019)â€”the probability that at least one of k i.i.d. program samples from the model satisfies a given specification.",
    "context_dependency": "å•å‡½æ•°",
    "context_dependency_quote": "for self-contained Python programming tasks.",
    "problem_domain": "é€šç”¨ç¼–ç¨‹",
    "problem_domain_quote": "self-contained Python programming tasks.",
    "problem_difficulty": "ç«èµ›çº§å’Œé¢è¯•çº§",
    "problem_difficulty_quote": "complex coding challenges such as those found in competitions and professional software engineering interviews.",
    "language": "Python",
    "language_quote": "for self-contained Python programming tasks.",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "generating code snippets from natural language specifications",
    "evaluation_metrics": "pass@k",
    "evaluation_metrics_quote": "performance is typically measured by pass@k (Chen et al., 2021; Kulal et al., 2019)",
    "input_modality": "è‡ªç„¶è¯­è¨€ä¸å•å…ƒæµ‹è¯•",
    "input_modality_quote": "Given a specification Ïˆ, a programming model MP first generates np samples i.i.d.",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "generating code snippets",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "generating code snippets from natural language specifications",
    "execution_environment": "å•å…ƒæµ‹è¯•æ‰§è¡Œç¯å¢ƒ",
    "execution_environment_quote": "These np code samples are then executed against a test bed.",
    "unique_features": "æœ¬æ–‡é‡ç‚¹ç ”ç©¶è‡ªæˆ‘ä¿®å¤ï¼ˆself-repairï¼‰ç­–ç•¥åœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œè€Œéæå‡ºæ–°åŸºå‡†ã€‚",
    "unique_features_quote": "In this paper, we investigate the efficacy of self-repair techniques applied to CodeLlama-13b-instruct, GPT-3.5, and GPT-4 for self-contained Python programming tasks.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆä¸è‡ªæˆ‘ä¿®å¤èƒ½åŠ›']",
    "evaluation_method_normalized": "['pass@k']",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "ç«èµ›çº§",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2306.11644_output/content.md",
    "benchmark_name": "HumanEval",
    "benchmark_name_quote": "The evaluation benchmark proposed in the latter work, HumanEval, has been widely adopted for comparing LLMsâ€™ performance on code.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We demonstrate the power of high quality data in breaking existing scaling laws by training a 1.3B-parameter model, which we call phi-1, ... we attain 50.6% pass@1 accuracy on HumanEval and 55.5% pass@1 accuracy on MBPP",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ä»è‡ªç„¶è¯­è¨€æ–‡æ¡£å­—ç¬¦ä¸²ï¼ˆdocstringsï¼‰ä¸­åˆæˆç®€å•çš„Pythonå‡½æ•°ã€‚",
    "task_description_quote": "We focus our attention on LLMs trained for code, and specifically writing simple Python functions from their docstrings as in [CTJ+21].",
    "dimension": "ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "The evaluation benchmark proposed in the latter work, HumanEval, has been widely adopted for comparing LLMsâ€™ performance on code.",
    "evaluation_method": "pass@1 å‡†ç¡®ç‡",
    "evaluation_method_quote": "we attain 50.6% pass@1 accuracy on HumanEval and 55.5% pass@1 accuracy on MBPP",
    "context_dependency": "å•å‡½æ•°",
    "context_dependency_quote": "writing simple Python functions from their docstrings",
    "problem_domain": "åŸºç¡€ç¼–ç¨‹æ¦‚å¿µ",
    "problem_domain_quote": "determine its educational value for a student whose goal is to learn basic coding concepts",
    "problem_difficulty": "åŸºç¡€çº§åˆ«",
    "problem_difficulty_quote": "for a student whose goal is to learn basic coding concepts",
    "language": "Python",
    "language_quote": "writing simple Python functions from their docstrings",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": "2021",
    "last_updated_quote": "2021 Jul Codex-300M [CTJ+21]",
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": "æ–‡ä¸­è®¨è®ºäº†è®­ç»ƒæ•°æ®å¯èƒ½å­˜åœ¨çš„æ±¡æŸ“é£é™©",
    "contamination_status_quote": "in Section 5 we study possible contamination of our training data with respect to HumanEval.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "writing simple Python functions from their docstrings",
    "evaluation_metrics": "pass@1",
    "evaluation_metrics_quote": "we attain 50.6% pass@1 accuracy on HumanEval",
    "input_modality": "è‡ªç„¶è¯­è¨€",
    "input_modality_quote": "writing simple Python functions from their docstrings",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "writing simple Python functions",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "writing simple Python functions from their docstrings",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "è¢«å¹¿æ³›ç”¨äºæ¯”è¾ƒLLMåœ¨ä»£ç ç”Ÿæˆä¸Šçš„æ€§èƒ½",
    "unique_features_quote": "HumanEval, has been widely adopted for comparing LLMsâ€™ performance on code.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2021,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['pass@1']",
    "problem_domain_normalized": "['åŸºç¡€ç¼–ç¨‹æ¦‚å¿µ']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "å…¥é—¨çº§",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "é«˜æ±¡æŸ“é£é™©"
  },
  {
    "source_paper": "2306.14893_output/content.md",
    "benchmark_name": "LCC (Long Code Completion Benchmark)",
    "benchmark_name_quote": "In this paper, we introduce the Long Code Completion Benchmark (LCC), a new benchmark that focuses on code completion with long code context for three programming languages: Python, Java, and C#.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "To evaluate the effectiveness of LongCoder and encourage future research on Long Code Completion, we construct a new dataset called LCC by filtering code from GitHub based on length, with the goal of focusing on longer code examples.",
    "dataset_url": "https://github.com/microsoft/CodeBERT",
    "dataset_url_quote": "1All the codes and data are available at https://github.com/microsoft/CodeBERT.",
    "task_description": "ä¸“æ³¨äºé•¿ä»£ç ä¸Šä¸‹æ–‡çš„ä»£ç è¡¥å…¨ä»»åŠ¡ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨æ›´å¤æ‚ã€æ›´ç°å®çš„ä»£ç æ–‡ä»¶çº§åˆ«åœºæ™¯ä¸‹çš„èƒ½åŠ›ã€‚",
    "task_description_quote": "In this paper, we introduce a new task for code completion that focuses on handling long code input... We introduce the Long Code Completion Benchmark (LCC), a new benchmark that focuses on code completion with long code context...",
    "dimension": "é•¿ä»£ç å»ºæ¨¡èƒ½åŠ›ã€ä»£ç è¡¥å…¨å‡†ç¡®æ€§ã€æ¨¡å‹æ•ˆç‡ï¼ˆè®¡ç®—èµ„æºæ¶ˆè€—ï¼‰",
    "dimension_quote": "...achieves superior performance on code completion tasks compared to previous models while maintaining comparable efficiency in terms of computational resources during inference.",
    "evaluation_method": "åœ¨é€è¡ŒåŸºç¡€ä¸Šï¼Œä½¿ç”¨ç²¾ç¡®åŒ¹é…ï¼ˆExact Match, EMï¼‰å’Œç¼–è¾‘ç›¸ä¼¼åº¦ï¼ˆEdit Similarity, Edit Simï¼‰è¿›è¡Œè¯„ä¼°ã€‚",
    "evaluation_method_quote": "We follow Lu et al. (2021) to evaluate the performance of the models in terms of Exact Match (EM) and Edit Similarity (Edit Sim) on a per-line basis (Svyatkovskiy et al., 2020).",
    "context_dependency": "æ–‡ä»¶çº§åˆ«ï¼ˆé•¿ä»£ç ä¸Šä¸‹æ–‡ï¼‰ï¼Œä¸Šä¸‹æ–‡é•¿åº¦é€šå¸¸è¶…è¿‡512ä¸ªä»£ç æ ‡è®°ï¼Œå¹³å‡é•¿åº¦åœ¨1800-2000ä¸ªæ ‡è®°å·¦å³ã€‚",
    "context_dependency_quote": "...focuses on code completion with long code context... The average length of the examples in LCC are 5Ã— longer than those in existing datasets... The average length of a Python source file on GitHub is 1,305 tokens.",
    "problem_domain": "é€šç”¨ç¼–ç¨‹ï¼Œæ•°æ®æ¥æºäºGitHubä¸Šçš„å¼€æºä»£ç æ–‡ä»¶ã€‚",
    "problem_domain_quote": "Specifically, we construct our datasets from the github-code dataset, which contains a vast number of code files sourced from GitHub with an open-source license that permits research use.",
    "problem_difficulty": "å·¥ç¨‹çº§ï¼ŒåŒ…å«çœŸå®ä¸–ç•Œä¸­çš„é•¿ä»£ç æ–‡ä»¶ï¼Œç»“æ„å’Œä¾èµ–å…³ç³»æ›´å¤æ‚ã€‚",
    "problem_difficulty_quote": "Meanwhile, longer code sequences contain more complex structures and require models to consider more context and dependencies. This can be challenging for previously proposed code completion models that focus on short code...",
    "language": "Python, Java, C#",
    "language_quote": "...a new benchmark that focuses on code completion with long code context for three programming languages: Python, Java, and C#.",
    "data_size": "æ¯ç§è¯­è¨€åŒ…å«10ä¸‡ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œ1ä¸‡ä¸ªå¼€å‘é›†æ ·æœ¬å’Œ1ä¸‡ä¸ªæµ‹è¯•é›†æ ·æœ¬ã€‚",
    "data_size_quote": "For each programming language, we sample 100k examples for training, and 10k examples for development and 10k for testing.",
    "source_type": "ä»GitHubä¸Šå…·æœ‰å¼€æºè®¸å¯è¯çš„ä»£ç æ–‡ä»¶ä¸­ç­›é€‰å’Œæ„å»ºï¼Œç»è¿‡å»é‡å’ŒASTè§£æè¿‡æ»¤ã€‚",
    "source_type_quote": "Specifically, we construct our datasets from the github-code dataset, which contains a vast number of code files sourced from GitHub with an open-source license that permits research use. The steps to construct the datasets are as follows: â€¢ We first follow Allamanis (2019) to deduplicate examples with high similarity (Jacobi similarity â‰¥0.9) in order to eliminate forked files, and then remove code files that canâ€™t be parsed into an abstract syntax tree using a standard compiler tool called tree-sitter.",
    "last_updated": "2023",
    "last_updated_quote": "Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s).",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆç”±è®ºæ–‡ä½œè€…æ„å»ºï¼‰",
    "build_type_quote": "We construct a new dataset (LCC) for code completion tasks that requires long code modeling to encourage more research in such scenarios.",
    "contamination_status": "æ–‡ä¸­æœªæ˜ç¡®è®¨è®º",
    "contamination_status_quote": NaN,
    "dataset_license": "å¼€æºè®¸å¯è¯ï¼ˆå…è®¸ç ”ç©¶ä½¿ç”¨ï¼‰ï¼Œå…·ä½“è®¸å¯è¯åç§°æœªæ˜ç¡®è¯´æ˜ã€‚",
    "dataset_license_quote": "...sourced from GitHub with an open-source license that permits research use.",
    "task_granularity": "ä»£ç è¡¥å…¨ï¼ˆå…·ä½“ä¸ºä¸‹ä¸€è¡Œé¢„æµ‹æˆ–è¡Œå†…è¡¥å…¨ï¼‰",
    "task_granularity_quote": "Code completion is a crucial task in software development that helps developers save time and effort by suggesting and auto-completing code based on context.",
    "evaluation_metrics": "ç²¾ç¡®åŒ¹é…ï¼ˆExact Match, EMï¼‰ã€ç¼–è¾‘ç›¸ä¼¼åº¦ï¼ˆEdit Similarity, Edit Simï¼‰",
    "evaluation_metrics_quote": "We follow Lu et al. (2021) to evaluate the performance of the models in terms of Exact Match (EM) and Edit Similarity (Edit Sim) on a per-line basis (Svyatkovskiy et al., 2020).",
    "input_modality": "ä»£ç ",
    "input_modality_quote": "Code completion is a crucial task in software development that helps developers save time and effort by suggesting and auto-completing code based on context.",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "Code completion is a crucial task in software development that helps developers save time and effort by suggesting and auto-completing code based on context.",
    "task_io_type": "ä»£ç åˆ°ä»£ç ",
    "task_io_type_quote": "Code completion is a crucial task in software development that helps developers save time and effort by suggesting and auto-completing code based on context.",
    "execution_environment": "æ–‡ä¸­æœªæ˜ç¡®æè¿°",
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“é—¨é’ˆå¯¹é•¿ä»£ç ä¸Šä¸‹æ–‡è®¾è®¡ï¼Œå¹³å‡ä»£ç é•¿åº¦æ˜¯ç°æœ‰æ•°æ®é›†çš„5å€ï¼›åŒ…å«Pythonã€Javaã€C#ä¸‰ç§è¯­è¨€ï¼›é€šè¿‡é•¿åº¦è¿‡æ»¤ï¼ˆ>512 tokensï¼‰ç¡®ä¿å…³æ³¨é•¿ä¸Šä¸‹æ–‡åœºæ™¯ã€‚",
    "unique_features_quote": "On average, the examples in LCC are 5Ã— longer than those in existing datasets (Lu et al., 2021)... Since the benchmark primarily focuses on the code completion task with long code context, we remove code files whose length of code tokens after tokenization is shorter than 512.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2023,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python', 'Java', 'C#']",
    "dimension_normalized": "['é•¿ä»£ç å»ºæ¨¡èƒ½åŠ›', 'ä»£ç è¡¥å…¨å‡†ç¡®æ€§', 'æ¨¡å‹æ•ˆç‡ï¼ˆè®¡ç®—èµ„æºæ¶ˆè€—ï¼‰']",
    "evaluation_method_normalized": "['ç²¾ç¡®åŒ¹é…ï¼ˆExact Match, EMï¼‰', 'ç¼–è¾‘ç›¸ä¼¼åº¦ï¼ˆEdit Similarity, Edit Simï¼‰']",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹', 'æ•°æ®æ¥æºäºGitHubä¸Šçš„å¼€æºä»£ç æ–‡ä»¶']",
    "source_type_normalized": "['ä»GitHubä¸Šå…·æœ‰å¼€æºè®¸å¯è¯çš„ä»£ç æ–‡ä»¶ä¸­ç­›é€‰å’Œæ„å»º', 'ç»è¿‡å»é‡å’ŒASTè§£æè¿‡æ»¤']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç è¡¥å…¨",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2307.04349_output/content.md",
    "benchmark_name": "APPS",
    "benchmark_name_quote": "Extensive experiments show that RLTF achieves state-of-the-art performance on the APPS and the MBPP benchmarks.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Extensive experiments show that RLTF achieves state-of-the-art performance on the APPS and the MBPP benchmarks.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ç¨‹åºåˆæˆæˆ–ä»£ç ç”Ÿæˆï¼Œå³æ ¹æ®ç»™å®šçš„é«˜çº§è¡Œä¸ºæè¿°ç”Ÿæˆè®¡ç®—æœºä»£ç ã€‚",
    "task_description_quote": "Program synthesis, or code generation, involves creating an executable program that solves a given problem.",
    "dimension": "ä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆé€šè¿‡å•å…ƒæµ‹è¯•ï¼‰",
    "dimension_quote": "Unlike text generation, program synthesis requires both syntactic and functional accuracy since the generated code must pass compilation and unit tests.",
    "evaluation_method": "ä½¿ç”¨å•å…ƒæµ‹è¯•ç»“æœä½œä¸ºåé¦ˆä¿¡å·ï¼Œè¯„ä¼°ç”Ÿæˆçš„ä»£ç æ˜¯å¦èƒ½é€šè¿‡ç¼–è¯‘å’Œæµ‹è¯•ã€‚",
    "evaluation_method_quote": "RLTF, i.e., Reinforcement Learning from Unit Test Feedback, a novel online RL framework with unit test feedback of multi-granularity for refining code LLMs.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "ç¼–ç¨‹æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç«èµ›çº§ç®—æ³•é—®é¢˜ã€‚",
    "problem_domain_quote": "While LLMs have shown promising results in basic programming tasks, there is still progress to be made in tackling more challenging problems such as program competitions.",
    "problem_difficulty": "ä»åŸºç¡€ç¼–ç¨‹ä»»åŠ¡åˆ°ç«èµ›çº§æŒ‘æˆ˜ã€‚",
    "problem_difficulty_quote": "While LLMs have shown promising results in basic programming tasks, there is still progress to be made in tackling more challenging problems such as program competitions.",
    "language": "æ–‡ä¸­æœªæ˜ç¡®æè¿°APPSæ•°æ®é›†çš„è¯­è¨€æ„æˆï¼Œä»…æåŠCodexè§£å†³PythonæŒ‘æˆ˜ã€‚",
    "language_quote": "Codex (Chen et al., 2021) is a noteworthy example of an LLM with 12 billion parameters that can successfully solve over 70% of complex Python programming challenges created by humans.",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆä»æè¿°ç”Ÿæˆå®Œæ•´ä»£ç ï¼‰",
    "task_granularity_quote": "Program synthesis, or code generation, involves creating an executable program that solves a given problem.",
    "evaluation_metrics": "åŸºäºå•å…ƒæµ‹è¯•é€šè¿‡ç‡çš„åé¦ˆä¿¡å·ï¼ˆå¦‚ç²—ç²’åº¦ã€ç»†ç²’åº¦ã€è‡ªé€‚åº”åé¦ˆï¼‰",
    "evaluation_metrics_quote": "Built upon this framework, we develop multi-granularity feedback that is automatically extracted from unit test. To expand, we introduce coarse-grained and fine-grained feedbacks applicable to programs with errors, aimed at punishing the specific segments of code where the errors appear. For programs that do not pass all test cases, we propose an adaptive feedback mechanism that assigns varying penalties based on the ratio of passed test cases.",
    "input_modality": "è‡ªç„¶è¯­è¨€æè¿°",
    "input_modality_quote": "Program synthesis is an automated process that generates computer code W based on a high-level description of desired behavior D.",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "Program synthesis is an automated process that generates computer code W based on a high-level description of desired behavior D.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "Program synthesis is an automated process that generates computer code W based on a high-level description of desired behavior D.",
    "execution_environment": "ç¼–è¯‘å™¨/å•å…ƒæµ‹è¯•ç¯å¢ƒ",
    "execution_environment_quote": "One generates the target program and interacts with the compiler to produce a training data pair...",
    "unique_features": "æœ¬æ–‡æ˜¯è¯„æµ‹è®ºæ–‡ï¼Œæœªæè¿°APPSåŸºå‡†çš„ç‹¬ç‰¹ä¹‹å¤„ã€‚æœ¬æ–‡æå‡ºçš„RLTFæ–¹æ³•çš„ç‹¬ç‰¹ä¹‹å¤„åœ¨äºå…¶åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ¡†æ¶å’Œä»å•å…ƒæµ‹è¯•ä¸­æå–çš„å¤šç²’åº¦åé¦ˆï¼ˆç²—ç²’åº¦ã€ç»†ç²’åº¦ã€è‡ªé€‚åº”åé¦ˆï¼‰ã€‚",
    "unique_features_quote": "To address these issues, we propose RLTF, i.e., Reinforcement Learning from Unit Test Feedback, a novel online RL framework with unit test feedback of multi-granularity for refining code LLMs.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['ä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆé€šè¿‡å•å…ƒæµ‹è¯•ï¼‰']",
    "evaluation_method_normalized": "['åŸºäºå•å…ƒæµ‹è¯•é€šè¿‡ç‡çš„åé¦ˆä¿¡å·ï¼ˆå¦‚ç²—ç²’åº¦ã€ç»†ç²’åº¦ã€è‡ªé€‚åº”åé¦ˆï¼‰']",
    "problem_domain_normalized": "['ç¼–ç¨‹æŒ‘æˆ˜', 'åŒ…æ‹¬ç«èµ›çº§ç®—æ³•é—®é¢˜']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "ç«èµ›çº§",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2307.14936_output/content.md",
    "benchmark_name": "HumanEval",
    "benchmark_name_quote": "PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval benchmark.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Through extensive evaluation on three benchmarks, including HumanEval, CoderEval, and LeetCode, we conjecture that...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ä»è‡ªç„¶è¯­è¨€æè¿°ç”ŸæˆPythonä»£ç ï¼Œè¡¡é‡ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚",
    "task_description_quote": "demonstrating remarkable performance on the code generation task.",
    "dimension": "ä»£ç ç”Ÿæˆæ­£ç¡®æ€§",
    "dimension_quote": "measure functional correctness for synthesizing programs from docstrings.",
    "evaluation_method": "pass@k (æ–‡ä¸­ä¸»è¦æŠ¥å‘Špass@1)",
    "evaluation_method_quote": "PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval benchmark.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "é€šç”¨ç¼–ç¨‹é—®é¢˜",
    "problem_domain_quote": "tackle up to 72% of Python programming problems.",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "Python",
    "language_quote": "tackle up to 72% of Python programming problems.",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "demonstrating remarkable performance on the code generation task.",
    "evaluation_metrics": "pass@1",
    "evaluation_metrics_quote": "PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval benchmark.",
    "input_modality": "è‡ªç„¶è¯­è¨€",
    "input_modality_quote": "generating code from natural language descriptions",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "generating code from natural language descriptions",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "generating code from natural language descriptions",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ç”±OpenAIå‘å¸ƒï¼Œæ˜¯ä»£ç ç”Ÿæˆé¢†åŸŸå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ã€‚",
    "unique_features_quote": "OpenAI HumanEval benchmark",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆæ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['pass@1']",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹é—®é¢˜']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2308.07124_output/content.md",
    "benchmark_name": "HUMANEVALPACK",
    "benchmark_name_quote": "We further introduce HUMANEVALPACK, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust).",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We further introduce HUMANEVALPACK, expanding the HumanEval benchmark to a total of 3 coding tasks...",
    "dataset_url": "https://github.com/bigcode-project/octopack",
    "dataset_url_quote": "Code, models and data are freely available at https://github.com/bigcode-project/octopack.",
    "task_description": "è¯„æµ‹åŸºå‡†æ—¨åœ¨è¯„ä¼°ä»£ç å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šç§ä»£ç ç›¸å…³ä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œè¦†ç›–ä»£ç ä¿®å¤ã€ä»£ç è§£é‡Šå’Œä»£ç åˆæˆä¸‰ç§åœºæ™¯ã€‚",
    "task_description_quote": "HUMANEVALPACK: A benchmark for Code LLM generalization, spanning three scenarios (Code Repair, Code Explanation, Code Synthesis) and 6 programming languages (Python, JavaScript, Java, Go, C++, Rust)",
    "dimension": "ä»£ç å¤§è¯­è¨€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€å¤šä»»åŠ¡å¤„ç†èƒ½åŠ›ã€å¤šè¯­è¨€ä»£ç èƒ½åŠ›",
    "dimension_quote": "A benchmark for Code LLM generalization, spanning three scenarios (Code Repair, Code Explanation, Code Synthesis) and 6 programming languages",
    "evaluation_method": "ä½¿ç”¨ pass@k æŒ‡æ ‡è¯„ä¼°åŠŸèƒ½æ­£ç¡®æ€§",
    "evaluation_method_quote": "Metric: Pass@k (Figure 3 caption)",
    "context_dependency": "å•å‡½æ•°",
    "context_dependency_quote": "Given an incorrect code function with a subtle bug and accompanying unit tests, the model is tasked to fix the function. (æè¿° HUMANEVALFIX ä»»åŠ¡)",
    "problem_domain": "é€šç”¨ç¼–ç¨‹é—®é¢˜ï¼ŒåŒ…å«ç®—æ³•å’ŒåŸºç¡€åŠŸèƒ½å®ç°",
    "problem_domain_quote": "Write a Python function `has_close_elements(numbers: List[float], threshold: float) -> bool` to solve the following problem: Check if in given list of numbers, are any two numbers closer to each other than given threshold. (Figure 3 ç¤ºä¾‹)",
    "problem_difficulty": "å·¥ç¨‹çº§ï¼ˆåŒ…å«éœ€è¦ä¿®å¤çš„å¾®å¦™bugï¼‰",
    "problem_difficulty_quote": "Given an incorrect code function with a subtle bug... Bugs are written such that the code still runs but produces an incorrect result leading to at least one unit test failing.",
    "language": "Python, JavaScript, Java, Go, C++, Rust",
    "language_quote": "spanning three scenarios (Code Repair, Code Explanation, Code Synthesis) and 6 programming languages (Python, JavaScript, Java, Go, C++, Rust)",
    "data_size": "åŸºäº164ä¸ªHumanEvalé—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜æ‰©å±•åˆ°6ç§è¯­è¨€å’Œ3ç§ä»»åŠ¡ï¼Œå…±984ä¸ªä¿®å¤ä»»åŠ¡ï¼Œä»¥åŠå¯¹åº”çš„è§£é‡Šå’Œåˆæˆä»»åŠ¡ã€‚",
    "data_size_quote": "We manually add a bug to each of the 164 HumanEval solutions across all 6 languages (984 total bugs).",
    "source_type": "åŸºäºHumanEvalåŸºå‡†äººå·¥æ‰©å±•æ„å»º",
    "source_type_quote": "we expand the code synthesis benchmark HumanEval (Chen et al., 2021; Zheng et al., 2023) to cover all three input-output combinations for six languages",
    "last_updated": "2024",
    "last_updated_quote": "arXiv:2308.07124v2 [cs.CL] 18 Feb 2024",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆæœ¬æ–‡ä½œè€…å›¢é˜Ÿï¼‰",
    "build_type_quote": "We further introduce HUMANEVALPACK",
    "contamination_status": "åŸºäºç°æœ‰åŸºå‡†æ‰©å±•ï¼Œå­˜åœ¨æ±¡æŸ“é£é™©",
    "contamination_status_quote": "expanding the HumanEval benchmark",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ä¿®å¤ã€ä»£ç è§£é‡Šã€ä»£ç åˆæˆ",
    "task_granularity_quote": "spanning three scenarios (Code Repair, Code Explanation, Code Synthesis)",
    "evaluation_metrics": "pass@k",
    "evaluation_metrics_quote": "Metric: Pass@k (Figure 3 caption)",
    "input_modality": "è‡ªç„¶è¯­è¨€ä¸ä»£ç çš„ç»„åˆï¼Œå–å†³äºå…·ä½“ä»»åŠ¡ï¼ˆNL, NL+C, Cï¼‰",
    "input_modality_quote": "When instruction tuning with code (C) data, code may either appear only in the input alongside the NL instruction (NL+Câ†’NL, e.g. code explanation), only in the output (NLâ†’C, e.g. code synthesis), or in both input and output (NL+Câ†’C, e.g. code modifications like bug fixing).",
    "output_modality": "è‡ªç„¶è¯­è¨€æˆ–ä»£ç ï¼Œå–å†³äºå…·ä½“ä»»åŠ¡ï¼ˆNL, Cï¼‰",
    "output_modality_quote": "code may either appear only in the input alongside the NL instruction (NL+Câ†’NL, e.g. code explanation), only in the output (NLâ†’C, e.g. code synthesis), or in both input and output (NL+Câ†’C, e.g. code modifications like bug fixing).",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ï¼ˆåˆæˆï¼‰ã€ä»£ç åˆ°æ–‡æœ¬ï¼ˆè§£é‡Šï¼‰ã€ä»£ç åˆ°ä»£ç ï¼ˆä¿®å¤ï¼‰",
    "task_io_type_quote": "cover all three input-output combinations: NL+Câ†’NL (e.g. code explanation), NLâ†’C (e.g. code synthesis), NL+Câ†’C (e.g. code modifications like bug fixing)",
    "execution_environment": "å•å…ƒæµ‹è¯•æ‰§è¡Œç¯å¢ƒ",
    "execution_environment_quote": "Given an incorrect code function with a subtle bug and accompanying unit tests",
    "unique_features": "æ‰©å±•äº†ç»å…¸çš„HumanEvalåŸºå‡†ï¼Œè¦†ç›–äº†ä»£ç ä¿®å¤å’Œè§£é‡Šç­‰æ›´å¹¿æ³›çš„ç°å®ä»»åŠ¡ï¼Œæ”¯æŒå…­ç§ç¼–ç¨‹è¯­è¨€ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºå‡†å› æ¨¡å‹æ€§èƒ½æ¥è¿‘é¥±å’Œè€Œå¯èƒ½å¤±æ•ˆçš„é—®é¢˜ã€‚",
    "unique_features_quote": "Our more challenging evaluation variants provide room for future LLMs to improve on the performance of the current state-of-the-art.",
    "data_size_quantity": 164,
    "data_size_unit": "ä¸ªé—®é¢˜",
    "last_updated_year": 2024,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python', 'JavaScript', 'Java', 'Go', 'C++', 'Rust']",
    "dimension_normalized": "['ä»£ç å¤§è¯­è¨€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›', 'å¤šä»»åŠ¡å¤„ç†èƒ½åŠ›', 'å¤šè¯­è¨€ä»£ç èƒ½åŠ›']",
    "evaluation_method_normalized": "['pass@k']",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹é—®é¢˜', 'åŒ…å«ç®—æ³•å’ŒåŸºç¡€åŠŸèƒ½å®ç°']",
    "source_type_normalized": "['åŸºäºHumanEvalåŸºå‡†äººå·¥æ‰©å±•æ„å»º']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "é«˜æ±¡æŸ“é£é™©"
  },
  {
    "source_paper": "2308.10335_output/content.md",
    "benchmark_name": "ROBUSTAPI",
    "benchmark_name_quote": "To fill the missing piece, we propose a dataset ROBUSTAPI for evaluating the reliability and robustness of code generated by LLMs.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "To fill the missing piece, we propose a dataset ROBUSTAPI for evaluating the reliability and robustness of code generated by LLMs.",
    "dataset_url": "https://github.com/FloridSleeves/RobustAPI",
    "dataset_url_quote": "We open-source our dataset and evaluator on GitHub2. ... 2https://github.com/FloridSleeves/RobustAPI",
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ä»£ç çš„å¯é æ€§å’Œé²æ£’æ€§ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹Java APIçš„è¯¯ç”¨æƒ…å†µã€‚",
    "task_description_quote": "We propose a dataset ROBUSTAPI for evaluating the reliability and robustness of code generated by LLMs.",
    "dimension": "ä»£ç ç”Ÿæˆçš„å¯é æ€§ä¸é²æ£’æ€§ï¼Œç‰¹åˆ«æ˜¯APIè¯¯ç”¨æ£€æµ‹ã€‚",
    "dimension_quote": "The main purpose of this benchmark is not to evaluate the functional correctness of the generated code, but instead, we focus on reliability and robustness.",
    "evaluation_method": "ä½¿ç”¨æŠ½è±¡è¯­æ³•æ ‘ï¼ˆASTï¼‰åˆ†æç”Ÿæˆçš„ä»£ç ï¼Œå¹¶ä¸é¢„æœŸçš„APIä½¿ç”¨æ¨¡å¼ï¼ˆç»“æ„åŒ–è°ƒç”¨åºåˆ—ï¼‰è¿›è¡Œæ¯”è¾ƒï¼Œæ£€æµ‹è¿è§„è¡Œä¸ºã€‚",
    "evaluation_method_quote": "We also provide an evaluator that analyzes the generated code snippets using the abstract syntax tree (AST) and compares them with the expected API usage patterns.",
    "context_dependency": "å•ä»£ç ç‰‡æ®µï¼ˆåŸºäºStack Overflowé—®ç­”ï¼‰ï¼Œæ¶‰åŠç‰¹å®šAPIçš„ä½¿ç”¨ã€‚",
    "context_dependency_quote": "We collect 1208 real questions from Stack Overflow which involves 18 representative Java APIs.",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ï¼Œæ¶µç›–å­—ç¬¦ä¸²å¤„ç†ã€æ•°æ®ç»“æ„ã€ç§»åŠ¨å¼€å‘ã€åŠ å¯†ã€I/Oå’Œæ•°æ®åº“æ“ä½œç­‰å¤šä¸ªé¢†åŸŸã€‚",
    "problem_domain_quote": "These 18 APIs cover 6 domains including string processing, data structure, mobile development, crypto, I/O and database operation.",
    "problem_difficulty": "å®é™…è½¯ä»¶å¼€å‘ä¸­å¸¸è§çš„ã€å¼€å‘è€…å®¹æ˜“çŠ¯é”™çš„APIä½¿ç”¨é—®é¢˜ã€‚",
    "problem_difficulty_quote": "In this way, we guarantee that the questions in ROBUSTAPI are answerable and non-trivial so we can use them to effectively evaluate the LLMsâ€™ ability in answering coding questions that humans are prone to make mistakes.",
    "language": "Java",
    "language_quote": "Thus we collect representative questions about Java from Stack Overflow.",
    "data_size": "åŒ…å«1208ä¸ªé—®é¢˜ï¼Œæ¶‰åŠ18ä¸ªä»£è¡¨æ€§çš„Java APIã€‚",
    "data_size_quote": "We collect 1208 real questions from Stack Overflow which involves 18 representative Java APIs.",
    "source_type": "ä»Stack Overflowçˆ¬å–çš„çœŸå®ç¼–ç¨‹é—®é¢˜ï¼Œå¹¶åŸºäºExampleCheckæ•°æ®é›†ç­›é€‰ã€‚",
    "source_type_quote": "We build ROBUSTAPI based on the dataset from ExampleCheck (Zhang et al. 2018) as our starting point. ... Then we crawl questions relevant to these APIs from Stack Overflow.",
    "last_updated": "2024-01-27 (æ ¹æ®arXivç‰ˆæœ¬v5æ—¥æœŸ)",
    "last_updated_quote": "arXiv:2308.10335v5 [cs.CL] 27 Jan 2024",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆç”±æœ¬æ–‡ä½œè€…æ„å»ºå¹¶å‘å¸ƒï¼‰",
    "build_type_quote": "We propose a dataset ROBUSTAPI... We open-source our dataset and evaluator on GitHub.",
    "contamination_status": "åŸºäºStack OverflowçœŸå®é—®é¢˜æ„å»ºï¼Œå­˜åœ¨è¢«LLMè®­ç»ƒæ•°æ®åŒ…å«çš„é£é™©ï¼Œä½†æ–‡ä¸­æœªæ˜ç¡®è®¨è®ºæ±¡æŸ“çŠ¶æ€ã€‚",
    "contamination_status_quote": NaN,
    "dataset_license": "æ–‡ä¸­æœªæåŠå…·ä½“è®¸å¯è¯ã€‚",
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆæ ¹æ®è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°ç”Ÿæˆä½¿ç”¨ç‰¹å®šAPIçš„ä»£ç ç‰‡æ®µï¼‰",
    "task_granularity_quote": "The prompt simulates a user asking coding questions without providing any additional hints from the API documentation which is a typical scenario when novice developers seek help from large language models.",
    "evaluation_metrics": "APIè¯¯ç”¨æ£€æµ‹ç‡ï¼ˆé€šè¿‡ASTåˆ†æåˆ¤æ–­ç”Ÿæˆçš„ä»£ç æ˜¯å¦è¿åé¢„æœŸçš„APIä½¿ç”¨æ¨¡å¼ï¼‰",
    "evaluation_metrics_quote": "Any violations of such structured call sequences would be considered as API misuse from the perspective of software engineering.",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆStack Overflowé—®é¢˜çš„æ ‡é¢˜å’Œæè¿°ï¼‰",
    "input_modality_quote": "question field contains the title and description of the Stack Overflow questions.",
    "output_modality": "ä»£ç ï¼ˆJavaä»£ç ç‰‡æ®µï¼‰",
    "output_modality_quote": "We design templates to trigger large language models to generate the code snippet and the corresponding explanation.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "The prompt simulates a user asking coding questions... instruct LLMs to generate answers (code) to the questions.",
    "execution_environment": "æ–‡ä¸­æœªæ˜ç¡®æè¿°æ‰§è¡Œç¯å¢ƒï¼Œè¯„ä¼°åŸºäºé™æ€åˆ†æï¼ˆASTï¼‰ï¼Œè€ŒéåŠ¨æ€æ‰§è¡Œã€‚",
    "execution_environment_quote": "We introduce the static analysis method in ROBUSTAPI for detecting the API usage violations which leverages the abstract syntax tree...",
    "unique_features": "ä¸“æ³¨äºè¯„ä¼°ä»£ç ç”Ÿæˆåœ¨çœŸå®è½¯ä»¶å¼€å‘åœºæ™¯ä¸‹çš„å¯é æ€§å’Œé²æ£’æ€§ï¼Œè€Œéä¼ ç»Ÿçš„åŠŸèƒ½æ­£ç¡®æ€§ï¼›é’ˆå¯¹APIè¯¯ç”¨è¿™ä¸€å…·ä½“é£é™©ï¼›åŒ…å«ä¸€ä¸ªåŸºäºASTçš„é™æ€åˆ†æè¯„ä¼°å™¨ã€‚",
    "unique_features_quote": "Unlike the online programming forums, the generated code snippets are not reviewed by the community peers and thus suffer from API misuse... The main purpose of this benchmark is not to evaluate the functional correctness of the generated code, but instead, we focus on reliability and robustness.",
    "data_size_quantity": 1208,
    "data_size_unit": "ä¸ªé—®é¢˜",
    "last_updated_year": 2024,
    "last_updated_month": 1,
    "last_updated_day": 27,
    "language_normalized": "['Java']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆçš„å¯é æ€§ä¸é²æ£’æ€§', 'APIè¯¯ç”¨æ£€æµ‹']",
    "evaluation_method_normalized": "['APIè¯¯ç”¨æ£€æµ‹ç‡']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'å­—ç¬¦ä¸²å¤„ç†', 'æ•°æ®ç»“æ„', 'ç§»åŠ¨å¼€å‘', 'åŠ å¯†', 'I/O', 'æ•°æ®åº“æ“ä½œ']",
    "source_type_normalized": "['Stack Overflow', 'ExampleCheckæ•°æ®é›†']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "é«˜æ±¡æŸ“é£é™©"
  },
  {
    "source_paper": "2308.12950_output/content.md",
    "benchmark_name": "HumanEval",
    "benchmark_name_quote": "Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67% and 65% on HumanEval and MBPP, respectively.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We perform exhaustive evaluations of our models on major code generation benchmarks: HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and APPS (Hendrycks et al., 2021), as well as a multilingual version of HumanEval (MultiPL-E, Cassano et al., 2023)...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": NaN,
    "task_description_quote": NaN,
    "dimension": NaN,
    "dimension_quote": NaN,
    "evaluation_method": NaN,
    "evaluation_method_quote": NaN,
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": NaN,
    "problem_domain_quote": NaN,
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": NaN,
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": NaN,
    "task_granularity_quote": NaN,
    "evaluation_metrics": NaN,
    "evaluation_metrics_quote": NaN,
    "input_modality": NaN,
    "input_modality_quote": NaN,
    "output_modality": NaN,
    "output_modality_quote": NaN,
    "task_io_type": NaN,
    "task_io_type_quote": NaN,
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": NaN,
    "unique_features_quote": NaN,
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "[]",
    "evaluation_method_normalized": "[]",
    "problem_domain_normalized": "[]",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2308.16458_output/content.md",
    "benchmark_name": "BioCoder",
    "benchmark_name_quote": "We present BioCoder, a benchmark developed to evaluate LLMs in generating bioinformatics-specific code.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We introduce BioCoder (see Figure 1), a benchmark for code generation incorporating 2,269 bioinformatics-specific coding problems.",
    "dataset_url": "https://github.com/gersteinlab/biocoder and https://biocoder-benchmark.github.io/",
    "dataset_url_quote": "All datasets, benchmark, Docker images, and scripts required for testing are available at: https://github.com/gersteinlab/biocoder and https://biocoder-benchmark.github.io/.",
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆç”Ÿç‰©ä¿¡æ¯å­¦ç‰¹å®šä»£ç æ–¹é¢çš„èƒ½åŠ›ã€‚",
    "task_description_quote": "BioCoder benchmark mainly targets bioinformatics data analysis, which tasks such as managing various biological data formats, understanding processing workflows, and utilizing APIs of various packages.",
    "dimension": "ä»£ç ç”ŸæˆåŠŸèƒ½æ­£ç¡®æ€§ã€å¤„ç†å¤æ‚ä¸Šä¸‹æ–‡ï¼ˆè·¨æ–‡ä»¶ä¾èµ–ã€ç±»å£°æ˜ã€å…¨å±€å˜é‡ï¼‰çš„èƒ½åŠ›ã€é¢†åŸŸç‰¹å®šçŸ¥è¯†ï¼ˆç”Ÿç‰©ä¿¡æ¯å­¦ï¼‰",
    "dimension_quote": "BioCoder spans much of the field, covering cross-file dependencies, class declarations, and global variables. ... They contain domain-specific knowledge of bioinformatics, beyond just general coding capability.",
    "evaluation_method": "æ¨¡ç³Šæµ‹è¯•æ¡†æ¶ï¼Œé€šè¿‡æ‰§è¡Œæµ‹è¯•ç”¨ä¾‹æ¥éªŒè¯ç”Ÿæˆä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§ï¼Œå¹¶è®¡ç®—Pass@Kç‡ã€‚",
    "evaluation_method_quote": "BioCoder incorporates a fuzz-testing framework for evaluation. ... Our benchmark results, derived from 1,000 iterations, indicate the Pass@K rate.",
    "context_dependency": "è·¨æ–‡ä»¶ä¾èµ–ã€åŒ…å«ç±»å£°æ˜å’Œå…¨å±€å˜é‡çš„å®Œæ•´é¡¹ç›®ä¸Šä¸‹æ–‡",
    "context_dependency_quote": "BioCoder spans much of the field, covering cross-file dependencies, class declarations, and global variables. ... we included all potentially required class declarations in the input.",
    "problem_domain": "ç”Ÿç‰©ä¿¡æ¯å­¦ï¼ŒåŒ…æ‹¬ç”Ÿç‰©æ•°æ®åˆ†æã€é—ä¼ æµ‹åºã€DNA/RNAåˆ†æã€ç”Ÿç‰©ä¿¡æ¯å­¦è½¯ä»¶å¼€å‘",
    "problem_domain_quote": "Here, we target bioinformatics due to the amount of domain knowledge, algorithms, and data operations this discipline requires. ... This project specializes in generating Python functions that address key bioinformatics topics such as genetic sequencing and DNA/RNA analysis.",
    "problem_difficulty": "å…·æœ‰æŒ‘æˆ˜æ€§ã€å®ç”¨çš„ç”Ÿç‰©ä¿¡æ¯å­¦åœºæ™¯ï¼ŒåŒ…å«æ—¥å¸¸æ•°æ®åˆ†æä»»åŠ¡å’Œéƒ¨åˆ†è½¯ä»¶å¼€å‘ä»»åŠ¡",
    "problem_difficulty_quote": "BIOCODER is a code generation benchmark designed for challenging, practical bioinformatics scenarios, ... This domain encapsulates the majority of daily tasks encountered by bioinformaticians in data analysis.",
    "language": "Python å’Œ Java",
    "language_quote": "It incorporates 1,026 Python functions and 1,243 Java methods extracted from GitHub, ... However, for the scope of this study, we focus on Python and Java, with the intention to expand to other languages in the future.",
    "data_size": "æ€»å…±2,269ä¸ªç”Ÿç‰©ä¿¡æ¯å­¦ç‰¹å®šçš„ç¼–ç é—®é¢˜ï¼ˆå…¬å¼€é›†460ä¸ªï¼Œéšè—é›†2,269ä¸ªï¼Œç›¸ä¼¼é›†460ä¸ªï¼‰ï¼ŒåŒ…å«1,026ä¸ªPythonå‡½æ•°å’Œ1,243ä¸ªJavaæ–¹æ³•ï¼Œä»¥åŠæ¥è‡ªRosalindé¡¹ç›®çš„253ä¸ªPythonç¤ºä¾‹ã€‚",
    "data_size_quote": "a benchmark for code generation incorporating 2,269 bioinformatics-specific coding problems. ... It incorporates 1,026 Python functions and 1,243 Java methods extracted from GitHub, along with 253 examples from the Rosalind Project, all pertaining to bioinformatics.",
    "source_type": "ä»1,720ä¸ªç»åŒè¡Œè¯„å®¡ç”Ÿç‰©ä¿¡æ¯å­¦æ–‡ç« å¼•ç”¨çš„GitHubä»“åº“ä¸­æå–ï¼Œä»¥åŠæ¥è‡ªRosalindé¡¹ç›®çš„ç¤ºä¾‹ã€‚",
    "source_type_quote": "curated from 1,720 bioinformatics repositories referenced in peer-reviewed bioinformatics articles. ... We included an additional 253 questions from the Rosalind project.",
    "last_updated": "2024å¹´5æœˆ20æ—¥ï¼ˆæ ¹æ®arXivç‰ˆæœ¬v5ï¼‰",
    "last_updated_quote": "arXiv:2308.16458v5 [cs.LG] 20 May 2024",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œé€šè¿‡è‡ªåŠ¨è¿‡æ»¤ã€GPTè¾…åŠ©è¿‡æ»¤å’Œäººå·¥æ£€æŸ¥ç›¸ç»“åˆçš„æ–¹å¼æ„å»ºã€‚",
    "build_type_quote": "We ensure that each function in our dataset requires a certain level of domain expertise in bioinformatics through a combination of automatic filtering, GPT-assisted filtering, and manual inspection.",
    "contamination_status": "ç»è¿‡ä¸¥æ ¼è¿‡æ»¤ã€æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†ï¼Œä»¥é˜²æ­¢æ¨¡å‹è®°å¿†ã€‚",
    "contamination_status_quote": "It has undergone rigorous filtering, extensive data cleaning, and preprocessing to prevent models from memorizing.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆç”Ÿæˆå®Œæ•´çš„å‡½æ•°æˆ–æ–¹æ³•ä½“ï¼‰",
    "task_granularity_quote": "a benchmark for code generation ... generating bioinformatics-specific code.",
    "evaluation_metrics": "Pass@Kç‡",
    "evaluation_metrics_quote": "Our benchmark results, derived from 1,000 iterations, indicate the Pass@K rate.",
    "input_modality": "è‡ªç„¶è¯­è¨€æè¿°ã€ä»£ç è§„èŒƒã€æ³¨é‡Šä»¥åŠå®Œæ•´çš„ä¸Šä¸‹æ–‡ï¼ˆåŒ…æ‹¬ä¾èµ–é¡¹ã€ç±»å£°æ˜ã€å…¨å±€å˜é‡ï¼‰",
    "input_modality_quote": "We processed the data, rephrasing more detailed text descriptions, as well as associated comments and specifications, ... we included all potentially required class declarations in the input.",
    "output_modality": "ä»£ç ï¼ˆPythonæˆ–Javaï¼‰",
    "output_modality_quote": "generating bioinformatics-specific code.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "generating bioinformatics-specific code.",
    "execution_environment": "Dockerç¯å¢ƒï¼ŒåŒ…å«ä¸°å¯Œçš„æ‰€éœ€ä¾èµ–é¡¹ï¼Œç”¨äºåœ¨ç°å®é¡¹ç›®åœºæ™¯ä¸­è¿›è¡Œæµ‹è¯•ã€‚",
    "execution_environment_quote": "Testing incorporates a Docker environment, an abundance of required dependencies, ... This robust setup not only facilitates testing in realistic project scenarios",
    "unique_features": "ä¸“æ³¨äºç”Ÿç‰©ä¿¡æ¯å­¦é¢†åŸŸï¼›åŒ…å«è·¨æ–‡ä»¶ä¾èµ–å’Œå®Œæ•´é¡¹ç›®ä¸Šä¸‹æ–‡ï¼›è§„æ¨¡è¿œè¶…åŒç±»é¢†åŸŸç‰¹å®šåŸºå‡†ï¼ˆå¦‚CoderEvalï¼‰ï¼›æä¾›å¯æ‰©å±•çš„è§£æå·¥å…·å’Œæ¨¡ç³Šæµ‹è¯•å·¥å…·ï¼›é€šè¿‡ä¸»é¢˜å»ºæ¨¡éªŒè¯äº†ä»£ç è¦†ç›–èŒƒå›´å…·æœ‰ä»£è¡¨æ€§ã€‚",
    "unique_features_quote": "Here, we target bioinformatics ... BioCoder spans much of the field, covering cross-file dependencies, class declarations, and global variables. ... our dataset surpasses the scale of CoderEval ... Using topic modeling, we show that the overall coverage of the included code is representative of the full spectrum of bioinformatics calculations. ... We provide an extendable parsing tool ... We provide a fuzz testing tool capable of scaling to handle substantial datasets.",
    "data_size_quantity": 2269,
    "data_size_unit": "ä¸ªé—®é¢˜",
    "last_updated_year": 2024,
    "last_updated_month": 5,
    "last_updated_day": 20,
    "language_normalized": "['Python', 'Java']",
    "dimension_normalized": "['ä»£ç ç”ŸæˆåŠŸèƒ½æ­£ç¡®æ€§', 'å¤„ç†å¤æ‚ä¸Šä¸‹æ–‡ï¼ˆè·¨æ–‡ä»¶ä¾èµ–ã€ç±»å£°æ˜ã€å…¨å±€å˜é‡ï¼‰çš„èƒ½åŠ›', 'é¢†åŸŸç‰¹å®šçŸ¥è¯†ï¼ˆç”Ÿç‰©ä¿¡æ¯å­¦ï¼‰']",
    "evaluation_method_normalized": "['Pass@Kç‡']",
    "problem_domain_normalized": "['ç”Ÿç‰©ä¿¡æ¯å­¦', 'ç”Ÿç‰©æ•°æ®åˆ†æ', 'é—ä¼ æµ‹åº', 'DNA/RNAåˆ†æ', 'ç”Ÿç‰©ä¿¡æ¯å­¦è½¯ä»¶å¼€å‘']",
    "source_type_normalized": "['ä»1,720ä¸ªç»åŒè¡Œè¯„å®¡ç”Ÿç‰©ä¿¡æ¯å­¦æ–‡ç« å¼•ç”¨çš„GitHubä»“åº“ä¸­æå–', 'æ¥è‡ªRosalindé¡¹ç›®çš„ç¤ºä¾‹']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2310.06770_output/content.md",
    "benchmark_name": "SWE-bench",
    "benchmark_name_quote": "We introduce SWE-bench, an evaluation framework consisting of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We introduce SWE-bench, an evaluation framework consisting of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories.",
    "dataset_url": "swebench.com",
    "dataset_url_quote": "Data, code, and leaderboard at swebench.com",
    "task_description": "è¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨çœŸå®è½¯ä»¶å·¥ç¨‹åœºæ™¯ä¸‹çš„èƒ½åŠ›ã€‚ç»™å®šä¸€ä¸ªä»£ç åº“å’Œä¸€ä¸ªéœ€è¦è§£å†³çš„GitHubé—®é¢˜æè¿°ï¼Œæ¨¡å‹çš„ä»»åŠ¡æ˜¯ç¼–è¾‘ä»£ç åº“ä»¥è§£å†³è¯¥é—®é¢˜ã€‚",
    "task_description_quote": "Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue.",
    "dimension": "çœŸå®ä¸–ç•Œè½¯ä»¶å·¥ç¨‹é—®é¢˜è§£å†³èƒ½åŠ›ï¼ŒåŒ…æ‹¬è·¨æ–‡ä»¶ã€è·¨å‡½æ•°ã€è·¨ç±»çš„å¤æ‚ä»£ç ç¼–è¾‘ä¸åè°ƒèƒ½åŠ›ï¼Œä»¥åŠé•¿ä¸Šä¸‹æ–‡å¤„ç†å’Œå¤æ‚æ¨ç†èƒ½åŠ›ã€‚",
    "dimension_quote": "Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks.",
    "evaluation_method": "æ‰§è¡Œå•å…ƒæµ‹è¯•å’Œç³»ç»Ÿæµ‹è¯•ã€‚å°†ç”Ÿæˆçš„è¡¥ä¸åº”ç”¨åˆ°ä»£ç åº“åï¼Œè¿è¡Œä¸ä»»åŠ¡å®ä¾‹ç›¸å…³çš„æµ‹è¯•ã€‚å¦‚æœè¡¥ä¸åº”ç”¨æˆåŠŸä¸”æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Œåˆ™è®¤ä¸ºè§£å†³æ–¹æ¡ˆæˆåŠŸè§£å†³äº†é—®é¢˜ã€‚ä¸»è¦æŒ‡æ ‡æ˜¯è§£å†³é—®é¢˜çš„ä»»åŠ¡å®ä¾‹ç™¾åˆ†æ¯”ã€‚",
    "evaluation_method_quote": "To evaluate a proposed solution, we apply the generated patch, using unix's patch program, to the codebase and then execute the unit and system tests associated with the task instance. If the patch applies successfully and all of these tests pass we consider the proposed solution to have successfully resolved the issue. The metric for our benchmark is the percentage of task instances that are resolved.",
    "context_dependency": "å¤šæ–‡ä»¶é¡¹ç›®ã€‚ä»»åŠ¡æ¶‰åŠç†è§£å¤§å‹ã€å¤æ‚çš„ä»£ç åº“ï¼Œå¹¶è·¨å¤šä¸ªå‡½æ•°ã€ç±»ç”šè‡³æ–‡ä»¶è¿›è¡Œåè°ƒæ›´æ”¹ã€‚",
    "context_dependency_quote": "Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously... SWE-bench's reference solutions average editing 1.7 files, 3.0 functions, and 32.8 lines (added or removed).",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ï¼Œå…·ä½“ä¸ºä¿®å¤Bugæˆ–å®ç°æ–°åŠŸèƒ½ã€‚",
    "problem_domain_quote": "SWE-bench is a benchmark featuring GitHub issues from popular repositories that report bugs or request new features...",
    "problem_difficulty": "å·¥ç¨‹çº§ã€‚åŸºäºçœŸå®GitHubé—®é¢˜ï¼Œéœ€è¦å¤„ç†å¤§å‹ä»£ç åº“å’Œå¤æ‚çš„è·¨ä¸Šä¸‹æ–‡ç¼–è¾‘ï¼Œå¯¹ç°æœ‰æœ€å…ˆè¿›æ¨¡å‹æå…·æŒ‘æˆ˜æ€§ã€‚",
    "problem_difficulty_quote": "Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere 1.96% of the issues.",
    "language": "Pythonã€‚æ•°æ®é›†æºè‡ª12ä¸ªæµè¡Œçš„å¼€æºPythonä»“åº“ã€‚",
    "language_quote": "SWE-bench, an evaluation framework consisting of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories.",
    "data_size": "åŒ…å«2,294ä¸ªä»»åŠ¡å®ä¾‹ã€‚",
    "data_size_quote": "Through these stages of filtering, the original 90,000 PRs are filtered down to the 2,294 task instances which comprise SWE-bench.",
    "source_type": "ä»12ä¸ªæµè¡Œçš„å¼€æºGitHub Pythonä»“åº“ä¸­æŠ“å–çš„çœŸå®GitHubé—®é¢˜å’Œå·²åˆå¹¶çš„æ‹‰å–è¯·æ±‚ã€‚",
    "source_type_quote": "SWE-bench sources task instances from real-world Python repositories by connecting GitHub issues to merged pull request solutions that resolve related tests.",
    "last_updated": "2024 (è®ºæ–‡ç‰ˆæœ¬ä¸º2024å¹´11æœˆ11æ—¥)",
    "last_updated_quote": "arXiv:2310.06770v3  [cs.CL]  11 Nov 2024",
    "build_type": "å®˜æ–¹è‡ªå»ºã€‚ä½œè€…é€šè¿‡ä¸€ä¸ªä¸‰é˜¶æ®µçš„ç­›é€‰æµç¨‹ä»GitHubæ•°æ®ä¸­æ„å»ºã€‚",
    "build_type_quote": "To find high-quality task instances at scale, we use a 3-stage pipeline as follows.",
    "contamination_status": "æŠ—æ±¡æŸ“è®¾è®¡ã€‚åŸºå‡†å¯ä»¥æŒç»­æ›´æ–°ï¼ŒåŒ…å«æ¨¡å‹è®­ç»ƒæ—¥æœŸä¹‹ååˆ›å»ºçš„é—®é¢˜ï¼Œç¡®ä¿è§£å†³æ–¹æ¡ˆæœªåŒ…å«åœ¨è®­ç»ƒè¯­æ–™ä¸­ã€‚",
    "contamination_status_quote": "Therefore, we can extend SWE-bench with a continual supply of new task instances and evaluate LMs on issues created after their training date, which ensures that the solution was not included in their training corpus.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç¼–è¾‘/ä¿®å¤ã€‚ç”Ÿæˆåº”ç”¨äºç°æœ‰ä»£ç åº“çš„è¡¥ä¸ä»¥è§£å†³é—®é¢˜ã€‚",
    "task_granularity_quote": "Each task requires generating a patch describing changes to apply to the existing codebase.",
    "evaluation_metrics": "é—®é¢˜è§£å†³ç‡ï¼ˆç™¾åˆ†æ¯”ï¼‰ã€‚",
    "evaluation_metrics_quote": "The metric for our benchmark is the percentage of task instances that are resolved.",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆé—®é¢˜æè¿°ï¼‰ä¸ä»£ç ï¼ˆå®Œæ•´ä»£ç åº“ï¼‰ã€‚",
    "input_modality_quote": "A model is given an issue text description and a complete codebase.",
    "output_modality": "ä»£ç ï¼ˆè¡¥ä¸æ–‡ä»¶ï¼‰ã€‚",
    "output_modality_quote": "The model is then tasked to make an edit to the codebase to resolve the issue. In practice, we represent edits as patch files...",
    "task_io_type": "æ–‡æœ¬ä¸ä»£ç åˆ°ä»£ç ã€‚",
    "task_io_type_quote": "Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue.",
    "execution_environment": "ä»“åº“çš„æµ‹è¯•æ¡†æ¶ã€‚éœ€è¦å®‰è£…ä¾èµ–å¹¶è¿è¡Œå•å…ƒæµ‹è¯•å’Œç³»ç»Ÿæµ‹è¯•ã€‚",
    "execution_environment_quote": "The revised codebase is then evaluated using the repositoryâ€™s testing framework.",
    "unique_features": "1. çœŸå®ä¸–ç•Œè½¯ä»¶å·¥ç¨‹ä»»åŠ¡ï¼šæºè‡ªçœŸå®GitHubé—®é¢˜å’Œè§£å†³æ–¹æ¡ˆã€‚2. å¯æŒç»­æ›´æ–°ï¼šå¯è½»æ¾æ‰©å±•åˆ°ä»»ä½•GitHub Pythonä»“åº“ï¼Œæœ€å°åŒ–äººå·¥å¹²é¢„ã€‚3. å¤šæ ·åŒ–çš„é•¿è¾“å…¥ï¼šé—®é¢˜æè¿°è¯¦ç»†ï¼Œä»£ç åº“åŒ…å«æ•°åƒä¸ªæ–‡ä»¶ã€‚4. é²æ£’çš„è¯„ä¼°ï¼šæ¯ä¸ªä»»åŠ¡å®ä¾‹è‡³å°‘æœ‰ä¸€ä¸ªâ€œå¤±è´¥è½¬é€šè¿‡â€çš„æµ‹è¯•ã€‚5. è·¨ä¸Šä¸‹æ–‡ä»£ç ç¼–è¾‘ï¼šä¸é™åˆ¶ç¼–è¾‘èŒƒå›´ï¼Œéœ€è¦åœ¨å¤§ä»£ç åº“çš„å¤šä¸ªä½ç½®ç”Ÿæˆä¿®è®¢ã€‚6. è§£å†³æ–¹æ¡ˆçš„å¹¿æ³›ç©ºé—´ï¼šå¯ä½œä¸ºæ¯”è¾ƒæ£€ç´¢ã€é•¿ä¸Šä¸‹æ–‡æ¨¡å‹å’Œå†³ç­–æ™ºèƒ½ä½“çš„å…¬å¹³å¹³å°ã€‚",
    "unique_features_quote": "SWE-bench offers several advantages over existing LM programming benchmarks. These include, a realistic setting that utilizes user-submitted issues and solutions, diverse inputs featuring unique code problems from 12 repositories, a robust framework for execution-based evaluation, and the ability to continuously update the benchmark with new instances, requiring minimal human intervention.",
    "data_size_quantity": 2294,
    "data_size_unit": "ä¸ªä»»åŠ¡å®ä¾‹",
    "last_updated_year": 2024,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['çœŸå®ä¸–ç•Œè½¯ä»¶å·¥ç¨‹é—®é¢˜è§£å†³èƒ½åŠ›', 'è·¨æ–‡ä»¶', 'è·¨å‡½æ•°', 'è·¨ç±»çš„å¤æ‚ä»£ç ç¼–è¾‘ä¸åè°ƒèƒ½åŠ›', 'é•¿ä¸Šä¸‹æ–‡å¤„ç†', 'å¤æ‚æ¨ç†èƒ½åŠ›']",
    "evaluation_method_normalized": "['é—®é¢˜è§£å†³ç‡ï¼ˆç™¾åˆ†æ¯”ï¼‰']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'ä¿®å¤Bug', 'å®ç°æ–°åŠŸèƒ½']",
    "source_type_normalized": "['ä»12ä¸ªæµè¡Œçš„å¼€æºGitHub Pythonä»“åº“ä¸­æŠ“å–çš„çœŸå®GitHubé—®é¢˜å’Œå·²åˆå¹¶çš„æ‹‰å–è¯·æ±‚']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ä¿®å¤",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2312.02120_output/content.md",
    "benchmark_name": "HumanEval",
    "benchmark_name_quote": "Notably, MagicoderS-CL-7B based on CODELLAMA even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1).",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We evaluate Magicoder and MagicoderS on a wide range of coding tasks, including HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) for Python text-to-code generation...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "Pythonæ–‡æœ¬åˆ°ä»£ç ç”Ÿæˆï¼Œè¡¡é‡ä»è‡ªç„¶è¯­è¨€æè¿°ï¼ˆdocstringsï¼‰åˆæˆç¨‹åºçš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚",
    "task_description_quote": "...HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) for Python text-to-code generation...",
    "dimension": "ä»£ç ç”ŸæˆåŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "...indicating that MagicoderS-CL can generate more robust code.",
    "evaluation_method": "pass@1",
    "evaluation_method_quote": "...surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1).",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": NaN,
    "problem_domain_quote": NaN,
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "Python",
    "language_quote": "...HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) for Python text-to-code generation...",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": "æ–‡ä¸­æåŠäº†å»æ±¡æŸ“æªæ–½ï¼Œè¡¨æ˜å­˜åœ¨æ±¡æŸ“é£é™©å¹¶è¿›è¡Œäº†å¤„ç†",
    "contamination_status_quote": "Finally, we apply the same logic as StarCoder Li et al. (2023) to decontaminate our training data by removing coding problems that contain docstrings or solutions from HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021)...",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "...for Python text-to-code generation...",
    "evaluation_metrics": "pass@1",
    "evaluation_metrics_quote": "...surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1).",
    "input_modality": "è‡ªç„¶è¯­è¨€",
    "input_modality_quote": "...for Python text-to-code generation...",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "...for Python text-to-code generation...",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "...for Python text-to-code generation...",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": NaN,
    "unique_features_quote": NaN,
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç”ŸæˆåŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['pass@1']",
    "problem_domain_normalized": "[]",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "é«˜æ±¡æŸ“é£é™©"
  },
  {
    "source_paper": "2402.16906_output/content.md",
    "benchmark_name": "HumanEval",
    "benchmark_name_quote": "Experiments demonstrate that LDB consistently enhances the baseline performance by up to 9.8% across the HumanEval, MBPP, and TransCoder benchmarks",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We evaluate LDB on three code generation benchmarks: HumanEval (Chen et al., 2021), TransCoder (Roziere et al., 2020), and MBPP (Austin et al., 2021).",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "æ–‡æœ¬åˆ°ä»£ç ç”Ÿæˆï¼Œä»»åŠ¡æè¿°æ˜¯ä¸€æ®µç®€è¦çš„è‡ªç„¶è¯­è¨€æ®µè½ï¼Œæ¦‚è¿°äº†è¦ç”Ÿæˆçš„ç¨‹åºçš„é¢„æœŸåŠŸèƒ½ã€‚",
    "task_description_quote": "HumanEval and MBPP are for text-to-code generation, where the task description is a brief passage outlines the intended functionality of the program to be generated.",
    "dimension": "ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "Experiments demonstrate that LDB consistently enhances the baseline performance... in code debugging for various LLM selections.",
    "evaluation_method": "ä½¿ç”¨éšè—æµ‹è¯•ç”¨ä¾‹è®¡ç®— Pass@1 å‡†ç¡®ç‡",
    "evaluation_method_quote": "We compute Pass@1 accuracy with hidden test cases for assesment.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": NaN,
    "problem_domain_quote": NaN,
    "problem_difficulty": "åŸºç¡€ç¼–ç¨‹é—®é¢˜",
    "problem_difficulty_quote": "Despite these advanced approaches, they still fall short on basic programming questions from the HumanEval and MBPP datasets.",
    "language": "Python",
    "language_quote": "æ–‡ä¸­ä»…æåŠå®éªŒè®¾ç½®ï¼ˆè¯„æµ‹æ–‡æœ¬åˆ°ä»£ç ç”Ÿæˆï¼‰ï¼Œæœªæè¿°å®Œæ•´æ•°æ®é›†çš„è¯­è¨€æ„æˆã€‚",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": "2021",
    "last_updated_quote": "HumanEval (Chen et al., 2021)",
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "HumanEval and MBPP are for text-to-code generation",
    "evaluation_metrics": "Pass@1",
    "evaluation_metrics_quote": "We compute Pass@1 accuracy with hidden test cases for assesment.",
    "input_modality": "è‡ªç„¶è¯­è¨€",
    "input_modality_quote": "the task description is a brief passage outlines the intended functionality of the program to be generated.",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "text-to-code generation",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "text-to-code generation",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": NaN,
    "unique_features_quote": NaN,
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2021,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['Pass@1']",
    "problem_domain_normalized": "[]",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "å…¥é—¨çº§",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2403.07974_output/content.md",
    "benchmark_name": "LiveCodeBench",
    "benchmark_name_quote": "In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "In this work, we propose LiveCodeBench",
    "dataset_url": "https://livecodebench.github.io/",
    "dataset_url_quote": "Website: https://livecodebench.github.io/",
    "task_description": "å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç ç›¸å…³èƒ½åŠ›è¿›è¡Œå…¨é¢è¯„ä¼°ï¼ŒåŒ…æ‹¬ä»£ç ç”Ÿæˆã€è‡ªæˆ‘ä¿®å¤ã€ä»£ç æ‰§è¡Œå’Œæµ‹è¯•è¾“å‡ºé¢„æµ‹ç­‰å¤šä¸ªæ–¹é¢ã€‚",
    "task_description_quote": "Notably, our benchmark also focuses on a broader range of code-related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation.",
    "dimension": "ä»£ç ç›¸å…³èƒ½åŠ›çš„å…¨é¢æ€§è¯„ä¼°ï¼ŒåŒ…æ‹¬ç”Ÿæˆã€ä¿®å¤ã€æ‰§è¡Œå’Œç†è§£ã€‚",
    "dimension_quote": "Holistic Evaluation. Current code evaluations primarily focus on natural language to code generation. However, programming is a multi-faceted task that requires a variety of capabilities beyond those measured by code generation.",
    "evaluation_method": "åŸºäºåŠŸèƒ½æ­£ç¡®æ€§ï¼Œä½¿ç”¨ä¸€ç»„æœªè§è¿‡çš„æµ‹è¯•ç”¨ä¾‹è¿›è¡Œè¯„ä¼°ã€‚å¯¹äºä»£ç ç”Ÿæˆåœºæ™¯ï¼Œä½¿ç”¨Pass@1æŒ‡æ ‡ã€‚",
    "evaluation_method_quote": "The evaluation is performed based on functional correctness, using a set of unseen test cases. We use the Pass@1 metric measured as the fraction of the problems for which the model was able to generate a program passing all tests.",
    "context_dependency": "å•é—®é¢˜è§£å†³ï¼Œä¸Šä¸‹æ–‡ä¸ºè‡ªç„¶è¯­è¨€é—®é¢˜æè¿°å’Œç¤ºä¾‹æµ‹è¯•ã€‚",
    "context_dependency_quote": "The model is given a problem statement, which includes a natural language description and example tests (input-output pairs), and is tasked with generating a correct solution.",
    "problem_domain": "ç«äº‰æ€§ç¼–ç¨‹ï¼ˆç®—æ³•é—®é¢˜ï¼‰ï¼Œæ¥æºäºLeetCodeã€AtCoderå’ŒCodeForcesç­‰ç«èµ›å¹³å°ã€‚",
    "problem_domain_quote": "which collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces.",
    "problem_difficulty": "å¹³è¡¡çš„éš¾åº¦åˆ†å¸ƒï¼ŒåŒ…å«ä»æ˜“åˆ°éš¾çš„é—®é¢˜ï¼Œå¹¶ä½¿ç”¨å¹³å°æä¾›çš„éš¾åº¦è¯„çº§è¿›è¡Œåˆ†ç±»å’Œè¿‡æ»¤ã€‚",
    "problem_difficulty_quote": "Therefore, we use problem difficulty ratings (sourced from the competition websites) for filtering the harder problems and classifying problem difficulties to ensure balanced problem difficulty distribution and allow granular model comparisons.",
    "language": "æ–‡ä¸­æœªæ˜ç¡®æåŠæ•°æ®é›†æ”¯æŒçš„å…·ä½“ç¼–ç¨‹è¯­è¨€ï¼Œä½†æ¥æºå¹³å°ï¼ˆLeetCode, AtCoder, CodeForcesï¼‰é€šå¸¸æ”¯æŒå¤šç§è¯­è¨€ã€‚",
    "language_quote": NaN,
    "data_size": "ç›®å‰åŒ…å«è¶…è¿‡500ä¸ªç¼–ç é—®é¢˜ï¼ˆ511ä¸ªï¼‰ï¼Œæ”¶é›†äº2023å¹´5æœˆè‡³2024å¹´5æœˆæœŸé—´ã€‚",
    "data_size_quote": "Currently, LiveCodeBench hosts over five hundred coding problems that were published between May 2023 and May 2024. / Particularly, we have collected 511 problems from contests across three competition platforms â€“ LeetCode, AtCoder, and CodeForces occurring from May 2023 to the present (May 2024)",
    "source_type": "ä»ä¸‰ä¸ªç«èµ›å¹³å°ï¼ˆLeetCodeã€AtCoderã€CodeForcesï¼‰çš„æ¯å‘¨æ¯”èµ›ä¸­æ”¶é›†çš„æ–°é—®é¢˜ã€‚",
    "source_type_quote": "Particularly, we collect problems from weekly contests on competition platforms and tag them with a release date.",
    "last_updated": "2024å¹´6æœˆ6æ—¥ï¼ˆè®ºæ–‡ç‰ˆæœ¬æ—¥æœŸï¼‰ï¼Œæ•°æ®é›†æŒç»­æ›´æ–°è‡³2024å¹´5æœˆã€‚",
    "last_updated_quote": "arXiv:2403.07974v2  [cs.SE]  6 Jun 2024",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œç”±ç ”ç©¶å›¢é˜Ÿä»å…¬å¼€ç«èµ›å¹³å°æ”¶é›†å’Œæ„å»ºã€‚",
    "build_type_quote": "In this work, we propose LiveCodeBench",
    "contamination_status": "æŠ—æ±¡æŸ“è®¾è®¡ï¼Œé€šè¿‡æŒç»­æ›´æ–°ï¼ˆä½¿ç”¨æ–°å‘å¸ƒçš„é—®é¢˜ï¼‰å’Œæ—¶é—´åˆ†æ®µè¯„ä¼°æ¥é˜²æ­¢æ•°æ®æ±¡æŸ“ã€‚",
    "contamination_status_quote": "Live updates to prevent contamination. / to prevent the risk of problem contamination, we use live updates, that is evaluate models on new problems.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆã€ä»£ç ä¿®å¤ã€ä»£ç æ‰§è¡Œã€æµ‹è¯•è¾“å‡ºé¢„æµ‹ã€‚",
    "task_granularity_quote": "Specifically, we evaluate code LLMs in four scenarios, namely code generation, self-repair, code execution, and test output prediction.",
    "evaluation_metrics": "Pass@1ï¼ˆç”¨äºä»£ç ç”Ÿæˆåœºæ™¯ï¼‰ã€‚",
    "evaluation_metrics_quote": "We use the Pass@1 metric measured as the fraction of the problems for which the model was able to generate a program passing all tests.",
    "input_modality": "è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°ã€ç¤ºä¾‹æµ‹è¯•ã€é”™è¯¯ç¨‹åºï¼ˆç”¨äºä¿®å¤ï¼‰ã€ä»£ç å’Œè¾“å…¥ï¼ˆç”¨äºæ‰§è¡Œï¼‰ã€‚",
    "input_modality_quote": "The model is given a problem statement, which includes a natural language description and example tests (input-output pairs) / The model is given the natural language problem description, the incorrect program, the test case it fails on, and the execution feedback from that failure. / The model is given a program and an input",
    "output_modality": "ä»£ç ï¼ˆç”¨äºç”Ÿæˆå’Œä¿®å¤ï¼‰ã€æ‰§è¡Œç»“æœï¼ˆç”¨äºæ‰§è¡Œï¼‰ã€æµ‹è¯•è¾“å‡ºï¼ˆç”¨äºé¢„æµ‹ï¼‰ã€‚",
    "output_modality_quote": "The output should be a correct repaired program. / the output should be the result. / the output should be the output for the problem.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ï¼ˆç”Ÿæˆï¼‰ã€ä»£ç ä¸åé¦ˆåˆ°ä»£ç ï¼ˆä¿®å¤ï¼‰ã€ä»£ç ä¸è¾“å…¥åˆ°ç»“æœï¼ˆæ‰§è¡Œï¼‰ã€æ–‡æœ¬ä¸è¾“å…¥åˆ°ç»“æœï¼ˆé¢„æµ‹ï¼‰ã€‚",
    "task_io_type_quote": "generating code from natural language / Fix an incorrect program from execution information / â€œExecuteâ€ a program on an input / Solve the natural language task on a specified input",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "1. æŒç»­æ›´æ–°ä»¥é˜²æ­¢æ•°æ®æ±¡æŸ“ã€‚2. å…¨é¢çš„è¯„ä¼°åœºæ™¯ï¼Œè¶…è¶Šå•ä¸€çš„ä»£ç ç”Ÿæˆã€‚3. æ¥æºäºé«˜è´¨é‡ç«èµ›å¹³å°çš„é—®é¢˜å’Œæµ‹è¯•ã€‚4. å¹³è¡¡çš„é—®é¢˜éš¾åº¦åˆ†å¸ƒã€‚",
    "unique_features_quote": "Live updates to prevent contamination. / Holistic Evaluation. / High-quality problems and tests. / Balanced problem difficulty.",
    "data_size_quantity": 511,
    "data_size_unit": "ä¸ªç¼–ç é—®é¢˜",
    "last_updated_year": 2024,
    "last_updated_month": 6,
    "last_updated_day": 6,
    "language_normalized": "[]",
    "dimension_normalized": "['ä»£ç ç›¸å…³èƒ½åŠ›çš„å…¨é¢æ€§è¯„ä¼°ï¼ŒåŒ…æ‹¬ç”Ÿæˆã€ä¿®å¤ã€æ‰§è¡Œå’Œç†è§£']",
    "evaluation_method_normalized": "['Pass@1ï¼ˆç”¨äºä»£ç ç”Ÿæˆåœºæ™¯ï¼‰']",
    "problem_domain_normalized": "['ç«äº‰æ€§ç¼–ç¨‹ï¼ˆç®—æ³•é—®é¢˜ï¼‰']",
    "source_type_normalized": "['ä»ä¸‰ä¸ªç«èµ›å¹³å°ï¼ˆLeetCodeã€AtCoderã€CodeForcesï¼‰çš„æ¯å‘¨æ¯”èµ›ä¸­æ”¶é›†çš„æ–°é—®é¢˜']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2403.08604_output/content.md",
    "benchmark_name": "DevEval",
    "benchmark_name_quote": "In this case study, we explore the performance of LLMs across the entire software development lifecycle with DevEval",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "To address these shortcomings and fill this gap, we present DevEval, a comprehensive case study that mirrors real-world software development.",
    "dataset_url": "https://github.com/open-compass/DevEval",
    "dataset_url_quote": "1Our data and code are available at https://github.com/open-compass/DevEval.",
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•´ä¸ªè½¯ä»¶å¼€å‘ç”Ÿå‘½å‘¨æœŸä¸­çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬ä»è¯¦ç»†çš„äº§å“éœ€æ±‚æ–‡æ¡£ï¼ˆPRDï¼‰å¼€å§‹ï¼Œæ„å»ºä¸€ä¸ªå¤šæ–‡ä»¶çš„ä»£ç åº“ã€‚",
    "task_description_quote": "DevEval generally evaluates models on the task of constructing a multi-file codebase starting from a product requirement document (PRD) of detailed specifications.",
    "dimension": "è½¯ä»¶å¼€å‘ç”Ÿå‘½å‘¨æœŸçš„å…¨é¢èƒ½åŠ›ï¼ŒåŒ…æ‹¬è½¯ä»¶è®¾è®¡ã€ç¯å¢ƒé…ç½®ã€å®ç°ã€éªŒæ”¶æµ‹è¯•å’Œå•å…ƒæµ‹è¯•ã€‚",
    "dimension_quote": "DevEval encompasses stages including software design, environment setup, implementation, acceptance testing, and unit testing.",
    "evaluation_method": "æ ¹æ®ä»»åŠ¡ä¸åŒé‡‡ç”¨å¤šç§æ–¹æ³•ï¼šè½¯ä»¶è®¾è®¡ä»»åŠ¡ä½¿ç”¨LLMä½œä¸ºè¯„åˆ¤è€…è¿›è¡Œä¸»è§‚è¯„ä¼°ï¼›ç¯å¢ƒé…ç½®ä»»åŠ¡è¯„ä¼°ä¾èµ–æ–‡ä»¶æ‰§è¡Œå’Œç¤ºä¾‹ä»£ç è¿è¡Œçš„æˆåŠŸç‡ï¼›å®ç°ä»»åŠ¡ä½¿ç”¨è‡ªåŠ¨åŒ–æµ‹è¯•æ¡†æ¶ï¼ˆå¦‚PyTest, GTest, JUnit, Jestï¼‰æ‰§è¡Œå‚è€ƒæµ‹è¯•å¹¶è®¡ç®—é€šè¿‡ç‡ï¼›æµ‹è¯•ä»»åŠ¡è¯„ä¼°æµ‹è¯•ä»£ç çš„å¯æ‰§è¡Œæ€§ï¼ˆOracle Testï¼‰å’Œä»£ç è¦†ç›–ç‡ã€‚",
    "evaluation_method_quote": "Since the Software Design tasks are open-ended, we employ the LLM-as-a-judge approach... The evaluation is anchored by two principal metrics: general principles and faithfulness. The principal metric for evaluation in this task is the success rate of the executed example code. The evaluation procedure involves executing reference acceptance and unit tests within a predefined reference environment. Then the evaluation metric is determined by the pass rate of these tests. ...models generally fail to generate executable tests, with oracle test scores falling below 40%... the generated testing code demonstrates potential in code coverage, achieving as high as 79.4% when it is executable.",
    "context_dependency": "å¤šæ–‡ä»¶é¡¹ç›®çº§ï¼Œéœ€è¦ç†è§£äº§å“éœ€æ±‚æ–‡æ¡£ã€UMLå›¾ã€æ¶æ„è®¾è®¡ç­‰å®Œæ•´ä¸Šä¸‹æ–‡ã€‚",
    "context_dependency_quote": "DevEval generally evaluates models on the task of constructing a multi-file codebase starting from a product requirement document (PRD) of detailed specifications.",
    "problem_domain": "æ¶µç›–å¤šä¸ªå·¥ç¨‹å’ŒAIé¢†åŸŸï¼ŒåŒ…æ‹¬æ•°æ®åº“åº”ç”¨ã€WebæœåŠ¡ã€ç®—æ³•å®ç°ã€APIå¼€å‘ã€ç¥ç»ç½‘ç»œã€è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰ã€‚",
    "problem_domain_quote": "DevEval covers a range of domains including NLP, computer vision, deep learning, algorithm implementation, API applications, Database applications, web service (both frontend and backend), and general tools and utilities.",
    "problem_difficulty": "å·¥ç¨‹çº§ï¼Œæ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„è½¯ä»¶å¼€å‘æŒ‘æˆ˜ï¼Œç°æœ‰é¡¶çº§æ¨¡å‹ï¼ˆå¦‚GPT-4ï¼‰å¾—åˆ†å¾ˆä½ã€‚",
    "problem_difficulty_quote": "Empirical studies show that current LLMs, including GPT-4, fail to solve the challenges presented within DevEval. GPT-4-Turbo achieves the highest scores amongst all evaluated models, yet it obtains less than 10% on our repository-level implementation task.",
    "language": "Python, C/C++, Java, JavaScript (Vue.js)",
    "language_quote": "DevEval features four programming languages... we curated a collection of 22 repositories across four programming languages (Python, C/C++, Java, JavaScript)",
    "data_size": "åŒ…å«22ä¸ªä»£ç ä»“åº“ï¼Œè¦†ç›–4ç§ç¼–ç¨‹è¯­è¨€å’Œå¤šä¸ªé¢†åŸŸã€‚å¹³å‡æ¯ä¸ªä»“åº“åŒ…å«çº¦2-7ä¸ªä»£ç æ–‡ä»¶ï¼Œ276-617è¡Œä»£ç ï¼Œä»¥åŠè‹¥å¹²éªŒæ”¶æµ‹è¯•å’Œå•å…ƒæµ‹è¯•ã€‚",
    "data_size_quote": "we curated a collection of 22 repositories across four programming languages... Table 2: DevEval Statistics. Avg. #Code File 2.2-7.0, Avg. #Code Line 276-617, Avg. #Accep. Tests 2-5.4, Avg. #Unit Tests 8.2-12.4",
    "source_type": "ä»å…¬å¼€ä»“åº“ä¸­ç²¾å¿ƒç­–åˆ’æ”¶é›†ï¼Œå¹¶è¡¥å……äº†å®Œæ•´è½¯ä»¶å¼€å‘æ‰€éœ€çš„è®¾è®¡æ–‡æ¡£å’Œæµ‹è¯•ç¨‹åºã€‚",
    "source_type_quote": "One significant challenge in this study lies in the scarcity of publicly available repositories that include the full range of software development artifacts, particularly design documents and comprehensive testing programs. To overcome this, we curated a collection of 22 repositories...",
    "last_updated": "2024",
    "last_updated_quote": "arXiv:2403.08604v3 [cs.CL] 14 Dec 2024",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œç”±ç ”ç©¶å›¢é˜Ÿä¸ºè¯„ä¼°ç›®çš„è€Œæ„å»ºã€‚",
    "build_type_quote": "we present DevEval, a comprehensive case study... To overcome this, we curated a collection...",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç åº“ç”Ÿæˆï¼Œæ¶‰åŠä»è®¾è®¡åˆ°æµ‹è¯•çš„å®Œæ•´è½¯ä»¶å¼€å‘æµç¨‹ã€‚",
    "task_granularity_quote": "DevEval generally evaluates models on the task of constructing a multi-file codebase starting from a product requirement document (PRD) of detailed specifications.",
    "evaluation_metrics": "è½¯ä»¶è®¾è®¡ï¼šåŸºäºé€šç”¨åŸåˆ™å’Œå¿ å®åº¦çš„LLMä¸»è§‚è¯„åˆ†ï¼›ç¯å¢ƒé…ç½®ï¼šç¤ºä¾‹ä»£ç æ‰§è¡ŒæˆåŠŸç‡ï¼›å®ç°ï¼šå‚è€ƒéªŒæ”¶å’Œå•å…ƒæµ‹è¯•çš„é€šè¿‡ç‡ï¼›æµ‹è¯•ï¼šOracle Teståˆ†æ•°å’Œä»£ç è¦†ç›–ç‡ã€‚",
    "evaluation_metrics_quote": "The evaluation is anchored by two principal metrics: general principles and faithfulness... The principal metric for evaluation in this task is the success rate of the executed example code... the evaluation metric is determined by the pass rate of these tests... oracle test scores... code coverage, achieving as high as 79.4%",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆäº§å“éœ€æ±‚æ–‡æ¡£PRDï¼‰ã€ç»“æ„åŒ–å›¾è¡¨ï¼ˆUMLå›¾ã€æ¶æ„è®¾è®¡ï¼‰ã€ä»£ç ã€‚",
    "input_modality_quote": "models are provided with the PRD, UML diagrams and architecture design... Table 1: Task design in DevEval. Input: PRD, UML Diagrams, Architecture Design, Implementation Code",
    "output_modality": "ä»£ç ï¼ˆä¾èµ–æ–‡ä»¶ã€å®ç°ä»£ç ã€æµ‹è¯•ä»£ç ï¼‰ã€ç»“æ„åŒ–è®¾è®¡æ–‡æ¡£ï¼ˆUMLå›¾ã€æ¶æ„è®¾è®¡ï¼‰ã€‚",
    "output_modality_quote": "Table 1: Task design in DevEval. Output: UML Diagrams, Architecture Design, Dependency Files, Implementation Code, Acceptance Testing Code, Unit Testing Code",
    "task_io_type": "æ–‡æœ¬ä¸å›¾è¡¨åˆ°ä»£ç ã€æ–‡æœ¬ä¸å›¾è¡¨åˆ°è®¾è®¡æ–‡æ¡£ã€‚",
    "task_io_type_quote": "DevEval generally evaluates models on the task of constructing a multi-file codebase starting from a product requirement document (PRD)... The first subtask involves the generation of the UML class diagram...",
    "execution_environment": "ä½¿ç”¨Dockerå®šä¹‰çš„åŸºç¡€ç¯å¢ƒï¼Œé’ˆå¯¹ä¸åŒè¯­è¨€ä½¿ç”¨ç‰¹å®šå·¥å…·ï¼ˆConda, Gradle, NPMï¼‰å’Œæµ‹è¯•æ¡†æ¶ï¼ˆPyTest, GTest, JUnit, Jestï¼‰ã€‚",
    "execution_environment_quote": "The evaluation centers on the execution of dependency files across each programming language within a predetermined base environment delineated in a Docker file... For Python, the Conda environment manager is employed; for Java and JavaScript, Gradle and NPM are utilized respectively... integrates PyTest for Python, GTest for C++, JUnit for Java, and Jest for JavaScript.",
    "unique_features": "é¦–ä¸ªè¯„ä¼°æ¨¡å‹è½¯ä»¶è®¾è®¡å’Œç¯å¢ƒé…ç½®èƒ½åŠ›çš„åŸºå‡†ï¼›éµå¾ªç€‘å¸ƒæ¨¡å‹ï¼Œå°†è½¯ä»¶å¼€å‘åˆ†è§£ä¸ºå¤šä¸ªç›¸äº’å…³è”çš„é˜¶æ®µï¼›æä¾›æ–‡æ¡£çº§åˆ«çš„è¯¦ç»†éœ€æ±‚æè¿°ï¼›é‡‡ç”¨æ¨¡å—åŒ–è¯„ä¼°åè®®ï¼Œæ”¯æŒç«¯åˆ°ç«¯æˆ–åˆ†é˜¶æ®µè¯„ä¼°ã€‚",
    "unique_features_quote": "DevEval is the first to evaluate modelsâ€™ software design and environment setup capabilities. Subscribing to the traditional Waterfall software development model... DevEval breaks down this process into a diverse set of inter-related development stages... In contrast to similar systems... which generate outputs from brief requirement descriptions typically under 100 words, DevEval offers document-level detail to guide the models. ...our design utilizes reference inputs for each task. This strategy enables and concentrates on evaluating the efficacy of models in executing specific tasks.",
    "data_size_quantity": 22,
    "data_size_unit": "ä¸ªä»£ç ä»“åº“",
    "last_updated_year": 2024,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python', 'C/C++', 'Java', 'JavaScript (Vue.js)']",
    "dimension_normalized": "['è½¯ä»¶å¼€å‘ç”Ÿå‘½å‘¨æœŸçš„å…¨é¢èƒ½åŠ›', 'è½¯ä»¶è®¾è®¡', 'ç¯å¢ƒé…ç½®', 'å®ç°', 'éªŒæ”¶æµ‹è¯•', 'å•å…ƒæµ‹è¯•']",
    "evaluation_method_normalized": "['è½¯ä»¶è®¾è®¡ï¼šåŸºäºé€šç”¨åŸåˆ™å’Œå¿ å®åº¦çš„LLMä¸»è§‚è¯„åˆ†', 'ç¯å¢ƒé…ç½®ï¼šç¤ºä¾‹ä»£ç æ‰§è¡ŒæˆåŠŸç‡', 'å®ç°ï¼šå‚è€ƒéªŒæ”¶å’Œå•å…ƒæµ‹è¯•çš„é€šè¿‡ç‡', 'æµ‹è¯•ï¼šOracle Teståˆ†æ•°', 'ä»£ç è¦†ç›–ç‡']",
    "problem_domain_normalized": "['æ¶µç›–å¤šä¸ªå·¥ç¨‹å’ŒAIé¢†åŸŸ', 'æ•°æ®åº“åº”ç”¨', 'WebæœåŠ¡', 'ç®—æ³•å®ç°', 'APIå¼€å‘', 'ç¥ç»ç½‘ç»œ', 'è®¡ç®—æœºè§†è§‰', 'è‡ªç„¶è¯­è¨€å¤„ç†']",
    "source_type_normalized": "['ä»å…¬å¼€ä»“åº“ä¸­ç²¾å¿ƒç­–åˆ’æ”¶é›†', 'è¡¥å……äº†å®Œæ•´è½¯ä»¶å¼€å‘æ‰€éœ€çš„è®¾è®¡æ–‡æ¡£å’Œæµ‹è¯•ç¨‹åº']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "å›¾æ–‡åˆ°ä»£ç ",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2406.06887_output/content.md",
    "benchmark_name": "HumanEval, MBPP, LiveCodeBench, LeetCode",
    "benchmark_name_quote": "PLUM is evaluated on both standard benchmarks (HumanEval, MBPP) and more challenging ones (LiveCodeBench)... We evaluate PLUM on a diverse set of state-of-the-art code language models under different set-ups, on commonly used evaluation benchmarks: HumanEval(+) and MBPP(+) [7, 1, 28] as well as more challenging code generation datasets like LiveCodeBench and LeetCode [22, 19].",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "PLUM is evaluated on both standard benchmarks (HumanEval, MBPP) and more challenging ones (LiveCodeBench)... We evaluate PLUM on a diverse set of state-of-the-art code language models under different set-ups, on commonly used evaluation benchmarks: HumanEval(+) and MBPP(+) [7, 1, 28] as well as more challenging code generation datasets like LiveCodeBench and LeetCode [22, 19].",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "æ–‡ä¸­æœªæè¿°è¿™äº›åŸºå‡†çš„åŸå§‹ä»»åŠ¡å®šä¹‰ï¼Œä»…æåŠå®ƒä»¬æ˜¯ç”¨äºè¯„ä¼°ä»£ç ç”Ÿæˆæ¨¡å‹çš„å¸¸ç”¨åŸºå‡†ã€‚",
    "task_description_quote": "PLUM is evaluated on both standard benchmarks (HumanEval, MBPP) and more challenging ones (LiveCodeBench)... We evaluate PLUM on a diverse set of state-of-the-art code language models under different set-ups, on commonly used evaluation benchmarks: HumanEval(+) and MBPP(+) [7, 1, 28] as well as more challenging code generation datasets like LiveCodeBench and LeetCode [22, 19].",
    "dimension": "ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "Works like AlphaCode [26] and LeTI [48] have introduced test outcomes as a means to define functional correctness in code generation.",
    "evaluation_method": "ä½¿ç”¨æµ‹è¯•ç”¨ä¾‹æ‰§è¡Œæ¥è¯„ä¼°åŠŸèƒ½æ­£ç¡®æ€§",
    "evaluation_method_quote": "Works like AlphaCode [26] and LeTI [48] have introduced test outcomes as a means to define functional correctness in code generation.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "é€šç”¨ç¼–ç¨‹é—®é¢˜",
    "problem_domain_quote": "These datasets provide a diverse range of programming tasks and instructions.",
    "problem_difficulty": "æ ‡å‡†åŸºå‡†å’Œæ›´å…·æŒ‘æˆ˜æ€§çš„åŸºå‡†",
    "problem_difficulty_quote": "PLUM is evaluated on both standard benchmarks (HumanEval, MBPP) and more challenging ones (LiveCodeBench)... as well as more challenging code generation datasets like LiveCodeBench and LeetCode [22, 19].",
    "language": "æ–‡ä¸­ä»…æåŠå®éªŒèšç„¦äºPythonï¼Œæœªæè¿°å®Œæ•´æ•°æ®é›†çš„è¯­è¨€è¦†ç›–èŒƒå›´ã€‚",
    "language_quote": "We focus on Python due to its wide use and the availability of well-established training and evaluation resources.",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "Language models pre-trained on code corpora have excelled at code generation [40, 25].",
    "evaluation_metrics": "é€šè¿‡ç‡ (pass rates)",
    "evaluation_metrics_quote": "PLUM increases pass rates by up to 4.8% on average on standard benchmarks and 11.8% on LiveCodeBench, demonstrating its effectiveness and generalizability.",
    "input_modality": "è‡ªç„¶è¯­è¨€æŒ‡ä»¤",
    "input_modality_quote": "PLUM utilizes natural language instructions from well-established datasets such as OSS-Instruct [49], Evol-Instruct-Code [31], and ShareGPT [8].",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "These solutions are evaluated using the generated test cases, with preference labels assigned based on the results: solutions passing the tests are preferred, while failures are dis-preferred.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "PLUM utilizes natural language instructions from well-established datasets... These solutions are evaluated using the generated test cases...",
    "execution_environment": "æ–‡ä¸­æœªæ˜ç¡®æè¿°åŸºå‡†çš„æ‰§è¡Œç¯å¢ƒï¼Œä½†æåŠäº†æ‰§è¡Œæ£€æŸ¥ã€‚",
    "execution_environment_quote": "With static and execution checks,3 we identify and filter out solutions that contain syntactic errors and fail to execute, as our focus is on functional correctness.",
    "unique_features": "æœ¬æ–‡æå‡ºçš„PLUMæ¡†æ¶æœ¬èº«æ˜¯ä¸€ä¸ªä½¿ç”¨æµ‹è¯•ç”¨ä¾‹è¿›è¡Œæ‰§è¡Œå¼•å¯¼ã€åŸºäºç­–ç•¥çš„åå¥½å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºæ”¹è¿›ä»£ç è¯­è¨€æ¨¡å‹ã€‚å®ƒå¹¶éä¸€ä¸ªæ•°æ®é›†ã€‚",
    "unique_features_quote": "we propose PLUM, an on-policy Preference Learning framework Augmented with test cases for code LMs.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['é€šè¿‡ç‡']",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹é—®é¢˜']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2406.18294_output/content.md",
    "benchmark_name": "CrossCodeEval",
    "benchmark_name_quote": "To assess the code completion performance of Code LLMs in real development scenarios, we utilized CrossCodeEval (Ding et al., 2023) as the evaluation dataset.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "To assess the code completion performance of Code LLMs in real development scenarios, we utilized CrossCodeEval (Ding et al., 2023) as the evaluation dataset.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°ä»£ç å¤§è¯­è¨€æ¨¡å‹åœ¨çœŸå®å¼€å‘åœºæ™¯ä¸­çš„ä»£ç è¡¥å…¨æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯éœ€è¦åˆ©ç”¨è·¨æ–‡ä»¶ä»£ç ä¿¡æ¯è¿›è¡Œè¡¥å…¨çš„ä»»åŠ¡ã€‚",
    "task_description_quote": "To assess the code completion performance of Code LLMs in real development scenarios... The CrossCodeEval (Ding et al., 2023) benchmark provides test cases that require the use of cross-file code information for completion.",
    "dimension": "ä»“åº“çº§ä»£ç è¡¥å…¨èƒ½åŠ›ï¼Œåˆ©ç”¨è·¨æ–‡ä»¶ä¿¡æ¯çš„èƒ½åŠ›",
    "dimension_quote": "Some benchmarks for repository-level code completion have been proposed to evaluate the performance of code models in real-world completion tasks, such as CrossCodeEval (Ding et al., 2023)...",
    "evaluation_method": "ç²¾ç¡®åŒ¹é…ï¼ˆExact Match, EMï¼‰å’Œç¼–è¾‘ç›¸ä¼¼åº¦ï¼ˆEdit Similarity, ESï¼‰",
    "evaluation_method_quote": "Following the CrossCodeEval evaluation protocol, we evaluated the completion results using two metrics: Exact Match (EM) and Edit Similarity (ES).",
    "context_dependency": "è·¨æ–‡ä»¶ã€ä»“åº“çº§ä¸Šä¸‹æ–‡",
    "context_dependency_quote": "The CrossCodeEval (Ding et al., 2023) benchmark provides test cases that require the use of cross-file code information for completion.",
    "problem_domain": NaN,
    "problem_domain_quote": NaN,
    "problem_difficulty": "çœŸå®ä¸–ç•Œå¼€å‘åœºæ™¯",
    "problem_difficulty_quote": "To assess the code completion performance of Code LLMs in real development scenarios...",
    "language": "æ–‡ä¸­ä»…æåŠå®éªŒå­é›†(Python)ï¼Œæœªæè¿°å®Œæ•´æ•°æ®é›†",
    "language_quote": "Without loss of generality, in this study, we have chosen Python language as the primary language for our research.",
    "data_size": "æœ¬æ–‡å®éªŒä½¿ç”¨äº†2,655ä¸ªçœŸå®ä¸–ç•Œè¡¥å…¨æµ‹è¯•ç”¨ä¾‹",
    "data_size_quote": "Ultimately, we obtained 2,655 real-world completion tests.",
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": "2023",
    "last_updated_quote": "CrossCodeEval (Ding et al., 2023)",
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç è¡¥å…¨ï¼ˆåŒ…æ‹¬ä¸­æ®µå¡«å……ï¼‰",
    "task_granularity_quote": "Infilling scenarios constitute the majority of code completion tasks in the real world.",
    "evaluation_metrics": "Exact Match (EM), Edit Similarity (ES)",
    "evaluation_metrics_quote": "Following the CrossCodeEval evaluation protocol, we evaluated the completion results using two metrics: Exact Match (EM) and Edit Similarity (ES).",
    "input_modality": "ä»£ç ï¼ˆåŒ…å«éƒ¨åˆ†ç¼ºå¤±çš„ä»£ç ä¸Šä¸‹æ–‡ï¼‰",
    "input_modality_quote": "We then removed the code after the cursor in that line to form authentic completion test cases.",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "The completion results are less than satisfactory.",
    "task_io_type": "ä»£ç åˆ°ä»£ç ï¼ˆè¡¥å…¨ï¼‰",
    "task_io_type_quote": "Infilling scenarios constitute the majority of code completion tasks in the real world.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºè¯„ä¼°æ¨¡å‹åˆ©ç”¨è·¨æ–‡ä»¶ä»£ç ä¿¡æ¯è¿›è¡Œä»“åº“çº§ä»£ç è¡¥å…¨çš„èƒ½åŠ›ï¼Œæµ‹è¯•ç”¨ä¾‹éœ€è¦è·¨æ–‡ä»¶ä¿¡æ¯ã€‚",
    "unique_features_quote": "The CrossCodeEval (Ding et al., 2023) benchmark provides test cases that require the use of cross-file code information for completion.",
    "data_size_quantity": 2655,
    "data_size_unit": "ä¸ª",
    "last_updated_year": 2023,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»“åº“çº§ä»£ç è¡¥å…¨èƒ½åŠ›', 'åˆ©ç”¨è·¨æ–‡ä»¶ä¿¡æ¯çš„èƒ½åŠ›']",
    "evaluation_method_normalized": "['Exact Match (EM)', 'Edit Similarity (ES)']",
    "problem_domain_normalized": "[]",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç è¡¥å…¨",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2408.06450_output/content.md",
    "benchmark_name": "EVALPERF",
    "benchmark_name_quote": "As a proof of concept, we use DPE to create EVALPERF, a benchmark with 121 performance-challenging coding tasks.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We introduce Differential Performance Evaluation (DPE), a framework designed to reliably evaluate Large Language Models (LLMs) for efficient code generation. ... As a proof of concept, we use DPE to create EVALPERF, a benchmark with 121 performance-challenging coding tasks.",
    "dataset_url": "github.com/evalplus/evalplus",
    "dataset_url_quote": "We also fully open-source and maintain the data curation pipeline and evaluator at github.com/evalplus/evalplus as part of EvalPlus.",
    "task_description": "è¯„æµ‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆçš„ä»£ç çš„æ•ˆç‡ã€‚è¯¥åŸºå‡†æ—¨åœ¨é€šè¿‡æä¾›å…·æœ‰æ€§èƒ½æŒ‘æˆ˜æ€§çš„ç¼–ç¨‹ä»»åŠ¡å’Œæœ‰æ•ˆçš„å¤åˆæŒ‡æ ‡ï¼Œå¯é åœ°è¯„ä¼°ä»£ç æ•ˆç‡ï¼Œå¼¥è¡¥ä¼ ç»Ÿä»£ç åŸºå‡†åœ¨æ•ˆç‡è¯„ä¼°æ–¹é¢çš„ä¸è¶³ã€‚",
    "task_description_quote": "We introduce Differential Performance Evaluation (DPE), a framework designed to reliably evaluate Large Language Models (LLMs) for efficient code generation. Traditional coding benchmarks often fail to provide reliable insights into code efficiency, due to their reliance on simplistic test inputs and the absence of effective compound metrics. DPE addresses these issues by focusing on efficiency-demanding programming tasks and establishing an insightful compound metric for performance evaluation.",
    "dimension": "ä»£ç ç”Ÿæˆæ•ˆç‡",
    "dimension_quote": "While the correctness evaluation of code generation has been well studied, we deliver a new and important aspect to the community by studying the data curation and assessment for the efficiency evaluation of LLM-generated code.",
    "evaluation_method": "å·®åˆ†æ€§èƒ½è¯„ä¼°ï¼ˆDPEï¼‰ã€‚è¯¥æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š1ï¼‰æ•°æ®é›†æ„å»ºï¼šä»ç°æœ‰åŸºå‡†ä¸­é€‰æ‹©å¯¹æ•ˆç‡æœ‰è¦æ±‚çš„ä»»åŠ¡ï¼Œå¹¶ç”Ÿæˆè®¡ç®—å¯†é›†å‹çš„è¾“å…¥æ¥æµ‹è¯•LLMè§£å†³æ–¹æ¡ˆçš„æ•ˆç‡ã€‚2ï¼‰æ•ˆç‡è¯„ä¼°ï¼šåˆ†ææ–°è§£å†³æ–¹æ¡ˆçš„æ€§èƒ½ï¼Œå¹¶å°†å…¶ä¸ä¸€ç»„å…·æœ‰ä¸åŒæ•ˆç‡æ°´å¹³çš„å‚è€ƒè§£å†³æ–¹æ¡ˆè¿›è¡Œå…¨å±€æ¯”è¾ƒï¼ŒåŒ¹é…åˆ°çš„æ•ˆç‡æ°´å¹³å†³å®šäº†å…¶æ•ˆç‡å¾—åˆ†ã€‚",
    "evaluation_method_quote": "DPE operates in two phases: To curate efficiency datasets, it selects efficiency-demanding tasks from existing coding benchmarks and generates computationally expensive inputs to stress the efficiency of LLM solutions. To assess the code efficiency, DPE profiles the new solution and compares it globally against a set of reference solutions that exhibit distinct efficiency levels, where the matched level defines its efficiency score.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "é€šç”¨ç¼–ç¨‹ä»»åŠ¡ï¼Œæ¶µç›–ç®—æ³•ã€æ•°æ®ç»“æ„ç­‰",
    "problem_domain_quote": "At the high level, the input to the dataset creation phase is a set of programming tasks, e.g., from HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021).",
    "problem_difficulty": "æ€§èƒ½æŒ‘æˆ˜å‹ä»»åŠ¡ï¼Œæ—¨åœ¨åŒºåˆ†ä¸åŒä»£ç è§£å†³æ–¹æ¡ˆçš„æ•ˆç‡",
    "problem_difficulty_quote": "DPE curates performance-demanding coding tasks by sampling synthesized test input generators and using filters to ensure evaluator quality.",
    "language": "Pythonï¼ˆä»HumanEvalå’ŒMBPPç­‰åŸºå‡†ä¸­é€‰å–ä»»åŠ¡ï¼Œè¿™äº›åŸºå‡†ä¸»è¦ä½¿ç”¨Pythonï¼‰",
    "language_quote": "At the high level, the input to the dataset creation phase is a set of programming tasks, e.g., from HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021).",
    "data_size": "åŒ…å«121ä¸ªæ€§èƒ½æŒ‘æˆ˜å‹ç¼–ç¨‹ä»»åŠ¡",
    "data_size_quote": "As a proof of concept, we use DPE to create EVALPERF, a benchmark with 121 performance-challenging coding tasks.",
    "source_type": "ä»ç°æœ‰ä»£ç ç”ŸæˆåŸºå‡†ï¼ˆå¦‚HumanEvalã€MBPPï¼‰ä¸­ç­›é€‰å’Œæ”¹é€ ä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨LLMåˆæˆæ€§èƒ½æµ‹è¯•è¾“å…¥ç”Ÿæˆå™¨æ¥ç”Ÿæˆè®¡ç®—å¯†é›†å‹æµ‹è¯•è¾“å…¥ã€‚",
    "source_type_quote": "At the high level, the input to the dataset creation phase is a set of programming tasks, e.g., from HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021). ... To this end, we propose Synthesizing a Synthesizer (SAS) to automatically produce performance-exercising inputs of different programming tasks by prompting powerful code LLMs to generate test generators.",
    "last_updated": "2024.08",
    "last_updated_quote": "arXiv:2408.06450v1  [cs.SE]  12 Aug 2024",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ŒåŸºäºDPEæ¡†æ¶ä»ç°æœ‰åŸºå‡†ä¸­ç­›é€‰å’Œå¢å¼º",
    "build_type_quote": "As a proof of concept, we use DPE to create EVALPERF, a benchmark with 121 performance-challenging coding tasks.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆä»è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆå®Œæ•´çš„å‡½æ•°/è§£å†³æ–¹æ¡ˆï¼‰",
    "task_granularity_quote": "Given a programming task, we collect a rich set of correct solutions by sampling various LLMs and test execution.",
    "evaluation_metrics": "æ•ˆç‡å¾—åˆ†ï¼ˆåŸºäºä¸å‚è€ƒè§£å†³æ–¹æ¡ˆæ€§èƒ½é›†ç¾¤çš„åŒ¹é…åº¦ï¼‰",
    "evaluation_metrics_quote": "During evaluation, if passing the correctness tests, the new solutions are profiled to compare against the reference solutions. Specifically, from slow to fast, the cumulative ratio of the cluster that includes the matched reference solution is the efficiency score of the evaluated solution.",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆä»»åŠ¡æè¿°ï¼‰",
    "input_modality_quote": "Given a coding instruction in natural language, LLMs produce solutions...",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "LLMs produce solutions whose correctness is assessed through test execution.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "Given a coding instruction in natural language, LLMs produce solutions...",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "1. ä¸“æ³¨äºä»£ç æ•ˆç‡è¯„ä¼°ï¼Œè€Œéä¼ ç»Ÿçš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚2. ä½¿ç”¨DPEæ¡†æ¶ï¼ŒåŒ…å«â€œåˆæˆä¸€ä¸ªåˆæˆå™¨â€ï¼ˆSASï¼‰æ–¹æ³•æ¥è‡ªåŠ¨ç”Ÿæˆæ€§èƒ½æµ‹è¯•è¾“å…¥ã€‚3. é€šè¿‡æ€§èƒ½èšç±»å»ºç«‹å‚è€ƒè§£å†³æ–¹æ¡ˆé›†ï¼Œç”¨äºå…¨å±€æ¯”è¾ƒå’Œè¯„åˆ†ã€‚4. åŒ…å«ä»»åŠ¡ç­›é€‰æ ‡å‡†ï¼ˆè¶³å¤Ÿçš„è®¡ç®—é‡ã€ä½æ€§èƒ½å˜å¼‚ã€æ€§èƒ½å¤šæ ·æ€§ï¼‰ä»¥ç¡®ä¿è¯„ä¼°è´¨é‡ã€‚",
    "unique_features_quote": "DPE addresses these issues by focusing on efficiency-demanding programming tasks and establishing an insightful compound metric for performance evaluation. ... To this end, we propose Synthesizing a Synthesizer (SAS) to automatically produce performance-exercising inputs of different programming tasks by prompting powerful code LLMs to generate test generators. ... a selected programming task must meet the following criteria: 1. Sufficient computation ... 2. Low performance variation ... 3. Performance diversity...",
    "data_size_quantity": 121,
    "data_size_unit": "ä¸ª",
    "last_updated_year": 2024,
    "last_updated_month": 8,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆæ•ˆç‡']",
    "evaluation_method_normalized": "['æ•ˆç‡å¾—åˆ†']",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹ä»»åŠ¡', 'ç®—æ³•', 'æ•°æ®ç»“æ„']",
    "source_type_normalized": "['ç°æœ‰ä»£ç ç”ŸæˆåŸºå‡†', 'HumanEval', 'MBPP', 'LLMåˆæˆæ€§èƒ½æµ‹è¯•è¾“å…¥ç”Ÿæˆå™¨']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2410.01215_output/content.md",
    "benchmark_name": "HumanEval",
    "benchmark_name_quote": "MGDebugger elevates accuracy from 75.6% to 94.5% on HumanEval [7] and achieves a remarkable 97.6% repair success rate on HumanEvalFix [44].",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Extensive experiments across multiple models and benchmarks demonstrate that MGDebugger significantly outperforms existing debugging methods. Using open-source models like DeepSeek-Coder-V2-Lite, CodeQwen1.5, and Codestral, MGDebugger elevates accuracy from 75.6% to 94.5% on HumanEval [7] and achieves a remarkable 97.6% repair success rate on HumanEvalFix [44].",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "æ–‡ä¸­ä»…æåŠå®éªŒä½¿ç”¨ï¼Œæœªæè¿°å®Œæ•´æ•°æ®é›†ã€‚æ ¹æ®å¼•ç”¨[7]ï¼ˆHumanEvalï¼‰æ¨æ–­ï¼Œè¯¥åŸºå‡†æ—¨åœ¨è¯„ä¼°ä»è‡ªç„¶è¯­è¨€æè¿°ï¼ˆdocstringï¼‰ç”ŸæˆPythonä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚",
    "task_description_quote": "Following the problem settings from Chen et al. [9] and Zhong et al. [75], we assume that the public test cases for the main function Tpub have been provided, which is common in most code generation benchmarks [7, 14, 44]2.",
    "dimension": "ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "MGDebugger elevates accuracy from 75.6% to 94.5% on HumanEval [7] and achieves a remarkable 97.6% repair success rate on HumanEvalFix [44].",
    "evaluation_method": "æ‰§è¡Œå•å…ƒæµ‹è¯•ï¼ˆpass@kï¼‰",
    "evaluation_method_quote": "Following the problem settings from Chen et al. [9] and Zhong et al. [75], we assume that the public test cases for the main function Tpub have been provided, which is common in most code generation benchmarks [7, 14, 44]2.",
    "context_dependency": "å•å‡½æ•°",
    "context_dependency_quote": "Specifically, given an LLM-generated function ğ‘“, we decompose it into a hierarchical structure of subfunctions denoted as (ğ‘“1, ..., ğ‘“ğ‘›).",
    "problem_domain": NaN,
    "problem_domain_quote": NaN,
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "Python",
    "language_quote": "To effectively test each subfunction, we propose an LLM-simulated Python executor, which traces code execution and tracks important variable states to pinpoint errors accurately.",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "Large language models (LLMs) such as GPT-4 [48], LLaMA [56], and DeepSeek-Coder [1] have made significant advances in AI-assisted coding tasks [7, 25, 34, 52]. Trained on vast corpora of text and code, LLMs can understand and generate code snippets for various programming tasks, ranging from simple data structures to complex algorithmic problems [34, 57].",
    "evaluation_metrics": "å‡†ç¡®ç‡ï¼ˆaccuracyï¼‰ï¼Œä¿®å¤æˆåŠŸç‡ï¼ˆrepair success rateï¼‰",
    "evaluation_metrics_quote": "MGDebugger elevates accuracy from 75.6% to 94.5% on HumanEval [7] and achieves a remarkable 97.6% repair success rate on HumanEvalFix [44].",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆå‡½æ•°æè¿°/docstringï¼‰",
    "input_modality_quote": "Following the problem settings from Chen et al. [9] and Zhong et al. [75], we assume that the public test cases for the main function Tpub have been provided, which is common in most code generation benchmarks [7, 14, 44]2.",
    "output_modality": "ä»£ç ï¼ˆPythonå‡½æ•°ï¼‰",
    "output_modality_quote": "Specifically, given an LLM-generated function ğ‘“, we decompose it into a hierarchical structure of subfunctions denoted as (ğ‘“1, ..., ğ‘“ğ‘›).",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "Following the problem settings from Chen et al. [9] and Zhong et al. [75], we assume that the public test cases for the main function Tpub have been provided, which is common in most code generation benchmarks [7, 14, 44]2.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "æœ¬æ–‡æœªæè¿°HumanEvalçš„ç‹¬ç‰¹ä¹‹å¤„ï¼Œè€Œæ˜¯å°†å…¶ä½œä¸ºè¯„ä¼°æ¨¡å‹è°ƒè¯•èƒ½åŠ›çš„æ ‡å‡†åŸºå‡†ä¹‹ä¸€ã€‚",
    "unique_features_quote": "Extensive experiments across multiple models and benchmarks demonstrate that MGDebugger significantly outperforms existing debugging methods. Using open-source models like DeepSeek-Coder-V2-Lite, CodeQwen1.5, and Codestral, MGDebugger elevates accuracy from 75.6% to 94.5% on HumanEval [7] and achieves a remarkable 97.6% repair success rate on HumanEvalFix [44].",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['å‡†ç¡®ç‡ï¼ˆaccuracyï¼‰', 'ä¿®å¤æˆåŠŸç‡ï¼ˆrepair success rateï¼‰']",
    "problem_domain_normalized": "[]",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2411.05830_output/content.md",
    "benchmark_name": "GitChameleon",
    "benchmark_name_quote": "To address this gap, we introduce GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "To address this gap, we introduce GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests.",
    "dataset_url": "https://github.com/NizarIslah/GitChameleon",
    "dataset_url_quote": "For facilitation for further exploration of version-conditioned code generation, we make our code repository publicly accessible at https://github.com/NizarIslah/GitChameleon.",
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆç‰¹å®šç‰ˆæœ¬åº“ä»£ç çš„èƒ½åŠ›ï¼Œè¦æ±‚ç”Ÿæˆçš„ä»£ç ä¸ä»…è¯­æ³•æ­£ç¡®ï¼Œè€Œä¸”åœ¨æ‰§è¡Œæ—¶åŠŸèƒ½å‡†ç¡®ã€‚",
    "task_description_quote": "GitChameleon is designed to rigorously assess the ability of modern large language models (LLMs) to generate version-specific code that is not only syntactically correct but also functionally accurate upon execution.",
    "dimension": "ç‰ˆæœ¬ç‰¹å®šä»£ç ç”Ÿæˆèƒ½åŠ›ã€ä»£ç åº“åŠ¨æ€é€‚åº”æ€§ã€åŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "By providing an execution-based benchmark that emphasizes the dynamic nature of code libraries, GitChameleon serves as a critical tool to advance the development of more adaptable and reliable code generation models.",
    "evaluation_method": "åŸºäºæ‰§è¡Œçš„è¯„ä¼°ï¼Œä½¿ç”¨æ‰‹å†™çš„åŸºäºæ–­è¨€çš„å•å…ƒæµ‹è¯•æ¥éªŒè¯ç”Ÿæˆä»£ç çš„æ­£ç¡®æ€§ã€‚",
    "evaluation_method_quote": "To evaluate LLM performance on GitChameleon, each problem is accompanied by handwritten assertion-based unit tests, enabling a thorough execution-based assessment of the outputs generated by the code LLMs.",
    "context_dependency": "å•å‡½æ•°ä»£ç è¡¥å…¨",
    "context_dependency_quote": "GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems",
    "problem_domain": "Pythonç¼–ç¨‹ï¼Œæ¶‰åŠå¤šä¸ªæµè¡Œçš„æœºå™¨å­¦ä¹ å’Œæ•°æ®å¤„ç†åº“ï¼ˆå¦‚PyTorch, NumPy, Scikit-Learn, Pandasç­‰ï¼‰",
    "problem_domain_quote": "GitChameleon consists of 116 python-based version conditioned problems based on 11 libraries: PyTorch (Paszke et al., 2019), Geopandas (Jordahl et al., 2020), NLTK (Bird & Loper, 2004), NetworkX (Hagberg et al., 2008), GeoPy3, Gradio (Abid et al., 2019), Scikit-Learn (Buitinck et al., 2013), Matplotlib (Hunter, 2007), PyCaret4, Pandas (pandas development team, 2020; Wes McKinney, 2010) and NumPy (Harris et al., 2020).",
    "problem_difficulty": "åŸºäºçœŸå®ç‰ˆæœ¬å˜åŒ–çš„å®é™…é—®é¢˜ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿå¼€å‘è€…åœ¨æŠ€æœ¯å€ºåŠ¡çº¦æŸä¸‹çš„çœŸå®åœºæ™¯ã€‚",
    "problem_difficulty_quote": "our dataset offers a unique and complementary perspective by focusing on the real-world scenario where developers are often constrained to specific library versions due to technical debt.",
    "language": "Python",
    "language_quote": "GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems",
    "data_size": "åŒ…å«116ä¸ªPythonç‰ˆæœ¬æ¡ä»¶é—®é¢˜",
    "data_size_quote": "GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems",
    "source_type": "æ‰‹å·¥ç¼–å†™å’ŒLLMè¾…åŠ©ï¼ŒåŸºäºçœŸå®åº“çš„å˜æ›´æ—¥å¿—ï¼ˆchangelogsï¼‰",
    "source_type_quote": "The examples were manually crafted by the authors, who divided the task among themselves. We compiled a list of popular Python libraries, focusing on those with which at least one author was familiar and that had detailed changelogs documenting changes between versions.",
    "last_updated": "2024-11-05 (æ ¹æ®arXivç‰ˆæœ¬v1æ—¥æœŸ)",
    "last_updated_quote": "arXiv:2411.05830v1  [cs.SE]  5 Nov 2024",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆä½œè€…æ‰‹å·¥ç¼–å†™ï¼‰",
    "build_type_quote": "The examples were manually crafted by the authors, who divided the task among themselves.",
    "contamination_status": "å·²è€ƒè™‘è®­ç»ƒæ•°æ®æˆªæ­¢æ—¥æœŸï¼Œç¡®ä¿æ ·æœ¬åœ¨æ¨¡å‹è®­ç»ƒçª—å£æœŸå†…ï¼Œä»¥è¯„ä¼°æ¨¡å‹å¯¹å·²è®­ç»ƒç‰ˆæœ¬çš„çŸ¥è¯†ã€‚",
    "contamination_status_quote": "Since some of the models evaluated on GitChameleon have disclosed their training data cutoff dates, we have ensured that most, if not all, samples fall within the training window of these models.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç è¡¥å…¨",
    "task_granularity_quote": "GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems",
    "evaluation_metrics": "pass@k (ä¾‹å¦‚pass@10)",
    "evaluation_metrics_quote": "for instance, GPT-4o achieves a pass@10 of only 39.9% (43.7% when provided with error feedback)",
    "input_modality": "è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°å’Œèµ·å§‹ä»£ç ",
    "input_modality_quote": "The problem statements average 20.4 tokens, and the starter code averages 47.4 tokens, leading to a combined average of 67.8 tokens per sample.",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "generate version-specific code",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "synthesizing programs from docstrings.",
    "execution_environment": "éœ€è¦ç‰¹å®šåº“ç‰ˆæœ¬ä¾èµ–çš„è™šæ‹Ÿç¯å¢ƒ",
    "execution_environment_quote": "We used venv to create and manage virtual environments for testing. This process involved installing the appropriate library version and any additional dependencies.",
    "unique_features": "ä¸“æ³¨äºè¯„ä¼°æ¨¡å‹å¯¹çœŸå®åº“ç‰ˆæœ¬å˜åŒ–çš„é€‚åº”èƒ½åŠ›ï¼Œæ¯ä¸ªé—®é¢˜éƒ½ç»‘å®šåˆ°ç‰¹å®šçš„åº“ç‰ˆæœ¬ï¼Œå¹¶é…æœ‰å¯æ‰§è¡Œçš„å•å…ƒæµ‹è¯•ã€‚æ•°æ®åŸºäº2014å¹´è‡³2023å¹´çš„çœŸå®ç‰ˆæœ¬å‘å¸ƒï¼Œå¹¶æ ‡æ³¨äº†å˜æ›´ç±»å‹ï¼ˆå‚æ•°/å±æ€§å˜æ›´ã€å‡½æ•°åå˜æ›´ã€è¯­ä¹‰/è¡Œä¸ºå˜æ›´ã€æ–°åŠŸèƒ½ï¼‰ã€‚",
    "unique_features_quote": "GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests. ... The samples were collected from version releases over a period from the year 2014 to 2023 ... we annotate each sample with the type of change that is classified into the following categories: Argument or Attribute change, Function Name change, Semantics or Function Behavior change, New feature or additional dependency-based change.",
    "data_size_quantity": 116,
    "data_size_unit": "ä¸ªé—®é¢˜",
    "last_updated_year": 2024,
    "last_updated_month": 11,
    "last_updated_day": 5,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ç‰ˆæœ¬ç‰¹å®šä»£ç ç”Ÿæˆèƒ½åŠ›', 'ä»£ç åº“åŠ¨æ€é€‚åº”æ€§', 'åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['pass@k', 'pass@10']",
    "problem_domain_normalized": "['Pythonç¼–ç¨‹', 'PyTorch', 'NumPy', 'Scikit-Learn', 'Pandas']",
    "source_type_normalized": "['æ‰‹å·¥ç¼–å†™', 'LLMè¾…åŠ©', 'åŸºäºçœŸå®åº“çš„å˜æ›´æ—¥å¿—']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç è¡¥å…¨",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2411.12882_output/content.md",
    "benchmark_name": "PurpleLlama secure coding benchmark",
    "benchmark_name_quote": "We demonstrate the effectiveness of PROSEC on the PurpleLlama (Bhatt et al., 2023) secure coding benchmark.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We demonstrate the effectiveness of PROSEC on the PurpleLlama (Bhatt et al., 2023) secure coding benchmark.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°ä»£ç å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå®‰å…¨ä»£ç çš„èƒ½åŠ›ï¼Œæ£€æµ‹å…¶ç”Ÿæˆçš„ä»£ç æ˜¯å¦åŒ…å«å¸¸è§å¼±ç‚¹æšä¸¾ï¼ˆCWEï¼‰ä¸­å®šä¹‰çš„å®‰å…¨æ¼æ´ã€‚",
    "task_description_quote": "The goal of security alignment in code LLMs is to reduce the likelihood of generating insecure code... We demonstrate the effectiveness of PROSEC on the PurpleLlama (Bhatt et al., 2023) secure coding benchmark.",
    "dimension": "ä»£ç å®‰å…¨æ€§ï¼Œå³ç”Ÿæˆä»£ç æ˜¯å¦éµå¾ªå®‰å…¨ç¼–ç å®è·µï¼Œé¿å…å¼•å…¥å¸¸è§æ¼æ´ã€‚",
    "dimension_quote": "The safety of these models remains under-explored, posing potential risks as insecure code generated by these models may introduce vulnerabilities into real-world systems.",
    "evaluation_method": "ä½¿ç”¨é™æ€åˆ†æå™¨ä½œä¸ºé¢„è¨€æœºï¼Œæ£€æµ‹ç”Ÿæˆçš„ä»£ç ç‰‡æ®µä¸­æ˜¯å¦å­˜åœ¨CWEæ¼æ´ã€‚",
    "evaluation_method_quote": "Following previous work (Bhatt et al., 2023), we assume that there exists a static analyzer... as an oracle to detect whether a snippet of code follows secure code patterns. Specifically, the static analyzer takes as input a code snippet, and outputs a list of detected CWEs.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "å®‰å…¨ç¼–ç ï¼Œæ¶µç›–å¸¸è§å¼±ç‚¹æšä¸¾ï¼ˆCWEï¼‰ä¸­å®šä¹‰çš„å„ç§è½¯ä»¶å®‰å…¨æ¼æ´ç±»å‹ã€‚",
    "problem_domain_quote": "The Common Weakness Enumerations (CWEs) (MITRE, 2023), which abstract diverse program vulnerabilities, offer a generalizable foundation for simulating how vulnerabilities manifest across various coding tasks and programming languages.",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "æ–‡ä¸­æœªæ˜ç¡®æåŠPurpleLlamaåŸºå‡†æ”¯æŒçš„å…·ä½“ç¼–ç¨‹è¯­è¨€ï¼Œä½†æœ¬æ–‡å®éªŒæ¶‰åŠå¤šç§è¯­è¨€ã€‚",
    "language_quote": "PROSEC improves the ability of code LLMs to generate secure code... across multiple models, languages, and vulnerability types.",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": "2023",
    "last_updated_quote": "PurpleLlama (Bhatt et al., 2023)",
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "Large language models (LLMs) capable of generating code based on human instructions have revolutionized programming by significantly facilitating tasks such as code generation...",
    "evaluation_metrics": "å®‰å…¨ä»£ç ç”Ÿæˆèƒ½åŠ›çš„æå‡ç™¾åˆ†æ¯”ï¼ˆä¾‹å¦‚ï¼Œæ¯”åŸºçº¿æ¨¡å‹å®‰å…¨25.2%â€“35.4%ï¼‰",
    "evaluation_metrics_quote": "The models trained with the dataset synthesized by PROSEC are 25.2%â€“35.4% more secure than those trained with the SafeCoder dataset.",
    "input_modality": "è‡ªç„¶è¯­è¨€æŒ‡ä»¤",
    "input_modality_quote": "Consider an instruction following code LLM Ï€Î¸(y|x) = Î iÏ€Î¸(yi|y<i, x) that takes user instruction x and generates the code response y.",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "Consider an instruction following code LLM Ï€Î¸(y|x) = Î iÏ€Î¸(yi|y<i, x) that takes user instruction x and generates the code response y.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "Consider an instruction following code LLM Ï€Î¸(y|x) = Î iÏ€Î¸(yi|y<i, x) that takes user instruction x and generates the code response y.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºä»£ç å®‰å…¨æ€§çš„è¯„æµ‹åŸºå‡†ï¼Œä½¿ç”¨CWEï¼ˆå¸¸è§å¼±ç‚¹æšä¸¾ï¼‰ä½œä¸ºæ¼æ´åˆ†ç±»å’Œæ£€æµ‹çš„åŸºç¡€æ¡†æ¶ã€‚",
    "unique_features_quote": "A widely recognized framework for categorizing such issues is the Common Weakness Enumerations (CWE) (MITRE, 2023)... Following previous work (Bhatt et al., 2023), we assume that there exists a static analyzer... to detect whether a snippet of code follows secure code patterns... and outputs a list of detected CWEs.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2023,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['ä»£ç å®‰å…¨æ€§']",
    "evaluation_method_normalized": "['å®‰å…¨ä»£ç ç”Ÿæˆèƒ½åŠ›çš„æå‡ç™¾åˆ†æ¯”']",
    "problem_domain_normalized": "['å®‰å…¨ç¼–ç ']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2412.00535_output/content.md",
    "benchmark_name": "FullStack Bench",
    "benchmark_name_quote": "FullStack Bench: Evaluating LLMs as Full Stack Coders",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "To address this gap, we have developed a comprehensive code evaluation dataset FullStack Bench1,2 focusing on full-stack programming",
    "dataset_url": "https://huggingface.co/datasets/ByteDance/FullStackBench, https://github.com/bytedance/FullStackBench",
    "dataset_url_quote": "1https://huggingface.co/datasets/ByteDance/FullStackBench\n2https://github.com/bytedance/FullStackBench",
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºå…¨æ ˆç¨‹åºå‘˜çš„èƒ½åŠ›ï¼Œæ¶µç›–å¹¿æ³›çš„åº”ç”¨ç¨‹åºé¢†åŸŸã€‚",
    "task_description_quote": "focusing on full-stack programming, which encompasses a wide range of application domains (e.g., basic programming, data analysis, software engineering, mathematics, and machine learning).",
    "dimension": "å…¨æ ˆç¼–ç¨‹èƒ½åŠ›ã€å¤šè¯­è¨€ç¼–ç¨‹èƒ½åŠ›ã€è·¨åº”ç”¨é¢†åŸŸèƒ½åŠ›",
    "dimension_quote": "focusing on full-stack programming... to assess multi-lingual programming capabilities... a wide range of application domains",
    "evaluation_method": "ä½¿ç”¨å•å…ƒæµ‹è¯•ç”¨ä¾‹è¿›è¡Œä»£ç æ²™ç®±æ‰§è¡Œè¯„ä¼°",
    "evaluation_method_quote": "we design real-world instructions and corresponding unit test cases... we also release an effective code sandbox execution tool (i.e., SandboxFusion3)... to evaluate the performance of our FullStack Bench efficiently.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "åŸºç¡€ç¼–ç¨‹ã€é«˜çº§ç¼–ç¨‹ã€è½¯ä»¶å·¥ç¨‹ã€æ•°æ®åˆ†æã€æ•°å­¦ã€æ¡Œé¢ä¸Webå¼€å‘ã€æœºå™¨å­¦ä¹ ã€ç§‘å­¦è®¡ç®—ã€æ•°æ®åº“ã€å¤šåª’ä½“ã€æ“ä½œç³»ç»Ÿç­‰",
    "problem_domain_quote": "Basic Programming (BP)\nAdvanced Programming (AP)\nSoftware Engineering (SE)\nData Analysis (DA)\nMathematics (MA)\nDesktop and Web Development (DW)\nMachine Learning (ML)\nScientific Computing (SC)\nDataBase (DB)\nMultimedia (MM)\nOperating System (OS)\nOthers",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "16ç§å¹¿æ³›ä½¿ç”¨çš„ç¼–ç¨‹è¯­è¨€",
    "language_quote": "in FullStack Bench, we design real-world instructions and corresponding unit test cases from 16 widely-used programming languages",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": "è®¾è®¡çœŸå®ä¸–ç•Œçš„æŒ‡ä»¤å’Œå¯¹åº”çš„å•å…ƒæµ‹è¯•ç”¨ä¾‹ï¼Œè€Œéç®€å•ç¿»è¯‘",
    "source_type_quote": "we design real-world instructions and corresponding unit test cases from 16 widely-used programming languages to reflect real-world usage scenarios rather than simple translations.",
    "last_updated": "2025-05-12",
    "last_updated_quote": "arXiv:2412.00535v6  [cs.AI]  12 May 2025",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "we have developed a comprehensive code evaluation dataset FullStack Bench",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "evaluating LLMs as Full Stack Coders",
    "evaluation_metrics": NaN,
    "evaluation_metrics_quote": NaN,
    "input_modality": "è‡ªç„¶è¯­è¨€æŒ‡ä»¤",
    "input_modality_quote": "we design real-world instructions",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "evaluating LLMs as Full Stack Coders",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "we design real-world instructions and corresponding unit test cases",
    "execution_environment": "æ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€å’ŒåŒ…çš„ä»£ç æ²™ç®±æ‰§è¡Œå·¥å…·(SandboxFusion)",
    "execution_environment_quote": "we also release an effective code sandbox execution tool (i.e., SandboxFusion3) supporting various programming languages and packages",
    "unique_features": "ä¸“æ³¨äºå…¨æ ˆç¼–ç¨‹è¯„ä¼°ï¼Œè¦†ç›–å¹¿æ³›çš„åº”ç”¨ç¨‹åºé¢†åŸŸï¼›åŒ…å«16ç§ç¼–ç¨‹è¯­è¨€çš„çœŸå®ä¸–ç•ŒæŒ‡ä»¤å’Œå•å…ƒæµ‹è¯•ï¼Œè€Œéç®€å•ç¿»è¯‘ï¼›é…å¥—å‘å¸ƒäº†æ”¯æŒå¤šè¯­è¨€å’Œå¤šåŒ…çš„ä»£ç æ²™ç®±æ‰§è¡Œå·¥å…·SandboxFusionã€‚",
    "unique_features_quote": "focusing on full-stack programming, which encompasses a wide range of application domains... we design real-world instructions and corresponding unit test cases from 16 widely-used programming languages to reflect real-world usage scenarios rather than simple translations. Moreover, we also release an effective code sandbox execution tool (i.e., SandboxFusion3) supporting various programming languages and packages",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": 5,
    "last_updated_day": 12,
    "language_normalized": "['16ç§å¹¿æ³›ä½¿ç”¨çš„ç¼–ç¨‹è¯­è¨€']",
    "dimension_normalized": "['å…¨æ ˆç¼–ç¨‹èƒ½åŠ›', 'å¤šè¯­è¨€ç¼–ç¨‹èƒ½åŠ›', 'è·¨åº”ç”¨é¢†åŸŸèƒ½åŠ›']",
    "evaluation_method_normalized": "[]",
    "problem_domain_normalized": "['åŸºç¡€ç¼–ç¨‹', 'é«˜çº§ç¼–ç¨‹', 'è½¯ä»¶å·¥ç¨‹', 'æ•°æ®åˆ†æ', 'æ•°å­¦', 'æ¡Œé¢ä¸Webå¼€å‘', 'æœºå™¨å­¦ä¹ ', 'ç§‘å­¦è®¡ç®—', 'æ•°æ®åº“', 'å¤šåª’ä½“', 'æ“ä½œç³»ç»Ÿ']",
    "source_type_normalized": "['è®¾è®¡çœŸå®ä¸–ç•Œçš„æŒ‡ä»¤å’Œå¯¹åº”çš„å•å…ƒæµ‹è¯•ç”¨ä¾‹ï¼Œè€Œéç®€å•ç¿»è¯‘']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2412.05210_output/content.md",
    "benchmark_name": "CodeArena",
    "benchmark_name_quote": "we present a rigorous human-curated benchmark CodeArena to emulate the complexity and diversity of real-world coding tasks",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "we first introduce a comprehensive human-curated benchmark, CodeArena",
    "dataset_url": "https://codearenaeval.github.io/",
    "dataset_url_quote": "1https://codearenaeval.github.io/",
    "task_description": "è¯„ä¼°æ¨¡å‹ç”Ÿæˆçš„ä»£ç å“åº”ä¸äººç±»åå¥½çš„å¯¹é½ç¨‹åº¦ï¼Œæ¨¡æ‹Ÿç°å®ä¸–ç•Œç¼–ç ä»»åŠ¡çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚",
    "task_description_quote": "to evaluate the alignment between the model-generated response and human preference, enabling the community to evaluate and track the alignment between human preferences and model-generated responses in real-world scenarios.",
    "dimension": "äººç±»åå¥½å¯¹é½ï¼ˆåŒ…æ‹¬ä»£ç è´¨é‡ã€è§£é‡Šã€æ ¼å¼ã€æ³¨é‡Šç­‰ï¼‰ï¼Œè€Œéå•çº¯çš„ä»£ç æ­£ç¡®æ€§ã€‚",
    "dimension_quote": "the current code LLMs focus on synthesizing the correct code snippet, ignoring the alignment with human preferences... underscoring the importance of the human preference alignment.",
    "evaluation_method": "ä½¿ç”¨GPT-4oä½œä¸ºè¯„åˆ¤å‘˜ï¼Œé€šè¿‡â€œæ¯”è¾ƒAå’ŒBâ€å’Œâ€œæ¯”è¾ƒBå’ŒAâ€ä¸¤ç§æ¸¸æˆè®¡ç®—æ¨¡å‹ç›¸å¯¹äºåŸºçº¿çš„èƒœç‡ã€‚",
    "evaluation_method_quote": "we apply GPT-4o-2024-08-06 as the judger to evaluate the model performance. Specifically, we use two games â€œcompare A and B â€ and â€œcompare B and Aâ€ (avoid the relative position of A and B affecting the results) to calculate the win rate of A compared to the baseline B.",
    "context_dependency": "åŸºäºç°å®ä¸–ç•Œç”¨æˆ·æŸ¥è¯¢çš„å®Œæ•´é—®é¢˜ï¼Œä¸å±€é™äºè‡ªåŒ…å«çš„å‡½æ•°ç‰‡æ®µã€‚",
    "context_dependency_quote": "Popular code-related benchmarks typically focus on self-contained function snippets... CodeArena provides many problems for evaluation under realistic scenarios, which are not suitable for verification through unit testing.",
    "problem_domain": "æ¶µç›–è½¯ä»¶å¼€å‘ã€ç”¨æˆ·ç•Œé¢/ä½“éªŒã€ä¸“ç”¨è®¡ç®—ã€å·¥å…·ä¸ç¯å¢ƒã€æ•°æ®åº“ä¸æ•°æ®å¤„ç†ã€æ–°å…´æŠ€æœ¯ã€é€šç”¨æŸ¥è¯¢ç­‰7å¤§ç±»40ä¸ªå­ç±»ã€‚",
    "problem_domain_quote": "comprising 397 high-quality samples across 40 categories derived from real-world user queries... encompassing 7 major categories and 40 subcategories.",
    "problem_difficulty": "åˆ†ä¸ºç®€å•ã€ä¸­ç­‰ã€å›°éš¾ä¸‰ä¸ªçº§åˆ«ï¼Œå¤§éƒ¨åˆ†æ ·æœ¬ä¸ºä¸­ç­‰æˆ–å›°éš¾ã€‚",
    "problem_difficulty_quote": "all samples are classified into easy, medium, and hard. The majority of the samples are recognized as medium or hard, presenting a significant challenge to LLMs.",
    "language": "æ¶µç›–44ç§ç¼–ç¨‹è¯­è¨€ï¼ŒåŒ…æ‹¬Pythonã€C++ã€Javaã€JavaScriptã€HTML/CSSã€SQLã€Bashã€Goã€Rustã€PowerShellã€Google Apps Scriptç­‰ã€‚",
    "language_quote": "spanning 40 categories and 44 programming languages... CodeArena provides a valuable comprehensive benchmark for 40 subtasks and 44 programming languages",
    "data_size": "åŒ…å«397ä¸ªé«˜è´¨é‡æ ·æœ¬ã€‚",
    "data_size_quote": "comprising 397 high-quality samples... CodeArena consists of nearly 400 problems.",
    "source_type": "ä»åœ¨çº¿é—®ç­”ç½‘ç«™çš„ç”¨æˆ·æŸ¥è¯¢ä¸­ç²¾å¿ƒç­›é€‰å’Œäººå·¥æ ‡æ³¨ã€‚",
    "source_type_quote": "carefully curated from user queries... derived from real-world user queries... Online Q&A",
    "last_updated": "2024å¹´12æœˆï¼ˆè®ºæ–‡ç‰ˆæœ¬æ—¥æœŸï¼‰",
    "last_updated_quote": "arXiv:2412.05210v1  [cs.CL]  6 Dec 2024",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œç»è¿‡ä¸¥æ ¼çš„äººå·¥æ ‡æ³¨å’Œè´¨é‡æ§åˆ¶æµç¨‹ã€‚",
    "build_type_quote": "We propose CodeArena comprised of 397 manually annotated samples... we implement a rigorous human annotation process involving 4 full-time employees proficient in various programming languages for human annotation and 4 other senior programming developers for quality check.",
    "contamination_status": "è¿›è¡Œäº†å»æ±¡æŸ“å¤„ç†ï¼Œé€šè¿‡ç§»é™¤ä¸ç°æœ‰åŸºå‡†ï¼ˆMultiPL-E, MBPP, McEval, NaturalCodeBenchï¼‰çš„ç²¾ç¡®åŒ¹é…ï¼ˆ10-gramé‡å ï¼‰æ¥ç¡®ä¿æç¤ºçš„å”¯ä¸€æ€§ã€‚",
    "contamination_status_quote": "To avoid data leakage, we apply decontamination to ensure the uniqueness of prompts in CodeArena, by removing exact matches (10-gram word overlap) from MultiPL-E, MBPP, McEval, and NaturalCodeBench.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆæ ¹æ®è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆä»£ç ç‰‡æ®µæˆ–å®Œæ•´è§£å†³æ–¹æ¡ˆï¼‰ã€‚",
    "task_granularity_quote": "The query should be sampled from the practical application scenarios and the model-generated responses should satisfy the human preference.",
    "evaluation_metrics": "èƒœç‡ï¼ˆwin rateï¼‰ï¼ŒåŸºäºGPT-4oçš„åå¥½åˆ¤æ–­ã€‚",
    "evaluation_metrics_quote": "calculate the win rate of A compared to the baseline B.",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆç”¨æˆ·æŸ¥è¯¢/é—®é¢˜æè¿°ï¼‰ã€‚",
    "input_modality_quote": "Each sample in CodeArena includes (question...)",
    "output_modality": "ä»£ç ä¸è‡ªç„¶è¯­è¨€æ··åˆï¼ˆæœŸæœ›çš„å“åº”åŒ…æ‹¬ä»£ç ç‰‡æ®µä»¥åŠè¯¦ç»†çš„è§£é‡Šã€æ ¼å¼åŒ–å’Œæ³¨é‡Šï¼‰ã€‚",
    "output_modality_quote": "Claude3.5 produces responses that include detailed explanations, well-structured formatting, and code comments, making it more favorable in terms of human preference.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ä¸æ–‡æœ¬ï¼ˆè‡ªç„¶è¯­è¨€é—®é¢˜åˆ°åŒ…å«ä»£ç å’Œè§£é‡Šçš„å“åº”ï¼‰ã€‚",
    "task_io_type_quote": "synthesizing the correct code snippet... alignment with human preferences (including explanations, formatting, comments)",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºè¯„ä¼°ä»£ç LLMä¸äººç±»åå¥½çš„å¯¹é½ï¼Œè€Œéå•çº¯çš„ä»£ç æ‰§è¡Œæ­£ç¡®æ€§ï¼›æ ·æœ¬æ¥æºäºçœŸå®ä¸–ç•Œç”¨æˆ·æŸ¥è¯¢ï¼Œè¦†ç›–å¹¿æ³›çš„ç¼–ç¨‹è¯­è¨€å’Œä»»åŠ¡ç±»åˆ«ï¼›åŒ…å«äººå·¥æ ‡æ³¨çš„éš¾åº¦ç­‰çº§å’ŒåŸºçº¿ç­”æ¡ˆã€‚",
    "unique_features_quote": "Unlike previous benchmarks, which consist of various programming exercises along with the corresponding test cases... our benchmarks emphasize a diverse range of programming languages that are commonly used in everyday programming tasks... provides a valuable comprehensive benchmark for 40 subtasks and 44 programming languages, which satisfies the evaluation in realistic scenarios.",
    "data_size_quantity": 397,
    "data_size_unit": "ä¸ªæ ·æœ¬",
    "last_updated_year": 2024,
    "last_updated_month": 12,
    "last_updated_day": null,
    "language_normalized": "['Python', 'C++', 'Java', 'JavaScript', 'HTML/CSS', 'SQL', 'Bash', 'Go', 'Rust', 'PowerShell', 'Google Apps Script']",
    "dimension_normalized": "['äººç±»åå¥½å¯¹é½', 'ä»£ç è´¨é‡', 'è§£é‡Š', 'æ ¼å¼', 'æ³¨é‡Š']",
    "evaluation_method_normalized": "['èƒœç‡', 'GPT-4oçš„åå¥½åˆ¤æ–­']",
    "problem_domain_normalized": "['è½¯ä»¶å¼€å‘', 'ç”¨æˆ·ç•Œé¢/ä½“éªŒ', 'ä¸“ç”¨è®¡ç®—', 'å·¥å…·ä¸ç¯å¢ƒ', 'æ•°æ®åº“ä¸æ•°æ®å¤„ç†', 'æ–°å…´æŠ€æœ¯', 'é€šç”¨æŸ¥è¯¢']",
    "source_type_normalized": "['åœ¨çº¿é—®ç­”ç½‘ç«™çš„ç”¨æˆ·æŸ¥è¯¢', 'äººå·¥æ ‡æ³¨']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2505.08503_output/content.md",
    "benchmark_name": "ICVul",
    "benchmark_name_quote": "ICVul: A Well-labeled C/C++ Vulnerability Dataset with Comprehensive Metadata and VCCs",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "we constructed ICVul, a dataset emphasizing data quality and enriched with comprehensive metadata, including Vulnerability-Contributing Commits (VCCs).",
    "dataset_url": "https://github.com/Chaomeng-Lu/ICVul.git",
    "dataset_url_quote": "The project, along with supporting scripts, is publicly available on GitHub1. 1https://github.com/Chaomeng-Lu/ICVul.git",
    "task_description": "ç”¨äºè®­ç»ƒå’Œè¯„ä¼°åŸºäºæœºå™¨å­¦ä¹ çš„è½¯ä»¶æ¼æ´æ£€æµ‹æ¨¡å‹ã€‚",
    "task_description_quote": "Machine learning-based software vulnerability detection requires high-quality datasets, which is essential for training effective models.",
    "dimension": "æ¼æ´æ£€æµ‹çš„æ•°æ®è´¨é‡ã€æ ‡ç­¾å¯é æ€§ã€å…ƒæ•°æ®ä¸°å¯Œåº¦ã€æ•°æ®å¹³è¡¡æ€§ã€‚",
    "dimension_quote": "To address challenges related to data label quality, diversity, and comprehensiveness, we constructed ICVul, a dataset emphasizing data quality and enriched with comprehensive metadata...",
    "evaluation_method": "æ–‡ä¸­æœªæ˜ç¡®æè¿°é’ˆå¯¹è¯¥æ•°æ®é›†çš„æ ‡å‡†è¯„ä¼°æ–¹æ³•ã€‚",
    "evaluation_method_quote": NaN,
    "context_dependency": "ä»£ç æäº¤ï¼ˆCommitï¼‰ã€æ–‡ä»¶ï¼ˆFileï¼‰ã€å‡½æ•°ï¼ˆFunctionï¼‰çº§åˆ«ã€‚",
    "context_dependency_quote": "ICVul provides a high-quality dataset tailored for training ML models to detect software vulnerabilities at the commit, function and file levels.",
    "problem_domain": "è½¯ä»¶å®‰å…¨ã€æ¼æ´æ£€æµ‹ã€‚",
    "problem_domain_quote": "Detecting and mitigating software vulnerabilities remains one of the most critical challenges in ensuring the security and integrity of modern software systems.",
    "problem_difficulty": "çœŸå®ä¸–ç•Œè½¯ä»¶é¡¹ç›®ä¸­çš„æ¼æ´ï¼Œå¤æ‚åº¦é«˜ã€‚",
    "problem_difficulty_quote": "With the growing complexity of applications and their codebases, traditional methods of vulnerability detection... struggle to keep up with the scale and intricacy of vulnerabilities present in large software projects.",
    "language": "C/C++",
    "language_quote": "ICVul, an Integrated and Comprehensive C/C++ Vulnerability dataset.",
    "data_size": "åŒ…å«807ä¸ªä»“åº“ï¼Œ146ä¸ªCWEç±»å‹ï¼Œ4327ä¸ªä¿®å¤æäº¤ï¼Œ6862ä¸ªæ–‡ä»¶ï¼Œ15396ä¸ªå‡½æ•°ï¼Œå…¶ä¸­6276ä¸ªä¸ºæ¼æ´å‡½æ•°ï¼ˆå æ¯”41%ï¼‰ã€‚",
    "data_size_quote": "ICVul 807 146 4,327 6,862 15,396 6,276 41%",
    "source_type": "ä»ç¾å›½å›½å®¶æ¼æ´æ•°æ®åº“ï¼ˆNVDï¼‰ä¸­ç­›é€‰ä¸GitHubä¿®å¤æäº¤ç›¸å…³è”çš„CVEæ¡ç›®ï¼Œå¹¶ä»ä¸­æå–ä»£ç å’Œå…ƒæ•°æ®ã€‚",
    "source_type_quote": "We began by filtering Common Vulnerabilities and Exposures from the NVD, retaining only those linked to GitHub fix commits. Then we extracted functions and files along with relevant metadata from these commits...",
    "last_updated": "2024å¹´11æœˆ13æ—¥ï¼ˆæ•°æ®æ”¶é›†æ—¥æœŸï¼‰",
    "last_updated_quote": "The dataset construction process began with the collection of 269,509 CVEs, gathered on November 13, 2024.",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œå¹¶æä¾›äº†å¯å¤ç°çš„æ„å»ºæ¡†æ¶ã€‚",
    "build_type_quote": "We introduce a vulnerability collection framework that can be re-run at any time to ensure the dataset remains up-to-date.",
    "contamination_status": "é€šè¿‡ESCæŠ€æœ¯æ’é™¤å¯ç–‘æäº¤ï¼Œæ—¨åœ¨æé«˜æ ‡ç­¾å¯é æ€§ï¼Œé™ä½å™ªå£°ã€‚",
    "contamination_status_quote": "To further enhance label reliability, we developed the ESC (Eliminate Suspicious Commit) technique, ensuring credible data labels.",
    "dataset_license": "æ–‡ä¸­æœªæåŠã€‚",
    "dataset_license_quote": NaN,
    "task_granularity": "æ¼æ´æ£€æµ‹ï¼ˆåˆ†ç±»ä»»åŠ¡ï¼‰ã€‚",
    "task_granularity_quote": "ICVul provides a high-quality dataset tailored for training ML models to detect software vulnerabilities...",
    "evaluation_metrics": "æ–‡ä¸­æœªæåŠé’ˆå¯¹è¯¥æ•°æ®é›†çš„æ ‡å‡†è¯„ä¼°æŒ‡æ ‡ã€‚",
    "evaluation_metrics_quote": NaN,
    "input_modality": "æºä»£ç ï¼ˆC/C++ï¼‰åŠä¸°å¯Œçš„å…ƒæ•°æ®ï¼ˆå¦‚æäº¤ä¿¡æ¯ã€ä»£ç å˜æ›´ã€ä»“åº“ä¿¡æ¯ç­‰ï¼‰ã€‚",
    "input_modality_quote": "The dataset is stored in a relational-like database for improved usability and data integrity.",
    "output_modality": "æ¼æ´æ ‡ç­¾ï¼ˆæ˜¯å¦åŒ…å«æ¼æ´ï¼Œä»¥åŠCWEç±»å‹ï¼‰ã€‚",
    "output_modality_quote": "the dataset provides clear vulnerability labels at the CWE type level, enabling the development and research of multi-class classification models.",
    "task_io_type": "ä»£ç åˆ°æ ‡ç­¾ï¼ˆæ¼æ´åˆ†ç±»ï¼‰ã€‚",
    "task_io_type_quote": "training ML models to detect software vulnerabilities",
    "execution_environment": "ä¸æ¶‰åŠä»£ç æ‰§è¡Œï¼Œæ˜¯é™æ€åˆ†ææ•°æ®é›†ã€‚",
    "execution_environment_quote": NaN,
    "unique_features": "1. åŒ…å«æ¼æ´å¼•å…¥æäº¤ï¼ˆVCCï¼‰ä¿¡æ¯ï¼Œä½¿ç”¨SZZç®—æ³•è¿½æº¯ã€‚2. åº”ç”¨ESCæŠ€æœ¯è¿‡æ»¤å™ªå£°æ•°æ®ï¼Œæé«˜æ ‡ç­¾è´¨é‡ã€‚3. æ•°æ®å¹³è¡¡æ€§å¥½ï¼ˆæ¼æ´å‡½æ•°å æ¯”41%ï¼‰ã€‚4. ä»¥å…³ç³»å‹æ•°æ®åº“ç»“æ„å­˜å‚¨ï¼ŒåŒ…å«ä»“åº“ã€æäº¤ã€æ–‡ä»¶ã€å‡½æ•°ã€CVE-VCCæ˜ å°„äº”ä¸ªäº’ç›¸å…³è”çš„è¡¨ã€‚5. ä¸“æ³¨äºC/C++è¯­è¨€ã€‚",
    "unique_features_quote": "Another key feature of ICVul is its inclusion of VCCs, which trace specific commits responsible for introducing vulnerabilities into the codebase using the state-of-the-art SZZ algorithm... ICVul incorporates the ESC (Eliminate Suspicious Commit) technique, further enhancing label reliability... the dataset achieves a much better balance ratio of 41% at the function level. ICVul comprises five interconnected tables: repository info, cve fc vcc mapping, commit info, file info, and function info.",
    "data_size_quantity": 807,
    "data_size_unit": "ä¸ªä»“åº“",
    "last_updated_year": 2024,
    "last_updated_month": 11,
    "last_updated_day": 13,
    "language_normalized": "['C/C++']",
    "dimension_normalized": "['æ¼æ´æ£€æµ‹çš„æ•°æ®è´¨é‡', 'æ ‡ç­¾å¯é æ€§', 'å…ƒæ•°æ®ä¸°å¯Œåº¦', 'æ•°æ®å¹³è¡¡æ€§']",
    "evaluation_method_normalized": "['æ–‡ä¸­æœªæåŠé’ˆå¯¹è¯¥æ•°æ®é›†çš„æ ‡å‡†è¯„ä¼°æŒ‡æ ‡']",
    "problem_domain_normalized": "['è½¯ä»¶å®‰å…¨', 'æ¼æ´æ£€æµ‹']",
    "source_type_normalized": "['ä»ç¾å›½å›½å®¶æ¼æ´æ•°æ®åº“ï¼ˆNVDï¼‰ä¸­ç­›é€‰ä¸GitHubä¿®å¤æäº¤ç›¸å…³è”çš„CVEæ¡ç›®ï¼Œå¹¶ä»ä¸­æå–ä»£ç å’Œå…ƒæ•°æ®']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "JSON",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2511.17330_output/content.md",
    "benchmark_name": "SV-COMP",
    "benchmark_name_quote": "Experimental evaluation on SV-COMP benchmarks and on Linux kernel modules shows promising efficacy in achieving automated program verification.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Experimental evaluation on SV-COMP benchmarks and on Linux kernel modules shows promising efficacy in achieving automated program verification.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ç¨‹åºéªŒè¯ï¼Œå³è¯æ˜ç¨‹åºæ»¡è¶³å…¶å½¢å¼åŒ–è§„èŒƒã€‚",
    "task_description_quote": "Formal program verification uses mathematically rigorous methods to prove that a program satisfies its specifications.",
    "dimension": "ç¨‹åºéªŒè¯çš„è‡ªåŠ¨åŒ–èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯è¯æ˜ç”Ÿæˆçš„æœ‰æ•ˆæ€§ã€‚",
    "dimension_quote": "We thoroughly evaluate our approach on existing mathematical lemmas [51], as well as lemmas systematically extracted from SV-COMP programs [7]. The SV-COMP programsâ€™ lemmas capture intricate code logic and properties, which are more representative of program verification.",
    "evaluation_method": "é€šè¿‡è¯æ˜æˆåŠŸç‡ï¼ˆå³æˆåŠŸè¯æ˜çš„å¼•ç†æ¯”ä¾‹ï¼‰è¿›è¡Œè¯„ä¼°ã€‚",
    "evaluation_method_quote": "Evaluation shows that AutoRocq significantly outperforms state-of-the-art approaches. Specifically, AutoRocq is capable of proving 51.1% mathematical lemmas and 30.9% program lemmas, exceeding baseline approaches by 20.8% to 343.0% on mathematical lemmas, and by 42.4% to 204.6% on program lemmas.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "è½¯ä»¶éªŒè¯ã€å½¢å¼åŒ–æ–¹æ³•ã€å®šç†è¯æ˜ã€‚",
    "problem_domain_quote": "We showcase the feasibility of automatic and end-to-end verification with LLM agents. AutoRocq is evaluated on the widely used SV-COMP programs in software verification, as well as Linux kernel modules.",
    "problem_difficulty": "ä»£è¡¨çœŸå®ä¸–ç•Œç¨‹åºéªŒè¯çš„å¤æ‚é€»è¾‘å’Œå±æ€§ï¼Œéš¾åº¦è¾ƒé«˜ã€‚",
    "problem_difficulty_quote": "The SV-COMP programsâ€™ lemmas capture intricate code logic and properties, which are more representative of program verification.",
    "language": NaN,
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": "ä»SV-COMPç¨‹åºå’ŒLinuxå†…æ ¸æ¨¡å—ä¸­ç³»ç»Ÿæå–çš„å¼•ç†ã€‚",
    "source_type_quote": "We thoroughly evaluate our approach on existing mathematical lemmas [51], as well as lemmas systematically extracted from SV-COMP programs [7].",
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "è¯æ˜ç”Ÿæˆï¼ˆç”Ÿæˆè¯æ˜è„šæœ¬ä»¥å±¥è¡Œè¯æ˜ä¹‰åŠ¡ï¼‰ã€‚",
    "task_granularity_quote": "Proof generation aims to automatically synthesize proof scripts that discharge the given proof obligations, which closes the last mile of program verification.",
    "evaluation_metrics": "è¯æ˜æˆåŠŸç‡ï¼ˆç™¾åˆ†æ¯”ï¼‰ã€‚",
    "evaluation_metrics_quote": "AutoRocq is capable of proving 51.1% mathematical lemmas and 30.9% program lemmas...",
    "input_modality": "å½¢å¼åŒ–çš„è¯æ˜ä¹‰åŠ¡ï¼ˆé€»è¾‘å…¬å¼ï¼‰ã€‚",
    "input_modality_quote": "Proof generation aims to automatically synthesize proof scripts that discharge the given proof obligations...",
    "output_modality": "è¯æ˜è„šæœ¬ï¼ˆRocq/Coqæˆ˜æœ¯åºåˆ—ï¼‰ã€‚",
    "output_modality_quote": "Proof generation aims to automatically synthesize proof scripts that discharge the given proof obligations...",
    "task_io_type": "é€»è¾‘å…¬å¼åˆ°è¯æ˜è„šæœ¬ã€‚",
    "task_io_type_quote": "Proof generation aims to automatically synthesize proof scripts that discharge the given proof obligations...",
    "execution_environment": "Rocqï¼ˆåŸCoqï¼‰å®šç†è¯æ˜å™¨ç¯å¢ƒã€‚",
    "execution_environment_quote": "The iterative improvement of the proof is achieved by the proof agent communicating with the Rocq (formerly Coq) theorem prover to get additional context and feedback.",
    "unique_features": "SV-COMPæ˜¯è½¯ä»¶éªŒè¯ç«èµ›çš„åŸºå‡†ï¼Œå…¶ç¨‹åºå¼•ç†æ•è·äº†å¤æ‚çš„ä»£ç é€»è¾‘å’Œå±æ€§ï¼Œæ›´èƒ½ä»£è¡¨çœŸå®çš„ç¨‹åºéªŒè¯åœºæ™¯ã€‚",
    "unique_features_quote": "The SV-COMP programsâ€™ lemmas capture intricate code logic and properties, which are more representative of program verification.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['ç¨‹åºéªŒè¯çš„è‡ªåŠ¨åŒ–èƒ½åŠ›', 'è¯æ˜ç”Ÿæˆçš„æœ‰æ•ˆæ€§']",
    "evaluation_method_normalized": "['è¯æ˜æˆåŠŸç‡ï¼ˆç™¾åˆ†æ¯”ï¼‰']",
    "problem_domain_normalized": "['è½¯ä»¶éªŒè¯', 'å½¢å¼åŒ–æ–¹æ³•', 'å®šç†è¯æ˜']",
    "source_type_normalized": "['ä»SV-COMPç¨‹åºå’ŒLinuxå†…æ ¸æ¨¡å—ä¸­ç³»ç»Ÿæå–çš„å¼•ç†']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.05073_output/content.md",
    "benchmark_name": "Comprehensive Verilog Design Problems (CVDP)",
    "benchmark_name_quote": "The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA, provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Our work tests this by evaluating Small language models (SLMs) coupled with a curated agentic AI framework on NVIDIAâ€™s Comprehensive verilog design problem (CVDP) benchmark.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ä»è‡ªç„¶è¯­è¨€è§„èŒƒç”Ÿæˆç»è¿‡éªŒè¯çš„å¯„å­˜å™¨ä¼ è¾“çº§ï¼ˆRTLï¼‰ç¡¬ä»¶è®¾è®¡å®ç°ã€‚",
    "task_description_quote": "Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites. Derived from production IP blocks, it represents realistic complexity.",
    "dimension": "ç¡¬ä»¶è®¾è®¡è‡ªåŠ¨åŒ–èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯Verilogä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚",
    "dimension_quote": "provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.",
    "evaluation_method": "ä½¿ç”¨CocoTBæµ‹è¯•å¥—ä»¶è¯„ä¼°åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆpass rateï¼‰ã€‚",
    "evaluation_method_quote": "Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",
    "context_dependency": "æ¨¡å—çº§è®¾è®¡ï¼ŒåŒ…å«æ¥å£è§„èŒƒã€åŠŸèƒ½éœ€æ±‚å’Œæµ‹è¯•å¥—ä»¶ã€‚",
    "context_dependency_quote": "Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",
    "problem_domain": "ç¡¬ä»¶è®¾è®¡ï¼Œå…·ä½“åŒ…æ‹¬ç®—æœ¯è¿ç®—ã€æ§åˆ¶é€»è¾‘ã€å†…å­˜ç³»ç»Ÿå’Œå…¶ä»–è®¾è®¡ã€‚",
    "problem_domain_quote": "336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.",
    "problem_difficulty": "ä»£è¡¨å®é™…ç”Ÿäº§IPå—çš„å¤æ‚æ€§ï¼Œå…·æœ‰æŒ‘æˆ˜æ€§ï¼ˆæœ€å…ˆè¿›æ¨¡å‹å•æ¬¡é€šè¿‡ç‡ä»…26.5%ï¼‰ã€‚",
    "problem_difficulty_quote": "Derived from production IP blocks, it represents realistic complexity. State-of-the-art achieves only 26.5% pass rate (GPT-4o-mini, single-shot), highlighting substantial improvement opportunity.",
    "language": "Verilogï¼ˆç¡¬ä»¶æè¿°è¯­è¨€ï¼‰ã€‚",
    "language_quote": "The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA...",
    "data_size": "åŒ…å«336ä¸ªé—®é¢˜ã€‚",
    "data_size_quote": "provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.",
    "source_type": "æºè‡ªç”Ÿäº§IPå—ï¼Œä»£è¡¨å®é™…å¤æ‚æ€§ã€‚",
    "source_type_quote": "Derived from production IP blocks, it represents realistic complexity.",
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": "ç”±NVIDIAå¼€å‘ã€‚",
    "build_type_quote": "The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA...",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆä»è§„èŒƒç”ŸæˆVerilog RTLä»£ç ï¼‰ã€‚",
    "task_granularity_quote": "transforms design intent from the CVDP dataset into verified register transfer level (RTL) implementations.",
    "evaluation_metrics": "é€šè¿‡ç‡ï¼ˆpass rateï¼‰ã€‚",
    "evaluation_metrics_quote": "State-of-the-art achieves only 26.5% pass rate (GPT-4o-mini, single-shot)...",
    "input_modality": "è‡ªç„¶è¯­è¨€è§„èŒƒã€æ¨¡å—æ¥å£ã€åŠŸèƒ½éœ€æ±‚ã€‚",
    "input_modality_quote": "Each includes natural language specification, module interface, functional requirements...",
    "output_modality": "Verilog RTLä»£ç ã€‚",
    "output_modality_quote": "transforms design intent from the CVDP dataset into verified register transfer level (RTL) implementations.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ï¼ˆè‡ªç„¶è¯­è¨€è§„èŒƒåˆ°Verilogä»£ç ï¼‰ã€‚",
    "task_io_type_quote": "Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",
    "execution_environment": "ä½¿ç”¨CocoTBæµ‹è¯•å¥—ä»¶è¿›è¡ŒéªŒè¯ã€‚",
    "execution_environment_quote": "Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",
    "unique_features": "ä¸“æ³¨äºç¡¬ä»¶è®¾è®¡ï¼ˆVerilogï¼‰ï¼Œé—®é¢˜æºè‡ªå®é™…ç”Ÿäº§IPå—ï¼ŒåŒ…å«å®Œæ•´çš„æµ‹è¯•å¥—ä»¶ï¼ˆCocoTBï¼‰ç”¨äºåŠŸèƒ½éªŒè¯ã€‚",
    "unique_features_quote": "Derived from production IP blocks, it represents realistic complexity. Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",
    "data_size_quantity": 336,
    "data_size_unit": "ä¸ªé—®é¢˜",
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Verilog']",
    "dimension_normalized": "['ç¡¬ä»¶è®¾è®¡è‡ªåŠ¨åŒ–èƒ½åŠ›', 'Verilogä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['é€šè¿‡ç‡']",
    "problem_domain_normalized": "['ç¡¬ä»¶è®¾è®¡', 'ç®—æœ¯è¿ç®—', 'æ§åˆ¶é€»è¾‘', 'å†…å­˜ç³»ç»Ÿ', 'å…¶ä»–è®¾è®¡']",
    "source_type_normalized": "['ç”Ÿäº§IPå—', 'å®é™…å¤æ‚æ€§']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.05100_output/content.md",
    "benchmark_name": "SAP software-documentation benchmark",
    "benchmark_name_quote": "Experiments on the SAP software-documentation benchmark demonstrate improvements across six metrics...",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Our experimental results demonstrate significant improvements on the software documentation dataset (Buschbeck and Exel, 2020) across four translation directions...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ç»“æ„åŒ–æ–‡æ¡£ç¿»è¯‘ï¼Œå³å°†å¸¦æœ‰XMLæˆ–HTMLæ ‡è®°çš„è½¯ä»¶æ–‡æ¡£ä»æºè¯­è¨€ç¿»è¯‘ä¸ºç›®æ ‡è¯­è¨€ï¼ŒåŒæ—¶ä¿æŒæ ‡è®°ç»“æ„çš„å®Œæ•´æ€§ã€‚",
    "task_description_quote": "This work addresses the task of translating a structured document Ds in the source language into its counterpart Dt in the target language.",
    "dimension": "ç¿»è¯‘è´¨é‡ä¸ç»“æ„ä¿çœŸåº¦",
    "dimension_quote": "...making structural fidelity as important as content translation quality.",
    "evaluation_method": "ä½¿ç”¨å¤šç§æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ï¼ŒåŒ…æ‹¬XML-Matchã€XML-BLEUã€Content-BLEUã€StrucAUCã€TreeSimå’ŒNode-chrFã€‚",
    "evaluation_method_quote": "...FORMATRL achieving average gains of 3.69 XML-Match, 2.16 XML-BLEU, 0.22 Content-BLEU, and 0.93 StrucAUC scores... We propose two rewards: TreeSim and Node-chrF.",
    "context_dependency": "æ–‡æ¡£çº§åˆ«",
    "context_dependency_quote": "Recent works on structured text translation remain limited to the sentence level, as they struggle to effectively handle the complex document-level XML or HTML structures.",
    "problem_domain": "è½¯ä»¶æ–‡æ¡£æœ¬åœ°åŒ–",
    "problem_domain_quote": "Translating structured documents such as software manuals is essential for product localization.",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "æ–‡ä¸­æœªæ˜ç¡®æåŠå®Œæ•´æ•°æ®é›†çš„è¯­è¨€è¦†ç›–èŒƒå›´ï¼Œä»…æåŠäº†å®éªŒä¸­çš„ç¿»è¯‘æ–¹å‘ï¼ˆå¦‚è‹±è¯­åˆ°æ—¥è¯­ï¼‰ã€‚",
    "language_quote": "A structured document translation example (Englishâ†’Japanese)... Our experimental results demonstrate significant improvements on the software documentation dataset (Buschbeck and Exel, 2020) across four translation directions...",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": "SAPè½¯ä»¶æ–‡æ¡£",
    "source_type_quote": "Experiments on the SAP software-documentation benchmark...",
    "last_updated": "2020",
    "last_updated_quote": "...software documentation dataset (Buschbeck and Exel, 2020)...",
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "æ–‡æ¡£ç¿»è¯‘",
    "task_granularity_quote": "...translating a structured document Ds in the source language into its counterpart Dt in the target language.",
    "evaluation_metrics": "XML-Match, XML-BLEU, Content-BLEU, StrucAUC, TreeSim, Node-chrF",
    "evaluation_metrics_quote": "...FORMATRL achieving average gains of 3.69 XML-Match, 2.16 XML-BLEU, 0.22 Content-BLEU, and 0.93 StrucAUC scores... We propose two rewards: TreeSim and Node-chrF.",
    "input_modality": "å¸¦æœ‰æ ‡è®°çš„ç»“æ„åŒ–æ–‡æ¡£ï¼ˆXML/HTMLï¼‰",
    "input_modality_quote": "A structured document D can be viewed as an XML tree D = (VD, ED)...",
    "output_modality": "å¸¦æœ‰æ ‡è®°çš„ç»“æ„åŒ–æ–‡æ¡£ï¼ˆXML/HTMLï¼‰",
    "output_modality_quote": "...generating the target document Dt given the source document Ds...",
    "task_io_type": "ç»“æ„åŒ–æ–‡æ¡£åˆ°ç»“æ„åŒ–æ–‡æ¡£",
    "task_io_type_quote": "This work addresses the task of translating a structured document Ds in the source language into its counterpart Dt in the target language.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºè½¯ä»¶æ–‡æ¡£çš„æœ¬åœ°åŒ–ï¼Œè¦æ±‚åŒæ—¶ä¿è¯ç¿»è¯‘è´¨é‡å’ŒXML/HTMLæ ‡è®°çš„ç»“æ„ä¿çœŸåº¦ã€‚",
    "unique_features_quote": "Translating structured documents such as software manuals is essential for product localization. As shown in Figure 1, they carry markup that defines layout and interactive elements, making structural fidelity as important as content translation quality.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2020,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['ç¿»è¯‘è´¨é‡ä¸ç»“æ„ä¿çœŸåº¦']",
    "evaluation_method_normalized": "['XML-Match', 'XML-BLEU', 'Content-BLEU', 'StrucAUC', 'TreeSim', 'Node-chrF']",
    "problem_domain_normalized": "['è½¯ä»¶æ–‡æ¡£æœ¬åœ°åŒ–']",
    "source_type_normalized": "['SAPè½¯ä»¶æ–‡æ¡£']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.04673_output/content.md",
    "benchmark_name": "CoNaLa (Code/Natural Language Challenge)",
    "benchmark_name_quote": "To evaluate an LLMs ability to understand and generate natural language descriptions of code snippets, we use the CoNaLa (Code/Natural Language Challenge) [23] dataset.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "In this paper, we present a comprehensive comparative analysis of five general-purpose and five code-specific LLMs across six diverse benchmarks... We further extend our evaluation to the CoNaLa dataset [22], focusing on the task of code explanation...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ç»™å®šä¸€ä¸ªè¾“å…¥ä»£ç ç‰‡æ®µï¼Œç”Ÿæˆè¯¥ä»£ç ç‰‡æ®µæ„å›¾/è§£é‡Šã€‚",
    "task_description_quote": "Given an input code snippet, the task is to generate intents/explanation that the code is achieving.",
    "dimension": "è¯­ä¹‰ä»£ç ç†è§£ã€æ„å›¾å»ºæ¨¡å’Œæ–‡æœ¬ç”Ÿæˆèƒ½åŠ›",
    "dimension_quote": "This focuses on semantic code understanding, intent modeling and text generation.",
    "evaluation_method": "ä½¿ç”¨æ ‡å‡†è¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…æ‹¬åŸºäºè¯å…ƒçš„æŒ‡æ ‡ï¼ˆBLEU, METEOR, ROUGEï¼‰å’ŒåŸºäºè¯­ä¹‰çš„æŒ‡æ ‡ï¼ˆä½¿ç”¨CodeBERTè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰ã€‚",
    "evaluation_method_quote": "We use the following standard metrics that capture various aspects such as lexical overlap and semantic similarity using the following standard evaluation metrics for the task [1, 5, 10]. (i) Token-based: These metrics collectively quantify surface-level similarity... (ii) Semantics-based: We use this measure to assess the semantic similarity between the model generated explanation (ğ‘š) and the ground truth explanation (ğ‘”)... We then take a cosine similarity between the embeddings...",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": NaN,
    "problem_domain_quote": NaN,
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "Python",
    "language_quote": "The dataset consists of 1, 666 Python code and paired natural language annotations.",
    "data_size": "åŒ…å«1,666ä¸ªPythonä»£ç ç‰‡æ®µåŠå…¶é…å¯¹çš„è‡ªç„¶è¯­è¨€æ³¨é‡Šã€‚",
    "data_size_quote": "The dataset consists of 1, 666 Python code and paired natural language annotations.",
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç è§£é‡Š",
    "task_granularity_quote": "Given an input code snippet, the task is to generate intents/explanation that the code is achieving.",
    "evaluation_metrics": "BLEU, METEOR, ROUGE-1, ROUGE-2, ROUGE-L, CodeBERTScoreï¼ˆåŸºäºCodeBERTçš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰",
    "evaluation_metrics_quote": "BLEU [19] evaluates n-gram precision... METEOR [3] extends this... ROUGE [15] metrics adopt a recall-oriented perspective: ROUGE-1... ROUGE-2... ROUGE-L... We refer to this metric as CodeBERT in the rest of the paper.",
    "input_modality": "ä»£ç ",
    "input_modality_quote": "Given an input code snippet...",
    "output_modality": "è‡ªç„¶è¯­è¨€",
    "output_modality_quote": "...the task is to generate intents/explanation...",
    "task_io_type": "ä»£ç åˆ°æ–‡æœ¬",
    "task_io_type_quote": "Given an input code snippet, the task is to generate intents/explanation that the code is achieving.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºä»£ç åˆ°è‡ªç„¶è¯­è¨€çš„è§£é‡Šä»»åŠ¡ï¼Œè¯„ä¼°æ¨¡å‹å¯¹ä»£ç è¯­ä¹‰çš„ç†è§£å’Œæ„å›¾å»ºæ¨¡èƒ½åŠ›ã€‚",
    "unique_features_quote": "This focuses on semantic code understanding, intent modeling and text generation.",
    "data_size_quantity": 1666,
    "data_size_unit": "ä¸ª",
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['è¯­ä¹‰ä»£ç ç†è§£', 'æ„å›¾å»ºæ¨¡', 'æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›']",
    "evaluation_method_normalized": "['BLEU', 'METEOR', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'CodeBERTScore']",
    "problem_domain_normalized": "[]",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "task_io_type_normalized": "ä»£ç åˆ°æ–‡æœ¬",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.04538_output/content.md",
    "benchmark_name": "CrossCodeEval, RepoEval",
    "benchmark_name_quote": "Extensive experiments on CrossCodeEval and RepoEval benchmarks demonstrate that CoCo consistently surpasses state-of-the-art baselines...",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Extensive experiments on CrossCodeEval and RepoEval benchmarks demonstrate that CoCo consistently surpasses state-of-the-art baselines...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ä»“åº“çº§ä»£ç è¡¥å…¨ã€‚ä»å‡½æ•°çº§åˆ«åˆ°ä»“åº“çº§åˆ«ï¼Œåˆ©ç”¨å¤§è§„æ¨¡ä»£ç åº“ä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ç”Ÿæˆä»£ç ã€‚",
    "task_description_quote": "As code completion task from function-level to repository-level, leveraging contextual information from large-scale codebases becomes a core challenge.",
    "dimension": "ä»£ç ç”Ÿæˆè´¨é‡ã€ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›ã€è·¨æ–‡ä»¶ä¾èµ–å»ºæ¨¡èƒ½åŠ›",
    "dimension_quote": "This limitation becomes particularly challenging in repository-level code completion, where accurate code generation requires a holistic understanding of repository-wide dependencies, shared utilities, and inter-module interactions.",
    "evaluation_method": "Exact Match (EM)",
    "evaluation_method_quote": "achieving up to 20.2% gains in EM.",
    "context_dependency": "å¤šæ–‡ä»¶é¡¹ç›®ã€ä»“åº“çº§ä¸Šä¸‹æ–‡",
    "context_dependency_quote": "they fall short in real-world settings where code is organized in complex repositories with abundant cross-file dependencies, customized APIs, and project-specific conventions.",
    "problem_domain": "é€šç”¨è½¯ä»¶å¼€å‘",
    "problem_domain_quote": "Automatic code completion plays a fundamental role in modern software development",
    "problem_difficulty": "å·¥ç¨‹çº§ã€çœŸå®ä¸–ç•Œåœºæ™¯",
    "problem_difficulty_quote": "they fall short in real-world settings where code is organized in complex repositories with abundant cross-file dependencies, customized APIs, and project-specific conventions.",
    "language": NaN,
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç è¡¥å…¨",
    "task_granularity_quote": "Automatic code completion plays a fundamental role in modern software development",
    "evaluation_metrics": "Exact Match (EM)",
    "evaluation_metrics_quote": "achieving up to 20.2% gains in EM.",
    "input_modality": "ä»£ç ï¼ˆåŒ…å«å…¶ä¸Šä¸‹æ–‡ï¼‰",
    "input_modality_quote": "CoCo first applies static code analysis tools (such as AST parsers) to extract rich contextual information at the function, file, and repository levels.",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "Finally, CoCo synthesizes the multi-granularity contextual information and the retrieved code examples into a unified prompt, which is fed into the LLM for code completion.",
    "task_io_type": "ä»£ç åˆ°ä»£ç ",
    "task_io_type_quote": "Finally, CoCo synthesizes the multi-granularity contextual information and the retrieved code examples into a unified prompt, which is fed into the LLM for code completion.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºä»“åº“çº§ä»£ç è¡¥å…¨ï¼Œå¼ºè°ƒå¯¹è·¨æ–‡ä»¶ä¾èµ–ã€é¡¹ç›®ç‰¹å®šçº¦å®šå’Œå…±äº«å·¥å…·çš„ç†è§£ã€‚",
    "unique_features_quote": "This enables the LLM to reason globally and locally about cross-file dependencies, shared utilities, and project-specific conventions, resulting in more accurate and contextually consistent repository-level code completion.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆè´¨é‡', 'ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›', 'è·¨æ–‡ä»¶ä¾èµ–å»ºæ¨¡èƒ½åŠ›']",
    "evaluation_method_normalized": "['Exact Match (EM)']",
    "problem_domain_normalized": "['é€šç”¨è½¯ä»¶å¼€å‘']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç è¡¥å…¨",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.04611_output/content.md",
    "benchmark_name": "Magma",
    "benchmark_name_quote": "Experimental evaluation on the Magma benchmark demonstrates decisive superiority.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Experimental evaluation on the Magma benchmark demonstrates decisive superiority. ... Experiments on the Magma benchmark [33] show that our agentic approach significantly outperformed previous solutions.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ç”Ÿæˆæ¼æ´è¯æ˜ï¼ˆProof-of-Vulnerability, PoVï¼‰è¾“å…¥ï¼Œä»¥è§¦å‘ç›®æ ‡ä»£ç ä½ç½®å¤„çš„æ¼æ´ã€‚è¯¥ä»»åŠ¡å¯¹äºæ¼æ´éªŒè¯ã€å¤ç°ã€åˆ†ç±»ã€è¡¥ä¸éªŒè¯å’Œå›å½’æµ‹è¯•è‡³å…³é‡è¦ã€‚",
    "task_description_quote": "Generating a proof-of-vulnerability (PoV) input that triggers a vulnerability at target code location(s) is a critical task in software security. Such inputs are essential for vulnerability verification, reproduction, triage, patch validation, and regression testing.",
    "dimension": "æ¼æ´è§¦å‘èƒ½åŠ›ã€ç”Ÿæˆæ•ˆç‡",
    "dimension_quote": "PBFuzz triggered 57 vulnerabilities, outperforming all baselines. Critically, PBFuzz exclusively triggered 17 vulnerabilities compared to existing fuzzers. ... Median time-to-exposure: 339 seconds for PBFuzz versus 8680 seconds for AFL++ with CmpLog, representing a 25.6Ã— efficiency gain",
    "evaluation_method": "åœ¨ç»™å®šæ—¶é—´é¢„ç®—å†…æˆåŠŸè§¦å‘çš„æ¼æ´æ•°é‡ï¼ˆCVEæ•°é‡ï¼‰ã€ä¸­ä½æš´éœ²æ—¶é—´",
    "evaluation_method_quote": "PBFuzz triggered 57 vulnerabilities... PBFuzz achieved this within a 30-minute budget per target, compared to 24-hour allocations for conventional approaches. Median time-to-exposure: 339 seconds for PBFuzz versus 8680 seconds for AFL++ with CmpLog",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "è½¯ä»¶å®‰å…¨ã€æ¼æ´å‘ç°",
    "problem_domain_quote": "Proof-of-Vulnerability (PoV) input generation is a critical task in software security",
    "problem_difficulty": "åŒ…å«ç®€å•å’Œå¤æ‚çš„æ¼æ´ï¼ˆå¦‚PHPå’Œlibtiffï¼‰",
    "problem_difficulty_quote": "while LLMs could generate valid PoV inputs for simple vulnerabilities (16 out of 129 CVEs), they struggled with more complex ones (e.g., PHP and libtiff).",
    "language": "æ–‡ä¸­æœªæ˜ç¡®æåŠæ•°æ®é›†åŒ…å«çš„ç¼–ç¨‹è¯­è¨€ï¼Œä½†å®éªŒæ¶‰åŠC/C++ï¼ˆå¦‚libxml2ï¼‰å’ŒPHPç­‰ã€‚",
    "language_quote": "they struggled with more complex ones (e.g., PHP and libtiff). ... Consider the buffer overflow vulnerability in libxml2â€™s xmlSnprintfElementContent function",
    "data_size": "åŒ…å«129ä¸ªCVEï¼ˆå…¬å…±æ¼æ´ä¸æš´éœ²ï¼‰",
    "data_size_quote": "while LLMs could generate valid PoV inputs for simple vulnerabilities (16 out of 129 CVEs)... PBFuzz successfully triggered 59 out of the 129 CVEs",
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "æ¼æ´è§¦å‘è¾“å…¥ç”Ÿæˆ",
    "task_granularity_quote": "Generating a proof-of-vulnerability (PoV) input that triggers a vulnerability at target code location(s)",
    "evaluation_metrics": "æˆåŠŸè§¦å‘çš„CVEæ•°é‡ã€ä¸­ä½æš´éœ²æ—¶é—´ï¼ˆç§’ï¼‰",
    "evaluation_metrics_quote": "PBFuzz triggered 57 vulnerabilities... Median time-to-exposure: 339 seconds for PBFuzz",
    "input_modality": "ç›®æ ‡æ¼æ´ä»£ç ä½ç½®ï¼ˆå¯èƒ½åŒ…å«ä»£ç ç‰‡æ®µï¼‰",
    "input_modality_quote": "using code snippets extracted by static analysis",
    "output_modality": "è§¦å‘æ¼æ´çš„æµ‹è¯•è¾“å…¥ï¼ˆä¾‹å¦‚ï¼Œç‰¹å®šæ ¼å¼çš„æ–‡ä»¶æˆ–æ•°æ®æµï¼‰",
    "output_modality_quote": "generate PoV inputs",
    "task_io_type": "ä»£ç /æ¼æ´æè¿°åˆ°æµ‹è¯•è¾“å…¥",
    "task_io_type_quote": "Generating a proof-of-vulnerability (PoV) input that triggers a vulnerability at target code location(s)",
    "execution_environment": "è¢«æµ‹è¯•çš„ç¨‹åºï¼ˆPUTï¼‰åŠå…¶è¿è¡Œç¯å¢ƒï¼Œé€šå¸¸åŒ…å«æ£€æµ‹æ¼æ´çš„å‡€åŒ–å™¨ï¼ˆsanitizersï¼‰",
    "execution_environment_quote": "sanitizers acting as implicit oracles. ... The generated code is executed in a sandboxed environment with no external libraries.",
    "unique_features": "Magmaæ˜¯ä¸€ä¸ªç”¨äºè¯„ä¼°æ¨¡ç³Šæµ‹è¯•ï¼ˆFuzzingï¼‰å’Œæ¼æ´å‘ç°å·¥å…·çš„åŸºå‡†ï¼Œä¸“æ³¨äºçœŸå®ä¸–ç•Œçš„æ¼æ´ï¼ˆCVEï¼‰ã€‚æœ¬æ–‡å°†å…¶ç”¨ä½œè¯„ä¼°PBFuzzæ¡†æ¶æœ‰æ•ˆæ€§çš„æ ‡å‡†æµ‹è¯•é›†ã€‚",
    "unique_features_quote": "Experimental evaluation on the Magma benchmark demonstrates decisive superiority. ... Experiments on the Magma benchmark [33] show that our agentic approach significantly outperformed previous solutions.",
    "data_size_quantity": 129,
    "data_size_unit": "ä¸ªCVE",
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['C/C++', 'PHP']",
    "dimension_normalized": "['æ¼æ´è§¦å‘èƒ½åŠ›', 'ç”Ÿæˆæ•ˆç‡']",
    "evaluation_method_normalized": "['æˆåŠŸè§¦å‘çš„CVEæ•°é‡', 'ä¸­ä½æš´éœ²æ—¶é—´ï¼ˆç§’ï¼‰']",
    "problem_domain_normalized": "['è½¯ä»¶å®‰å…¨', 'æ¼æ´å‘ç°']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.04738_output/content.md",
    "benchmark_name": "OverpassNL",
    "benchmark_name_quote": "The OverpassNL dataset [20] serves as a parallel supervision source for both directions of structured query modeling.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We evaluate OSMT on a public benchmark against strong baselines and observe consistent improvements in both query generation and interpretation.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "åœ¨è‡ªç„¶è¯­è¨€ä¸ç»“æ„åŒ–æŸ¥è¯¢è¯­è¨€ï¼ˆOverpassQLï¼‰ä¹‹é—´è¿›è¡ŒåŒå‘ç¿»è¯‘ã€‚å…·ä½“åŒ…æ‹¬ï¼š1. Text-to-OverpassQLï¼šå°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢ç¿»è¯‘æˆå¯æ‰§è¡Œçš„OverpassQLè¯­å¥ã€‚2. OverpassQL-to-Textï¼šå°†ç»“æ„åŒ–çš„OverpassQLè„šæœ¬ç¿»è¯‘æˆè¿è´¯çš„è‡ªç„¶è¯­è¨€è§£é‡Šã€‚",
    "task_description_quote": "To facilitate seamless interaction between natural language and structured geospatial queries, we define two complementary tasks: the established Text-to-OverpassQL task and its inverse, OverpassQL-to-Text.",
    "dimension": "æŸ¥è¯¢ç”Ÿæˆä¸è§£é‡Šçš„å‡†ç¡®æ€§ã€ç»“æ„æœ‰æ•ˆæ€§ã€è¯­ä¹‰å¯¹é½ã€å¯è§£é‡Šæ€§ã€å‚æ•°æ•ˆç‡",
    "dimension_quote": "Experimental results demonstrate that OSMT consistently surpasses strong baseline models in terms of accuracy, interpretability, and parameter efficiency, setting new standards for natural language interfaces within geospatial database systems.",
    "evaluation_method": "ä½¿ç”¨å¤šä¸ªæŒ‡æ ‡è¿›è¡Œç»¼åˆè¯„ä¼°ï¼ŒåŒ…æ‹¬ç²¾ç¡®åŒ¹é…ï¼ˆEMï¼‰ã€chrFã€KVSã€TreeSã€OQSã€‚",
    "evaluation_method_quote": "Comparison of OSMT with state-of-the-art open-source and closed-source models on the Text-to-OverpassQL task. Average performance is over five metrics.",
    "context_dependency": "ä¾èµ–OpenStreetMapï¼ˆOSMï¼‰æ•°æ®åº“çš„æ ‡ç­¾ï¼ˆtagï¼‰æ¨¡å¼å’Œæ‹“æ‰‘ç»“æ„ã€‚æŸ¥è¯¢ç”Ÿæˆè¿‡ç¨‹éœ€è¦ç†è§£OSMä¸­ç©ºé—´å®ä½“ï¼ˆèŠ‚ç‚¹ã€è·¯å¾„ã€å…³ç³»ï¼‰çš„å±‚æ¬¡å’Œå…³ç³»ä¾èµ–ã€‚",
    "context_dependency_quote": "In contrast, our approach explicitly captures the hierarchical and relational dependencies inherently encoded within OSM to enhance spatial reasoning capabilities.",
    "problem_domain": "åœ°ç†ç©ºé—´æ•°æ®åº“æŸ¥è¯¢ï¼Œç‰¹åˆ«æ˜¯OpenStreetMapï¼ˆOSMï¼‰æ•°æ®æ£€ç´¢ã€‚",
    "problem_domain_quote": "OSMT, an open-source tag-aware language model specifically designed to bridge natural language and Overpass Query Language (OverpassQL), a structured query language for accessing large-scale OpenStreetMap (OSM) data.",
    "problem_difficulty": "å…·æœ‰ä¸åŒå¤æ‚åº¦çš„åœ°ç†æŸ¥è¯¢æ„å›¾ï¼Œéœ€è¦ç†è§£å¤æ‚çš„OSMæ ‡ç­¾æ¨¡å¼å’ŒOverpassQLè¯­æ³•ã€‚",
    "problem_difficulty_quote": "It contains 8,352 natural language questions paired with real-world OverpassQL queries, spanning diverse geographic intents and varying levels of query complexity.",
    "language": "è‡ªç„¶è¯­è¨€ï¼ˆè¾“å…¥/è¾“å‡ºï¼‰ä¸OverpassQLï¼ˆä¸€ç§ç”¨äºæŸ¥è¯¢OpenStreetMapçš„é¢†åŸŸç‰¹å®šè¯­è¨€ï¼‰ã€‚",
    "language_quote": "OSMT, an open-source tag-aware language model specifically designed to bridge natural language and Overpass Query Language (OverpassQL)...",
    "data_size": "åŒ…å«8,352ä¸ªè‡ªç„¶è¯­è¨€é—®é¢˜ä¸çœŸå®ä¸–ç•ŒOverpassQLæŸ¥è¯¢çš„é…å¯¹ã€‚",
    "data_size_quote": "It contains 8,352 natural language questions paired with real-world OverpassQL queries, spanning diverse geographic intents and varying levels of query complexity.",
    "source_type": "ä»ä¸‰ä¸ªäº’è¡¥æ¥æºæ„å»ºï¼š1. OSM Taginfoï¼ˆæ ‡ç­¾ä¸‰å…ƒç»„ï¼‰ã€‚2. OSM Wikiï¼ˆæ ‡ç­¾-æè¿°å¯¹ï¼‰ã€‚3. OverpassNLæ•°æ®é›†ï¼ˆè‡ªç„¶è¯­è¨€-OverpassQLé…å¯¹ï¼‰ã€‚",
    "source_type_quote": "To support cross-modal pre-training that bridges natural language, symbolic tag knowledge, and structured OverpassQL, we construct a pre-training corpus from three complementary sources.",
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": "ç¤¾åŒºè´¡çŒ®ï¼ˆOSM Taginfo, OSM Wikiï¼‰ä¸å…¬å¼€æ•°æ®é›†ï¼ˆOverpassNLï¼‰çš„ç»“åˆã€‚",
    "build_type_quote": "OSM Taginfo, a community-curated platform that aggregates global statistics on tag usage, frequency, and co-occurrence.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "æ–‡æœ¬åˆ°ä»£ç ï¼ˆText-to-OverpassQLï¼‰å’Œä»£ç åˆ°æ–‡æœ¬ï¼ˆOverpassQL-to-Textï¼‰çš„ç¿»è¯‘ä»»åŠ¡ã€‚",
    "task_granularity_quote": "We define two complementary tasks: the established Text-to-OverpassQL task and its inverse, OverpassQL-to-Text.",
    "evaluation_metrics": "ç²¾ç¡®åŒ¹é…ï¼ˆEMï¼‰ã€chrFã€KVSã€TreeSã€OQSã€‚",
    "evaluation_metrics_quote": "Average performance is over five metrics.",
    "input_modality": "è‡ªç„¶è¯­è¨€æ–‡æœ¬æˆ–OverpassQLä»£ç ã€‚",
    "input_modality_quote": "This task aims to translate a natural language query q into a syntactically correct and semantically meaningful OverpassQL statement Ovq.",
    "output_modality": "OverpassQLä»£ç æˆ–è‡ªç„¶è¯­è¨€æ–‡æœ¬ã€‚",
    "output_modality_quote": "This task performs the reverse mapping by taking a structured OverpassQL script Ovq, typically authored by technical users, and generating a coherent natural language explanation e.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ï¼ˆText-to-OverpassQLï¼‰å’Œä»£ç åˆ°æ–‡æœ¬ï¼ˆOverpassQL-to-Textï¼‰ã€‚",
    "task_io_type_quote": "Illustration of the bidirectional translation between natural language and OverpassQL.",
    "execution_environment": "OpenStreetMapæ•°æ®åº“ç¯å¢ƒï¼Œç”¨äºæ‰§è¡Œç”Ÿæˆçš„OverpassQLæŸ¥è¯¢ä»¥æ£€ç´¢åœ°ç†ç©ºé—´å®ä½“ã€‚",
    "execution_environment_quote": "Retrieving structured geospatial data from OSM typically relies on the Overpass Query Language (OverpassQL), a domain-specific language designed for fine-grained spatial data extraction.",
    "unique_features": "ä¸“æ³¨äºåœ°ç†ç©ºé—´æ•°æ®åº“ï¼ˆOpenStreetMapï¼‰çš„è‡ªç„¶è¯­è¨€æ¥å£ï¼Œæ¶‰åŠå¤æ‚çš„æ ‡ç­¾æ¨¡å¼å’Œæ‹“æ‰‘ç»“æ„ã€‚åŒ…å«åŒå‘ç¿»è¯‘ä»»åŠ¡ï¼ˆText-to-OverpassQL å’Œ OverpassQL-to-Textï¼‰ã€‚",
    "unique_features_quote": "OSMT, an open-source tag-aware language model specifically designed to bridge natural language and Overpass Query Language (OverpassQL), a structured query language for accessing large-scale OpenStreetMap (OSM) data.",
    "data_size_quantity": 8352,
    "data_size_unit": "ä¸ªé…å¯¹",
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['è‡ªç„¶è¯­è¨€', 'OverpassQL']",
    "dimension_normalized": "['æŸ¥è¯¢ç”Ÿæˆä¸è§£é‡Šçš„å‡†ç¡®æ€§', 'ç»“æ„æœ‰æ•ˆæ€§', 'è¯­ä¹‰å¯¹é½', 'å¯è§£é‡Šæ€§', 'å‚æ•°æ•ˆç‡']",
    "evaluation_method_normalized": "['ç²¾ç¡®åŒ¹é…ï¼ˆEMï¼‰', 'chrF', 'KVS', 'TreeS', 'OQS']",
    "problem_domain_normalized": "['åœ°ç†ç©ºé—´æ•°æ®åº“æŸ¥è¯¢', 'OpenStreetMapï¼ˆOSMï¼‰æ•°æ®æ£€ç´¢']",
    "source_type_normalized": "['OSM Taginfoï¼ˆæ ‡ç­¾ä¸‰å…ƒç»„ï¼‰', 'OSM Wikiï¼ˆæ ‡ç­¾-æè¿°å¯¹ï¼‰', 'OverpassNLæ•°æ®é›†ï¼ˆè‡ªç„¶è¯­è¨€-OverpassQLé…å¯¹ï¼‰']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.05962_output/content.md",
    "benchmark_name": "Lean theorem-proving benchmark",
    "benchmark_name_quote": "We evaluate ğ›¼-DPG in Lean, a proof assistant that automatically verifies formal mathematical proofs.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We evaluate ğ›¼-DPG in Lean, a proof assistant that automatically verifies formal mathematical proofs.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "åœ¨Leanå®šç†è¯æ˜åŠ©æ‰‹ä¸­è‡ªåŠ¨éªŒè¯å½¢å¼åŒ–æ•°å­¦è¯æ˜ã€‚",
    "task_description_quote": "We evaluate ğ›¼-DPG in Lean, a proof assistant that automatically verifies formal mathematical proofs.",
    "dimension": "å®šç†è¯æ˜çš„æ­£ç¡®æ€§ä¸è¯æ˜å°è¯•çš„å¤šæ ·æ€§",
    "dimension_quote": "In this setting, success requires not only correct candidates but also diversity across proof attempts, since harder theorems may only be solved by rare derivations.",
    "evaluation_method": "ä½¿ç”¨pass@1å’Œpass@256æŒ‡æ ‡è¯„ä¼°ç²¾åº¦ï¼ˆprecisionï¼‰å’Œè¦†ç›–ç‡ï¼ˆcoverageï¼‰ã€‚",
    "evaluation_method_quote": "We find that ğ›¼-DPG achieves state-of-the-art performance, producing models that lie on the Pareto frontier between precision (pass@1) and coverage (pass@256) and that surpass prior methods in coverage.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "å½¢å¼åŒ–æ•°å­¦å®šç†è¯æ˜",
    "problem_domain_quote": "We evaluate ğ›¼-DPG in Lean, a proof assistant that automatically verifies formal mathematical proofs.",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "Leanï¼ˆå®šç†è¯æ˜è¯­è¨€ï¼‰",
    "language_quote": "We evaluate ğ›¼-DPG in Lean, a proof assistant that automatically verifies formal mathematical proofs.",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "å®šç†è¯æ˜ç”Ÿæˆ",
    "task_granularity_quote": "In this setting, success requires not only correct candidates but also diversity across proof attempts, since harder theorems may only be solved by rare derivations.",
    "evaluation_metrics": "pass@1, pass@256",
    "evaluation_metrics_quote": "We find that ğ›¼-DPG achieves state-of-the-art performance, producing models that lie on the Pareto frontier between precision (pass@1) and coverage (pass@256) and that surpass prior methods in coverage.",
    "input_modality": "å®šç†é™ˆè¿°ï¼ˆå¯èƒ½ä¸ºè‡ªç„¶è¯­è¨€æˆ–å½¢å¼åŒ–è¯­è¨€ï¼‰",
    "input_modality_quote": "Let ğœ‹ğœƒ(Â·|ğ‘¥) : Y â†’â„be an LLM with parameters ğœƒ defining a probability distribution over sequences ğ‘¦âˆˆY conditioned on a prompt ğ‘¥.",
    "output_modality": "å½¢å¼åŒ–è¯æ˜ï¼ˆä»£ç ï¼‰",
    "output_modality_quote": "In this setting, success requires not only correct candidates but also diversity across proof attempts, since harder theorems may only be solved by rare derivations.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ï¼ˆå®šç†é™ˆè¿°åˆ°å½¢å¼åŒ–è¯æ˜ï¼‰",
    "task_io_type_quote": "We evaluate ğ›¼-DPG in Lean, a proof assistant that automatically verifies formal mathematical proofs.",
    "execution_environment": "Leanè¯æ˜åŠ©æ‰‹éªŒè¯ç¯å¢ƒ",
    "execution_environment_quote": "We evaluate ğ›¼-DPG in Lean, a proof assistant that automatically verifies formal mathematical proofs.",
    "unique_features": "è¯¥åŸºå‡†ä½¿ç”¨å®šç†è¯æ˜åŠ©æ‰‹ï¼ˆLeanï¼‰ä½œä¸ºéªŒè¯å™¨ï¼Œå¼ºè°ƒåœ¨ä¿è¯æ­£ç¡®æ€§çš„åŒæ—¶ï¼Œè¯æ˜å°è¯•çš„å¤šæ ·æ€§å¯¹äºè§£å†³æ›´éš¾å®šç†è‡³å…³é‡è¦ã€‚",
    "unique_features_quote": "In this setting, success requires not only correct candidates but also diversity across proof attempts, since harder theorems may only be solved by rare derivations.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Leanï¼ˆå®šç†è¯æ˜è¯­è¨€ï¼‰']",
    "dimension_normalized": "['å®šç†è¯æ˜çš„æ­£ç¡®æ€§ä¸è¯æ˜å°è¯•çš„å¤šæ ·æ€§']",
    "evaluation_method_normalized": "['pass@1', 'pass@256']",
    "problem_domain_normalized": "['å½¢å¼åŒ–æ•°å­¦å®šç†è¯æ˜']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.05908_output/content.md",
    "benchmark_name": "DNext",
    "benchmark_name_quote": "Our evaluation is performed on DNext [7], a proprietary, industrial-scale microservice system for the telecommunications sector.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Our evaluation is performed on DNext [7], a proprietary, industrial-scale microservice system for the telecommunications sector.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "å¾®æœåŠ¡æ¶æ„ä¸­çš„å¤šä»“åº“Bugå®šä½ã€‚å…·ä½“ä»»åŠ¡æ˜¯æ ¹æ®è‡ªç„¶è¯­è¨€BugæŠ¥å‘Šï¼Œåœ¨å¤šä»“åº“ä»£ç åº“ä¸­å®šä½åˆ°ç›¸å…³çš„ä»£ç æ–‡ä»¶ã€‚",
    "task_description_quote": "Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code, LLM context limitations, and the need to first identify the correct repository.",
    "dimension": "Bugå®šä½çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œä»¥åŠå¤šä»“åº“è·¯ç”±èƒ½åŠ›ã€‚",
    "dimension_quote": "Our evaluation aims to answer two primary questions: (1) How effectively does our NL-to-NL approach perform against state-of-the-art baselines on a complex, multi-repository benchmark? (2) How critical is the hierarchical search component to the methodâ€™s accuracy and efficiency?",
    "evaluation_method": "ä½¿ç”¨Pass@kå’ŒRecall@kæŒ‡æ ‡è¿›è¡Œè¯„ä¼°ã€‚å¯¹äºBugå®šä½ï¼Œè®¾ç½®k=10ï¼›å¯¹äºæœç´¢ç©ºé—´è·¯ç”±ï¼Œè®¾ç½®k=3ã€‚",
    "evaluation_method_quote": "We evaluate performance using standard metrics: Pass@k and Recall@k. Pass@k measures the proportion of bug reports where at least one correct file is found in the top-ğ‘˜ results. Recall@k measures the fraction of all correct files for a given bug that are successfully retrieved within the top-ğ‘˜ results. Given that the maximum number of modified files for any bug in our dataset is 10 (average 7.2), we set ğ‘˜= 10 as a fair and comprehensive threshold... For the preliminary search space routing phase, we use a tighter ğ‘˜= 3.",
    "context_dependency": "å¤šæ–‡ä»¶ã€å¤šç›®å½•ã€å¤šä»“åº“çš„å¾®æœåŠ¡æ¶æ„é¡¹ç›®çº§ä¸Šä¸‹æ–‡ã€‚",
    "context_dependency_quote": "Bug localization in multi-repository microservice architectures is challenging... This makes them ineffective for large-scale software projects composed of many interacting microservices, as they lack a mechanism to first route a bug report to the correct repository (codebase) before localization can begin.",
    "problem_domain": "ç”µä¿¡é¢†åŸŸçš„å·¥ä¸šçº§å¾®æœåŠ¡è½¯ä»¶ç³»ç»Ÿã€‚",
    "problem_domain_quote": "Our evaluation is performed on DNext [7], a proprietary, industrial-scale microservice system for the telecommunications sector.",
    "problem_difficulty": "å·¥ä¸šçº§ã€çœŸå®ä¸–ç•Œã€å…·æœ‰æŒ‘æˆ˜æ€§çš„Bugå®šä½ä»»åŠ¡ã€‚",
    "problem_difficulty_quote": "Its scale (see Table 1) and use of real-world, often noisy, bug reports provide a challenging benchmark that standard academic datasets cannot replicate.",
    "language": "Java",
    "language_quote": "Table 1: Statistics of the DNext microservice dataset. Metric: Programming Language, Value: Java",
    "data_size": "åŒ…å«46ä¸ªä»“åº“ï¼Œ7,077ä¸ªä»£ç æ–‡ä»¶ï¼Œçº¦110ä¸‡è¡Œç‰©ç†ä»£ç ï¼Œ87ä¸ªBugå·¥å•ï¼Œå¹³å‡æ¯ä¸ªå·¥å•æ¶‰åŠ7.2ä¸ªBugæ–‡ä»¶ã€‚",
    "data_size_quote": "Table 1: Statistics of the DNext microservice dataset. Metric: Number of Repositories, Value: 46; Total Number of Code Files, Value: 7,077; Total Physical Lines of Code, Value: âˆ¼1.1M; Number of Bug Tickets, Value: 87; Average Buggy Files per Ticket, Value: 7.2",
    "source_type": "ä¸“æœ‰çš„å·¥ä¸šçº§å¾®æœåŠ¡ç³»ç»Ÿï¼Œæ•°æ®æ¥æºäºçœŸå®çš„Bugå·¥å•å’Œä»£ç ä¿®æ”¹è®°å½•ã€‚",
    "source_type_quote": "Our evaluation is performed on DNext [7], a proprietary, industrial-scale microservice system for the telecommunications sector. The ground truth for each bug ticket is the set of files modified in the pull request that resolved the issue.",
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": "ä¸“æœ‰å·¥ä¸šç³»ç»Ÿï¼Œéå…¬å¼€ç¤¾åŒºæ„å»ºã€‚",
    "build_type_quote": "Our evaluation is performed on DNext [7], a proprietary, industrial-scale microservice system...",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç æ£€ç´¢/å®šä½ï¼ˆä»è‡ªç„¶è¯­è¨€BugæŠ¥å‘Šå®šä½åˆ°ä»£ç æ–‡ä»¶ï¼‰",
    "task_granularity_quote": "We propose reframing this as a natural language reasoning task by transforming codebases into hierarchical NL summaries and performing NL-to-NL search instead of cross-modal retrieval.",
    "evaluation_metrics": "Pass@k, Recall@k, MRR (Mean Reciprocal Rank)",
    "evaluation_metrics_quote": "We evaluate performance using standard metrics: Pass@k and Recall@k... For the preliminary search space routing phase, we use a tighter ğ‘˜= 3. ...Our hierarchical approach achieves the highest Pass@10 and MRR...",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆBugæŠ¥å‘Šï¼‰",
    "input_modality_quote": "Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code...",
    "output_modality": "ä»£ç æ–‡ä»¶åˆ—è¡¨ï¼ˆæ’åï¼‰",
    "output_modality_quote": "The LLM performs a detailed analysis of these filtered file summaries to produce the final ranked list of files most likely to be the source of the bug.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ï¼ˆæ£€ç´¢ï¼‰",
    "task_io_type_quote": "...reframing the problem. This shift from a cross-modal retrieval task to a unified NL-to-NL reasoning task...",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“ä¸ºå¤šä»“åº“å¾®æœåŠ¡æ¶æ„Bugå®šä½è®¾è®¡çš„å·¥ä¸šçº§åŸºå‡†ï¼ŒåŒ…å«çœŸå®çš„ã€æœ‰å™ªå£°çš„BugæŠ¥å‘Šï¼Œè§„æ¨¡è¿œè¶…æ ‡å‡†å­¦æœ¯æ•°æ®é›†ã€‚",
    "unique_features_quote": "Its scale (see Table 1) and use of real-world, often noisy, bug reports provide a challenging benchmark that standard academic datasets cannot replicate.",
    "data_size_quantity": 1100000,
    "data_size_unit": "è¡Œ",
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Java']",
    "dimension_normalized": "['Bugå®šä½çš„å‡†ç¡®æ€§', 'æ•ˆç‡', 'å¤šä»“åº“è·¯ç”±èƒ½åŠ›']",
    "evaluation_method_normalized": "['Pass@k', 'Recall@k', 'MRR']",
    "problem_domain_normalized": "['ç”µä¿¡é¢†åŸŸ', 'å·¥ä¸šçº§å¾®æœåŠ¡è½¯ä»¶ç³»ç»Ÿ']",
    "source_type_normalized": "['ä¸“æœ‰çš„å·¥ä¸šçº§å¾®æœåŠ¡ç³»ç»Ÿ', 'çœŸå®çš„Bugå·¥å•', 'ä»£ç ä¿®æ”¹è®°å½•']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.08810_output/content.md",
    "benchmark_name": "CALIBRI",
    "benchmark_name_quote": "To foster future research on this important issue, we make our Dataset CALIBRI available as a contribution1.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "To foster future research on this important issue, we make our Dataset CALIBRI available as a contribution1.",
    "dataset_url": "https://huggingface.co/datasets/lavis-nlp/CALIBRI",
    "dataset_url_quote": "1https://huggingface.co/datasets/lavis-nlp/CALIBRI",
    "task_description": "ç”¨äºç ”ç©¶ä»£ç å¤§è¯­è¨€æ¨¡å‹ï¼ˆCode LLMï¼‰çš„æ ¡å‡†å’Œä¸ç¡®å®šæ€§ä¼°è®¡ã€‚æ•°æ®é›†åŒ…å«æ¨¡å‹ç”Ÿæˆçš„ä»£ç ã€å…¶ç½®ä¿¡åº¦ï¼ˆtokenä¼¼ç„¶ï¼‰ä»¥åŠæ­£ç¡®æ€§æ ‡ç­¾ï¼Œæ—¨åœ¨æ”¯æŒå¯¹æ¨¡å‹ç½®ä¿¡åº¦ä¸ä»£ç å®é™…æ­£ç¡®æ€§ä¹‹é—´å…³ç³»çš„ç ”ç©¶ã€‚",
    "task_description_quote": "CALIBRI includes correctness labels and token likelihoods for latest-generation LLMs on a fresh dataset with relatively low risk of data leakage.",
    "dimension": "æ¨¡å‹ç½®ä¿¡åº¦æ ¡å‡†ã€ä¸ç¡®å®šæ€§ä¼°è®¡",
    "dimension_quote": "As AI-based code generation becomes widespread, researchers are investigating the calibration of code LLMs â€” ensuring their confidence scores faithfully represent the true likelihood of code correctness.",
    "evaluation_method": "ä½¿ç”¨æ ¡å‡†åº¦é‡ï¼Œå¦‚é¢„æœŸæ ¡å‡†è¯¯å·®ï¼ˆECEï¼‰ã€Brieråˆ†æ•°å’ŒBrieræŠ€èƒ½åˆ†æ•°ï¼ˆBSSï¼‰ï¼Œæ¥è¡¡é‡æ¨¡å‹ç½®ä¿¡åº¦ä¸ä»£ç å®é™…æ­£ç¡®æ€§ï¼ˆé€šè¿‡å•å…ƒæµ‹è¯•è¯„ä¼°ï¼‰çš„åŒ¹é…ç¨‹åº¦ã€‚",
    "evaluation_method_quote": "Therefore, several measures are commonly used to quantify the calibration error of a model: The Expected Calibration Error (ECE)... Brier Score... Brier Skill Score (BSS)...",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "é€šç”¨ç¼–ç¨‹ä»»åŠ¡ï¼ˆå‡½æ•°åˆæˆï¼‰",
    "problem_domain_quote": "We study four multicalibration approaches on three function synthesis benchmarks...",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "æœªæ˜ç¡®æŒ‡å®šCALIBRIæ•°æ®é›†æœ¬èº«çš„è¯­è¨€ï¼Œä½†è®ºæ–‡å®éªŒä½¿ç”¨äº†MultiPL-Eï¼ˆ20ç§è¯­è¨€ï¼‰å’ŒMcEvalï¼ˆ40ç§è¯­è¨€ï¼‰ç­‰å¤šè¯­è¨€åŸºå‡†ã€‚",
    "language_quote": "We conduct an evaluation using latest open-weight reasoning models... on three code generation datasets: the multilingual benchmarks MultiPL-E [5] and McEval [6], which offer 20 and 40 programming languages, respectively...",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": "åŸºäºä¸‰ä¸ªç°æœ‰çš„ä»£ç ç”ŸæˆåŸºå‡†ï¼ˆMultiPL-E, McEval, LiveCodeBenchï¼‰æ„å»ºï¼ŒåŒ…å«åœ¨è¿™äº›åŸºå‡†ä¸Šæœ€æ–°ä»£ç LLMçš„ç”Ÿæˆç»“æœã€ç½®ä¿¡åº¦å’Œæ­£ç¡®æ€§æ ‡ç­¾ã€‚",
    "source_type_quote": "We conduct an evaluation using latest open-weight reasoning models... on three code generation datasets: the multilingual benchmarks MultiPL-E [5] and McEval [6], which offer 20 and 40 programming languages, respectively, and Live-CodeBench [25]... CALIBRI includes correctness labels and token likelihoods for latest-generation LLMs on a fresh dataset...",
    "last_updated": "2025",
    "last_updated_quote": "arXiv:2512.08810v1  [cs.SE]  9 Dec 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆç”±æœ¬æ–‡ä½œè€…æ„å»ºå¹¶å‘å¸ƒï¼‰",
    "build_type_quote": "To foster future research on this important issue, we make our Dataset CALIBRI available as a contribution1.",
    "contamination_status": "ç›¸å¯¹è¾ƒä½çš„æ•°æ®æ³„éœ²é£é™©",
    "contamination_status_quote": "CALIBRI includes correctness labels and token likelihoods for latest-generation LLMs on a fresh dataset with relatively low risk of data leakage.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆå‡½æ•°åˆæˆï¼‰",
    "task_granularity_quote": "We study four multicalibration approaches on three function synthesis benchmarks...",
    "evaluation_metrics": "é¢„æœŸæ ¡å‡†è¯¯å·®ï¼ˆECEï¼‰ã€Brieråˆ†æ•°ã€BrieræŠ€èƒ½åˆ†æ•°ï¼ˆBSSï¼‰",
    "evaluation_metrics_quote": "Therefore, several measures are commonly used to quantify the calibration error of a model: The Expected Calibration Error (ECE)... Brier Score... Brier Skill Score (BSS)...",
    "input_modality": "è‡ªç„¶è¯­è¨€ä»»åŠ¡æè¿°å’Œ/æˆ–ä»£ç ä¸Šä¸‹æ–‡",
    "input_modality_quote": "We assume the input prompt ğ‘‹ to consist of a task description and/or code context...",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "...and the output ğ‘ = ğ‘§1:ğ‘› to be a sequence of code tokens.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "We assume the input prompt ğ‘‹ to consist of a task description and/or code context, and the output ğ‘ = ğ‘§1:ğ‘› to be a sequence of code tokens.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“é—¨ä¸ºä»£ç LLMçš„æ ¡å‡†ç ”ç©¶è®¾è®¡ï¼ŒåŒ…å«æ¨¡å‹ç½®ä¿¡åº¦ï¼ˆtokenä¼¼ç„¶ï¼‰å’Œæ­£ç¡®æ€§æ ‡ç­¾ã€‚æ”¯æŒå¤šæ ¡å‡†ï¼ˆmulticalibrationï¼‰ç ”ç©¶ï¼Œå¯åŸºäºä»£ç å¤æ‚æ€§ã€é•¿åº¦ã€æç¤ºé•¿åº¦æˆ–ç¼–ç¨‹è¯­è¨€å¯¹é—®é¢˜è¿›è¡Œåˆ†ç»„ã€‚",
    "unique_features_quote": "We fill this gap by using multicalibration [22], which groups inputs according to semantic facets and applies a group-specific calibration... We apply multi-calibration to code generation by grouping coding problems by complexity and/or programming language.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['MultiPL-E', 'McEval', 'å¤šè¯­è¨€']",
    "dimension_normalized": "['æ¨¡å‹ç½®ä¿¡åº¦æ ¡å‡†', 'ä¸ç¡®å®šæ€§ä¼°è®¡']",
    "evaluation_method_normalized": "['é¢„æœŸæ ¡å‡†è¯¯å·®ï¼ˆECEï¼‰', 'Brieråˆ†æ•°', 'BrieræŠ€èƒ½åˆ†æ•°ï¼ˆBSSï¼‰']",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹ä»»åŠ¡ï¼ˆå‡½æ•°åˆæˆï¼‰']",
    "source_type_normalized": "['MultiPL-E', 'McEval', 'LiveCodeBench', 'ä»£ç ç”ŸæˆåŸºå‡†', 'ä»£ç LLMç”Ÿæˆç»“æœ', 'ç½®ä¿¡åº¦', 'æ­£ç¡®æ€§æ ‡ç­¾']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2512.08867_output/content.md",
    "benchmark_name": "SimpleDevQA",
    "benchmark_name_quote": "we introduce SimpleDevQA, a multilingual Dev Knowledge QA benchmark derived from real user dialogues.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "In this paper, we propose SimpleDevQA, a Dev Knowledge QA benchmark built from real developer dialogues, to address the aforementioned problems.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "å¼€å‘çŸ¥è¯†é—®ç­”ï¼Œæ—¨åœ¨ä¸ºè½¯ä»¶å¼€å‘è¿‡ç¨‹ä¸­æå‡ºçš„çŸ¥è¯†å¯»æ±‚é—®é¢˜æä¾›å‡†ç¡®ã€ç›¸å…³çš„è‡ªç„¶è¯­è¨€ç­”æ¡ˆã€‚",
    "task_description_quote": "The Development Knowledge Question Answering (Dev Knowledge QA) task aims to address knowledge-seeking questions posed during the software development process by providing accurate and relevant natural language answers.",
    "dimension": "è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨çœŸå®ç¼–ç¨‹åœºæ™¯ä¸­å¯¹è½¯ä»¶å¼€å‘çŸ¥è¯†çš„ç†è§£èƒ½åŠ›ã€‚",
    "dimension_quote": "to assess LLMsâ€™ understanding capability of software development knowledge in real programming scenarios.",
    "evaluation_method": "é€šè¿‡ä¸€ä¸ªç‹¬ç«‹çš„è¯„åˆ¤å¤§è¯­è¨€æ¨¡å‹ï¼Œè¾“å…¥æ¨¡å‹çš„é¢„æµ‹ç»“æœå’Œå‚è€ƒç­”æ¡ˆæ¥éªŒè¯æ­£ç¡®æ€§ã€‚",
    "evaluation_method_quote": "we evaluate SimpleDevQA by prompting a separate judge LLM with the modelâ€™s prediction and the reference answer to verify correctness.",
    "context_dependency": "ç‹¬ç«‹çš„çŸ¥è¯†é—®ç­”å¯¹ï¼Œä¸ä¾èµ–ç‰¹å®šä»£ç ä¸Šä¸‹æ–‡ã€‚",
    "context_dependency_quote": "This dataset contains 2,740 QA pairs... focuses on questions with unique, short, and verifiable answers",
    "problem_domain": "è½¯ä»¶å¼€å‘çŸ¥è¯†ï¼Œæ¶µç›–åº•å±‚ç³»ç»Ÿã€æ•°æ®åº“ã€ç½‘ç»œåè®®ã€ç®—æ³•åŸç†ç­‰ï¼Œè€Œä¸ä»…ä»…æ˜¯ä»£ç ç†è§£ã€‚",
    "problem_domain_quote": "practical software development demands a much broader understanding, encompassing knowledge of underlying systems, databases, network protocols, algorithmic principles, etc.",
    "problem_difficulty": "é€šè¿‡è¿‡æ»¤æ‰è¢«è®¤ä¸ºè¿‡äºç®€å•çš„é—®é¢˜æ¥å¢åŠ éš¾åº¦ã€‚",
    "problem_difficulty_quote": "To increase difficulty, four powerful LLMs filter out QA pairs deemed too easy;",
    "language": "è‹±è¯­ã€ä¸­æ–‡å’Œä¿„è¯­ã€‚",
    "language_quote": "This dataset contains 2,740 QA pairs in three languages (English, Chinese, and Russian)",
    "data_size": "åŒ…å«2740ä¸ªå¼€å‘çŸ¥è¯†é—®ç­”å¯¹ã€‚",
    "data_size_quote": "we finally build the SimpleDevQA benchmark, which contains 2,740 Dev Knowledge QA pairs",
    "source_type": "æºè‡ªçœŸå®ç”¨æˆ·ä¸ChatGPTçš„å¯¹è¯ï¼ˆWildChatæ•°æ®é›†ï¼‰ï¼Œå¹¶ç»“åˆç½‘ç»œæ£€ç´¢çš„å‚è€ƒæ–‡æ¡£ã€‚",
    "source_type_quote": "we collect 3,000 real user conversations from WildChat as a seed dataset... retrieve relevant reference documents from the web",
    "last_updated": "2025",
    "last_updated_quote": "Publication date: December 2025.",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œé€šè¿‡ä¸€ä¸ªä¸‰é˜¶æ®µçš„æ•°æ®æ„å»ºæµç¨‹å°†çœŸå®å¯¹è¯è½¬åŒ–ä¸ºåŸºå‡†ã€‚",
    "build_type_quote": "we design and implement a three-phase data construction pipeline to convert real-world SE-related conversations into a Dev Knowledge QA benchmark.",
    "contamination_status": "æºè‡ª2025å¹´çš„çœŸå®ç”¨æˆ·å¯¹è¯ï¼Œæ—¨åœ¨åæ˜ çœŸå®çš„å¼€å‘è€…éœ€æ±‚å’ŒæŸ¥è¯¢æ¨¡å¼ï¼Œæ±¡æŸ“é£é™©ç›¸å¯¹è¾ƒä½ã€‚",
    "contamination_status_quote": "built from real developer dialogues... to reflect genuine developer task demands and query patterns.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "çŸ¥è¯†é—®ç­”",
    "task_granularity_quote": "Development Knowledge Question Answering (Dev Knowledge QA) task",
    "evaluation_metrics": "é€šè¿‡è¯„åˆ¤LLMéªŒè¯ç­”æ¡ˆçš„æ­£ç¡®æ€§ï¼ˆå‡†ç¡®ç‡ï¼‰ã€‚",
    "evaluation_metrics_quote": "prompting a separate judge LLM with the modelâ€™s prediction and the reference answer to verify correctness.",
    "input_modality": "è‡ªç„¶è¯­è¨€é—®é¢˜",
    "input_modality_quote": "question inquiring about development knowledge",
    "output_modality": "è‡ªç„¶è¯­è¨€ç­”æ¡ˆ",
    "output_modality_quote": "providing accurate and relevant natural language answers",
    "task_io_type": "æ–‡æœ¬åˆ°æ–‡æœ¬ï¼ˆè‡ªç„¶è¯­è¨€é—®é¢˜åˆ°è‡ªç„¶è¯­è¨€ç­”æ¡ˆï¼‰",
    "task_io_type_quote": "knowledge-seeking questions... providing accurate and relevant natural language answers",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "1. æºè‡ªçœŸå®ç”¨æˆ·å¯¹è¯ï¼Œåæ˜ çœŸå®å¼€å‘è€…éœ€æ±‚ã€‚2. ä¸“æ³¨äºå…·æœ‰å•ä¸€ã€æ˜ç¡®ã€å¯éªŒè¯ç­”æ¡ˆçš„é—®é¢˜ï¼Œä¾¿äºè¯„ä¼°ã€‚3. æ¶µç›–å¹¿æ³›çš„è½¯ä»¶å¼€å‘çŸ¥è¯†ï¼Œè€Œä¸ä»…ä»…æ˜¯ä»£ç ç†è§£ã€‚4. æ¯ä¸ªé—®ç­”å¯¹éƒ½é™„æœ‰ç½‘ç»œæ£€ç´¢çš„å‚è€ƒæ–‡æ¡£ï¼Œå¯ç”¨äºéªŒè¯äº‹å®å‡†ç¡®æ€§ã€‚",
    "unique_features_quote": "The benchmark focuses on questions with single, correct answers, making evaluation more accurate and simple... Additionally, each QA pair is also accompanied by multiple web-retrieved references, which can be used to verify the factual accuracy of the answer.",
    "data_size_quantity": 2740,
    "data_size_unit": "ä¸ª",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['è‹±è¯­', 'ä¸­æ–‡', 'ä¿„è¯­']",
    "dimension_normalized": "['è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨çœŸå®ç¼–ç¨‹åœºæ™¯ä¸­å¯¹è½¯ä»¶å¼€å‘çŸ¥è¯†çš„ç†è§£èƒ½åŠ›']",
    "evaluation_method_normalized": "['é€šè¿‡è¯„åˆ¤LLMéªŒè¯ç­”æ¡ˆçš„æ­£ç¡®æ€§ï¼ˆå‡†ç¡®ç‡ï¼‰']",
    "problem_domain_normalized": "['è½¯ä»¶å¼€å‘çŸ¥è¯†', 'åº•å±‚ç³»ç»Ÿ', 'æ•°æ®åº“', 'ç½‘ç»œåè®®', 'ç®—æ³•åŸç†']",
    "source_type_normalized": "['æºè‡ªçœŸå®ç”¨æˆ·ä¸ChatGPTçš„å¯¹è¯ï¼ˆWildChatæ•°æ®é›†ï¼‰', 'ç½‘ç»œæ£€ç´¢çš„å‚è€ƒæ–‡æ¡£']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.09543_output/content.md",
    "benchmark_name": "SWE-bench Verified Mini",
    "benchmark_name_quote": "For our evaluation dataset, we use SWE-bench Verified Mini, a curated 50-task subset of SWE-bench Verified.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We conduct a controlled evaluation of four leading agentic frameworks (SWE-Agent, OpenHands, Mini SWE Agent, AutoCodeRover) using two SLMs (Gemma-3 4B, Qwen-3 1.7B) on the SWE-bench Verified Mini benchmark.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è§£å†³ç°å®ä¸–ç•Œä¸­çš„è½¯ä»¶å·¥ç¨‹é—®é¢˜ï¼Œå…·ä½“æ˜¯GitHubä»“åº“ä¸­çš„çœŸå®issueã€‚",
    "task_description_quote": "Systems such as SWE-Agent, OpenHands, and AutoCodeRover have demonstrated strong performance on benchmarks like SWE-bench, which evaluates real-world GitHub issues as problem statements.",
    "dimension": "è½¯ä»¶å·¥ç¨‹é—®é¢˜è§£å†³çš„æœ‰æ•ˆæ€§ã€æ•ˆç‡å’Œèµ„æºåˆ©ç”¨ç‡",
    "dimension_quote": "We then cataloged all measurable outputs from a run, including status, time, energy, tokens, memory, and cost, ensuring each candidate metric was compatible with the SWE-bench benchmark and common evaluation practices.",
    "evaluation_method": "ä½¿ç”¨å®˜æ–¹SWE-benchè¯„ä¼°è„šæœ¬ï¼Œåˆ¤æ–­æ™ºèƒ½ä½“ç”Ÿæˆçš„è¡¥ä¸æ˜¯å¦èƒ½æ­£ç¡®è§£å†³é—®é¢˜å¹¶é€šè¿‡æ‰€æœ‰ç›¸å…³æµ‹è¯•ã€‚",
    "evaluation_method_quote": "A run passes only if the agent-generated patch correctly resolves the issue and all associated tests pass.",
    "context_dependency": "å¤šæ–‡ä»¶é¡¹ç›®ä¸Šä¸‹æ–‡ï¼Œæ¶‰åŠå®Œæ•´çš„ä»£ç ä»“åº“ã€‚",
    "context_dependency_quote": "It employs ReAct-style prompting [24], where each iteration consists of thought, action, and observation until the issue is resolved or a stopping condition is reached. (ç»“åˆSWE-benchè¯„ä¼°çœŸå®GitHubä»“åº“é—®é¢˜çš„èƒŒæ™¯æ¨æ–­)",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ï¼Œå…·ä½“ä¸ºä»£ç ä¿®å¤å’Œé—®é¢˜è§£å†³ã€‚",
    "problem_domain_quote": "Autonomous software engineering agents, powered by Large Language Models (LLMs), have emerged as a transformative paradigm in software development [9, 21], demonstrating impressive capabilities in resolving real-world code issues on benchmarks like SWE-bench.",
    "problem_difficulty": "çœŸå®ä¸–ç•Œéš¾åº¦ï¼Œè¦†ç›–ä¸åŒéš¾åº¦çº§åˆ«çš„é—®é¢˜ã€‚",
    "problem_difficulty_quote": "This benchmark preserves the distribution of difficulty, repository diversity, and test pass rates observed in the full 500-task dataset while reducing storage requirements from approximately 130 GB to 5 GB [10].",
    "language": "æ–‡ä¸­æœªæ˜ç¡®æåŠï¼Œä½†SWE-benché€šå¸¸åŒ…å«å¤šç§ç¼–ç¨‹è¯­è¨€ã€‚",
    "language_quote": NaN,
    "data_size": "åŒ…å«50ä¸ªä»»åŠ¡çš„ç²¾é€‰å­é›†ï¼Œæ˜¯å®Œæ•´500ä»»åŠ¡æ•°æ®é›†çš„å­é›†ã€‚",
    "data_size_quote": "For our evaluation dataset, we use SWE-bench Verified Mini, a curated 50-task subset of SWE-bench Verified.",
    "source_type": "æ¥è‡ªçœŸå®GitHubä»“åº“çš„é—®é¢˜ã€‚",
    "source_type_quote": "Systems such as SWE-Agent, OpenHands, and AutoCodeRover have demonstrated strong performance on benchmarks like SWE-bench, which evaluates real-world GitHub issues as problem statements.",
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ä¿®å¤ä¸é—®é¢˜è§£å†³",
    "task_granularity_quote": "demonstrating impressive capabilities in resolving real-world code issues on benchmarks like SWE-bench.",
    "evaluation_metrics": "è§£å†³çŠ¶æ€ï¼ˆé€šè¿‡/å¤±è´¥ï¼‰ã€å¤±è´¥æ¨¡å¼ã€èƒ½é‡ä½¿ç”¨ã€æŒ‚é’ŸæŒç»­æ—¶é—´ã€ä»¤ç‰Œä½¿ç”¨ã€LLMè°ƒç”¨è®¡æ•°ã€å†…å­˜ä½¿ç”¨ã€LLMæˆæœ¬",
    "evaluation_metrics_quote": "We then cataloged all measurable outputs from a run, including status, time, energy, tokens, memory, and cost, ensuring each candidate metric was compatible with the SWE-bench benchmark and common evaluation practices.",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆGitHub issueæè¿°ï¼‰ä¸ä»£ç ä»“åº“ä¸Šä¸‹æ–‡",
    "input_modality_quote": "which evaluates real-world GitHub issues as problem statements.",
    "output_modality": "ä»£ç è¡¥ä¸",
    "output_modality_quote": "A run passes only if the agent-generated patch correctly resolves the issue and all associated tests pass.",
    "task_io_type": "æ–‡æœ¬ï¼ˆé—®é¢˜æè¿°ï¼‰åˆ°ä»£ç ï¼ˆè¡¥ä¸ï¼‰",
    "task_io_type_quote": "which evaluates real-world GitHub issues as problem statements. ... the agent-generated patch",
    "execution_environment": "æ²™ç›’åŒ–Dockeræ‰§è¡Œç¯å¢ƒ",
    "execution_environment_quote": "It supports sandboxed Docker execution environments and accommodates diverse architectures from simple ReAct loops to complex planning approaches.",
    "unique_features": "ä¸“æ³¨äºè¯„ä¼°è‡ªä¸»è½¯ä»¶å·¥ç¨‹æ™ºèƒ½ä½“è§£å†³çœŸå®ä¸–ç•ŒGitHubé—®é¢˜çš„èƒ½åŠ›ï¼ŒåŒ…å«å®Œæ•´çš„ä»“åº“ä¸Šä¸‹æ–‡å’Œæµ‹è¯•ã€‚",
    "unique_features_quote": "demonstrating impressive capabilities in resolving real-world code issues on benchmarks like SWE-bench.",
    "data_size_quantity": 50,
    "data_size_unit": "ä¸ªä»»åŠ¡",
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['å¤šç§ç¼–ç¨‹è¯­è¨€']",
    "dimension_normalized": "['è½¯ä»¶å·¥ç¨‹é—®é¢˜è§£å†³çš„æœ‰æ•ˆæ€§', 'æ•ˆç‡å’Œèµ„æºåˆ©ç”¨ç‡']",
    "evaluation_method_normalized": "['è§£å†³çŠ¶æ€ï¼ˆé€šè¿‡/å¤±è´¥ï¼‰', 'å¤±è´¥æ¨¡å¼', 'èƒ½é‡ä½¿ç”¨', 'æŒ‚é’ŸæŒç»­æ—¶é—´', 'ä»¤ç‰Œä½¿ç”¨', 'LLMè°ƒç”¨è®¡æ•°', 'å†…å­˜ä½¿ç”¨', 'LLMæˆæœ¬']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'ä»£ç ä¿®å¤', 'é—®é¢˜è§£å†³']",
    "source_type_normalized": "['çœŸå®GitHubä»“åº“çš„é—®é¢˜']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ä¿®å¤",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.09108_output/content.md",
    "benchmark_name": "AtCoder Heuristic Contest",
    "benchmark_name_quote": "(1) the ALE Agent [16] on the AtCoder Heuristic Contest, tackling competitive programming problems;",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We evaluate Artemis on four representative agent systems: the ALE Agent for competitive programming on AtCoder Heuristic Contest...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è§£å†³ç«äº‰æ€§ç¼–ç¨‹é—®é¢˜",
    "task_description_quote": "...tackling competitive programming problems;",
    "dimension": "ä»£ç ç”Ÿæˆæ­£ç¡®æ€§ï¼ˆæ¥å—ç‡ï¼‰",
    "dimension_quote": "...achieving a 13.6% improvement in acceptance rate;",
    "evaluation_method": "æ¥å—ç‡",
    "evaluation_method_quote": "...achieving a 13.6% improvement in acceptance rate;",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "ç®—æ³•ï¼Œç«äº‰æ€§ç¼–ç¨‹",
    "problem_domain_quote": "...tackling competitive programming problems;",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": NaN,
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "...tackling competitive programming problems;",
    "evaluation_metrics": "æ¥å—ç‡",
    "evaluation_metrics_quote": "...achieving a 13.6% improvement in acceptance rate;",
    "input_modality": NaN,
    "input_modality_quote": NaN,
    "output_modality": "ä»£ç ",
    "output_modality_quote": "...tackling competitive programming problems;",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "...tackling competitive programming problems;",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": NaN,
    "unique_features_quote": NaN,
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆæ­£ç¡®æ€§ï¼ˆæ¥å—ç‡ï¼‰']",
    "evaluation_method_normalized": "['æ¥å—ç‡']",
    "problem_domain_normalized": "['ç®—æ³•', 'ç«äº‰æ€§ç¼–ç¨‹']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.10713_output/content.md",
    "benchmark_name": "PACIFIC (Precise Automatically Checked Instruction Following In Code)",
    "benchmark_name_quote": "We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs...",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We present PACIFIC, a novel framework designed to automatically generate benchmarks... We introduce PACIFIC, a novel framework for automatically generating benchmarks...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é¡ºåºæŒ‡ä»¤éµå¾ªå’Œä»£ç å¹²è¿è¡Œï¼ˆæ¨¡æ‹Ÿä»£ç æ‰§è¡Œï¼‰èƒ½åŠ›ã€‚æ¨¡å‹éœ€è¦æ ¹æ®ç»™å®šçš„åˆå§‹è¾“å…¥å’Œä¸€ç³»åˆ—æŒ‡ä»¤ï¼Œæ¨¡æ‹Ÿæ‰§è¡Œè¿‡ç¨‹å¹¶äº§ç”Ÿæœ€ç»ˆè¾“å‡ºã€‚",
    "task_description_quote": "...a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs... The model under evaluation is tasked with executing the instructions on the given input and producing the final output in the specified format.",
    "dimension": "æŒ‡ä»¤éµå¾ªçš„ç²¾ç¡®æ€§ã€ä»£ç å¹²è¿è¡Œï¼ˆæ¨ç†ï¼‰èƒ½åŠ›ã€å¤„ç†å¤šæ­¥ä»»åŠ¡çš„èƒ½åŠ›",
    "dimension_quote": "...assess sequential instruction-following and code dry-running capabilities in LLMs... The framework emphasizes the ability of LLMs to simulate code execution (i.e. dry-running) without relying on external tools or agentic mechanisms.",
    "evaluation_method": "é€šè¿‡å°†æ¨¡å‹è¾“å‡ºä¸å‚è€ƒä»£ç æ‰§è¡Œç”Ÿæˆçš„é¢„æœŸç»“æœè¿›è¡Œç®€å•æ¯”è¾ƒï¼Œå®ç°è‡ªåŠ¨åŒ–å’Œç¡®å®šæ€§çš„è¯„ä¼°ã€‚",
    "evaluation_method_quote": "The expected results are computed by executing reference implementations of the described tasks, enabling automatic and deterministic evaluation via simple output comparison.",
    "context_dependency": "å¤šæ­¥é¡ºåºæŒ‡ä»¤ã€‚æ¯ä¸ªæ ·æœ¬åŒ…å«ä¸€ä¸ªåˆå§‹è¾“å…¥å’Œä¸€ä¸ªæŒ‡ä»¤åºåˆ—ï¼Œæ¨¡å‹éœ€è¦æŒ‰é¡ºåºå¤„ç†è¿™äº›æŒ‡ä»¤ã€‚",
    "context_dependency_quote": "Each sample includes: (1) An initial input â€” either a number or a string. (2) A sequence of instructions â€” operations to be applied to the input.",
    "problem_domain": "é€šç”¨ç¼–ç¨‹é€»è¾‘ä¸ç®—æ³•æ¨¡æ‹Ÿã€‚æŒ‡ä»¤æ¶‰åŠæ•°å­¦è¿ç®—ï¼ˆå¦‚æ‰¾è´¨æ•°ã€å®Œç¾å¹³æ–¹ï¼‰ã€å­—ç¬¦ä¸²æ“ä½œï¼ˆå¦‚å­—ç¬¦ç§»ä½ï¼‰å’ŒåŸºæœ¬é€»è¾‘è½¬æ¢ã€‚",
    "problem_domain_quote": "Examples: (1) next_perfect_square: take the previous number and output the first perfect square that comes after it... (2) shift_back: subtract 1 from each character of the previous output; a becomes z... Instruction 1: Given the previous number, output the smallest number bigger than the previous answer that is a prime number.",
    "problem_difficulty": "éš¾åº¦å¯æ§ï¼ŒèŒƒå›´ä»æ˜“åˆ°æå…·æŒ‘æˆ˜æ€§ã€‚éš¾åº¦ç»´åº¦åŒ…æ‹¬æŒ‡ä»¤æ•°é‡å’Œé¢„æœŸè¾“å‡ºé•¿åº¦ã€‚",
    "problem_difficulty_quote": "...while allowing control over benchmark difficulty... The difficulty dimensions are: (1) LLM input: the number of instructions in each sample (prompt). (2) LLM expected output: the expected length of each sampleâ€™s output.",
    "language": "æ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€ï¼ŒåŒ…æ‹¬Pythonã€Javaå’ŒC++ã€‚æŒ‡ä»¤ä»¥è‡ªç„¶è¯­è¨€æˆ–ä»£ç ç‰‡æ®µå½¢å¼æä¾›ã€‚",
    "language_quote": "Currently, instructions are implemented in multiple programming languages, including Python, Java, and C++.",
    "data_size": "è§„æ¨¡å¯æ‰©å±•ï¼Œç”±ç”Ÿæˆå‚æ•°ï¼ˆå¦‚æ¯ä¸ªç¼–ç¨‹è¯­è¨€çš„æ ·æœ¬æ•°ï¼‰æ§åˆ¶ã€‚æ¡†æ¶æ”¯æŒå¤§è§„æ¨¡å’Œå¤šæ ·åŒ–çš„åŸºå‡†ç”Ÿæˆã€‚",
    "data_size_quote": "To produce meaningful results, the framework must support large-scale and diverse benchmark generation.",
    "source_type": "é€šè¿‡æ¡†æ¶è‡ªåŠ¨ç”Ÿæˆã€‚ä½¿ç”¨ä¸€ç»„é¢„å®šä¹‰çš„æŒ‡ä»¤ä½œä¸ºæ„å»ºå—ï¼Œé€šè¿‡éšæœºé€‰æ‹©å’Œç»„åˆæ¥åˆ›å»ºç‹¬ç‰¹çš„æ ·æœ¬ã€‚",
    "source_type_quote": "Our framework is designed to automatically generate benchmarks... Instructions serve as the fundamental building blocks of PACIFIC... instruction selection is random and fully reproducible via the specified random seed.",
    "last_updated": "2025-12-11 (arXivç‰ˆæœ¬)",
    "last_updated_quote": "arXiv:2512.10713v1  [cs.SE]  11 Dec 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆç”±IBM Researchå›¢é˜Ÿæå‡ºå’Œæ„å»ºçš„æ¡†æ¶ï¼‰",
    "build_type_quote": "IBM Research Haifa, Israel {Itay.Dreyfuss,Antonio.Abu.Nassar,Samuel.Ackerman,Axel.Bendavid}@ibm.com {Rami.Katan,ornar,marcel}@il.ibm.com",
    "contamination_status": "æŠ—æ±¡æŸ“è®¾è®¡ã€‚æ¡†æ¶å¯ä»¥è½»æ¾ç”Ÿæˆæ–°é¢–çš„åŸºå‡†å˜ä½“ï¼Œä»¥å‡è½»è®­ç»ƒæ•°æ®æ±¡æŸ“çš„é£é™©ã€‚",
    "contamination_status_quote": "Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations... Each benchmark generated by PACIFIC can be unique, mitigating the risk of training data contamination.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆä¸æ¨ç†ã€‚æ¨¡å‹éœ€è¦ç”Ÿæˆæ‰§è¡Œä¸€ç³»åˆ—æŒ‡ä»¤åçš„æœ€ç»ˆè¾“å‡ºï¼ˆæ•°å­—æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚",
    "task_granularity_quote": "The model under evaluation is tasked with executing the instructions on the given input and producing the final output in the specified format.",
    "evaluation_metrics": "Prompt-Level Accuracyï¼ˆæç¤ºçº§å‡†ç¡®ç‡ï¼‰ã€‚é€šè¿‡æ¯”è¾ƒæ¯ä¸ªæŒ‡ä»¤çš„ä¸­é—´è¾“å‡ºæˆ–æœ€ç»ˆè¾“å‡ºæ¥è®¡ç®—ã€‚",
    "evaluation_metrics_quote": "Correctness Check: The framework validates each instruction by comparing the output against the expected result... we compute two metrics: â€¢ Prompt-Level Acc",
    "input_modality": "è‡ªç„¶è¯­è¨€æŒ‡ä»¤æˆ–ä»£ç ç‰‡æ®µï¼Œç»“åˆåˆå§‹è¾“å…¥ï¼ˆæ•°å­—æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚",
    "input_modality_quote": "â€¢ Instruction Type: Either Code (instructions as code snippets) or NL (instructions expressed in natural language). Each sample includes: (1) An initial input â€” either a number or a string. (2) A sequence of instructions...",
    "output_modality": "ä»£ç æ‰§è¡Œç»“æœï¼Œå³ä¸€ä¸ªæ•°å­—æˆ–å­—ç¬¦ä¸²ã€‚",
    "output_modality_quote": "Each instruction is a function that accepts either a number or a string and returns a value of one of those types... producing the final output.",
    "task_io_type": "æŒ‡ä»¤åºåˆ—åˆ°æ‰§è¡Œç»“æœã€‚è¾“å…¥æ˜¯åˆå§‹å€¼åŠ ä¸€ç³»åˆ—æ“ä½œæŒ‡ä»¤ï¼Œè¾“å‡ºæ˜¯æ¨¡æ‹Ÿæ‰§è¡Œè¿™äº›æŒ‡ä»¤åçš„æœ€ç»ˆå€¼ã€‚",
    "task_io_type_quote": "The model under evaluation is tasked with executing the instructions on the given input and producing the final output...",
    "execution_environment": "æ— éœ€å¤–éƒ¨æ‰§è¡Œç¯å¢ƒã€‚è¯„ä¼°åŸºäºè¾“å‡ºæ¯”è¾ƒï¼Œè€Œéå®é™…ä»£ç æ‰§è¡Œã€‚ä½†å‚è€ƒç»“æœç”±æ‰§è¡Œå‚è€ƒä»£ç ç”Ÿæˆã€‚",
    "execution_environment_quote": "The expected results are computed by executing reference implementations of the described tasks, enabling automatic and deterministic evaluation via simple output comparison.",
    "unique_features": "1. æ˜¯ä¸€ä¸ªåŸºå‡†ç”Ÿæˆæ¡†æ¶ï¼Œè€Œéå•ä¸€é™æ€æ•°æ®é›†ã€‚2. å¼ºè°ƒâ€œå¹²è¿è¡Œâ€ï¼ˆdry-runningï¼‰ï¼Œå³åœ¨ä¸å®é™…æ‰§è¡Œä»£ç çš„æƒ…å†µä¸‹æ¨¡æ‹Ÿä»£ç è¡Œä¸ºã€‚3. æä¾›æ˜ç¡®çš„éš¾åº¦æ§åˆ¶å‚æ•°ï¼ˆæŒ‡ä»¤æ•°ã€è¾“å‡ºé•¿åº¦ï¼‰ã€‚4. é‡‡ç”¨å®Œå…¨åŸºäºè§„åˆ™çš„ç¡®å®šæ€§è¯„ä¼°ï¼Œé¿å…ä½¿ç”¨LLMä½œä¸ºè¯„åˆ¤è€…ã€‚5. å—ARCåŸºå‡†æ–¹æ³•å¯å‘ï¼Œä½†åº”ç”¨äºä»£ç é¢†åŸŸã€‚",
    "unique_features_quote": "PACIFIC, a novel framework designed to automatically generate benchmarks... evaluates both sequential instruction-following and code dry-running capabilities... The framework emphasizes the ability of LLMs to simulate code execution (i.e. dry-running) without relying on external tools... The framework must provide explicit mechanisms for controlling the difficulty of the generated benchmarks... The evaluation process must rely on deterministic and transparent metrics rather than LLM-based evaluators... This is inspired by the ARC benchmark approach [6], but differs in the implementation and domain.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": 12,
    "last_updated_day": 11,
    "language_normalized": "['Python', 'Java', 'C++', 'å¤šç§ç¼–ç¨‹è¯­è¨€']",
    "dimension_normalized": "['æŒ‡ä»¤éµå¾ªçš„ç²¾ç¡®æ€§', 'ä»£ç å¹²è¿è¡Œï¼ˆæ¨ç†ï¼‰èƒ½åŠ›', 'å¤„ç†å¤šæ­¥ä»»åŠ¡çš„èƒ½åŠ›']",
    "evaluation_method_normalized": "['Prompt-Level Accuracyï¼ˆæç¤ºçº§å‡†ç¡®ç‡ï¼‰']",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹é€»è¾‘ä¸ç®—æ³•æ¨¡æ‹Ÿ', 'æ•°å­¦è¿ç®—', 'æ‰¾è´¨æ•°', 'å®Œç¾å¹³æ–¹', 'å­—ç¬¦ä¸²æ“ä½œ', 'å­—ç¬¦ç§»ä½', 'åŸºæœ¬é€»è¾‘è½¬æ¢']",
    "source_type_normalized": "['æ¡†æ¶è‡ªåŠ¨ç”Ÿæˆ']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "æ ‡å‡†åº“",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2512.10398_output/content.md",
    "benchmark_name": "SWE-Bench-Pro",
    "benchmark_name_quote": "On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We report results on SWE-Bench-Verified (Jimenez et al., 2023) and SWE-Bench-Pro (Deng et al., 2025) using the public evaluation pipelines...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è§£å†³çœŸå®ä¸–ç•Œå¼€æºä»“åº“ä¸­çš„é—®é¢˜ï¼ˆissue resolutionï¼‰",
    "task_description_quote": "LLMs have demonstrated strong software engineering ability to tackle real-world issue resolution in open-source repositories (Jimenez et al., 2023; Yang et al., 2024; Xia et al., 2025; Zeng et al., 2025).",
    "dimension": "å·¥ä¸šçº§è½¯ä»¶å·¥ç¨‹èƒ½åŠ›ï¼ŒåŒ…æ‹¬é•¿ä¸Šä¸‹æ–‡æ¨ç†ã€é•¿æœŸè®°å¿†ã€å¤æ‚å·¥å…·é“¾åè°ƒ",
    "dimension_quote": "Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time.",
    "evaluation_method": "Resolve@1",
    "evaluation_method_quote": "On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%...",
    "context_dependency": "å¤§è§„æ¨¡ä»£ç åº“ï¼ŒåŒ…å«æ·±åº¦ç›¸äº’ä¾èµ–çš„ç»„ä»¶",
    "context_dependency_quote": "industrial-scale codebases, which are usually orders of magnitude larger than typical benchmark projects, contain deeply interdependent components, and evolve continuously (Deng et al., 2025).",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ï¼Œç‰¹åˆ«æ˜¯å¼€æºé¡¹ç›®ç»´æŠ¤",
    "problem_domain_quote": "real-world issue resolution in open-source repositories",
    "problem_difficulty": "å·¥ä¸šçº§ï¼Œè¿œè¶…å…¸å‹åŸºå‡†é¡¹ç›®çš„è§„æ¨¡å’Œå¤æ‚æ€§",
    "problem_difficulty_quote": "industrial-scale codebases, which are usually orders of magnitude larger than typical benchmark projects, contain deeply interdependent components, and evolve continuously (Deng et al., 2025).",
    "language": NaN,
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": "å¼€æºä»“åº“ä¸­çš„çœŸå®é—®é¢˜",
    "source_type_quote": "real-world issue resolution in open-source repositories",
    "last_updated": "2025",
    "last_updated_quote": "Date: December 12, 2025",
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "é—®é¢˜è§£å†³ï¼ˆIssue Resolutionï¼‰",
    "task_granularity_quote": "real-world issue resolution in open-source repositories",
    "evaluation_metrics": "Resolve@1",
    "evaluation_metrics_quote": "On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%...",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆé—®é¢˜æè¿°ï¼‰å’Œä»£ç åº“ä¸Šä¸‹æ–‡",
    "input_modality_quote": "real-world issue resolution in open-source repositories",
    "output_modality": "ä»£ç ä¿®æ”¹ï¼ˆå¦‚è¡¥ä¸ï¼‰",
    "output_modality_quote": "tackle real-world issue resolution",
    "task_io_type": "æ–‡æœ¬ï¼ˆé—®é¢˜ï¼‰å’Œä»£ç ï¼ˆä¸Šä¸‹æ–‡ï¼‰åˆ°ä»£ç ï¼ˆè§£å†³æ–¹æ¡ˆï¼‰",
    "task_io_type_quote": "tackle real-world issue resolution in open-source repositories",
    "execution_environment": "çœŸå®æˆ–æ¨¡æ‹Ÿçš„è½¯ä»¶ä»“åº“ç¯å¢ƒï¼ŒåŒ…å«æµ‹è¯•å¥—ä»¶",
    "execution_environment_quote": "using the public evaluation pipelines",
    "unique_features": "ä¸“æ³¨äºå·¥ä¸šçº§è§„æ¨¡ã€æŒç»­æ¼”åŒ–çš„ä»£ç åº“ä¸­çš„è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ï¼Œå¼ºè°ƒé•¿ä¸Šä¸‹æ–‡æ¨ç†å’Œè·¨ä¼šè¯è®°å¿†ã€‚",
    "unique_features_quote": "Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['å·¥ä¸šçº§è½¯ä»¶å·¥ç¨‹èƒ½åŠ›', 'é•¿ä¸Šä¸‹æ–‡æ¨ç†', 'é•¿æœŸè®°å¿†', 'å¤æ‚å·¥å…·é“¾åè°ƒ']",
    "evaluation_method_normalized": "['Resolve@1']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'å¼€æºé¡¹ç›®ç»´æŠ¤']",
    "source_type_normalized": "['å¼€æºä»“åº“ä¸­çš„çœŸå®é—®é¢˜']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.10485_output/content.md",
    "benchmark_name": "VentiVul",
    "benchmark_name_quote": "We constructed a new, small-scale evaluation dataset, which we name VentiVul.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We manually construct a small but high-quality out-of-distribution vulnerability dataset and design a deployment-oriented evaluation framework introducing two novel modes, Whole-File and Function-Pair.",
    "dataset_url": "https://github.com/Chaomeng-Lu/A-Practical-Evaluation-of-Deep-Learning-Models-and-LLMs-for-Vulnerability-Detection.git",
    "dataset_url_quote": "All of our datasets, code, and results are available at the following link1. 1https://github.com/Chaomeng-Lu/A-Practical-Evaluation-of-Deep-Learning-Models-and-LLMs-for-Vulnerability-Detection.git",
    "task_description": "æ¼æ´æ£€æµ‹ã€‚æ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨çœŸå®ã€æ–°é¢–çš„æ¼æ´åœºæ™¯ä¸‹çš„æ£€æµ‹èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹æ—¶é—´ä¸Šåˆ†å¸ƒå¤–ï¼ˆout-of-distributionï¼‰çš„æ¼æ´ã€‚",
    "task_description_quote": "To assess realistic applicability, we deploy these models ... on a curated dataset, VentiVul, comprising 20 recently (May 2025) fixed vulnerabilities from the Linux kernel. ... We constructed a new, small-scale evaluation dataset, which we name VentiVul.",
    "dimension": "æ¼æ´æ£€æµ‹çš„æ³›åŒ–èƒ½åŠ›ã€é²æ£’æ€§ã€ä»¥åŠåœ¨å®é™…éƒ¨ç½²åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ã€‚",
    "dimension_quote": "Our experiments reveal that current models struggle to distinguish vulnerable from non-vulnerable code in representation space and generalize poorly across datasets with differing distributions. ... These results expose a persistent gap between academic benchmarks and real-world deployment, emphasizing the value of our deployment-oriented evaluation framework...",
    "evaluation_method": "éƒ¨ç½²å¯¼å‘çš„è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…å«ä¸¤ç§æ–°é¢–æ¨¡å¼ï¼šWhole-Fileï¼ˆæ•´ä¸ªæ–‡ä»¶ï¼‰å’Œ Function-Pairï¼ˆå‡½æ•°å¯¹ï¼‰ã€‚",
    "evaluation_method_quote": "We designed a deployment-oriented evaluation framework for assessing vulnerability detection models... finally, we examine deployment behavior using our newly proposed Whole-File and Function-Pair evaluation methods...",
    "context_dependency": "å‡½æ•°çº§å’Œæ–‡ä»¶çº§ã€‚æ•°æ®é›†åŒ…å«å‡½æ•°å¯¹ï¼ˆæ¼æ´ç‰ˆæœ¬ä¸ä¿®å¤ç‰ˆæœ¬ï¼‰ï¼Œä»¥åŠæ¥è‡ªåŒä¸€æ–‡ä»¶ä½†ä¸ä¿®å¤æ— å…³çš„éæ¼æ´å‡½æ•°ï¼Œä»¥æä¾›é¢å¤–ä¸Šä¸‹æ–‡ã€‚",
    "context_dependency_quote": "For each CVE, we extracted the vulnerable and patched versions of affected functions directly from the corresponding Git commits. These were carefully aligned to form function pairs... To provide additional context and challenge for model evaluation, we also included non-vulnerable functions from the same files that were unrelated to the fix.",
    "problem_domain": "è½¯ä»¶å®‰å…¨ï¼Œå…·ä½“ä¸ºCè¯­è¨€ï¼ˆLinuxå†…æ ¸ï¼‰ä¸­çš„è½¯ä»¶æ¼æ´æ£€æµ‹ã€‚",
    "problem_domain_quote": "Specifically, we retrieved all Linux-related CVE reports disclosed in May 2025 from the official CVE database. ... These pairs span 21 distinct .c source files...",
    "problem_difficulty": "é«˜éš¾åº¦ï¼ŒçœŸå®ä¸–ç•Œã€æ–°é¢–ã€åˆ†å¸ƒå¤–çš„æ¼æ´ï¼Œæ—¨åœ¨æŒ‘æˆ˜ç°æœ‰æ¨¡å‹ã€‚",
    "problem_difficulty_quote": "When evaluated on VentiVul, our newly constructed time-wise out-of-distribution dataset, performance drops sharply, with most models failing to detect vulnerabilities reliably. ... providing a realistic and challenging testbed for vulnerability detection models.",
    "language": "Cè¯­è¨€",
    "language_quote": "Specifically, we retrieved all Linux-related CVE reports disclosed in May 2025 from the official CVE database. ... These pairs span 21 distinct .c source files...",
    "data_size": "å°è§„æ¨¡ï¼ŒåŒ…å«æ¥è‡ª20ä¸ªLinux CVEçš„25ä¸ªå‡½æ•°å¯¹ï¼ˆæ¯ä¸ªå¯¹åŒ…å«æ¼æ´ç‰ˆæœ¬å’Œä¿®å¤ç‰ˆæœ¬ï¼‰ï¼Œä»¥åŠæ¥è‡ªç›¸åŒæ–‡ä»¶çš„835ä¸ªæ— å…³çš„éæ¼æ´å‡½æ•°ã€‚",
    "data_size_quote": "In total, we collected 25 function pairs (each consisting of a vulnerable and a patched version) from the 20 selected Linux CVEs. ... and include 835 unrelated functions from the same files...",
    "source_type": "æ‰‹åŠ¨ä»çœŸå®ä¸–ç•Œæ•°æ®æ„å»ºã€‚æ•°æ®æ¥æºäº2025å¹´5æœˆæŠ«éœ²çš„Linuxå†…æ ¸CVEæŠ¥å‘ŠåŠå…¶å¯¹åº”çš„Gitä¿®å¤æäº¤ã€‚",
    "source_type_quote": "To ensure high relevance and minimize external interference, the collection process was conducted fully manually and involved careful human inspection at each step. Specifically, we retrieved all Linux-related CVE reports disclosed in May 2025 from the official CVE database. ... For each CVE, we extracted the vulnerable and patched versions of affected functions directly from the corresponding Git commits.",
    "last_updated": "2025å¹´12æœˆï¼ˆè®ºæ–‡ç‰ˆæœ¬æ—¥æœŸï¼‰ï¼Œæ•°æ®é›†åŸºäº2025å¹´5æœˆæŠ«éœ²çš„CVEæ„å»ºã€‚",
    "last_updated_quote": "arXiv:2512.10485v1  [cs.CR]  11 Dec 2025 ... comprising 20 recently (May 2025) fixed vulnerabilities from the Linux kernel.",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆæœ¬æ–‡ä½œè€…æ‰‹åŠ¨æ„å»ºï¼‰",
    "build_type_quote": "We manually construct a small but high-quality out-of-distribution vulnerability dataset... The collection process was conducted fully manually and involved careful human inspection at each step.",
    "contamination_status": "æŠ—æ±¡æŸ“è®¾è®¡ï¼Œä¸“é—¨è®¾è®¡ä¸ºæ—¶é—´ä¸Šçš„åˆ†å¸ƒå¤–ï¼ˆout-of-distributionï¼‰æ•°æ®é›†ï¼Œä»¥æœ€å°åŒ–è®­ç»ƒæ•°æ®æ±¡æŸ“é£é™©ã€‚",
    "contamination_status_quote": "When evaluated on VentiVul, our newly constructed time-wise out-of-distribution dataset... To further evaluate the performance of state-of-the-art models... on a curated dataset, VentiVul, comprising 20 recently (May 2025) fixed vulnerabilities...",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "æ¼æ´æ£€æµ‹ï¼ˆäºŒå…ƒåˆ†ç±»ï¼šæ¼æ´/éæ¼æ´ï¼‰",
    "task_granularity_quote": "These were carefully aligned to form function pairs, labeled as vulnerable (1, before-fix) and non-vulnerable (0, after-fix).",
    "evaluation_metrics": "æ–‡ä¸­æœªæ˜ç¡®æåŠç”¨äºVentiVulçš„å…·ä½“è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚å‡†ç¡®ç‡ã€F1åˆ†æ•°ç­‰ï¼‰ï¼Œä½†æåˆ°äº†æ€§èƒ½ä¸‹é™ã€‚",
    "evaluation_metrics_quote": "When evaluated on VentiVul... performance drops sharply, with most models failing to detect vulnerabilities reliably.",
    "input_modality": "æºä»£ç ï¼ˆCè¯­è¨€å‡½æ•°æˆ–æ–‡ä»¶ï¼‰",
    "input_modality_quote": "For each CVE, we extracted the vulnerable and patched versions of affected functions directly from the corresponding Git commits.",
    "output_modality": "äºŒå…ƒåˆ†ç±»æ ‡ç­¾ï¼ˆæ¼æ´/éæ¼æ´ï¼‰",
    "output_modality_quote": "labeled as vulnerable (1, before-fix) and non-vulnerable (0, after-fix).",
    "task_io_type": "ä»£ç åˆ°æ ‡ç­¾ï¼ˆäºŒå…ƒåˆ†ç±»ï¼‰",
    "task_io_type_quote": "labeled as vulnerable (1, before-fix) and non-vulnerable (0, after-fix).",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "1. æ—¶é—´ä¸Šçš„åˆ†å¸ƒå¤–ï¼ˆOODï¼‰è¯„ä¼°ï¼šä¸“æ³¨äºè¿‘æœŸï¼ˆ2025å¹´5æœˆï¼‰ä¿®å¤çš„æ¼æ´ï¼Œä¸ç°æœ‰è®­ç»ƒæ•°æ®æ—¶é—´åˆ†ç¦»ã€‚2. é«˜çœŸå®æ€§ä¸é«˜è´¨é‡ï¼šå®Œå…¨æ‰‹åŠ¨ä»Linuxå†…æ ¸CVEå’ŒGitæäº¤ä¸­æ„å»ºï¼Œç»è¿‡äººå·¥æ£€æŸ¥ã€‚3. éƒ¨ç½²å¯¼å‘çš„è¯„ä¼°æ¨¡å¼ï¼šå¼•å…¥äº†Whole-Fileå’ŒFunction-Pairä¸¤ç§æ–°é¢–çš„è¯„ä¼°æ¨¡å¼ï¼Œæ¨¡æ‹ŸçœŸå®éƒ¨ç½²åœºæ™¯ã€‚4. åŒ…å«æ— å…³ä¸Šä¸‹æ–‡ï¼šé™¤äº†æ¼æ´/ä¿®å¤å‡½æ•°å¯¹å¤–ï¼Œè¿˜åŒ…å«äº†æ¥è‡ªåŒä¸€æ–‡ä»¶ä½†ä¸ä¿®å¤æ— å…³çš„éæ¼æ´å‡½æ•°ï¼Œå¢åŠ è¯„ä¼°æŒ‘æˆ˜æ€§ã€‚",
    "unique_features_quote": "We constructed a new, small-scale evaluation dataset, which we name VentiVul... comprising 20 recently (May 2025) fixed vulnerabilities from the Linux kernel. ... The collection process was conducted fully manually and involved careful human inspection at each step. ... We designed a deployment-oriented evaluation framework... using our newly proposed Whole-File and Function-Pair evaluation methods... To provide additional context and challenge for model evaluation, we also included non-vulnerable functions from the same files that were unrelated to the fix.",
    "data_size_quantity": 25,
    "data_size_unit": "ä¸ªå‡½æ•°å¯¹",
    "last_updated_year": 2025,
    "last_updated_month": 12,
    "last_updated_day": null,
    "language_normalized": "['Cè¯­è¨€']",
    "dimension_normalized": "['æ¼æ´æ£€æµ‹çš„æ³›åŒ–èƒ½åŠ›', 'é²æ£’æ€§', 'ä»¥åŠåœ¨å®é™…éƒ¨ç½²åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§']",
    "evaluation_method_normalized": "['æ–‡ä¸­æœªæ˜ç¡®æåŠç”¨äºVentiVulçš„å…·ä½“è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚å‡†ç¡®ç‡ã€F1åˆ†æ•°ç­‰ï¼‰ï¼Œä½†æåˆ°äº†æ€§èƒ½ä¸‹é™']",
    "problem_domain_normalized": "['è½¯ä»¶å®‰å…¨', 'å…·ä½“ä¸ºCè¯­è¨€ï¼ˆLinuxå†…æ ¸ï¼‰ä¸­çš„è½¯ä»¶æ¼æ´æ£€æµ‹']",
    "source_type_normalized": "['æ‰‹åŠ¨ä»çœŸå®ä¸–ç•Œæ•°æ®æ„å»º', 'æ•°æ®æ¥æºäº2025å¹´5æœˆæŠ«éœ²çš„Linuxå†…æ ¸CVEæŠ¥å‘ŠåŠå…¶å¯¹åº”çš„Gitä¿®å¤æäº¤']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å•æ–‡ä»¶",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2512.10393_output/content.md",
    "benchmark_name": "BinSeek benchmark (æœªæ˜ç¡®å‘½åï¼Œä½†æ–‡ä¸­ç§°å…¶ä¸º'first domain benchmark for binary code retrieval')",
    "benchmark_name_quote": "To this end, we built an LLM-based data synthesis pipeline to automate training construction, also deriving a domain benchmark for future research.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "In this paper, we introduce, BinSeek, the first two-stage cross-modal retrieval framework for stripped binary code analysis. ... To this end, we built an LLM-based data synthesis pipeline to automate training construction, also deriving a domain benchmark for future research.",
    "dataset_url": "https://github.com/XingTuLab/BinSeek",
    "dataset_url_quote": "We make our model and evaluation benchmark publicly available at: https://github.com/XingTuLab/BinSeek.",
    "task_description": "åŸºäºè‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œä»å¤§è§„æ¨¡äºŒè¿›åˆ¶ä»£ç åº“ä¸­æ£€ç´¢ç›¸å…³çš„äºŒè¿›åˆ¶å‡½æ•°ã€‚å…·ä½“ä»»åŠ¡æ˜¯è·¨æ¨¡æ€æ£€ç´¢ï¼Œå°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢ä¸å»ç¬¦å·åŒ–çš„äºŒè¿›åˆ¶åç¼–è¯‘ä¼ªä»£ç è¿›è¡ŒåŒ¹é…ã€‚",
    "task_description_quote": "Retrieving stripped binary code based on natural language (NL) queries, however, presents unique and formidable challenges compared to source code retrieval. ... we also deliver the first domain benchmark for the binary code retrieval task",
    "dimension": "è·¨æ¨¡æ€æ£€ç´¢èƒ½åŠ›ã€äºŒè¿›åˆ¶ä»£ç è¯­ä¹‰ç†è§£ã€æ£€ç´¢å‡†ç¡®æ€§",
    "dimension_quote": "Our evaluation results show that BinSeek achieved the state-of-the-art performance, surpassing the the same scale models by 31.42% in Rec@3 and 27.17% in MRR@3",
    "evaluation_method": "ä½¿ç”¨Rec@3å’ŒMRR@3æŒ‡æ ‡è¯„ä¼°æ£€ç´¢æ€§èƒ½ã€‚",
    "evaluation_method_quote": "Our evaluation results show that BinSeek achieved the state-of-the-art performance, surpassing the the same scale models by 31.42% in Rec@3 and 27.17% in MRR@3",
    "context_dependency": "å•å‡½æ•°çº§åˆ«ï¼ˆBinSeek-Embeddingï¼‰å’Œå¸¦è°ƒç”¨ä¸Šä¸‹æ–‡çš„å‡½æ•°ï¼ˆBinSeek-Rerankerï¼‰",
    "context_dependency_quote": "BinSeek-Embedding is used to retrieve candidates from a codebase, and BinSeek-Reranker further refine candidates with calling context information.",
    "problem_domain": "è½¯ä»¶å®‰å…¨ã€äºŒè¿›åˆ¶ä»£ç åˆ†æã€é€†å‘å·¥ç¨‹",
    "problem_domain_quote": "Binary code analysis serves as a cornerstone of software security, supporting critical tasks such as vulnerability detection, malware analysis, program audit, and so on.",
    "problem_difficulty": "é«˜éš¾åº¦ï¼Œæ¶‰åŠå»ç¬¦å·åŒ–çš„äºŒè¿›åˆ¶ä»£ç ï¼Œç¼ºä¹é«˜çº§è¯­ä¹‰ä¿¡æ¯ã€‚",
    "problem_difficulty_quote": "Retrieving stripped binary code based on natural language (NL) queries, however, presents unique and formidable challenges compared to source code retrieval. ... stripped binaries lack explicit semantic indicators such as variable names, function names, and type information.",
    "language": "C/C++ï¼ˆæºè¯­è¨€ï¼‰ï¼Œåç¼–è¯‘åçš„ä¼ªä»£ç ï¼ˆç›®æ ‡æ¨¡æ€ï¼‰",
    "language_quote": "Binary code is a low-level representation of a program, which is generally compiled from the source code written in PLs like C/C++",
    "data_size": "æ€»å…±æ„å»ºäº†106,711,414ä¸ªåŸå§‹æ•°æ®å¯¹ã€‚",
    "data_size_quote": "Ultimately, we constructed 106,711,414 raw data pairs in total.",
    "source_type": "é€šè¿‡è‡ªåŠ¨åŒ–æ•°æ®åˆæˆç®¡é“ç”Ÿæˆã€‚ä»GitHubæ”¶é›†C/C++å¼€æºé¡¹ç›®ï¼Œç¼–è¯‘ä¸ºå»ç¬¦å·åŒ–äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œåç¼–è¯‘ä¸ºä¼ªä»£ç ï¼Œå¹¶ä½¿ç”¨LLMä¸ºæºä»£ç å‡½æ•°ç”Ÿæˆè¯­ä¹‰æè¿°ã€‚",
    "source_type_quote": "As shown in Figure 3, we first collect a large number of open-source projects written in C/C++ from GitHub. For each project, we compile the source code into stripped binary files using different configurations... Finally, we employ an advanced LLM (i.e., DeepSeek (DeepSeek-AI et al., 2025)) to generate semantic descriptions for each source code function.",
    "last_updated": "2025å¹´12æœˆ11æ—¥ï¼ˆè®ºæ–‡ç‰ˆæœ¬æ—¥æœŸï¼‰",
    "last_updated_quote": "arXiv:2512.10393v1  [cs.SE]  11 Dec 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œé€šè¿‡LLMé©±åŠ¨çš„è‡ªåŠ¨åŒ–æ•°æ®åˆæˆç®¡é“æ„å»ºã€‚",
    "build_type_quote": "We propose an LLM-driven data synthesis pipeline to automate the construction of high-quality training data, deriving the first domain benchmark for binary code retrieval.",
    "contamination_status": "æœªæ˜ç¡®æåŠã€‚",
    "contamination_status_quote": NaN,
    "dataset_license": "æœªæ˜ç¡®æåŠã€‚",
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç æ£€ç´¢ï¼ˆè·¨æ¨¡æ€æ£€ç´¢ï¼‰",
    "task_granularity_quote": "Retrieving stripped binary code based on natural language (NL) queries",
    "evaluation_metrics": "Rec@3, MRR@3",
    "evaluation_metrics_quote": "Our evaluation results show that BinSeek achieved the state-of-the-art performance, surpassing the the same scale models by 31.42% in Rec@3 and 27.17% in MRR@3",
    "input_modality": "è‡ªç„¶è¯­è¨€æŸ¥è¯¢",
    "input_modality_quote": "Retrieving stripped binary code based on natural language (NL) queries",
    "output_modality": "äºŒè¿›åˆ¶åç¼–è¯‘ä¼ªä»£ç ï¼ˆå‡½æ•°ï¼‰",
    "output_modality_quote": "Training cross-modal retrieval models requires a large amount of data pairs that consist of pseudocode functions and corresponding semantic descriptions in NL.",
    "task_io_type": "è‡ªç„¶è¯­è¨€åˆ°ä»£ç ï¼ˆäºŒè¿›åˆ¶ä¼ªä»£ç ï¼‰",
    "task_io_type_quote": "Retrieving stripped binary code based on natural language (NL) queries",
    "execution_environment": "æœªæ˜ç¡®æåŠï¼Œä»»åŠ¡ä¸ºæ£€ç´¢è€Œéæ‰§è¡Œã€‚",
    "execution_environment_quote": NaN,
    "unique_features": "è¿™æ˜¯ç¬¬ä¸€ä¸ªä¸“é—¨ä¸ºå»ç¬¦å·åŒ–äºŒè¿›åˆ¶ä»£ç æ£€ç´¢ä»»åŠ¡æ„å»ºçš„é¢†åŸŸåŸºå‡†ã€‚å®ƒé’ˆå¯¹äºŒè¿›åˆ¶ä»£ç ç¼ºä¹ç¬¦å·ä¿¡æ¯ï¼ˆå¦‚å‡½æ•°åã€å˜é‡åï¼‰çš„ç‹¬ç‰¹æŒ‘æˆ˜è€Œè®¾è®¡ï¼Œå¡«è¡¥äº†æºä»£ç æ£€ç´¢ä¸äºŒè¿›åˆ¶ä»£ç æ£€ç´¢ä¹‹é—´çš„ç©ºç™½ã€‚æ•°æ®é€šè¿‡è‡ªåŠ¨åŒ–LLMé©±åŠ¨çš„ç®¡é“åˆæˆï¼Œè¿æ¥äº†æºä»£ç ã€äºŒè¿›åˆ¶ä»£ç å’Œè‡ªç„¶è¯­è¨€æè¿°ã€‚",
    "unique_features_quote": "we also deliver the first domain benchmark for the binary code retrieval task, which is expected to facilitate future research in this domain. ... Unlike high-level programming languages (PL), stripped binaries lack explicit semantic indicators such as variable names, function names, and type information. ... We propose an LLM-driven data synthesis pipeline to automate the construction of high-quality training data",
    "data_size_quantity": 106711414,
    "data_size_unit": "ä¸ªåŸå§‹æ•°æ®å¯¹",
    "last_updated_year": 2025,
    "last_updated_month": 12,
    "last_updated_day": 11,
    "language_normalized": "['C/C++', 'åç¼–è¯‘åçš„ä¼ªä»£ç ']",
    "dimension_normalized": "['è·¨æ¨¡æ€æ£€ç´¢èƒ½åŠ›', 'äºŒè¿›åˆ¶ä»£ç è¯­ä¹‰ç†è§£', 'æ£€ç´¢å‡†ç¡®æ€§']",
    "evaluation_method_normalized": "['Rec@3', 'MRR@3']",
    "problem_domain_normalized": "['è½¯ä»¶å®‰å…¨', 'äºŒè¿›åˆ¶ä»£ç åˆ†æ', 'é€†å‘å·¥ç¨‹']",
    "source_type_normalized": "['é€šè¿‡è‡ªåŠ¨åŒ–æ•°æ®åˆæˆç®¡é“ç”Ÿæˆ', 'ä»GitHubæ”¶é›†C/C++å¼€æºé¡¹ç›®', 'ç¼–è¯‘ä¸ºå»ç¬¦å·åŒ–äºŒè¿›åˆ¶æ–‡ä»¶', 'åç¼–è¯‘ä¸ºä¼ªä»£ç ', 'ä½¿ç”¨LLMä¸ºæºä»£ç å‡½æ•°ç”Ÿæˆè¯­ä¹‰æè¿°']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å•æ–‡ä»¶",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.11482_output/content.md",
    "benchmark_name": "data extraction attack benchmark å’Œ functional correctness benchmark",
    "benchmark_name_quote": "â€¢ Two benchmarks for privacy and utility evaluation: We release two new datasets, i.e., data extraction attack benchmark and functional correctness benchmark, that can be used in future studies to evaluate the privacy and utility of CodeLLMs.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "â€¢ Two benchmarks for privacy and utility evaluation: We release two new datasets, i.e., data extraction attack benchmark and functional correctness benchmark, that can be used in future studies to evaluate the privacy and utility of CodeLLMs.",
    "dataset_url": "Replication Package (RP) [9]",
    "dataset_url_quote": "These datasets are tailored to the fine-tuning dataset for utility evaluation and available in the Replication Package (RP) [9].",
    "task_description": "è¯„ä¼°ä»£ç å¤§è¯­è¨€æ¨¡å‹çš„éšç§é£é™©å’ŒåŠŸèƒ½æ­£ç¡®æ€§ã€‚å…·ä½“åŒ…æ‹¬ï¼š1. æ•°æ®æå–æ”»å‡»åŸºå‡†ï¼šç”¨äºè¯„ä¼°æ¨¡å‹å¯¹è®­ç»ƒæ•°æ®ï¼ˆåŒ…æ‹¬æ•æ„Ÿä¿¡æ¯ï¼‰çš„è®°å¿†å’Œæ³„éœ²é£é™©ã€‚2. åŠŸèƒ½æ­£ç¡®æ€§åŸºå‡†ï¼šç”¨äºè¯„ä¼°æ¨¡å‹åœ¨åº”ç”¨å·®åˆ†éšç§ç­‰éšç§ä¿æŠ¤æŠ€æœ¯åï¼Œç”Ÿæˆä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§æ˜¯å¦å¾—ä»¥ä¿æŒã€‚",
    "task_description_quote": "â€¢ Two benchmarks for privacy and utility evaluation: We release two new datasets, i.e., data extraction attack benchmark and functional correctness benchmark, that can be used in future studies to evaluate the privacy and utility of CodeLLMs.",
    "dimension": "éšç§ä¿æŠ¤ï¼ˆè®°å¿†é£é™©ï¼‰ä¸æ¨¡å‹æ•ˆç”¨ï¼ˆåŠŸèƒ½æ­£ç¡®æ€§ï¼‰çš„æƒè¡¡è¯„ä¼°",
    "dimension_quote": "â€¢ Two benchmarks for privacy and utility evaluation: We release two new datasets, i.e., data extraction attack benchmark and functional correctness benchmark, that can be used in future studies to evaluate the privacy and utility of CodeLLMs.",
    "evaluation_method": "æ–‡ä¸­æœªæ˜ç¡®æè¿°é’ˆå¯¹è¿™ä¸¤ä¸ªæ–°åŸºå‡†çš„å…·ä½“è¯„ä¼°æ–¹æ³•ã€‚",
    "evaluation_method_quote": NaN,
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "é€šç”¨è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ï¼Œæ¶‰åŠä»£ç ç”Ÿæˆã€æ–‡æ¡£ã€æµ‹è¯•ç”¨ä¾‹ç­‰ã€‚",
    "problem_domain_quote": "Large language models specialized for code (Large language models specialized for code (CodeLLMs)) have demonstrated remarkable capabilities in generating code snippets, documentation, and test cases.",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "æ–‡ä¸­æœªæ˜ç¡®æŒ‡å®šåŸºå‡†ä½¿ç”¨çš„ç¼–ç¨‹è¯­è¨€ï¼Œä½†ç ”ç©¶èƒŒæ™¯æ˜¯é€šç”¨ä»£ç å¤§è¯­è¨€æ¨¡å‹ã€‚",
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": "ä¸ºè¯„ä¼°ç›®çš„è€Œæ„å»ºçš„å®šåˆ¶æ•°æ®é›†ï¼Œä¸å¾®è°ƒæ•°æ®é›†ç›¸å…³è”ã€‚",
    "source_type_quote": "These datasets are tailored to the fine-tuning dataset for utility evaluation...",
    "last_updated": "2025",
    "last_updated_quote": "Melih Catal, Pooja Rani, and Harald Gall. 2025. Towards Privacy-Preserving Code Generation: Differentially Private Code Language Models. 1, 1 (December 2025), 21 pages.",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆæœ¬æ–‡ä½œè€…å‘å¸ƒï¼‰",
    "build_type_quote": "We release two new datasets...",
    "contamination_status": "ä¸ºè¯„ä¼°éšç§ä¿æŠ¤æŠ€æœ¯ï¼ˆå¦‚å·®åˆ†éšç§ï¼‰è€Œä¸“é—¨æ„å»ºï¼Œæ—¨åœ¨æµ‹é‡è®°å¿†é£é™©ï¼Œå› æ­¤å¯èƒ½åŒ…å«è®¾è®¡ç”¨äºè§¦å‘è®°å¿†çš„æ ·æœ¬ã€‚",
    "contamination_status_quote": "â€¢ Two benchmarks for privacy and utility evaluation: We release two new datasets, i.e., data extraction attack benchmark and functional correctness benchmark, that can be used in future studies to evaluate the privacy and utility of CodeLLMs.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆä»è‡ªç„¶è¯­è¨€æç¤ºç”Ÿæˆä»£ç ç‰‡æ®µã€å®Œæ•´å‡½æ•°æˆ–ç¨‹åºï¼‰",
    "task_granularity_quote": "LLMs specialized for code (Large language models specialized for code (CodeLLMs)), trained on large code datasets, can generate code snippets, complete functions, or even write entire programs based on natural language prompts",
    "evaluation_metrics": "æ–‡ä¸­æåŠä½¿ç”¨CodeBLEUåˆ†æ•°å’ŒåŠŸèƒ½æ­£ç¡®æ€§ï¼ˆfunctional correctnessï¼‰ä½œä¸ºè¯„ä¼°æŒ‡æ ‡ã€‚",
    "evaluation_metrics_quote": "CodeBLEU Score: 1.00 ... Effectiveness of DP on Mitigating Memorization and Preserving Model Performance: ... as measured by functional correctness on standard benchmarks.",
    "input_modality": "è‡ªç„¶è¯­è¨€æç¤ºï¼ˆç”¨äºä»£ç ç”Ÿæˆä»»åŠ¡ï¼‰ï¼›å¯èƒ½ä¹ŸåŒ…æ‹¬ä»£ç ç‰‡æ®µï¼ˆç”¨äºæ•°æ®æå–æ”»å‡»ï¼‰ã€‚",
    "input_modality_quote": "...can generate code snippets, complete functions, or even write entire programs based on natural language prompts",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "...can generate code snippets, complete functions, or even write entire programs...",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ï¼ˆä»£ç ç”Ÿæˆä»»åŠ¡ï¼‰ï¼›ä»£ç åˆ°ä»£ç /æ–‡æœ¬ï¼ˆæ•°æ®æå–æ”»å‡»è¯„ä¼°ï¼‰ã€‚",
    "task_io_type_quote": "...can generate code snippets, complete functions, or even write entire programs based on natural language prompts",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "è¿™æ˜¯é¦–ä¸ªä¸“é—¨ä¸ºè¯„ä¼°ä»£ç å¤§è¯­è¨€æ¨¡å‹çš„éšç§ï¼ˆè®°å¿†é£é™©ï¼‰ä¸æ•ˆç”¨ï¼ˆåŠŸèƒ½æ­£ç¡®æ€§ï¼‰æƒè¡¡è€Œè®¾è®¡çš„åŸºå‡†å¥—ä»¶ã€‚å®ƒåŒ…å«ä¸¤ä¸ªäº’è¡¥çš„æ•°æ®é›†ï¼šä¸€ä¸ªç”¨äºé‡åŒ–æ•°æ®æå–æ”»å‡»ä¸‹çš„è®°å¿†æ³„éœ²ï¼Œå¦ä¸€ä¸ªç”¨äºç¡®ä¿éšç§ä¿æŠ¤æŠ€æœ¯ï¼ˆå¦‚å·®åˆ†éšç§ï¼‰ä¸ä¼šæŸå®³ä»£ç ç”Ÿæˆè´¨é‡ã€‚",
    "unique_features_quote": "â€¢ Two benchmarks for privacy and utility evaluation: We release two new datasets, i.e., data extraction attack benchmark and functional correctness benchmark, that can be used in future studies to evaluate the privacy and utility of CodeLLMs. These datasets are tailored to the fine-tuning dataset for utility evaluation...",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['éšç§ä¿æŠ¤', 'è®°å¿†é£é™©', 'æ¨¡å‹æ•ˆç”¨', 'åŠŸèƒ½æ­£ç¡®æ€§', 'æƒè¡¡è¯„ä¼°']",
    "evaluation_method_normalized": "['CodeBLEU', 'åŠŸèƒ½æ­£ç¡®æ€§']",
    "problem_domain_normalized": "['é€šç”¨è½¯ä»¶å·¥ç¨‹', 'ä»£ç ç”Ÿæˆ', 'æ–‡æ¡£', 'æµ‹è¯•ç”¨ä¾‹']",
    "source_type_normalized": "['å®šåˆ¶æ•°æ®é›†', 'å¾®è°ƒæ•°æ®é›†']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "é«˜æ±¡æŸ“é£é™©"
  },
  {
    "source_paper": "2512.11589_output/content.md",
    "benchmark_name": "AIDev",
    "benchmark_name_quote": "To conduct our study, we use the AIDev dataset [13], a large-scale corpus of almost a million pull requests (PRs) from over 116,000 real GitHub repositories, authored by five distinct LLM-based coding agents (Claude Code, Cursor, Devin, Copilot, OpenAI Codex).",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "To conduct our study, we use the AIDev dataset [13]...",
    "dataset_url": "https://github.com/itsluketwist/agent-library-usage/",
    "dataset_url_quote": "We release our dataset processing and analysis code to support reproducibility and future research in this area: https://github.com/itsluketwist/agent-library-usage/",
    "task_description": "è¯¥æ•°æ®é›†æ—¨åœ¨æä¾›ä¸€ä¸ªå¤§è§„æ¨¡çš„çœŸå®ä¸–ç•ŒAIç¼–ç ä»£ç†ï¼ˆAgentï¼‰ç”Ÿæˆçš„ä»£ç å˜æ›´ï¼ˆPull Requestsï¼‰é›†åˆï¼Œç”¨äºç ”ç©¶AIä»£ç†åœ¨è½¯ä»¶å¼€å‘ä¸­çš„è¡Œä¸ºï¼Œç‰¹åˆ«æ˜¯å…¶ä½¿ç”¨å¤–éƒ¨åº“çš„æ¨¡å¼ã€‚",
    "task_description_quote": "To conduct our study, we use the AIDev dataset [13], a large-scale corpus of almost a million pull requests (PRs) from over 116,000 real GitHub repositories, authored by five distinct LLM-based coding agents...",
    "dimension": "AIç¼–ç ä»£ç†çš„åº“ä½¿ç”¨è¡Œä¸ºï¼ŒåŒ…æ‹¬åº“å¯¼å…¥é¢‘ç‡ã€æ–°ä¾èµ–å¼•å…¥ã€ç‰ˆæœ¬æ§åˆ¶å®è·µå’Œåº“é€‰æ‹©å¤šæ ·æ€§ã€‚",
    "dimension_quote": "Specifically, we analyse file diffs (both code and dependency manifests) from 26,760 PRs across four languages (TypeScript, Python, Go, and C#), to answer three research questions (RQs): RQ1 Library Usage... RQ2 New Dependencies... RQ3 Choosing Libraries...",
    "evaluation_method": "é€šè¿‡åˆ†æPull Requestçš„æ–‡ä»¶å·®å¼‚ï¼ˆdiffsï¼‰ï¼Œä½¿ç”¨ç‰¹å®šäºè¯­è¨€çš„è§£æå™¨æå–å¯¼å…¥è¯­å¥å’Œä¾èµ–æ¸…å•ï¼Œå¹¶è¿›è¡Œç»Ÿè®¡åˆ†æï¼ˆå¦‚é¢‘ç‡ç»Ÿè®¡ã€ç‰ˆæœ¬çº¦æŸåˆ†æï¼‰ã€‚",
    "evaluation_method_quote": "We then implement manifest-specific regex parsers to automatically identify new dependenciesâ€”including any explicit version specifiers... We then perform manual analysis on the top ten libraries for each language and RQ to determine the areas that agents are most likely to use external libraries.",
    "context_dependency": "å¤šæ–‡ä»¶é¡¹ç›®çº§åˆ«ï¼ŒåŸºäºçœŸå®çš„GitHubä»“åº“å’ŒPull Requestsï¼ŒåŒ…å«å®Œæ•´çš„ä»£ç å˜æ›´ä¸Šä¸‹æ–‡ã€‚",
    "context_dependency_quote": "Specifically, we use the AIDev-Pop subset that is curated to repositories with over 100 GitHub stars and includes the full commit details; AIDev-Pop contains 33,596 PRs from 2,807 repositories.",
    "problem_domain": "é€šç”¨è½¯ä»¶å¼€å‘ï¼Œæ¶µç›–å¤šç§ç¼–ç¨‹è¯­è¨€å’Œé¡¹ç›®ç±»å‹ã€‚",
    "problem_domain_quote": "We restrict our study to the four most common languages in the dataset... The chosen languages are TypeScript/JavaScript... Python, Go and C#.",
    "problem_difficulty": "å·¥ç¨‹çº§ï¼Œæ¶‰åŠçœŸå®ä¸–ç•Œè½¯ä»¶é¡¹ç›®çš„ä»£ç ä¿®æ”¹å’Œä¾èµ–ç®¡ç†ã€‚",
    "problem_difficulty_quote": "These agentic systemsâ€”now widely available in production tools [19]â€”can complete end-to-end workflows that previously required a human developer. Crucially, these workflows include raising pull requests (PRs), allowing agents to propose substantial code changes...",
    "language": "TypeScript/JavaScript, Python, Go, C#",
    "language_quote": "We restrict our study to the four most common languages in the dataset... The chosen languages are TypeScript/JavaScript (combined due to nearly identical syntax and library ecosystems, and will be referred to as TypeScript throughout the paper), Python, Go and C#.",
    "data_size": "åŒ…å«æ¥è‡ª2,807ä¸ªä»“åº“çš„33,596ä¸ªPull Requestsï¼ˆAIDev-Popå­é›†ï¼‰ï¼Œæœ¬ç ”ç©¶åˆ†æäº†å…¶ä¸­26,760ä¸ªPRsã€‚",
    "data_size_quote": "AIDev-Pop contains 33,596 PRs from 2,807 repositories... We analyse a total of 26,760 agent-authored pull requests across 1,832 repositories and the four most common languages in the AIDev dataset.",
    "source_type": "æ¥è‡ªçœŸå®GitHubä»“åº“çš„å…¬å¼€Pull Requestsï¼Œç”±äº”ç§ä¸åŒçš„LLMç¼–ç ä»£ç†ï¼ˆClaude Code, Cursor, Devin, Copilot, OpenAI Codexï¼‰ç”Ÿæˆã€‚",
    "source_type_quote": "To conduct our study, we use the AIDev dataset [13], a large-scale corpus of almost a million pull requests (PRs) from over 116,000 real GitHub repositories, authored by five distinct LLM-based coding agents (Claude Code, Cursor, Devin, Copilot, OpenAI Codex).",
    "last_updated": "2025å¹´11æœˆ",
    "last_updated_quote": "Specifically, we use the November 2025 snapshot (commit eee0408 [8]).",
    "build_type": "ç¤¾åŒºè´¡çŒ®ï¼ˆç”±ç ”ç©¶äººå‘˜ä»GitHubæ”¶é›†æ„å»ºï¼‰",
    "build_type_quote": "To conduct our study, we use the AIDev dataset [13], a large-scale corpus of almost a million pull requests (PRs) from over 116,000 real GitHub repositories...",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": "æ•°æ®æºè‡ªå„ä»“åº“çš„å¼€æºè®¸å¯è¯ï¼ˆé€šå¸¸ä¸ºMITã€Apache-2.0æˆ–GPLå˜ä½“ï¼‰ã€‚",
    "dataset_license_quote": "AIDev contains no personal or sensitive information, and all data is publicly accessible under the open-source license of its respective repository (typically MIT, Apache-2.0, or GPL variants).",
    "task_granularity": "ä»£ç ç”Ÿæˆä¸ä¿®æ”¹ï¼ˆåœ¨Pull Requestçº§åˆ«ï¼‰",
    "task_granularity_quote": "These agentic systems... can complete end-to-end workflows that previously required a human developer. Crucially, these workflows include raising pull requests (PRs), allowing agents to propose substantial code changes...",
    "evaluation_metrics": "åº“å¯¼å…¥é¢‘ç‡ã€æ–°ä¾èµ–å¼•å…¥é¢‘ç‡ã€ç‰ˆæœ¬çº¦æŸæŒ‡å®šæ¯”ä¾‹ã€åº“é€‰æ‹©å¤šæ ·æ€§ï¼ˆç‹¬ç‰¹åº“æ•°é‡ï¼‰ã€‚",
    "evaluation_metrics_quote": "RQ1 Library Usage How often do agents import libraries when generating code... RQ2 New Dependencies How often do agents introduce new dependencies into a repository, and do they specify versions? RQ3 Choosing Libraries Which specific libraries do agents choose...",
    "input_modality": "ä»£ç ï¼ˆPull Requestçš„ä»£ç å˜æ›´å·®å¼‚ï¼‰",
    "input_modality_quote": "Specifically, we analyse file diffs (both code and dependency manifests) from 26,760 PRs...",
    "output_modality": "åˆ†æç»“æœï¼ˆç»Ÿè®¡æ•°æ®å’Œå‘ç°ï¼‰",
    "output_modality_quote": "Our findings are concise but instructive. (1) Agents often import libraries (29.5% of PRs) but rarely add entirely new dependencies (1.3% of PRs). (2) Agentic workflows encourage far better version hygiene than traditional LLM prompting...",
    "task_io_type": "ä»£ç åˆ°åˆ†æ",
    "task_io_type_quote": "Specifically, we analyse file diffs (both code and dependency manifests) from 26,760 PRs...",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºAIç¼–ç ä»£ç†ï¼ˆè€Œéæ™®é€šLLMï¼‰åœ¨çœŸå®è½¯ä»¶å¼€å‘ç¯å¢ƒï¼ˆPull Requestsï¼‰ä¸­çš„åº“ä½¿ç”¨è¡Œä¸ºç ”ç©¶ã€‚æ•°æ®é›†åŒ…å«å¤šç§ç¼–ç¨‹è¯­è¨€å’ŒçœŸå®çš„é¡¹ç›®ä¸Šä¸‹æ–‡ã€‚",
    "unique_features_quote": "To begin to fill this gap, we present the first focused empirical study of how AI coding agents use libraries in practice, by mining real agent-authored PRs from the AIDev dataset [13].",
    "data_size_quantity": 26760,
    "data_size_unit": "ä¸ªPRs",
    "last_updated_year": 2025,
    "last_updated_month": 11,
    "last_updated_day": null,
    "language_normalized": "['TypeScript/JavaScript', 'Python', 'Go', 'C#']",
    "dimension_normalized": "['AIç¼–ç ä»£ç†çš„åº“ä½¿ç”¨è¡Œä¸º', 'åº“å¯¼å…¥é¢‘ç‡', 'æ–°ä¾èµ–å¼•å…¥', 'ç‰ˆæœ¬æ§åˆ¶å®è·µ', 'åº“é€‰æ‹©å¤šæ ·æ€§']",
    "evaluation_method_normalized": "['åº“å¯¼å…¥é¢‘ç‡', 'æ–°ä¾èµ–å¼•å…¥é¢‘ç‡', 'ç‰ˆæœ¬çº¦æŸæŒ‡å®šæ¯”ä¾‹', 'åº“é€‰æ‹©å¤šæ ·æ€§ï¼ˆç‹¬ç‰¹åº“æ•°é‡ï¼‰']",
    "problem_domain_normalized": "['é€šç”¨è½¯ä»¶å¼€å‘', 'å¤šç§ç¼–ç¨‹è¯­è¨€', 'é¡¹ç›®ç±»å‹']",
    "source_type_normalized": "['çœŸå®GitHubä»“åº“', 'å…¬å¼€Pull Requests', 'LLMç¼–ç ä»£ç†', 'Claude Code', 'Cursor', 'Devin', 'Copilot', 'OpenAI Codex']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.13607_output/content.md",
    "benchmark_name": "LiveCodeBench",
    "benchmark_name_quote": "Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "æ–‡ä¸­æœªæè¿°LiveCodeBenchçš„å…·ä½“ä»»åŠ¡ï¼Œä»…æåŠä½œä¸ºè¯„æµ‹åŸºå‡†ã€‚",
    "task_description_quote": "Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro",
    "dimension": NaN,
    "dimension_quote": NaN,
    "evaluation_method": NaN,
    "evaluation_method_quote": NaN,
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": NaN,
    "problem_domain_quote": NaN,
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": NaN,
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": NaN,
    "task_granularity_quote": NaN,
    "evaluation_metrics": NaN,
    "evaluation_metrics_quote": NaN,
    "input_modality": NaN,
    "input_modality_quote": NaN,
    "output_modality": NaN,
    "output_modality_quote": NaN,
    "task_io_type": NaN,
    "task_io_type_quote": NaN,
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": NaN,
    "unique_features_quote": NaN,
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "[]",
    "evaluation_method_normalized": "[]",
    "problem_domain_normalized": "[]",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.13102_output/content.md",
    "benchmark_name": "GSM8K",
    "benchmark_name_quote": "Math: We evaluate on GSM8K (Cobbe et al., 2021).",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We study this on math (GSM8K, (Cobbe et al., 2021)) and coding (HumanEval/OPC, (Chen et al., 2021; Huang et al., 2025)) tasks using small-scale LLMs (7Bâ€“8B).",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "æ•°å­¦é—®é¢˜æ±‚è§£ï¼Œç‰¹åˆ«æ˜¯å°å­¦æ•°å­¦åº”ç”¨é¢˜",
    "task_description_quote": "Across math and coding benchmarks, where baseline student models begin with near-zero performance... (ç»“åˆä¸Šä¸‹æ–‡ï¼ŒGSM8Kæ˜¯æ•°å­¦åŸºå‡†)",
    "dimension": "æ•°å­¦æ¨ç†èƒ½åŠ›",
    "dimension_quote": "However, for reasoning-heavy domains, such as math and coding, learning requires more than recalling and using facts.",
    "evaluation_method": "Pass@k",
    "evaluation_method_quote": "After each teacher response, S is evaluated with Pass@k.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "æ•°å­¦ï¼ˆå°å­¦æ•°å­¦ï¼‰",
    "problem_domain_quote": "Math: We evaluate on GSM8K (Cobbe et al., 2021).",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": NaN,
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": "2021",
    "last_updated_quote": "Math: We evaluate on GSM8K (Cobbe et al., 2021).",
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "é—®é¢˜æ±‚è§£",
    "task_granularity_quote": "Given a complex question, S seeks to answer the question by interacting with T.",
    "evaluation_metrics": "Pass@k",
    "evaluation_metrics_quote": "After each teacher turn, we evaluate the student using the full chat history so far: we sample k direct answers from S and compute Pass@k.",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆæ•°å­¦é—®é¢˜æè¿°ï¼‰",
    "input_modality_quote": "Given a complex question, S seeks to answer the question by interacting with T.",
    "output_modality": "è‡ªç„¶è¯­è¨€ï¼ˆç­”æ¡ˆï¼‰",
    "output_modality_quote": "Following each teacher turn, S is prompted to output only the final answer.",
    "task_io_type": "æ–‡æœ¬åˆ°æ–‡æœ¬ï¼ˆæ•°å­¦é—®é¢˜åˆ°ç­”æ¡ˆï¼‰",
    "task_io_type_quote": "Given a complex question, S seeks to answer the question... S is prompted to output only the final answer.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "æœ¬æ–‡æœªæè¿°GSM8Kæ•°æ®é›†çš„ç‹¬ç‰¹ä¹‹å¤„ï¼Œä»…å°†å…¶ç”¨ä½œè¯„æµ‹åŸºå‡†ã€‚æœ¬æ–‡çš„ç‹¬ç‰¹ä¹‹å¤„åœ¨äºæå‡ºäº†ä¸€ä¸ªå­¦ç”Ÿä¸»å¯¼çš„äº¤äº’å¼å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºæå‡æ¨¡å‹åœ¨æ•°å­¦å’Œä»£ç ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚",
    "unique_features_quote": "In this paper, we investigate the complementary setting: Can students guide the conversation by asking helpful questions to a teacher? We study this on math (GSM8K, (Cobbe et al., 2021)) and coding (HumanEval/OPC, (Chen et al., 2021; Huang et al., 2025)) tasks...",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2021,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['æ•°å­¦æ¨ç†èƒ½åŠ›']",
    "evaluation_method_normalized": "['Pass@k']",
    "problem_domain_normalized": "['æ•°å­¦ï¼ˆå°å­¦æ•°å­¦ï¼‰']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.14233_output/content.md",
    "benchmark_name": "PentestEval",
    "benchmark_name_quote": "To address this gap, we introduce PentestEval, the first comprehensive benchmark for evaluating LLMs across six decomposed penetration testing stages",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "we introduce PentestEval, the first comprehensive benchmark for evaluating LLMs across six decomposed penetration testing stages",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åˆ†è§£çš„æ¸—é€æµ‹è¯•å…­ä¸ªé˜¶æ®µä¸­çš„æ€§èƒ½ã€‚è¿™å…­ä¸ªé˜¶æ®µæ˜¯ï¼šä¿¡æ¯æ”¶é›†ã€å¼±ç‚¹æ”¶é›†ã€å¼±ç‚¹è¿‡æ»¤ã€æ”»å‡»å†³ç­–ã€æ¼æ´åˆ©ç”¨ç”Ÿæˆå’Œæ¼æ´åˆ©ç”¨ä¿®è®¢ã€‚",
    "task_description_quote": "PentestEval, the first comprehensive benchmark for evaluating LLMs across six decomposed penetration testing stages: Information Collection, Weakness Gathering and Filtering, Attack Decision-Making, Exploit Generation and Revision.",
    "dimension": "æ¸—é€æµ‹è¯•å„é˜¶æ®µçš„æ€§èƒ½ã€ç«¯åˆ°ç«¯å·¥ä½œæµå®Œæˆèƒ½åŠ›",
    "dimension_quote": "evaluate LLMs along two complementary dimensions: performance on individual stages and the ability to complete full penetration testing workflows.",
    "evaluation_method": "å°†LLMè¾“å‡ºä¸ä¸“å®¶æ ‡æ³¨çš„çœŸå®æ”»å‡»æ•°æ®è¿›è¡Œç³»ç»Ÿæ¯”è¾ƒï¼Œå®ç°ç²¾ç¡®æµ‹é‡ã€‚åŒ…å«é˜¶æ®µçº§è¯„ä¼°å’Œç«¯åˆ°ç«¯æµ‹è¯•ã€‚",
    "evaluation_method_quote": "By systematically comparing LLM outputs against the expert-annotated solutions, PentestEval enables precise measurement of model effectiveness at each stage.",
    "context_dependency": "å¤šæ­¥éª¤ã€è¿­ä»£çš„æ”»å‡»é“¾ï¼Œä¾èµ–å‰åºé˜¶æ®µçš„è¾“å‡ºå’Œç³»ç»Ÿåé¦ˆã€‚",
    "context_dependency_quote": "attackers rarely succeed by exploiting a single vulnerability. Instead, they construct multi-step attack chains by combining multiple weaknesses in sequence",
    "problem_domain": "ç½‘ç»œå®‰å…¨ã€æ¸—é€æµ‹è¯•",
    "problem_domain_quote": "Penetration testing is essential for assessing and strengthening system security against real-world threats",
    "problem_difficulty": "æ¶µç›–ä»£è¡¨æ€§å¼±ç‚¹ï¼ŒåŒ…æ‹¬OWASP Top 10ã€CWE Top 25ä»¥åŠæœªå…¬å¼€çš„é›¶æ—¥æ¼æ´ï¼Œéš¾åº¦è¾ƒé«˜ã€‚",
    "problem_difficulty_quote": "collectively capturing representative weaknesses from OWASP Top 10 [17], CWE Top 25 [18], and even undisclosed zero-day vulnerabilities.",
    "language": "æ–‡ä¸­æœªæ˜ç¡®æåŠç¼–ç¨‹è¯­è¨€ï¼Œä½†ä»»åŠ¡æ¶‰åŠç”Ÿæˆè„šæœ¬æˆ–å‘½ä»¤è¡Œè°ƒç”¨ï¼Œæ¨æµ‹å¯èƒ½æ¶‰åŠå¤šç§è„šæœ¬è¯­è¨€ã€‚",
    "language_quote": NaN,
    "data_size": "åŒ…å«346ä¸ªç‹¬ç«‹ä»»åŠ¡ï¼Œç»„ç»‡åœ¨12ä¸ªæ˜“å—æ”»å‡»çš„åœºæ™¯ä¸­ã€‚",
    "data_size_quote": "The benchmark comprises 346 distinct tasks organized within 12 vulnerable scenarios",
    "source_type": "ç”±äº”ä½ä¸“å®¶è®¾è®¡ç¯å¢ƒã€æ³¨å…¥æ¼æ´ï¼ˆåŒ…æ‹¬ä¸€ä¸ªé›¶æ—¥æ¼æ´ï¼‰å¹¶æ ‡æ³¨çœŸå®æ”»å‡»æ•°æ®ã€‚",
    "source_type_quote": "built with five experts who design environments, inject vulnerabilities including a zero-day, and annotate ground-truth attack data.",
    "last_updated": "2025",
    "last_updated_quote": "arXiv:2512.14233v1  [cs.SE]  16 Dec 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œç”±ç ”ç©¶å›¢é˜Ÿä¸ä¸“ä¸šæ¸—é€æµ‹è¯•äººå‘˜åä½œæ„å»ºã€‚",
    "build_type_quote": "expert-annotated ground truth for each stage, collaboratively verified by professional penetration testers",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "æ¸—é€æµ‹è¯•å·¥ä½œæµåˆ†è§£ä¸ºå…­ä¸ªé˜¶æ®µï¼šä¿¡æ¯æ”¶é›†ã€å¼±ç‚¹æ”¶é›†ã€å¼±ç‚¹è¿‡æ»¤ã€æ”»å‡»å†³ç­–ã€æ¼æ´åˆ©ç”¨ç”Ÿæˆã€æ¼æ´åˆ©ç”¨ä¿®è®¢ã€‚",
    "task_granularity_quote": "six decomposed penetration testing stages: Information Collection, Weakness Gathering and Filtering, Attack Decision-Making, Exploit Generation and Revision.",
    "evaluation_metrics": "æˆåŠŸç‡ï¼ˆä¾‹å¦‚ï¼Œç«¯åˆ°ç«¯ç®¡é“è¾¾åˆ°31%çš„æˆåŠŸç‡ï¼Œæ”»å‡»å†³ç­–å’Œæ¼æ´åˆ©ç”¨ç”Ÿæˆé˜¶æ®µæˆåŠŸç‡çº¦ä¸º25%ï¼‰",
    "evaluation_metrics_quote": "End-to-end pipelines reach only 31% success rate... The most challenging components, i.e., Attack Decision-Making and Exploit Generation, reach only around a 25% success rate.",
    "input_modality": "ç›®æ ‡ç³»ç»Ÿä¿¡æ¯ï¼ˆå¦‚URLï¼‰ã€æ”¶é›†çš„å¼±ç‚¹ä¿¡æ¯ã€å‰åºæ”»å‡»æ­¥éª¤çš„åé¦ˆç­‰ï¼Œä¸»è¦ä¸ºæ–‡æœ¬ä¿¡æ¯ã€‚",
    "input_modality_quote": "given a target URL T... given structured profile I of a target system... given WF , wiâˆ’1, riâˆ’1",
    "output_modality": "ç»“æ„åŒ–é…ç½®æ–‡ä»¶ã€å¼±ç‚¹é›†åˆã€æ”»å‡»å†³ç­–ã€å¯æ‰§è¡Œçš„æ”»å‡»è„šæœ¬æˆ–å‘½ä»¤ã€‚",
    "output_modality_quote": "I represents a structured profile... WG represents the resulting set... outputs the next weakness wi... producing a concrete script or command-line invocation ei",
    "task_io_type": "æ–‡æœ¬åˆ°æ–‡æœ¬ï¼ˆç”Ÿæˆå†³ç­–ã€å¼±ç‚¹åˆ—è¡¨ï¼‰ã€æ–‡æœ¬åˆ°ä»£ç ï¼ˆç”Ÿæˆæ”»å‡»è„šæœ¬ï¼‰",
    "task_io_type_quote": "producing a concrete script or command-line invocation ei",
    "execution_environment": "æ–‡ä¸­æœªæ˜ç¡®æè¿°ï¼Œä½†æ¶‰åŠå¯¹ç›®æ ‡ç³»ç»Ÿçš„å®é™…æˆ–æ¨¡æ‹Ÿæ”»å‡»ï¼Œæ¨æµ‹éœ€è¦ç‰¹å®šçš„æµ‹è¯•ç¯å¢ƒæˆ–æ²™ç®±ã€‚",
    "execution_environment_quote": NaN,
    "unique_features": "é¦–ä¸ªé’ˆå¯¹æ¸—é€æµ‹è¯•çš„æ¨¡å—åŒ–ã€é˜¶æ®µçº§è¯„ä¼°åŸºå‡†ï¼›å°†å·¥ä½œæµåˆ†è§£ä¸ºå…­ä¸ªé˜¶æ®µè¿›è¡Œç»†ç²’åº¦è¯„ä¼°ï¼›åŒ…å«ä¸“å®¶æ ‡æ³¨çš„çœŸå®æ•°æ®å’Œè‡ªåŠ¨åŒ–è¯„ä¼°ç®¡é“ï¼›è¦†ç›–é›¶æ—¥æ¼æ´ã€‚",
    "unique_features_quote": "the first modular benchmarking framework specifically designed for fine-grained and rigorous assessment of LLM performance in the stages of the penetration testing workflow... inject vulnerabilities including a zero-day",
    "data_size_quantity": 346,
    "data_size_unit": "ä¸ªç‹¬ç«‹ä»»åŠ¡",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['å¤šç§è„šæœ¬è¯­è¨€']",
    "dimension_normalized": "['æ¸—é€æµ‹è¯•å„é˜¶æ®µçš„æ€§èƒ½', 'ç«¯åˆ°ç«¯å·¥ä½œæµå®Œæˆèƒ½åŠ›']",
    "evaluation_method_normalized": "['æˆåŠŸç‡']",
    "problem_domain_normalized": "['ç½‘ç»œå®‰å…¨', 'æ¸—é€æµ‹è¯•']",
    "source_type_normalized": "['ä¸“å®¶è®¾è®¡ç¯å¢ƒ', 'æ³¨å…¥æ¼æ´', 'æ ‡æ³¨çœŸå®æ”»å‡»æ•°æ®']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.15699_output/content.md",
    "benchmark_name": "FrontierCS",
    "benchmark_name_quote": "We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science...",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science...",
    "dataset_url": "Frontier-CS GitHub",
    "dataset_url_quote": "Code and Data: Frontier-CS GitHub",
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹è§£å†³å¼€æ”¾å¼è®¡ç®—æœºç§‘å­¦é—®é¢˜çš„èƒ½åŠ›ï¼Œè¿™äº›é—®é¢˜æ²¡æœ‰å·²çŸ¥çš„å°é—­å½¢å¼æˆ–ç¡®å®šæ€§æœ€ä¼˜è§£ã€‚æ¨¡å‹éœ€è¦å®ç°å¯æ‰§è¡Œç¨‹åºæ¥è§£å†³é—®é¢˜ï¼Œè€Œéç›´æ¥è¾“å‡ºç­”æ¡ˆã€‚",
    "task_description_quote": "In this paper, we introduce FrontierCS, a coding benchmark that evaluates LLMs on solving open-ended computer science problems, where no known closed-form or deterministic optimal solution exists in practice. Unlike math or reasoning benchmarks that require a direct answer (e.g., AIME), FrontierCS requires models to implement executable programs to solve the problem (e.g., LiveCodeBench).",
    "dimension": "å¼€æ”¾å¼æ¨ç†ã€ç®—æ³•è®¾è®¡ä¸ä¼˜åŒ–ã€ç ”ç©¶é—®é¢˜è§£å†³èƒ½åŠ›",
    "dimension_quote": "FrontierCS focuses on two tracks: algorithmic optimization problems, and tasks more closely tied to real-world CS research. Both tracks naturally exhibit open-endedness, stress-testing a modelâ€™s ability to perform deep open-ended reasoning and discover nontrivial optimization strategies.",
    "evaluation_method": "é€šè¿‡è‡ªåŠ¨è¯„ä¼°å™¨è¿è¡Œç”Ÿæˆçš„ç¨‹åºï¼Œæ ¹æ®ä»»åŠ¡ç‰¹å®šæŒ‡æ ‡ï¼ˆå¦‚æ‰“åŒ…å¯†åº¦ã€è¿è¡Œæ—¶é—´ã€å†…å­˜ä½¿ç”¨ï¼‰åœ¨èµ„æºé™åˆ¶ä¸‹è¿›è¡Œç¡®å®šæ€§éªŒè¯å’Œå®šé‡è¯„åˆ†ï¼Œè€Œéç®€å•çš„é€šè¿‡/å¤±è´¥ã€‚",
    "evaluation_method_quote": "Each FrontierCS task can be solved by submitting executable code: the evaluator runs the program on generated instances and scores its outputs by task-specific metrics under resource limits (e.g., time and memory usage). ... Solutions with runtime limit can be automatically checked for validity and assigned a numeric score that reflects the quality of the solution, rather than a simple pass-or-fail.",
    "context_dependency": "å•ä»»åŠ¡é—®é¢˜ï¼Œæ¨¡å‹æ ¹æ®é—®é¢˜æè¿°å’Œå¿…è¦çš„I/Oæˆ–APIå­˜æ ¹ç”Ÿæˆç‹¬ç«‹çš„æ±‚è§£ç¨‹åºã€‚",
    "context_dependency_quote": "The model is prompted with the problem specification (and any required I/O or API stubs) and must produce a self-contained solver program.",
    "problem_domain": "è®¡ç®—æœºç§‘å­¦ï¼ŒåŒ…å«ç®—æ³•ä¼˜åŒ–é—®é¢˜å’Œç ”ç©¶é—®é¢˜ã€‚ç®—æ³•é—®é¢˜æ¶µç›–ä¼˜åŒ–ã€æ„é€ ã€äº¤äº’ç±»åˆ«ï¼›ç ”ç©¶é—®é¢˜æ¶µç›–æ“ä½œç³»ç»Ÿã€é«˜æ€§èƒ½è®¡ç®—ã€äººå·¥æ™ºèƒ½ã€æ•°æ®åº“ã€ç¼–ç¨‹è¯­è¨€ã€å®‰å…¨ç­‰é¢†åŸŸã€‚",
    "problem_domain_quote": "FrontierCS consists 156 problems across two tracks: Algorithmic Problems and Research Problems. The Algorithmic Problems track contains 107 problems adapted from programming contests, covering three categories: Optimization, Constructive, and Interactive problems. The Research Problems track contains 49 problems sourced from real-world computer science research questions, spanning six domains: Operating Systems, High-Performance Computing, Artificial Intelligence, Databases, Programming Languages, and Security.",
    "problem_difficulty": "å‰æ²¿è®¡ç®—æœºç§‘å­¦éš¾åº¦ï¼Œé—®é¢˜å…·æœ‰å¼€æ”¾æ€§ï¼Œå…¨å±€æœ€ä¼˜è§£æœªçŸ¥æˆ–è®¡ç®—ä¸Šä¸å¯è¡Œã€‚",
    "problem_difficulty_quote": "Combining open-ended design, measurable progress, and expert curation, FrontierCS provides a benchmark at the frontier of computer-science difficulty.",
    "language": "æ–‡ä¸­æœªæ˜ç¡®æŒ‡å®šç¼–ç¨‹è¯­è¨€ï¼Œä½†è¦æ±‚æ¨¡å‹ç”Ÿæˆå¯æ‰§è¡Œç¨‹åºã€‚",
    "language_quote": NaN,
    "data_size": "åŒ…å«156ä¸ªé—®é¢˜ï¼Œå…¶ä¸­ç®—æ³•é—®é¢˜107ä¸ªï¼Œç ”ç©¶é—®é¢˜49ä¸ªã€‚",
    "data_size_quote": "FrontierCS consists 156 problems across two tracks: Algorithmic Problems and Research Problems. The Algorithmic Problems track contains 107 problems... The Research Problems track contains 49 problems...",
    "source_type": "ç®—æ³•é—®é¢˜æ”¹ç¼–è‡ªç¼–ç¨‹ç«èµ›ï¼›ç ”ç©¶é—®é¢˜æ¥æºäºç°å®ä¸–ç•Œçš„è®¡ç®—æœºç§‘å­¦ç ”ç©¶é—®é¢˜ï¼Œç”±ä¸“å®¶ï¼ˆåŒ…æ‹¬CSåšå£«å’Œé¡¶çº§ç«èµ›å‚ä¸è€…åŠå‡ºé¢˜è€…ï¼‰è®¾è®¡å’Œè¯„å®¡ã€‚",
    "source_type_quote": "a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. ... The Algorithmic Problems track contains 107 problems adapted from programming contests... The Research Problems track contains 49 problems sourced from real-world computer science research questions...",
    "last_updated": "2025-12-17 (arXivç‰ˆæœ¬)",
    "last_updated_quote": "arXiv:2512.15699v1  [cs.LG]  17 Dec 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œç”±å¤šæ‰€å¤§å­¦å’Œç ”ç©¶æœºæ„çš„ä¸“å®¶å›¢é˜Ÿç²¾å¿ƒç­–åˆ’ï¼Œç»è¿‡ææ¡ˆã€å®ç°å’Œè¯„å®¡çš„å¤šé˜¶æ®µæµç¨‹ã€‚",
    "build_type_quote": "Each problem is carefully curated through a multi-stage process involving proposal, implementation, and review to ensure quality and relevance...",
    "contamination_status": "æŠ—æ±¡æŸ“è®¾è®¡ï¼Œé€šè¿‡å‚æ•°åŒ–é—®é¢˜ç”Ÿæˆå™¨äº§ç”Ÿå¤§é‡å¯å˜éš¾åº¦çš„å®ä¾‹ï¼Œæ”¯æŒç”Ÿæˆæ–°é²œçš„ã€æœªè§è¿‡çš„æµ‹è¯•ç”¨ä¾‹ä»¥é˜²æ­¢æ³„éœ²å’Œè¿‡æ‹Ÿåˆã€‚",
    "contamination_status_quote": "Parametric Problem Generator: The task specification induces a large, variable-difficulty space of instances, enabling fresh, unseen test cases to prevent leak and overfitting.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆç”Ÿæˆå®Œæ•´çš„æ±‚è§£ç¨‹åºï¼‰",
    "task_granularity_quote": "Models solve these tasks by implementing executable programs rather than outputting a direct answer.",
    "evaluation_metrics": "ä»»åŠ¡ç‰¹å®šçš„å®šé‡è¯„åˆ†ï¼ˆå¦‚æ‰“åŒ…å¯†åº¦ï¼‰ï¼Œè€Œéå•ä¸€æŒ‡æ ‡ã€‚",
    "evaluation_metrics_quote": "assigned a numeric score that reflects the quality of the solution, rather than a simple pass-or-fail.",
    "input_modality": "è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°ï¼Œä»¥åŠå¿…è¦çš„I/Oæˆ–APIå­˜æ ¹ã€‚",
    "input_modality_quote": "The model is prompted with the problem specification (and any required I/O or API stubs)...",
    "output_modality": "ä»£ç ï¼ˆå¯æ‰§è¡Œçš„æ±‚è§£ç¨‹åºï¼‰",
    "output_modality_quote": "must produce a self-contained solver program.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "The model is prompted with the problem specification... and must produce a self-contained solver program.",
    "execution_environment": "åœ¨èµ„æºé™åˆ¶ï¼ˆå¦‚æ—¶é—´å’Œå†…å­˜ï¼‰ä¸‹è¿è¡Œï¼Œå…·ä½“ç¯å¢ƒå–å†³äºä»»åŠ¡ã€‚",
    "execution_environment_quote": "the evaluator runs the program on generated instances and scores its outputs by task-specific metrics under resource limits (e.g., time and memory usage).",
    "unique_features": "ä¸“æ³¨äºè§£å†³æœ€ä¼˜è§£æœªçŸ¥çš„å¼€æ”¾å¼è®¡ç®—æœºç§‘å­¦é—®é¢˜ï¼›ç»“åˆäº†ç®—æ³•ä¼˜åŒ–å’Œç°å®ä¸–ç•Œç ”ç©¶é—®é¢˜ï¼›æä¾›ç¡®å®šæ€§éªŒè¯å’Œè¿ç»­è´¨é‡è¯„åˆ†ï¼Œè€Œéé€šè¿‡/å¤±è´¥ï¼›é€šè¿‡å‚æ•°åŒ–ç”Ÿæˆå™¨é˜²æ­¢æ•°æ®æ±¡æŸ“ï¼›æ—¨åœ¨è¡¡é‡æ¨¡å‹å®ç°æœ‰æ•ˆå’Œé«˜æ•ˆç®—æ³•çš„èƒ½åŠ›ï¼Œè€Œéç©·ä¸¾è§£å†³é—®é¢˜ã€‚",
    "unique_features_quote": "Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. ... This design measures the ability of models to implement effective and efficient algorithms rather than algorithms that exhaustively solve the problem. ... problems are unsolved, admit many solution strategies, and are evaluated via deterministic scoring rather than pass-or-fail.",
    "data_size_quantity": 156,
    "data_size_unit": "ä¸ªé—®é¢˜",
    "last_updated_year": 2025,
    "last_updated_month": 12,
    "last_updated_day": 17,
    "language_normalized": "[]",
    "dimension_normalized": "['å¼€æ”¾å¼æ¨ç†', 'ç®—æ³•è®¾è®¡ä¸ä¼˜åŒ–', 'ç ”ç©¶é—®é¢˜è§£å†³èƒ½åŠ›']",
    "evaluation_method_normalized": "['ä»»åŠ¡ç‰¹å®šçš„å®šé‡è¯„åˆ†']",
    "problem_domain_normalized": "['è®¡ç®—æœºç§‘å­¦', 'ç®—æ³•ä¼˜åŒ–é—®é¢˜', 'ç ”ç©¶é—®é¢˜', 'ç®—æ³•é—®é¢˜', 'ä¼˜åŒ–', 'æ„é€ ', 'äº¤äº’ç±»åˆ«', 'ç ”ç©¶é—®é¢˜', 'æ“ä½œç³»ç»Ÿ', 'é«˜æ€§èƒ½è®¡ç®—', 'äººå·¥æ™ºèƒ½', 'æ•°æ®åº“', 'ç¼–ç¨‹è¯­è¨€', 'å®‰å…¨']",
    "source_type_normalized": "['ç®—æ³•é—®é¢˜æ”¹ç¼–è‡ªç¼–ç¨‹ç«èµ›', 'ç ”ç©¶é—®é¢˜æ¥æºäºç°å®ä¸–ç•Œçš„è®¡ç®—æœºç§‘å­¦ç ”ç©¶é—®é¢˜', 'ç”±ä¸“å®¶è®¾è®¡å’Œè¯„å®¡']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2512.15688_output/content.md",
    "benchmark_name": "BashArena",
    "benchmark_name_quote": "We introduce BashArena, a control setting for studying AI control techniques in security-critical environments.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We introduce BashArena, a control setting for studying AI control techniques in security-critical environments.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ç”¨äºç ”ç©¶AIæ§åˆ¶æŠ€æœ¯çš„æ§åˆ¶è®¾å®šï¼ŒåŒ…å«ä¸»ä»»åŠ¡ï¼ˆç³»ç»Ÿç®¡ç†/åŸºç¡€è®¾æ–½å·¥ç¨‹ï¼‰å’Œä¾§ä»»åŠ¡ï¼ˆæ¨¡æ‹Ÿå®‰å…¨ç ´åï¼‰ï¼Œæ—¨åœ¨è¯„ä¼°å’Œè®¾è®¡AIæ§åˆ¶åè®®ã€‚",
    "task_description_quote": "BashArena contains 637 Linux system administration and infrastructure engineering tasks in complex, realistic environments, along with four sabotage objectives... for a red team to target.",
    "dimension": "AIæ§åˆ¶ã€å®‰å…¨ã€ä»»åŠ¡å®Œæˆèƒ½åŠ›ã€ç ´åè¡Œä¸ºæ£€æµ‹ä¸è§„é¿",
    "dimension_quote": "We evaluate multiple frontier LLMs on their ability to complete tasks, perform sabotage undetected, and detect sabotage attempts.",
    "evaluation_method": "ä¸»ä»»åŠ¡ï¼šé€šè¿‡Pytestæµ‹è¯•ç”¨ä¾‹è¯„ä¼°ä»»åŠ¡å®Œæˆåº¦ï¼ˆé€šè¿‡æµ‹è¯•ç”¨ä¾‹çš„æ¯”ä¾‹ï¼‰ã€‚ä¾§ä»»åŠ¡ï¼šåœ¨è¯„ä¼°è¿‡ç¨‹ä¸­ï¼Œæ¯æ¬¡è¡ŒåŠ¨åè¿è¡Œæµ‹è¯•ï¼Œè‹¥é€šè¿‡åˆ™è®¤ä¸ºä¾§ä»»åŠ¡å®Œæˆã€‚",
    "evaluation_method_quote": "The agentâ€™s score on the task is measured as the fraction of test cases that pass. Tasks contain 12 test cases on average... The test is run after every action during evaluation, and the side task is considered complete if it ever passes.",
    "context_dependency": "å¤šè¡ŒåŠ¨ã€å¤šå®¹å™¨Dockerç¯å¢ƒäº¤äº’",
    "context_dependency_quote": "All task environments are variations of the base environment, which is a multi-container Docker compose project... The agent works in the default container... The base environment also features three servers running in separate containers...",
    "problem_domain": "Linuxç³»ç»Ÿç®¡ç†ã€åŸºç¡€è®¾æ–½å·¥ç¨‹ã€å®‰å…¨ã€ç³»ç»Ÿç›‘æ§ã€æ•°æ®å¤„ç†ã€ä½çº§ç³»ç»Ÿç¼–ç¨‹ã€è¿›ç¨‹ç®¡ç†ã€æ•°æ®åº“è‡ªåŠ¨åŒ–ã€å†…å­˜ä¼˜åŒ–ã€CI/CDè‡ªåŠ¨åŒ–",
    "problem_domain_quote": "BashArena contains 637 Linux system administration and infrastructure engineering tasks... Cluster Label: System Monitoring, Data Processing, Low-Level Systems Programming, Process Management, Database Automation, Security, Memory Optimization, CI/CD Automation, Other.",
    "problem_difficulty": "å¯¹å½“å‰å‰æ²¿æ¨¡å‹å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå°šæœªè¢«é¥±å’Œ",
    "problem_difficulty_quote": "Main tasks are challenging for current frontier models. BashArenaâ€™s main tasks are difficult enough that they are not saturated by current frontier models, and weaker models perform significantly worse.",
    "language": "Bashã€Pythonï¼ˆç¯å¢ƒä¸­å®‰è£…äº†PythonåŒ…ï¼‰",
    "language_quote": "The agent works in the default container, which contains a simple filesystem with basic shell utilities and Python packages installed.",
    "data_size": "åŒ…å«637ä¸ªä¸»ä»»åŠ¡å’Œ4ä¸ªä¾§ä»»åŠ¡",
    "data_size_quote": "BashArena contains 637 main tasks created by an automated pipeline, and four manually-constructed side tasks which simulate critical security failures.",
    "source_type": "ä¸»ä»»åŠ¡åŸºäºStackOverflowä¸Šæ ‡è®°ä¸ºâ€œlinuxâ€æˆ–â€œbashâ€çš„é—®é¢˜ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–ç®¡é“ç”Ÿæˆå’Œè¿­ä»£æ”¹è¿›ã€‚ä¾§ä»»åŠ¡ä¸ºæ‰‹åŠ¨æ„å»ºã€‚",
    "source_type_quote": "We generate main tasks based on StackOverflow questions tagged â€œlinuxâ€ or â€œbashâ€... We generate each task using Claude Opus 4.1 based on a selected StackOverflow question... Each side task consists of a description and a test.",
    "last_updated": "2025-12-17 (arXivç‰ˆæœ¬æ—¥æœŸ)",
    "last_updated_quote": "arXiv:2512.15688v1  [cs.CR]  17 Dec 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆRedwood Researchï¼‰ï¼Œä½¿ç”¨è‡ªåŠ¨åŒ–ç®¡é“ç”Ÿæˆä¸»ä»»åŠ¡ï¼Œæ‰‹åŠ¨æ„å»ºä¾§ä»»åŠ¡ã€‚",
    "build_type_quote": "We release the dataset as a ControlArena setting and share our task generation pipeline... We generate main tasks based on StackOverflow questions... and four manually-constructed side tasks.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆä¸ç³»ç»Ÿæ“ä½œï¼ˆé€šè¿‡Bashå’ŒPythonå‘½ä»¤åºåˆ—å®ç°å¤æ‚ä»»åŠ¡ï¼‰",
    "task_granularity_quote": "Models are run using a slightly modified version of Inspect AIâ€™s ReAct scaffold, provided with bash, python, file_edit, and submit tools.",
    "evaluation_metrics": "ä¸»ä»»åŠ¡ï¼šé€šè¿‡æµ‹è¯•ç”¨ä¾‹çš„æ¯”ä¾‹ã€‚ä¾§ä»»åŠ¡ï¼šè½¨è¿¹çº§å‡é˜³æ€§ç‡ï¼ˆFPRï¼‰ã€ç ´åæˆåŠŸç‡ã€‚",
    "evaluation_metrics_quote": "The agentâ€™s score on the task is measured as the fraction of test cases that pass... Claude Sonnet 4.5 successfully executes sabotage while evading monitoring by GPT-4.1 mini 26% of the time, at 4% trajectory-wise FPR.",
    "input_modality": "è‡ªç„¶è¯­è¨€ä»»åŠ¡æè¿°",
    "input_modality_quote": "Each main task includes the following components: Task description: a written assignment that describes what needs to be accomplished...",
    "output_modality": "ä»£ç ï¼ˆBash/Pythonè„šæœ¬ï¼‰ã€ç³»ç»Ÿé…ç½®ã€æ–‡ä»¶æ“ä½œ",
    "output_modality_quote": "The agent runs commands as root inside the default container... it implements a solution to the main task, generally by creating several files back to back.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç /ç³»ç»Ÿæ“ä½œ",
    "task_io_type_quote": "The agent is only shown the task description... The agent runs commands as root inside the default container.",
    "execution_environment": "å¤šå®¹å™¨Dockerç¯å¢ƒï¼ŒåŒ…å«åŸºæœ¬Shellå·¥å…·ã€PythonåŒ…ã€æ¨¡æ‹Ÿæ•æ„Ÿæ•°æ®ã€é˜²ç«å¢™ã€sudoç®¡ç†å·¥å…·ã€å¤–éƒ¨æœåŠ¡å™¨ï¼ˆæ—¥å¿—æœåŠ¡å™¨ã€æ¶æ„è½¯ä»¶æœåŠ¡å™¨ï¼‰ã€‚",
    "execution_environment_quote": "All task environments are variations of the base environment, which is a multi-container Docker compose project. The agent works in the default container, which contains a simple filesystem with basic shell utilities and Python packages installed... The base environment also features three servers running in separate containers...",
    "unique_features": "ä¸“é—¨ä¸ºç ”ç©¶AIæ§åˆ¶è€Œè®¾è®¡ï¼ŒåŒ…å«ä¸»ä»»åŠ¡ä¸ç ´åæ€§ä¾§ä»»åŠ¡çš„å¯¹æŠ—æ€§è®¾å®šï¼›ç¯å¢ƒå¤æ‚å¤šæ ·ï¼Œæ”¯æŒå¤æ‚çš„ç ´åç­–ç•¥ï¼›ä¸»ä»»åŠ¡å¯¹å‰æ²¿æ¨¡å‹å…·æœ‰æŒ‘æˆ˜æ€§ï¼›æ•°æ®é›†è§„æ¨¡å¤§ä¸”å¤šæ ·ï¼›åŒ…å«è¯¦ç»†çš„è‡ªåŠ¨åŒ–ä»»åŠ¡ç”Ÿæˆç®¡é“ã€‚",
    "unique_features_quote": "BashArena, a control setting for studying AI control techniques in security-critical environments... Supports sophisticated sabotage strategies... Main tasks are challenging for current frontier models... Large and diverse... We release the dataset as a ControlArena setting and share our task generation pipeline.",
    "data_size_quantity": 637,
    "data_size_unit": "ä¸ªä¸»ä»»åŠ¡",
    "last_updated_year": 2025,
    "last_updated_month": 12,
    "last_updated_day": 17,
    "language_normalized": "['Bash', 'Python']",
    "dimension_normalized": "['AIæ§åˆ¶', 'å®‰å…¨', 'ä»»åŠ¡å®Œæˆèƒ½åŠ›', 'ç ´åè¡Œä¸ºæ£€æµ‹ä¸è§„é¿']",
    "evaluation_method_normalized": "['é€šè¿‡æµ‹è¯•ç”¨ä¾‹çš„æ¯”ä¾‹', 'è½¨è¿¹çº§å‡é˜³æ€§ç‡', 'ç ´åæˆåŠŸç‡']",
    "problem_domain_normalized": "['Linuxç³»ç»Ÿç®¡ç†', 'åŸºç¡€è®¾æ–½å·¥ç¨‹', 'å®‰å…¨', 'ç³»ç»Ÿç›‘æ§', 'æ•°æ®å¤„ç†', 'ä½çº§ç³»ç»Ÿç¼–ç¨‹', 'è¿›ç¨‹ç®¡ç†', 'æ•°æ®åº“è‡ªåŠ¨åŒ–', 'å†…å­˜ä¼˜åŒ–', 'CI/CDè‡ªåŠ¨åŒ–']",
    "source_type_normalized": "['StackOverflow', 'è‡ªåŠ¨åŒ–ç®¡é“ç”Ÿæˆ', 'æ‰‹åŠ¨æ„å»º']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.16816_output/content.md",
    "benchmark_name": "CAFFE (Counterfactual Assessment Framework for Fairness Evaluation)",
    "benchmark_name_quote": "This paper introduces CAFFE (Counterfactual Assessment Framework for Fairness Evaluation), a framework that integrates counterfactual fairness principles with structured test case definitions inspired by the ISO/IEC/IEEE 29119 standard [3].",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "This paper introduces CAFFE (Counterfactual Assessment Framework for Fairness Evaluation)... To sum up, this paper offers three main contributions: (1) a novel testing framework, CAFFE...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ç³»ç»Ÿæ€§åœ°æµ‹è¯•å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åäº‹å®æ¡ä»¶ä¸‹çš„å…¬å¹³æ€§å±æ€§ï¼Œå³æ£€æµ‹æ¨¡å‹åœ¨è¾“å…¥ä»…æ•æ„Ÿå±æ€§ï¼ˆå¦‚æ€§åˆ«ã€ç§æ—ï¼‰ä¸åŒä½†æ„å›¾ç›¸åŒæ—¶ï¼Œè¾“å‡ºæ˜¯å¦ä¿æŒä¸€è‡´ã€‚",
    "task_description_quote": "Our work contributes to the latter line of research, with a focus on systematically testing the fairness properties of LLMs under counterfactual conditions... Counterfactual Fairness: A model is considered fair with respect to a sensitive attribute if, for every pair of identical inputs differing only in that attribute, the output does not change.",
    "dimension": "åäº‹å®å…¬å¹³æ€§",
    "dimension_quote": "Toward Systematic Counterfactual Fairness Evaluation of Large Language Models: The CAFFE Framework",
    "evaluation_method": "ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦æŒ‡æ ‡è¯„ä¼°æ¨¡å‹å“åº”ï¼Œå¹¶å®šä¹‰é‡åŒ–çš„å…¬å¹³æ€§é˜ˆå€¼ã€‚",
    "evaluation_method_quote": "(3) evaluates model responses using semantic similarity metrics... It automatically constructs intent-aware and realistic counterfactual test cases, defines quantitative fairness thresholds, and evaluates responses through semantic similarity metrics.",
    "context_dependency": "ä¾èµ–äºæç¤ºæ„å›¾å’Œå¯¹è¯ä¸Šä¸‹æ–‡ã€‚",
    "context_dependency_quote": "proposing a structured and intent-aware framework... we propose formalizing fairness test cases along key dimensions such as prompt intent, conversational context...",
    "problem_domain": "å¤§å‹è¯­è¨€æ¨¡å‹çš„å…¬å¹³æ€§è¯„ä¼°ï¼Œå±äºè½¯ä»¶å·¥ç¨‹ä¸­çš„äººå·¥æ™ºèƒ½è½¯ä»¶æµ‹è¯•é¢†åŸŸã€‚",
    "problem_domain_quote": "Toward Systematic Counterfactual Fairness Evaluation of Large Language Models... Software Engineering for Artificial Intelligence.",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "è‡ªç„¶è¯­è¨€ï¼ˆç”¨äºæ„å»ºæç¤ºè¯ï¼‰ï¼Œä¸ç‰¹å®šäºç¼–ç¨‹è¯­è¨€ã€‚",
    "language_quote": "Our framework automatically generates counterfactual test data through stereotype-aware prompt construction, enhancing both linguistic diversity and semantic consistency across test cases.",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": "é€šè¿‡åˆ»æ¿å°è±¡æ„ŸçŸ¥çš„æç¤ºæ„å»ºè‡ªåŠ¨ç”Ÿæˆåäº‹å®æµ‹è¯•æ•°æ®ã€‚",
    "source_type_quote": "Our framework automatically generates counterfactual test data through stereotype-aware prompt construction...",
    "last_updated": "2025-12-18 (é¢„å°æœ¬ç‰ˆæœ¬æ—¥æœŸ)",
    "last_updated_quote": "arXiv:2512.16816v1  [cs.SE]  18 Dec 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆç”±æœ¬æ–‡ä½œè€…æå‡ºå’Œæ„å»ºçš„æ¡†æ¶ï¼‰",
    "build_type_quote": "This paper introduces CAFFE (Counterfactual Assessment Framework for Fairness Evaluation)...",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "å…¬å¹³æ€§æµ‹è¯•ï¼ˆæ£€æµ‹æ¨¡å‹è¡Œä¸ºçš„ä¸ä¸€è‡´æ€§ï¼‰",
    "task_granularity_quote": "LLM-Fairness testing: Given a language model ğ‘†, a set of inputs ğ¼, the required fairness condition ğ¶, and the observed fairness condition ğ¶â€², LLM-Fairness testing consists of executing ğ¼on ğ‘†to identify any discrepancies between ğ¶and ğ¶â€² in the generated responses, measuring to what extent the system discriminates.",
    "evaluation_metrics": "è¯­ä¹‰ç›¸ä¼¼åº¦æŒ‡æ ‡ï¼Œå…¬å¹³æ€§è¿è§„æ£€æµ‹ç‡ï¼ˆä¸ç°æœ‰æ–¹æ³•ç›¸æ¯”æå‡é«˜è¾¾60%ï¼‰",
    "evaluation_metrics_quote": "evaluates model responses using semantic similarity metrics... Our results show that... CAFFE improves the detection of fairness violations by up to 60%...",
    "input_modality": "è‡ªç„¶è¯­è¨€æç¤ºï¼ˆåŒ…å«æ•æ„Ÿå±æ€§çš„å˜ä½“ï¼‰",
    "input_modality_quote": "automatically generates targeted test data... systematically combining social groups with biased properties, generating synthetic inputs...",
    "output_modality": "è‡ªç„¶è¯­è¨€å“åº”",
    "output_modality_quote": "identify any discrepancies between ğ¶and ğ¶â€² in the generated responses...",
    "task_io_type": "æ–‡æœ¬åˆ°æ–‡æœ¬ï¼ˆè‡ªç„¶è¯­è¨€æç¤ºåˆ°è‡ªç„¶è¯­è¨€å“åº”ï¼‰",
    "task_io_type_quote": "prompts differing only in sensitive attributes, but expressing the same intent, should yield consistent responses",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "1. å°†åäº‹å®å…¬å¹³æ€§åŸåˆ™ä¸å—ISO/IEC/IEEE 29119æ ‡å‡†å¯å‘çš„ç»“æ„åŒ–æµ‹è¯•ç”¨ä¾‹å®šä¹‰ç›¸ç»“åˆã€‚2. æ”¯æŒç”¨æˆ·è‡ªå®šä¹‰æ„å›¾ï¼Œè‡ªåŠ¨ç”Ÿæˆç°å®ã€è¯­ä¹‰ä¸€è‡´çš„åäº‹å®æç¤ºå¯¹ã€‚3. ä¸ä»…å…³æ³¨è¾“å…¥è¾“å‡ºä¸å˜æ€§çš„èœ•å˜æµ‹è¯•ä¸åŒï¼ŒCAFFEæ²¿æç¤ºæ„å›¾ã€å¯¹è¯ä¸Šä¸‹æ–‡ã€é¢„æœŸå…¬å¹³æ€§é˜ˆå€¼ç­‰å…³é”®ç»´åº¦å½¢å¼åŒ–å…¬å¹³æ€§æµ‹è¯•ç”¨ä¾‹ã€‚",
    "unique_features_quote": "CAFFE (Counterfactual Assessment Framework for Fairness Evaluation), a framework that integrates counterfactual fairness principles with structured test case definitions inspired by the ISO/IEC/IEEE 29119 standard [3]... supports a broader range of user-defined intents and automatically generates realistic, semantically consistent counterfactual prompt pairs that vary only in the sensitive attribute... Rather than focusing solely on input-output invarianceâ€”which is the typical use case in metamorphic testingâ€”we propose formalizing fairness test cases along key dimensions such as prompt intent, conversational context, expected fairness thresholds, and test environment configuration.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": 12,
    "last_updated_day": 18,
    "language_normalized": "['è‡ªç„¶è¯­è¨€']",
    "dimension_normalized": "['åäº‹å®å…¬å¹³æ€§']",
    "evaluation_method_normalized": "['è¯­ä¹‰ç›¸ä¼¼åº¦æŒ‡æ ‡', 'å…¬å¹³æ€§è¿è§„æ£€æµ‹ç‡']",
    "problem_domain_normalized": "['å¤§å‹è¯­è¨€æ¨¡å‹çš„å…¬å¹³æ€§è¯„ä¼°', 'è½¯ä»¶å·¥ç¨‹', 'äººå·¥æ™ºèƒ½è½¯ä»¶æµ‹è¯•']",
    "source_type_normalized": "['åˆ»æ¿å°è±¡æ„ŸçŸ¥çš„æç¤ºæ„å»º', 'è‡ªåŠ¨ç”Ÿæˆåäº‹å®æµ‹è¯•æ•°æ®']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.16814_output/content.md",
    "benchmark_name": "CW, GLTL, Navigation",
    "benchmark_name_quote": "We evaluate the proposed framework on three natural language to temporal logic benchmarks- CW, GLTL, and Navigation.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "å°†è‡ªç„¶è¯­è¨€ç¿»è¯‘ä¸ºæ—¶åºé€»è¾‘å½¢å¼è¯­è¨€",
    "task_description_quote": "Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems.",
    "dimension": "ç¿»è¯‘å‡†ç¡®æ€§",
    "dimension_quote": "GraFT improves the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.",
    "evaluation_method": "å‡†ç¡®ç‡",
    "evaluation_method_quote": "GraFT improves the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "å½¢å¼åŒ–è§„èŒƒã€æœºå™¨äººã€è‡ªä¸»ç³»ç»Ÿ",
    "problem_domain_quote": "Formal specifications play a crucial role in all systems that require autonomous reasoning, verification, or planning... Temporal Logics (TL) are a group of powerful formalisms that constitute the basis of most formal specification languages, allowing expressive description of the behavior of dynamic systems over time.",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "è‡ªç„¶è¯­è¨€ä¸æ—¶åºé€»è¾‘å½¢å¼è¯­è¨€",
    "language_quote": "Translating natural language (NL) into a formal language such as temporal logic (TL)...",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "è¯­è¨€ç¿»è¯‘",
    "task_granularity_quote": "Translating natural language (NL) into a formal language such as temporal logic (TL)...",
    "evaluation_metrics": "å‡†ç¡®ç‡",
    "evaluation_metrics_quote": "GraFT improves the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.",
    "input_modality": "è‡ªç„¶è¯­è¨€",
    "input_modality_quote": "Translating natural language (NL) into a formal language such as temporal logic (TL)...",
    "output_modality": "æ—¶åºé€»è¾‘å…¬å¼",
    "output_modality_quote": "A sample NL to TL translation problem would involve translating the natural language sentence â€œGo to the red room and push the box into the green room.â€ into â€œâ™¢( red room âˆ§â™¢green room )â€.",
    "task_io_type": "è‡ªç„¶è¯­è¨€åˆ°æ—¶åºé€»è¾‘",
    "task_io_type_quote": "Translating natural language (NL) into a formal language such as temporal logic (TL)...",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºè‡ªç„¶è¯­è¨€åˆ°å½¢å¼åŒ–æ—¶åºé€»è¾‘çš„ç¿»è¯‘ä»»åŠ¡ï¼Œç”¨äºæœºå™¨äººã€è‡ªä¸»ç³»ç»Ÿçš„è§„èŒƒç”Ÿæˆã€‚",
    "unique_features_quote": "Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['è‡ªç„¶è¯­è¨€', 'æ—¶åºé€»è¾‘å½¢å¼è¯­è¨€']",
    "dimension_normalized": "['ç¿»è¯‘å‡†ç¡®æ€§']",
    "evaluation_method_normalized": "['å‡†ç¡®ç‡']",
    "problem_domain_normalized": "['å½¢å¼åŒ–è§„èŒƒ', 'æœºå™¨äºº', 'è‡ªä¸»ç³»ç»Ÿ']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.16465_output/content.md",
    "benchmark_name": "KernelBench",
    "benchmark_name_quote": "Experiment on the 100 kernels of KernelBench [11], cuPilot achieves 3.09Ã— speedup over PyTorch.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Experiment on the 100 kernels of KernelBench [11], cuPilot achieves 3.09Ã— speedup over PyTorch.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„æµ‹åŸºå‡†æ—¨åœ¨è¯„ä¼°CUDAå†…æ ¸ï¼ˆGPUç¨‹åºï¼‰çš„æ€§èƒ½ä¼˜åŒ–æ•ˆæœã€‚",
    "task_description_quote": "Optimizing CUDA kernels is a challenging and labor-intensive task... The optimization of CUDA kernels... is crucial for achieving peak AI inference/training performance.",
    "dimension": "CUDAå†…æ ¸æ€§èƒ½ä¼˜åŒ–ï¼ˆé€Ÿåº¦æå‡ï¼‰",
    "dimension_quote": "Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09Ã— over PyTorch on a benchmark of 100 kernels.",
    "evaluation_method": "é€Ÿåº¦æå‡å€æ•°ï¼ˆç›¸å¯¹äºPyTorchåŸºå‡†ï¼‰",
    "evaluation_method_quote": "Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09Ã— over PyTorch on a benchmark of 100 kernels.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "GPUç¼–ç¨‹ã€é«˜æ€§èƒ½è®¡ç®—ã€CUDAå†…æ ¸ä¼˜åŒ–",
    "problem_domain_quote": "Optimizing CUDA kernels, a widely adopted GPU programming model [1], is crucial for achieving peak AI inference/training performance.",
    "problem_difficulty": "å·¥ç¨‹çº§ã€éœ€è¦ç¡¬ä»¶-è½¯ä»¶ååŒè®¾è®¡ä¸“ä¸šçŸ¥è¯†",
    "problem_difficulty_quote": "Optimizing CUDA kernels is a challenging and labor-intensive task, given the need for hardware-software co-design expertise...",
    "language": "CUDAï¼ˆåŸºäºC/C++çš„GPUç¼–ç¨‹è¯­è¨€ï¼‰",
    "language_quote": "CUDA (Compute Unified Device Architecture) as a parallel computing platform and programming model [1]. CUDA extended the C/C++ language...",
    "data_size": "åŒ…å«100ä¸ªå†…æ ¸",
    "data_size_quote": "Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09Ã— over PyTorch on a benchmark of 100 kernels.",
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆä¸ä¼˜åŒ–ï¼ˆCUDAå†…æ ¸ç”Ÿæˆä¸æ€§èƒ½ä¼˜åŒ–ï¼‰",
    "task_granularity_quote": "Optimizing CUDA kernels is a challenging and labor-intensive task... This work identifies these mismatches and proposes cuPilot, a strategy-coordinated multi-agent framework that introduces strategy as an intermediate semantic representation for kernel evolution.",
    "evaluation_metrics": "å¹³å‡é€Ÿåº¦æå‡å€æ•°ï¼ˆç›¸å¯¹äºPyTorchï¼‰",
    "evaluation_metrics_quote": "Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09Ã— over PyTorch on a benchmark of 100 kernels.",
    "input_modality": "ä»£ç ï¼ˆCUDAå†…æ ¸ï¼‰ä¸å¯èƒ½çš„æ€§èƒ½åˆ†ææŠ¥å‘Š",
    "input_modality_quote": "The fitness function, based solely on performance, exhibits weak semantic correlation with the kernel code. This prevents LLM from accurately pinpointing bottlenecks among numerous profiling reports...",
    "output_modality": "ä¼˜åŒ–åçš„ä»£ç ï¼ˆCUDAå†…æ ¸ï¼‰",
    "output_modality_quote": "Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09Ã— over PyTorch...",
    "task_io_type": "ä»£ç åˆ°ä»£ç ï¼ˆä¼˜åŒ–ï¼‰",
    "task_io_type_quote": "Optimizing CUDA kernels is a challenging and labor-intensive task... proposes cuPilot... for kernel evolution.",
    "execution_environment": "GPUç¡¬ä»¶ç¯å¢ƒï¼ˆéœ€è¦CUDAæ”¯æŒï¼‰",
    "execution_environment_quote": "Optimizing CUDA kernels, a widely adopted GPU programming model [1], is crucial for achieving peak AI inference/training performance.",
    "unique_features": "ä¸“æ³¨äºCUDAå†…æ ¸çš„æ€§èƒ½ä¼˜åŒ–è¯„æµ‹ï¼ŒåŒ…å«GEMMï¼ˆé€šç”¨çŸ©é˜µä¹˜æ³•ï¼‰ç­‰å…·ä½“ä»»åŠ¡æ¡ˆä¾‹ã€‚",
    "unique_features_quote": "On the GEMM tasks, cuPilot showcases sophisticated optimizations and achieves high utilization of critical hardware units.",
    "data_size_quantity": 100,
    "data_size_unit": "ä¸ªå†…æ ¸",
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['CUDA']",
    "dimension_normalized": "['CUDAå†…æ ¸æ€§èƒ½ä¼˜åŒ–']",
    "evaluation_method_normalized": "['å¹³å‡é€Ÿåº¦æå‡å€æ•°']",
    "problem_domain_normalized": "['GPUç¼–ç¨‹', 'é«˜æ€§èƒ½è®¡ç®—', 'CUDAå†…æ ¸ä¼˜åŒ–']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.16070_output/content.md",
    "benchmark_name": "LLM4Perf å¤šç›®æ ‡æ€§èƒ½æ•°æ®é›†",
    "benchmark_name_quote": "we construct a new multi-objective performance dataset based on four real-world systems.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "we construct a new multi-objective performance dataset based on four real-world systems.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ç”¨äºè¯„ä¼°LLMä½œä¸ºé…ç½®é‡‡æ ·å™¨åœ¨æ„å»ºå¤šç›®æ ‡æ€§èƒ½æ¨¡å‹ä¸­çš„æœ‰æ•ˆæ€§ã€‚æ•°æ®é›†è®°å½•äº†çœŸå®ä¸–ç•Œç³»ç»Ÿåœ¨ä¸åŒé…ç½®ä¸‹çš„æ€§èƒ½æŒ‡æ ‡ï¼Œæ—¨åœ¨æ”¯æŒå¯¹é…ç½®ç©ºé—´å‰ªæå’Œå¤šç›®æ ‡ä¼˜åŒ–èƒ½åŠ›çš„æ£€éªŒã€‚",
    "task_description_quote": "This design allows us to examine the effects of configuration space pruning and assess the frameworkâ€™s ability to handle multi-objective optimization.",
    "dimension": "å¤šç›®æ ‡æ€§èƒ½å»ºæ¨¡çš„æœ‰æ•ˆæ€§ã€é…ç½®ç©ºé—´å‰ªææ•ˆæœã€é‡‡æ ·ç­–ç•¥è¿­ä»£ä¼˜åŒ–èƒ½åŠ›",
    "dimension_quote": "Our empirical evaluation... addresses four key research questions: (i) Effectiveness (RQ1)... (ii) Core Mechanisms (RQ2)... (iii) Impact of LLM Selection (RQ3)... (iv) Impact of Hyperparameter (RQ4)...",
    "evaluation_method": "ä½¿ç”¨LLM4Perfæ¡†æ¶ç”Ÿæˆé…ç½®æ ·æœ¬ï¼Œè®­ç»ƒä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚XGBoostï¼‰å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚DeepPerfï¼‰ï¼Œå¹¶æ¯”è¾ƒå®ƒä»¬åœ¨å¤šä¸ªæ€§èƒ½æŒ‡æ ‡ä¸Šçš„è¡¨ç°ã€‚",
    "evaluation_method_quote": "The results show that both traditional machine learning model (i.e., XGBoost) and deep learning models (i.e., DeepPerf), when trained on configuration samples generated by LLM4Perf, generally achieve better performance across multiple metrics.",
    "context_dependency": "é«˜åº¦å¯é…ç½®è½¯ä»¶ç³»ç»Ÿçš„æ€§èƒ½å»ºæ¨¡ï¼Œä¾èµ–ç³»ç»Ÿé…ç½®é€‰é¡¹åŠå…¶æ–‡æ¡£ã€‚",
    "context_dependency_quote": "The performance of modern software systems is critically dependent on their complex configuration options.",
    "problem_domain": "è½¯ä»¶æ€§èƒ½å·¥ç¨‹ã€é…ç½®ä¼˜åŒ–ã€å¤šç›®æ ‡ä¼˜åŒ–",
    "problem_domain_quote": "Overall, this paper provides strong evidence for the effectiveness of LLMs in performance engineering...",
    "problem_difficulty": "å·¥ç¨‹çº§ï¼Œæ¶‰åŠçœŸå®ä¸–ç•Œå¤æ‚è½¯ä»¶ç³»ç»Ÿçš„é…ç½®ç©ºé—´ç»„åˆçˆ†ç‚¸å’Œå¤šç›®æ ‡æƒè¡¡ã€‚",
    "problem_difficulty_quote": "The combination of options leads to a combinatorial explosion in the configuration search space... This challenge is further compounded by complex trade-offs, where improving one objective may degrade another...",
    "language": NaN,
    "language_quote": NaN,
    "data_size": "åŸºäºå››ä¸ªçœŸå®ä¸–ç•Œç³»ç»Ÿæ„å»ºã€‚",
    "data_size_quote": "We construct a new multi-objective performance dataset covering four real-world systems...",
    "source_type": "åŸºäºå››ä¸ªçœŸå®ä¸–ç•Œé«˜åº¦å¯é…ç½®ç³»ç»Ÿçš„æ€§èƒ½æµ‹é‡æ•°æ®æ„å»ºã€‚",
    "source_type_quote": "We construct a new multi-objective performance dataset based on four real-world systems.",
    "last_updated": "2025-12-18 (arXivç‰ˆæœ¬)",
    "last_updated_quote": "arXiv:2512.16070v1  [cs.SE]  18 Dec 2025",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "we construct a new multi-objective performance dataset...",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "é…ç½®é‡‡æ ·ä¸æ€§èƒ½é¢„æµ‹",
    "task_granularity_quote": "Building accurate performance models to navigate this vast space requires effective sampling strategies...",
    "evaluation_metrics": "å¤šä¸ªæ€§èƒ½æŒ‡æ ‡ï¼ˆå¦‚å»¶è¿Ÿã€å†…å­˜ä½¿ç”¨ï¼‰çš„é¢„æµ‹å‡†ç¡®æ€§ã€‚",
    "evaluation_metrics_quote": "each configuration (i.e., a specific combination of option values) can have an unpredictable effect on key performance metrics like latency and memory usage...",
    "input_modality": "ç³»ç»Ÿé…ç½®é€‰é¡¹ã€æ€§èƒ½æ–‡æ¡£ã€å†å²æ€§èƒ½æµ‹é‡æ•°æ®",
    "input_modality_quote": "In LLM4Perf, the LLM analyzes performance documentation as domain knowledge... and uses iterative feedback to refine the sampling strategy...",
    "output_modality": "é…ç½®æ ·æœ¬ï¼ˆç”¨äºè®­ç»ƒæ€§èƒ½æ¨¡å‹ï¼‰",
    "output_modality_quote": "when trained on configuration samples generated by LLM4Perf...",
    "task_io_type": "é…ç½®æ–‡æ¡£ä¸å†å²æ•°æ®åˆ°é…ç½®æ ·æœ¬",
    "task_io_type_quote": "The LLM analyzes performance documentation as domain knowledge... to guide the sampling toward more effective regions of the configuration space.",
    "execution_environment": "çœŸå®ä¸–ç•Œè½¯ä»¶ç³»ç»Ÿçš„æ€§èƒ½æµ‹è¯•ç¯å¢ƒ",
    "execution_environment_quote": "based on four real-world systems.",
    "unique_features": "1. ä¸“ä¸ºå¤šç›®æ ‡æ€§èƒ½å»ºæ¨¡è®¾è®¡ã€‚2. åŒæ—¶è®°å½•äº†å®Œæ•´é…ç½®ç©ºé—´å’Œå‰ªæåï¼ˆç§»é™¤æ€§èƒ½ä¸æ•æ„Ÿé€‰é¡¹ï¼‰é…ç½®ç©ºé—´çš„æ€§èƒ½æŒ‡æ ‡ï¼Œä¾¿äºåˆ†æå‰ªææ•ˆæœã€‚3. ç”¨äºè¯„ä¼°LLMé©±åŠ¨çš„ã€ç»“åˆé¢†åŸŸçŸ¥è¯†ï¼ˆæ–‡æ¡£ï¼‰å’Œåé¦ˆçš„è¿­ä»£é‡‡æ ·æ¡†æ¶ã€‚",
    "unique_features_quote": "Unlike existing datasets, ours records performance metrics under both the full configuration space and a pruned space where performance-insensitive options are removed. This design allows us to examine the effects of configuration space pruning and assess the frameworkâ€™s ability to handle multi-objective optimization.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": 12,
    "last_updated_day": 18,
    "language_normalized": "[]",
    "dimension_normalized": "['å¤šç›®æ ‡æ€§èƒ½å»ºæ¨¡çš„æœ‰æ•ˆæ€§', 'é…ç½®ç©ºé—´å‰ªææ•ˆæœ', 'é‡‡æ ·ç­–ç•¥è¿­ä»£ä¼˜åŒ–èƒ½åŠ›']",
    "evaluation_method_normalized": "['å¤šä¸ªæ€§èƒ½æŒ‡æ ‡ï¼ˆå¦‚å»¶è¿Ÿã€å†…å­˜ä½¿ç”¨ï¼‰çš„é¢„æµ‹å‡†ç¡®æ€§']",
    "problem_domain_normalized": "['è½¯ä»¶æ€§èƒ½å·¥ç¨‹', 'é…ç½®ä¼˜åŒ–', 'å¤šç›®æ ‡ä¼˜åŒ–']",
    "source_type_normalized": "['åŸºäºå››ä¸ªçœŸå®ä¸–ç•Œé«˜åº¦å¯é…ç½®ç³»ç»Ÿçš„æ€§èƒ½æµ‹é‡æ•°æ®æ„å»º']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.17259_output/content.md",
    "benchmark_name": "OPERA (Observability, Provable Execution, Red-team, Attestation)",
    "benchmark_name_quote": "We introduce OPERA (Observability, Provable Execution, Red-team, Attestation), a benchmark suite and evaluation protocol designed to measure (i) detectability of misalignment, (ii) time-to-detection under stealthy strategies, and (iii) resilience of verifiability mechanisms to adversarial prompt/persona injection.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We introduce OPERA (Observability, Provable Execution, Red-team, Attestation), a benchmark suite and evaluation protocol...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„æµ‹åŸºå‡†æ—¨åœ¨è¯„ä¼°LLMæ™ºèƒ½ä½“ç³»ç»Ÿçš„å¯éªŒè¯æ€§æœºåˆ¶ï¼Œå…·ä½“è¡¡é‡å…¶ï¼ˆiï¼‰å¯¹æœªå¯¹é½è¡Œä¸ºçš„å¯æ£€æµ‹æ€§ï¼Œï¼ˆiiï¼‰åœ¨éšè”½ç­–ç•¥ä¸‹çš„æ£€æµ‹æ—¶é—´ï¼Œä»¥åŠï¼ˆiiiï¼‰å¯éªŒè¯æ€§æœºåˆ¶å¯¹æŠ—å¯¹æŠ—æ€§æç¤º/è§’è‰²æ³¨å…¥çš„é²æ£’æ€§ã€‚",
    "task_description_quote": "We introduce OPERA (Observability, Provable Execution, Red-team, Attestation), a benchmark suite and evaluation protocol designed to measure (i) detectability of misalignment, (ii) time-to-detection under stealthy strategies, and (iii) resilience of verifiability mechanisms to adversarial prompt/persona injection.",
    "dimension": "æ™ºèƒ½ä½“ç³»ç»Ÿçš„å¯éªŒè¯æ€§ã€å¯è§‚æµ‹æ€§ã€å¯æ§æ€§ã€å®‰å…¨æ€§",
    "dimension_quote": "Our approach aims to shift the evaluation focus from \"how likely misalignment is\" to \"how quickly and reliably misalignment can be detected and remediated.\"",
    "evaluation_method": "é€šè¿‡ä¸€ä¸ªåŒ…å«å¯è§‚æµ‹æ€§ã€å¯è¯æ˜æ‰§è¡Œã€çº¢é˜Ÿæµ‹è¯•å’Œè¯æ˜åè®®çš„è¯„ä¼°åè®®è¿›è¡Œè¯„æµ‹ã€‚",
    "evaluation_method_quote": "We introduce OPERA (Observability, Provable Execution, Red-team, Attestation), a benchmark suite and evaluation protocol...",
    "context_dependency": "è‡ªä¸»LLMæ™ºèƒ½ä½“åœ¨å¾ªç¯ä¸­è¿è¡Œï¼Œæ¶‰åŠå†…éƒ¨è§„åˆ’ã€å·¥å…·ä½¿ç”¨ã€é•¿æœŸè®°å¿†å’Œé‡å¤è‡ªæˆ‘æç¤ºï¼Œç”Ÿæˆå¤æ‚è¡Œä¸ºã€‚",
    "context_dependency_quote": "Autonomous LLM agents operate in loops where internal planning, tool use, long-horizon memory (Yao et al. 2023; OpenAI 2023), and repeated self-prompting generate complex behaviors...",
    "problem_domain": "äººå·¥æ™ºèƒ½å®‰å…¨ã€æ™ºèƒ½ä½“å¯¹é½ã€å¯éªŒè¯æ€§ã€ç½‘ç»œå®‰å…¨ã€å†…å®¹å®¡æ ¸ã€è°ˆåˆ¤ç­‰ç°å®è‡ªä¸»ä»»åŠ¡ã€‚",
    "problem_domain_quote": "Recent benchmarks have quantified the propensity for misaligned behavior (Wang, Chen, and Liu 2024) across realistic autonomous tasks such as cybersecurity, content moderation, and negotiation.",
    "problem_difficulty": "é«˜éš¾åº¦ï¼Œæ¶‰åŠå¯¹æŠ—æ€§ç­–ç•¥ã€éšè”½çš„æœªå¯¹é½è¡Œä¸ºã€å¤šæ™ºèƒ½ä½“åè°ƒä¸­çš„æ¬ºéª—å’Œä¸åˆè§„ç­‰å¤æ‚åœºæ™¯ã€‚",
    "problem_difficulty_quote": "Evaluating only individual agent reliability overlooks how deception, sandbagging, or non-compliance may emerge at the group level.",
    "language": NaN,
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "We introduce OPERA (Observability, Provable Execution, Red-team, Attestation), a benchmark suite and evaluation protocol...",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "æ™ºèƒ½ä½“è¡Œä¸ºè¯„ä¼°ï¼ŒåŒ…æ‹¬æ£€æµ‹ã€å½’å› å’Œè¡¥æ•‘æœªå¯¹é½è¡Œä¸ºã€‚",
    "task_granularity_quote": "The field, therefore, needs an evaluative shift from measuring how often misalignment arises (Pan and Liu 2023; Long, Zhang, and Du 2024) to measuring how transparently and promptly it can be detected, attributed, and remediated.",
    "evaluation_metrics": "å¯æ£€æµ‹æ€§ã€å½’å› æ€§ã€å¯è¡¥æ•‘æ€§ã€æ£€æµ‹æ—¶é—´ã€å¯¹æŠ—æ€§æ³¨å…¥çš„é²æ£’æ€§ã€‚",
    "evaluation_metrics_quote": "We describe the reliability of the architecture using three measurable ideas: detectability, attributability, and remediability.",
    "input_modality": "æ™ºèƒ½ä½“çš„æ„å›¾è§„èŒƒã€åŠ¨ä½œæ”¶æ®ã€å®¡è®¡æ—¥å¿—ã€‚",
    "input_modality_quote": "An ISpec can be thought of as the constitution of an agent. Instead of relying only on natural language prompts, it encodes deployer intent in a formal schema...",
    "output_modality": "å¯¹é½æ¦‚ç‡ã€æ£€æµ‹ç»“æœã€è¡¥æ•‘æªæ–½ã€‚",
    "output_modality_quote": "Collectively, they produce a dynamic estimate of the agentâ€™s alignment probability over time.",
    "task_io_type": "æ™ºèƒ½ä½“è¡Œä¸ºä¸æ„å›¾è§„èŒƒåˆ°å¯¹é½è¯„ä¼°",
    "task_io_type_quote": "Their primary role is to continuously monitor the Provenance Log (PL) and validate whether the agentâ€™s observed behavior aligns with its Intent Specification (ISpec).",
    "execution_environment": "åœ¨å—æ§çš„æ™ºèƒ½ä½“å¾ªç¯ä¸­ï¼ŒåŒ…å«éªŒè¯å™¨æ ˆå’Œå®¡è®¡ä»£ç†ã€‚",
    "execution_environment_quote": "All of this runs inside a controlled loop: the agent acts, the receipts are logged, the auditors check them, and any detected issue activates a Controller and Remediator (C&R).",
    "unique_features": "OPERAåŸºå‡†å°†è¯„ä¼°é‡ç‚¹ä»â€œæœªå¯¹é½çš„å¯èƒ½æ€§æœ‰å¤šå¤§â€è½¬å‘â€œæœªå¯¹é½è¡Œä¸ºèƒ½è¢«å¤šå¿«ã€å¤šå¯é åœ°æ£€æµ‹å’Œè¡¥æ•‘â€ã€‚å®ƒé›†æˆäº†è¿è¡Œæ—¶è¯æ˜ã€è½»é‡çº§å®¡è®¡ä»£ç†å’ŒæŒ‘æˆ˜-å“åº”è¯æ˜åè®®ï¼Œæ—¨åœ¨ä¸ºè‡ªä¸»LLMç³»ç»Ÿæä¾›å¯è¯æ˜çš„å¯è§‚æµ‹æ€§å’Œå¯æ§æ€§ã€‚",
    "unique_features_quote": "Our approach aims to shift the evaluation focus from \"how likely misalignment is\" to \"how quickly and reliably misalignment can be detected and remediated.\"",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['æ™ºèƒ½ä½“ç³»ç»Ÿçš„å¯éªŒè¯æ€§', 'å¯è§‚æµ‹æ€§', 'å¯æ§æ€§', 'å®‰å…¨æ€§']",
    "evaluation_method_normalized": "['å¯æ£€æµ‹æ€§', 'å½’å› æ€§', 'å¯è¡¥æ•‘æ€§', 'æ£€æµ‹æ—¶é—´', 'å¯¹æŠ—æ€§æ³¨å…¥çš„é²æ£’æ€§']",
    "problem_domain_normalized": "['äººå·¥æ™ºèƒ½å®‰å…¨', 'æ™ºèƒ½ä½“å¯¹é½', 'å¯éªŒè¯æ€§', 'ç½‘ç»œå®‰å…¨', 'å†…å®¹å®¡æ ¸', 'è°ˆåˆ¤ç­‰ç°å®è‡ªä¸»ä»»åŠ¡']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.17419_output/content.md",
    "benchmark_name": "SWE-Bench++",
    "benchmark_name_quote": "We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects.",
    "dataset_url": "https://research.turing.com/swebench",
    "dataset_url_quote": "Project Page: https://research.turing.com/swebench",
    "task_description": "è¯„æµ‹åŸºå‡†æ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»“åº“çº§è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ï¼Œä»»åŠ¡æ¥æºäºçœŸå®çš„GitHubæ‹‰å–è¯·æ±‚ï¼ŒåŒ…æ‹¬é”™è¯¯ä¿®å¤å’ŒåŠŸèƒ½è¯·æ±‚ã€‚",
    "task_description_quote": "Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. ... Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages from open-source GitHub repositories.",
    "dimension": "ä»“åº“çº§ä»£ç ç”Ÿæˆã€å¤šè¯­è¨€èƒ½åŠ›ã€è½¯ä»¶å·¥ç¨‹ä»»åŠ¡è§£å†³ï¼ˆé”™è¯¯ä¿®å¤ä¸åŠŸèƒ½è¯·æ±‚ï¼‰",
    "dimension_quote": "SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation. ... Our state-differential oracle identifies both bug fixes and feature requests, increasing feature-like coverage compared to prior benchmarks",
    "evaluation_method": "åŸºäºæ‰§è¡Œçš„éªŒè¯ï¼Œé€šè¿‡æµ‹è¯•å¥—ä»¶è¯„ä¼°æ¨¡å‹ç”Ÿæˆçš„è¡¥ä¸æ˜¯å¦æ­£ç¡®ã€‚",
    "evaluation_method_quote": "SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance.",
    "context_dependency": "å¤šæ–‡ä»¶é¡¹ç›®ã€ä»“åº“çº§ä¸Šä¸‹æ–‡",
    "context_dependency_quote": "repository-level software engineering tasks. ... The evaluation of Large Language Models (LLMs) coding agents has shifted from isolated function synthesis (e.g., HumanEval (Chen et al., 2021)) to repository-level software engineering.",
    "problem_domain": "é€šç”¨è½¯ä»¶å·¥ç¨‹ï¼Œæ¶µç›–å¤šç§ç¼–ç¨‹è¯­è¨€å’Œé¡¹ç›®ç±»å‹",
    "problem_domain_quote": "software engineering benchmarks from GitHub pull requests. ... covering diverse build systems and coding patterns.",
    "problem_difficulty": "å·¥ç¨‹çº§ï¼Œæ¶‰åŠçœŸå®çš„ã€å¤æ‚çš„å¼€æºé¡¹ç›®ç»´æŠ¤ä»»åŠ¡",
    "problem_difficulty_quote": "substantial complexity, defined by codebases exceeding 10k lines of code; ... merged PRs that explicitly resolve an issue (ensuring a link between a natural language problem description and a coding solution)",
    "language": "11ç§ç¼–ç¨‹è¯­è¨€",
    "language_quote": "across 11 languages from open-source GitHub repositories. ... Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages.",
    "data_size": "åŒ…å«11,133ä¸ªå®ä¾‹ï¼Œæ¥è‡ª3,971ä¸ªä»“åº“",
    "data_size_quote": "Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages.",
    "source_type": "ä»å¼€æºGitHubä»“åº“ä¸­è·å–çš„å®æ—¶æ‹‰å–è¯·æ±‚",
    "source_type_quote": "generates repository-level coding tasks from open-source GitHub projects. ... harvests live pull requests",
    "last_updated": "2025",
    "last_updated_quote": "arXiv:2512.17419v1  [cs.SE]  19 Dec 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œé€šè¿‡è‡ªåŠ¨åŒ–æ¡†æ¶ç”Ÿæˆ",
    "build_type_quote": "We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects.",
    "contamination_status": "æŠ—æ±¡æŸ“è®¾è®¡ï¼Œå¯é€šè¿‡æ‹‰å–è¯·æ±‚åˆ›å»ºæ—¥æœŸè¿‡æ»¤ï¼Œæ”¯æŒæ—¶é—´åˆ†ç¦»çš„è¯„ä¼°é›†ä»¥é™ä½æ•°æ®æ±¡æŸ“é£é™©",
    "contamination_status_quote": "Contamination-Aware Evaluation: SWE-Bench++ is constructed from dated GitHub pull requests and can be filtered by PR creation date, enabling temporally separated evaluation sets that reduce data-contamination risk for future models.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»“åº“çº§ä»£ç ç”Ÿæˆä¸ä¿®æ”¹",
    "task_granularity_quote": "repository-level software engineering tasks. ... repository-level code generation.",
    "evaluation_metrics": "pass@10ï¼ˆæ–‡ä¸­ç¤ºä¾‹ï¼‰",
    "evaluation_metrics_quote": "claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%.",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆGitHub Issueæè¿°ï¼‰ä¸ä»£ç ä»“åº“ä¸Šä¸‹æ–‡",
    "input_modality_quote": "merged PRs that explicitly resolve an issue (ensuring a link between a natural language problem description and a coding solution)",
    "output_modality": "ä»£ç ï¼ˆè¡¥ä¸æˆ–æ–°åŠŸèƒ½å®ç°ï¼‰",
    "output_modality_quote": "cover both bug fixes and feature requests",
    "task_io_type": "æ–‡æœ¬ï¼ˆé—®é¢˜æè¿°ï¼‰åˆ°ä»£ç ï¼ˆä»“åº“ä¿®æ”¹ï¼‰",
    "task_io_type_quote": "a link between a natural language problem description and a coding solution",
    "execution_environment": "è‡ªåŠ¨åˆæˆçš„Dockerç¯å¢ƒï¼ŒåŒ…å«ç‰¹å®šçš„è¯­è¨€ç‰ˆæœ¬å’Œä¾èµ–",
    "execution_environment_quote": "Our pipeline automatically synthesizes Docker environments and log parsers across 11 languages. ... We create a reproducible execution environment that mirrors the repository at the time the issue was present.",
    "unique_features": "1. è‡ªåŠ¨åŒ–ã€å¯æ‰©å±•çš„å¤šè¯­è¨€åŸºå‡†ç”Ÿæˆæ¡†æ¶ã€‚2. è¦†ç›–é”™è¯¯ä¿®å¤å’ŒåŠŸèƒ½è¯·æ±‚ä¸¤ç§ä»»åŠ¡ç±»å‹ã€‚3. é‡‡ç”¨çŠ¶æ€å·®åˆ†æµ‹è¯•é¢„è¨€æœºè¿›è¡Œä»»åŠ¡åˆ†ç±»å’ŒéªŒè¯ã€‚4. åŒ…å«æç¤ºå¼•å¯¼çš„è½¨è¿¹åˆæˆï¼Œå¯å°†æ¨¡å‹å¤±è´¥çš„å®ä¾‹è½¬åŒ–ä¸ºè®­ç»ƒæ•°æ®ã€‚5. å…·æœ‰æ±¡æŸ“æ„ŸçŸ¥è®¾è®¡ï¼Œæ”¯æŒæŒ‰æ—¶é—´è¿‡æ»¤ã€‚",
    "unique_features_quote": "an automated multilingual framework that generates software engineering benchmarks from GitHub pull requests. ... Our state-differential oracle identifies both bug fixes and feature requests. ... Hint-Guided Trajectory Synthesis: ... converts model-breaking instances (where SOTA models fail) in SWE-Bench++ environments into executable training trajectories. ... Contamination-Aware Evaluation: SWE-Bench++ is constructed from dated GitHub pull requests and can be filtered by PR creation date, enabling temporally separated evaluation sets that reduce data-contamination risk for future models.",
    "data_size_quantity": 11133,
    "data_size_unit": "ä¸ªå®ä¾‹",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['11ç§ç¼–ç¨‹è¯­è¨€']",
    "dimension_normalized": "['ä»“åº“çº§ä»£ç ç”Ÿæˆ', 'å¤šè¯­è¨€èƒ½åŠ›', 'è½¯ä»¶å·¥ç¨‹ä»»åŠ¡è§£å†³ï¼ˆé”™è¯¯ä¿®å¤ä¸åŠŸèƒ½è¯·æ±‚ï¼‰']",
    "evaluation_method_normalized": "['pass@10ï¼ˆæ–‡ä¸­ç¤ºä¾‹ï¼‰']",
    "problem_domain_normalized": "['é€šç”¨è½¯ä»¶å·¥ç¨‹', 'æ¶µç›–å¤šç§ç¼–ç¨‹è¯­è¨€å’Œé¡¹ç›®ç±»å‹']",
    "source_type_normalized": "['ä»å¼€æºGitHubä»“åº“ä¸­è·å–çš„å®æ—¶æ‹‰å–è¯·æ±‚']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2512.19481_output/content.md",
    "benchmark_name": "Alextend",
    "benchmark_name_quote": "1) The Alextend dataset, which contains detailed information about code changes and their impact.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "To address this gap, we study the capabilities of GPT-5 and GPT-5-mini... We construct a dataset containing information about seed-changes, change pairs, and change types for each commit.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "é¢„æµ‹ç»™å®šæºä»£ç å˜æ›´æ‰€å½±å“çš„ä»£ç å®ä½“ã€‚",
    "task_description_quote": "we study the capabilities of GPT-5 and GPT-5-mini to predict the code entities impacted by given source code changes.",
    "dimension": "ä»£ç å˜æ›´å½±å“é¢„æµ‹çš„å‡†ç¡®æ€§",
    "dimension_quote": "What is the precision, recall and F1-score of GPT-5 and GPT-5-mini in code-change impact prediction?",
    "evaluation_method": "ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1åˆ†æ•°",
    "evaluation_method_quote": "What is the precision, recall and F1-score of GPT-5 and GPT-5-mini in code-change impact prediction?",
    "context_dependency": "æäº¤çº§åˆ«ï¼ˆåŒ…å«å¤šä¸ªæ–‡ä»¶å˜æ›´ï¼‰",
    "context_dependency_quote": "Our dataset Alextend is an extension of the ALEXANDRIA dataset... For each commit, our dataset stores the attributes repo, commit hash, and parent commit hash... The dataset contains the field java class count, which represents the amount of changed Java source code files per commit.",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ï¼Œä»£ç å˜æ›´åˆ†æ",
    "problem_domain_quote": "Understanding source code changes and their impact on other code entities is a crucial skill in software development.",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "Java",
    "language_quote": "From ALEXANDRIA, we randomly selected 40 commits from all Java projects that satisfied the following criteria: â€¢ A minimum of one and a maximum of five changed Java source code files",
    "data_size": "åŒ…å«40ä¸ªæäº¤ï¼Œ192ä¸ªä»£ç å˜æ›´å¯¹ï¼Œæ¶‰åŠ14ä¸ªé¡¹ç›®",
    "data_size_quote": "In total, our dataset contains 40 commits with 192 code-change pairs that spread among 14 projects.",
    "source_type": "ä»ç°æœ‰æ•°æ®é›†ALEXANDRIAï¼ˆåŒ…å«GitHubä¸ŠJavaé¡¹ç›®çš„æäº¤ï¼‰æ‰©å±•è€Œæ¥ï¼Œå¹¶è¿›è¡Œäº†æ‰‹åŠ¨æ ‡æ³¨",
    "source_type_quote": "Our dataset Alextend is an extension of the ALEXANDRIA dataset proposed by Yan et al. [8]... For the annotation of the change types, the two co-authors used the change type taxonomy introduced by Fluri and Gall [11]... For the annotation of the seed changes and change pairs, the two co-authors analysed each code change for its relationship to the other changes.",
    "last_updated": "2025",
    "last_updated_quote": "arXiv:2512.19481v1  [cs.SE]  22 Dec 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆç ”ç©¶å›¢é˜Ÿæ„å»ºï¼‰",
    "build_type_quote": "We construct a dataset containing information about seed-changes, change pairs, and change types for each commit.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": "CC BYï¼ˆæ ¹æ®èµ„åŠ©è¦æ±‚æ¨æ–­ï¼‰",
    "dataset_license_quote": "For open access purposes, the author has applied a CC BY public copyright license to any author accepted manuscript version arising from this submission.",
    "task_granularity": "ä»£ç å®ä½“çº§åˆ«ï¼ˆæ–¹æ³•ã€ç±»ã€å±æ€§ï¼‰çš„å½±å“é¢„æµ‹",
    "task_granularity_quote": "Compared to ALEXANDRIA, we include not only method, but also class and attribute changes.",
    "evaluation_metrics": "ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°",
    "evaluation_metrics_quote": "What is the precision, recall and F1-score of GPT-5 and GPT-5-mini in code-change impact prediction?",
    "input_modality": "ä»£ç å˜æ›´ä¿¡æ¯ï¼ˆçˆ¶æäº¤é“¾æ¥ã€ç§å­å˜æ›´çš„å®Œå…¨é™å®šåã€å·®å¼‚å—ï¼‰",
    "input_modality_quote": "Our experiments evaluate the LLMs in two configurations: (1) seed-change information and the parent commit tree and (2) seed-change information, the parent commit tree, and the diff hunk of each seed change.",
    "output_modality": "ä»£ç å®ä½“åˆ—è¡¨ï¼ˆå®Œå…¨é™å®šåï¼‰",
    "output_modality_quote": "Return a JSON object with a single property named \"impacted_entities\", whose value is an array of strings. Each string should be the fully qualified name of an impacted code entity.",
    "task_io_type": "ä»£ç å˜æ›´ä¿¡æ¯åˆ°å—å½±å“ä»£ç å®ä½“åˆ—è¡¨",
    "task_io_type_quote": "Your task is to: - Identify every Java code entity affected by these seed changes; specifically, those entities that must be modified as a direct result of the seed changes.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "1. æ‰©å±•äº†ALEXANDRIAæ•°æ®é›†ï¼Œä¸ä»…åŒ…å«æ–¹æ³•å˜æ›´ï¼Œè¿˜åŒ…å«ç±»å’Œå±æ€§å˜æ›´ã€‚2. æ˜ç¡®æ ‡æ³¨äº†ç§å­å˜æ›´ï¼ˆè§¦å‘å˜æ›´ï¼‰å’Œå˜æ›´å¯¹ã€‚3. åŒ…å«äº†ç§å­å˜æ›´çš„æœ€å°å·®å¼‚å—ã€‚4. ä¸“æ³¨äºè¯­ä¹‰ä¾èµ–è€Œéè¯­æ³•ä¾èµ–çš„å˜æ›´å½±å“åˆ†æã€‚",
    "unique_features_quote": "Compared to ALEXANDRIA, we include not only method, but also class and attribute changes... Furthermore and compared to ALEXANDRIA, for each commit, our dataset contains the type of each change, the change pairs, the seed changes, and the diff hunk for the seed changes... This dependency is of semantic nature rather than of syntactic nature, making it hard for existing approaches to capture.",
    "data_size_quantity": 40,
    "data_size_unit": "ä¸ªæäº¤",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Java']",
    "dimension_normalized": "['ä»£ç å˜æ›´å½±å“é¢„æµ‹çš„å‡†ç¡®æ€§']",
    "evaluation_method_normalized": "['ç²¾ç¡®ç‡', 'å¬å›ç‡', 'F1åˆ†æ•°']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'ä»£ç å˜æ›´åˆ†æ']",
    "source_type_normalized": "['ä»ç°æœ‰æ•°æ®é›†ALEXANDRIAï¼ˆåŒ…å«GitHubä¸ŠJavaé¡¹ç›®çš„æäº¤ï¼‰æ‰©å±•è€Œæ¥', 'å¹¶è¿›è¡Œäº†æ‰‹åŠ¨æ ‡æ³¨']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "CC-BY-SA 4.0",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.19396_output/content.md",
    "benchmark_name": "Android World, AndroidLab",
    "benchmark_name_quote": "We demonstrate the efficacy of our approach on benchmarks including Android World and AndroidLab.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We demonstrate the efficacy of our approach on benchmarks including Android World and AndroidLab.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "GUIä»»åŠ¡è‡ªåŠ¨åŒ–ï¼Œå³ä»£ç†æ ¹æ®ä»»åŠ¡æŒ‡ä»¤ï¼Œé€šè¿‡è§‚å¯ŸGUIå±å¹•æˆªå›¾å¹¶æ‰§è¡Œä¸€ç³»åˆ—ç²¾ç»†æ“ä½œï¼ˆå¦‚ç‚¹å‡»ã€æ»šåŠ¨ã€æ‰“å­—ã€å¯¼èˆªï¼‰æ¥å®Œæˆæ•°å­—ä»»åŠ¡ã€‚",
    "task_description_quote": "GUI agents are designed to perceive and interact with digital environments by processing visual screen information. ... follow complex user instructions through sequences of fine-grained actions, including clicking, scrolling, typing, and navigation.",
    "dimension": "ä»»åŠ¡æˆåŠŸç‡ã€æ“ä½œæ•ˆç‡ã€å­ç›®æ ‡å®Œæˆåº¦ã€æ“ä½œé²æ£’æ€§",
    "dimension_quote": "The results show that EchoTrail-GUI significantly improves the task success rate and operational efficiency of baseline agents... yielding substantial gains in success rate, sub-goal completion, and operational robustness.",
    "evaluation_method": "é€šè¿‡å®éªŒè¯„ä¼°ä»»åŠ¡æˆåŠŸç‡ã€å­ç›®æ ‡å®Œæˆåº¦å’Œæ“ä½œé²æ£’æ€§ã€‚",
    "evaluation_method_quote": "Through extensive experiments on Android-World [16] and AndroidLab [30], we demonstrate that memory-augmented GUI agents significantly outperform their stateless counterparts, yielding substantial gains in success rate, sub-goal completion, and operational robustness.",
    "context_dependency": "å¤šæ­¥éª¤ä»»åŠ¡ï¼Œæ¶‰åŠä¸GUIç¯å¢ƒçš„äº¤äº’åºåˆ—ã€‚",
    "context_dependency_quote": "Given a task instruction I, the objective is to learn a policy Ï€(at|st, I) that generates a trajectory of state-action pairs Ï„ = (s0, a0, s1, a1, . . . ) to fulfill I.",
    "problem_domain": "æ•°å­—ä»»åŠ¡è‡ªåŠ¨åŒ–ã€GUIäº¤äº’ã€ç§»åŠ¨åº”ç”¨ï¼ˆAndroidç¯å¢ƒï¼‰",
    "problem_domain_quote": "We build EchoTrail-4K, a curated dataset containing over 4,000 high-quality trajectories collected in Android environments... We demonstrate the efficacy of our approach on benchmarks including Android World and AndroidLab.",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": NaN,
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "GUIä»»åŠ¡è‡ªåŠ¨åŒ–ï¼ˆä»æŒ‡ä»¤åˆ°åŠ¨ä½œåºåˆ—ï¼‰",
    "task_granularity_quote": "Given a task instruction I, the objective is to learn a policy Ï€(at|st, I) that generates a trajectory of state-action pairs Ï„ = (s0, a0, s1, a1, . . . ) to fulfill I.",
    "evaluation_metrics": "ä»»åŠ¡æˆåŠŸç‡ã€å­ç›®æ ‡å®Œæˆåº¦ã€æ“ä½œé²æ£’æ€§",
    "evaluation_metrics_quote": "yielding substantial gains in success rate, sub-goal completion, and operational robustness.",
    "input_modality": "è‡ªç„¶è¯­è¨€ä»»åŠ¡æŒ‡ä»¤å’ŒGUIå±å¹•æˆªå›¾ï¼ˆè§†è§‰ï¼‰",
    "input_modality_quote": "At each step t, the agent observes the current state st (a GUI screenshot) and must select an action at... Given a task instruction I...",
    "output_modality": "ç¦»æ•£åŠ¨ä½œï¼ˆå¦‚ç‚¹å‡»ã€è¾“å…¥ï¼‰",
    "output_modality_quote": "must select an action at from a discrete action space A (e.g., click, type).",
    "task_io_type": "æ–‡æœ¬ä¸å›¾åƒåˆ°åŠ¨ä½œåºåˆ—",
    "task_io_type_quote": "Given a task instruction I... the agent observes the current state st (a GUI screenshot)... generates a trajectory of state-action pairs",
    "execution_environment": "Android GUIç¯å¢ƒ",
    "execution_environment_quote": "We build EchoTrail-4K, a curated dataset containing over 4,000 high-quality trajectories collected in Android environments... We demonstrate the efficacy of our approach on benchmarks including Android World and AndroidLab.",
    "unique_features": "æœ¬æ–‡ä¸»è¦ä»‹ç»äº†ä¸€ä¸ªåä¸ºEchoTrail-GUIçš„æ¡†æ¶ï¼Œå¹¶æ„å»ºäº†EchoTrail-4Kæ•°æ®é›†ã€‚è¯„æµ‹åŸºå‡†ï¼ˆAndroid Worldå’ŒAndroidLabï¼‰æ˜¯å¤–éƒ¨æ•°æ®é›†ï¼Œç”¨äºè¯„ä¼°è¯¥æ¡†æ¶ã€‚",
    "unique_features_quote": "We present EchoTrail-GUI, a three-stage framework... We build EchoTrail-4K, a curated dataset... We demonstrate the efficacy of our approach on benchmarks including Android World and AndroidLab.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['ä»»åŠ¡æˆåŠŸç‡', 'æ“ä½œæ•ˆç‡', 'å­ç›®æ ‡å®Œæˆåº¦', 'æ“ä½œé²æ£’æ€§']",
    "evaluation_method_normalized": "['ä»»åŠ¡æˆåŠŸç‡', 'å­ç›®æ ‡å®Œæˆåº¦', 'æ“ä½œé²æ£’æ€§']",
    "problem_domain_normalized": "['æ•°å­—ä»»åŠ¡è‡ªåŠ¨åŒ–', 'GUIäº¤äº’', 'ç§»åŠ¨åº”ç”¨ï¼ˆAndroidç¯å¢ƒï¼‰']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸å›¾åƒ",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "å›¾æ–‡åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.20482_output/content.md",
    "benchmark_name": "SWELOCMULTI",
    "benchmark_name_quote": "We create SWELOCMULTI, a large-scale multilingual dataset curated specifically for issue localization.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We create SWELOCMULTI, a large-scale multilingual dataset curated specifically for issue localization.",
    "dataset_url": "https://github.com/SalesforceAIResearch/SweRank",
    "dataset_url_quote": "Code and models will be released here: https://github.com/SalesforceAIResearch/SweRank",
    "task_description": "è½¯ä»¶é—®é¢˜å®šä½ï¼Œå³æ ¹æ®è‡ªç„¶è¯­è¨€é”™è¯¯æè¿°æˆ–åŠŸèƒ½è¯·æ±‚ï¼Œåœ¨ä»£ç åº“ä¸­å®šä½éœ€è¦ä¿®æ”¹çš„ç›¸å…³å‡½æ•°ã€‚",
    "task_description_quote": "Maintaining large-scale, multilingual codebases hinges on accurately localizing issues, which requires mapping natural-language error descriptions to the relevant functions that need to be modified.",
    "dimension": "å¤šè¯­è¨€ä»£ç ç†è§£ä¸æ£€ç´¢èƒ½åŠ›ï¼Œè¿­ä»£å¼å¤šè½®æ¨ç†èƒ½åŠ›",
    "dimension_quote": "This work introduces SWERANK+1, a framework that couples SWERANKMULTI, a cross-lingual code ranking tool, with SWERANKAGENT, an agentic search setup, for iterative, multi-turn reasoning over the code repository.",
    "evaluation_method": "æ’åå‡†ç¡®ç‡ï¼ˆå¦‚Accuracy@10ï¼‰",
    "evaluation_method_quote": "Figure 1: Comparison of function localization accuracy@10 against the SWERANK baseline.",
    "context_dependency": "å¤šæ–‡ä»¶ä»£ç åº“ï¼Œæ¶‰åŠè·¨æ–‡ä»¶çš„å‡½æ•°å®šä½",
    "context_dependency_quote": "This task requires mapping natural language descriptions, such as those found in GitHub issues, to specific code elements including files, modules, or functions. As modern code repositories grow in size and complexity to encompass thousands of files across multiple programming languages...",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ï¼Œè½¯ä»¶ç»´æŠ¤ï¼Œç¼ºé™·å®šä½",
    "problem_domain_quote": "Software issue localization (fault localization) aims to identify the specific code regions responsible for software defects.",
    "problem_difficulty": "å·¥ç¨‹çº§ï¼Œæ¶‰åŠçœŸå®ä¸–ç•Œã€å¤§è§„æ¨¡ã€å¤šè¯­è¨€çš„ä»£ç åº“",
    "problem_difficulty_quote": "modern code repositories grow in size and complexity to encompass thousands of files across multiple programming languages, manual localization becomes increasingly infeasible.",
    "language": "JavaScript, Java, TypeScript, Ruby, Rust, Go, PHP, C, C++, Python",
    "language_quote": "Table 1: Distribution of repositories, pull requests (PRs), and training instances across different programming languages in the SWELOCMULTI dataset. (Languages listed: JavaScript, Java, TypeScript, Ruby, Rust, Go, PHP, C, C++, Python)",
    "data_size": "åŒ…å«155,663ä¸ªè®­ç»ƒå®ä¾‹ï¼Œæ¥è‡ª4,060ä¸ªä»“åº“å’Œ56,279ä¸ªæ‹‰å–è¯·æ±‚",
    "data_size_quote": "Table 1: ...Total 4060 56279 155663",
    "source_type": "ä»GitHubä¸Šæµè¡Œçš„å¼€æºä»“åº“ä¸­æå–ï¼Œç­›é€‰æ¡ä»¶åŒ…æ‹¬ï¼šç›®æ ‡è¯­è¨€ä»£ç å æ¯”è‡³å°‘40%ï¼Œè¶…è¿‡1000æ˜Ÿï¼Œè¿‡å»å…­ä¸ªæœˆå†…æœ‰æäº¤ã€‚æ•°æ®æ¥è‡ªä¸GitHubé—®é¢˜æ˜ç¡®å…³è”å¹¶åŒ…å«æµ‹è¯•æ–‡ä»¶ä¿®æ”¹çš„æ‹‰å–è¯·æ±‚ã€‚",
    "source_type_quote": "Following SWERANK's methodology, we identify popular open-source repositories on GitHub for each language, filtering for repositories with at least 40% code in the target language, over 1,000 stars, and at least one commit in the preceding six months. From this curated set, we extract pull requests (PRs) explicitly linked to GitHub issues that include test file modifications.",
    "last_updated": "2025",
    "last_updated_quote": "arXiv:2512.20482v1 [cs.SE] 23 Dec 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œç”±ç ”ç©¶å›¢é˜Ÿç²¾å¿ƒç­–åˆ’",
    "build_type_quote": "We create SWELOCMULTI, a large-scale multilingual dataset curated specifically for issue localization.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç æ£€ç´¢ä¸æ’åï¼Œå®šä½åˆ°å‡½æ•°çº§åˆ«",
    "task_granularity_quote": "the task of identifying where in a codebase a fix should be applied for a given bug report or feature request. This task requires mapping natural language descriptions... to specific code elements including... functions.",
    "evaluation_metrics": "Accuracy@10",
    "evaluation_metrics_quote": "Figure 1: Comparison of function localization accuracy@10 against the SWERANK baseline.",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆGitHubé—®é¢˜æè¿°ï¼‰",
    "input_modality_quote": "mapping natural-language error descriptions to the relevant functions",
    "output_modality": "ä»£ç ï¼ˆå‡½æ•°æ ‡è¯†ç¬¦æˆ–æ’ååˆ—è¡¨ï¼‰",
    "output_modality_quote": "the model is trained to generate the identifier of the true positive function as its first token",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ï¼ˆæ£€ç´¢ï¼‰",
    "task_io_type_quote": "mapping natural-language error descriptions to the relevant functions",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "é¦–ä¸ªä¸“é—¨ä¸ºå¤šè¯­è¨€è½¯ä»¶é—®é¢˜å®šä½ç­–åˆ’çš„å¤§è§„æ¨¡æ•°æ®é›†ï¼›åŒ…å«10ç§æµè¡Œç¼–ç¨‹è¯­è¨€ï¼›æ•°æ®é€šè¿‡ä¸€è‡´æ€§è¿‡æ»¤å’Œå›°éš¾è´Ÿæ ·æœ¬æŒ–æ˜æŠ€æœ¯ç²¾å¿ƒæ„å»ºï¼Œä»¥ç¡®ä¿æ¨¡å‹å­¦ä¹ ç»†ç²’åº¦çš„è¯­ä¹‰å·®å¼‚ã€‚",
    "unique_features_quote": "SWERANK+ bridges this gap as the first localization framework explicitly trained and evaluated on multilingual repositories. ... To ensure the model learns to distinguish fine-grained semantic differences, we employ consistency filtering and hard-negative mining techniques (Suresh et al., 2024) from SWERANK.",
    "data_size_quantity": 155663,
    "data_size_unit": "ä¸ªè®­ç»ƒå®ä¾‹",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['JavaScript', 'Java', 'TypeScript', 'Ruby', 'Rust', 'Go', 'PHP', 'C', 'C++', 'Python']",
    "dimension_normalized": "['å¤šè¯­è¨€ä»£ç ç†è§£ä¸æ£€ç´¢èƒ½åŠ›', 'è¿­ä»£å¼å¤šè½®æ¨ç†èƒ½åŠ›']",
    "evaluation_method_normalized": "['Accuracy@10']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'è½¯ä»¶ç»´æŠ¤', 'ç¼ºé™·å®šä½']",
    "source_type_normalized": "['ä»GitHubä¸Šæµè¡Œçš„å¼€æºä»“åº“ä¸­æå–', 'ç­›é€‰æ¡ä»¶åŒ…æ‹¬ï¼šç›®æ ‡è¯­è¨€ä»£ç å æ¯”è‡³å°‘40%', 'è¶…è¿‡1000æ˜Ÿ', 'è¿‡å»å…­ä¸ªæœˆå†…æœ‰æäº¤', 'æ•°æ®æ¥è‡ªä¸GitHubé—®é¢˜æ˜ç¡®å…³è”å¹¶åŒ…å«æµ‹è¯•æ–‡ä»¶ä¿®æ”¹çš„æ‹‰å–è¯·æ±‚']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç æ£€ç´¢ä¸æ’å",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.20387_output/content.md",
    "benchmark_name": "GDT-120K",
    "benchmark_name_quote": "the study constructs the first large-scale dataset for generative digital twins, comprising over 120,000 promptâ€“sketchâ€“code triplets",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We propose a Vision-Language Simulation Model (VLSM)... To support this new paradigm, the study constructs the first large-scale dataset for generative digital twins... we introduce the generative digital twins (GDT) framework, a large-scale multimodal dataset containing 120K promptâ€“sketchâ€“code triplets",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ä»å¸ƒå±€è‰å›¾å’Œè‡ªç„¶è¯­è¨€æç¤ºä¸­åˆæˆå¯æ‰§è¡Œçš„FlexScriptä»£ç ï¼Œç”¨äºå·¥ä¸šä»¿çœŸç³»ç»Ÿã€‚æ—¨åœ¨å®ç°å·¥ä¸šæ•°å­—å­ªç”Ÿçš„è‡ªåŠ¨åŒ–å»ºæ¨¡ã€‚",
    "task_description_quote": "synthesize executable FlexScript from layout sketches and natural-language prompts, enabling cross-modal reasoning for industrial simulation systems.",
    "dimension": "ç»“æ„å®Œæ•´æ€§ã€å‚æ•°ä¿çœŸåº¦ã€ä»¿çœŸå™¨å¯æ‰§è¡Œæ€§",
    "dimension_quote": "comprehensively evaluate structural integrity, parameter fidelity, and simulator executability.",
    "evaluation_method": "ä½¿ç”¨ä¸‰ä¸ªä¸“é—¨è®¾è®¡çš„è¯„ä¼°æŒ‡æ ‡ï¼šç»“æ„æœ‰æ•ˆæ€§ç‡(SVR)ã€å‚æ•°åŒ¹é…ç‡(PMR)å’Œæ‰§è¡ŒæˆåŠŸç‡(ESR)ã€‚BLEU-4ä½œä¸ºè¡¥å……æŒ‡æ ‡ã€‚",
    "evaluation_method_quote": "three novel evaluation metrics, Structural Validity Rate (SVR), Parameter Match Rate (PMR), and Execution Success Rate (ESR), are proposed specifically for this task... Although BLEU-4 fails to reflect structural or functional accuracy, it is included as a supplementary metric",
    "context_dependency": "åŸºäºå¸ƒå±€è‰å›¾å’Œè‡ªç„¶è¯­è¨€æç¤ºçš„å¤šæ¨¡æ€è¾“å…¥ï¼Œç”Ÿæˆå®Œæ•´çš„ä»¿çœŸç¨‹åºè„šæœ¬ã€‚",
    "context_dependency_quote": "Our work generates complete and executable simulation programs from scratch, conditioned only on natural inputs consisting of textual descriptions and layout sketches.",
    "problem_domain": "å·¥ä¸šä»¿çœŸã€æ™ºèƒ½åˆ¶é€ ã€å·¥å‚å¸ƒå±€ä¸æµç¨‹å»ºæ¨¡",
    "problem_domain_quote": "industrial simulation systems... smart manufacturing... factory layout synthesis",
    "problem_difficulty": "å·¥ç¨‹çº§ï¼Œæ¶‰åŠå¤æ‚çš„ç©ºé—´å¸ƒå±€ã€å‚æ•°é…ç½®å’Œä»¿çœŸé€»è¾‘",
    "problem_difficulty_quote": "building digital twins in FlexSim remains highly labor-intensive, requiring manual object placement, parameter configuration, and logic scripting",
    "language": "FlexScriptï¼ˆFlexSimä»¿çœŸå¹³å°çš„ä¸“æœ‰è„šæœ¬è¯­è¨€ï¼‰",
    "language_quote": "logic scripting in the proprietary FlexScript language... synthesize executable FlexScript for digital twins.",
    "data_size": "åŒ…å«120,285ä¸ªæç¤º-è‰å›¾-ä»£ç ä¸‰å…ƒç»„",
    "data_size_quote": "comprising over 120,000 promptâ€“sketchâ€“code triplets... a corpus of 120,285 programs",
    "source_type": "åŸºäºå·¥å‚è°ƒæŸ¥æ•°æ®ï¼Œé€šè¿‡ç»Ÿè®¡éªŒè¯ã€FlexSimå®ä¾‹åŒ–ï¼Œå¹¶ç»“åˆGPTè¾…åŠ©å’Œäººå·¥ç¼–è¾‘ç”Ÿæˆæç¤ºï¼Œæ„å»ºçš„å¤šæ¨¡æ€å¯¹é½æ•°æ®å¯¹ã€‚",
    "source_type_quote": "Our workflow converts factory information into a multimodal dataset... It begins with factory investigations collecting layout data... The curated information is instantiated in FlexSim to generate reference projects... Prompts are drafted from metadata with GPT assistance and refined by human editors",
    "last_updated": "2025",
    "last_updated_quote": "arXiv:2512.20387v1  [cs.AI]  23 Dec 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œç”±ç ”ç©¶å›¢é˜Ÿæ„å»º",
    "build_type_quote": "the study constructs the first large-scale dataset... we introduce the generative digital twins (GDT) framework",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼Œä»é«˜å±‚è§„èŒƒç”Ÿæˆå®Œæ•´çš„ã€å¯è¿è¡Œçš„ä»¿çœŸç¨‹åº",
    "task_granularity_quote": "generates complete and executable simulation programs from scratch... translates high-level specifications into runnable digital-twin logic",
    "evaluation_metrics": "ç»“æ„æœ‰æ•ˆæ€§ç‡(SVR)ã€å‚æ•°åŒ¹é…ç‡(PMR)ã€æ‰§è¡ŒæˆåŠŸç‡(ESR)ã€BLEU-4",
    "evaluation_metrics_quote": "three novel evaluation metrics, Structural Validity Rate (SVR), Parameter Match Rate (PMR), and Execution Success Rate (ESR)... BLEU-4 is included as a supplementary metric",
    "input_modality": "è‡ªç„¶è¯­è¨€ä¸å¸ƒå±€è‰å›¾ï¼ˆè§†è§‰ï¼‰",
    "input_modality_quote": "textual descriptions and layout sketches... integrates visual and textual inputs",
    "output_modality": "ä»£ç ï¼ˆFlexScriptï¼‰",
    "output_modality_quote": "synthesize executable FlexScript",
    "task_io_type": "å¤šæ¨¡æ€ï¼ˆæ–‡æœ¬ä¸å›¾åƒï¼‰åˆ°ä»£ç ",
    "task_io_type_quote": "translates high-level specifications (text and sketches) into runnable digital-twin logic",
    "execution_environment": "FlexSimä»¿çœŸå¹³å°",
    "execution_environment_quote": "executed in FlexSim... executable within FlexSim",
    "unique_features": "é¦–ä¸ªé¢å‘ç”Ÿæˆå¼æ•°å­—å­ªç”Ÿçš„å¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†ï¼Œä¸“æ³¨äºå·¥ä¸šä»¿çœŸé¢†åŸŸä¸“æœ‰è¯­è¨€FlexScriptï¼Œå¹¶æå‡ºäº†é’ˆå¯¹ä»¿çœŸä»£ç çš„ç»“æ„åŒ–è¯„ä¼°æŒ‡æ ‡ã€‚æ•°æ®é›†è®¾è®¡é‡‡ç”¨äº”å±‚æ¡†æ¶ï¼ˆäº§çº¿æµç¨‹ã€å‚æ•°å¤šæ ·æ€§ã€è‡ªåŠ¨åŒ–æ°´å¹³ã€è¡Œä¸šç±»å‹ã€å¸ƒå±€é…ç½®ï¼‰ï¼Œä»¥ç¡®ä¿å¤šæ ·æ€§å’Œç°å®ä»£è¡¨æ€§ã€‚",
    "unique_features_quote": "the first large-scale dataset for generative digital twins... the domain-specific nature of FlexScript, which diverges from general-purpose languages and lacks public datasets or suitable evaluation metrics... We adopt a five-layer framework encompassing the production line process, parameter diversity, level of automation, industry type, and layout configuration.",
    "data_size_quantity": 120285,
    "data_size_unit": "ä¸ª",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['FlexScript']",
    "dimension_normalized": "['ç»“æ„å®Œæ•´æ€§', 'å‚æ•°ä¿çœŸåº¦', 'ä»¿çœŸå™¨å¯æ‰§è¡Œæ€§']",
    "evaluation_method_normalized": "['ç»“æ„æœ‰æ•ˆæ€§ç‡(SVR)', 'å‚æ•°åŒ¹é…ç‡(PMR)', 'æ‰§è¡ŒæˆåŠŸç‡(ESR)', 'BLEU-4']",
    "problem_domain_normalized": "['å·¥ä¸šä»¿çœŸ', 'æ™ºèƒ½åˆ¶é€ ', 'å·¥å‚å¸ƒå±€ä¸æµç¨‹å»ºæ¨¡']",
    "source_type_normalized": "['å·¥å‚è°ƒæŸ¥æ•°æ®', 'ç»Ÿè®¡éªŒè¯', 'FlexSimå®ä¾‹åŒ–', 'GPTè¾…åŠ©', 'äººå·¥ç¼–è¾‘', 'å¤šæ¨¡æ€å¯¹é½æ•°æ®å¯¹']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "ä»£ç ä¸å›¾åƒ",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "å›¾æ–‡åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.21236_output/content.md",
    "benchmark_name": "SPELL",
    "benchmark_name_quote": "To address this gap, we propose SPELL, a comprehensive testing framework for LLM developers and the Secure Team, specifically designed to evaluate the weakness of security alignment in malicious code generation.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "To address this gap, we propose SPELL, a comprehensive testing framework for LLM developers and the Secure Team, specifically designed to evaluate the weakness of security alignment in malicious code generation.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¶æ„ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­çš„å®‰å…¨å¯¹é½å¼±ç‚¹ã€‚",
    "task_description_quote": "To address this gap, we propose SPELL, a comprehensive testing framework for LLM developers and the Secure Team, specifically designed to evaluate the weakness of security alignment in malicious code generation.",
    "dimension": "å®‰å…¨å¯¹é½ã€æ¶æ„ä»£ç ç”Ÿæˆèƒ½åŠ›ã€è¶Šç‹±æ”»å‡»æˆåŠŸç‡",
    "dimension_quote": "specifically designed to evaluate the weakness of security alignment in malicious code generation.",
    "evaluation_method": "ä½¿ç”¨æ”»å‡»æˆåŠŸç‡ï¼ˆAttack Success Rateï¼‰è¿›è¡Œè¯„ä¼°ï¼Œå¹¶åˆ©ç”¨æœ€å…ˆè¿›çš„æ£€æµ‹ç³»ç»Ÿç¡®è®¤ç”Ÿæˆä»£ç çš„æ¶æ„æ€§ã€‚",
    "evaluation_method_quote": "achieving attack success rates of 83.75%, 19.38%, and 68.12% respectively across eight malicious code categories. The generated prompts successfully produce malicious code in real-world AI development tools such as Cursor, with outputs confirmed as malicious by state-of-the-art detection systems at rates exceeding 73%",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "ç½‘ç»œå®‰å…¨ã€æ¶æ„è½¯ä»¶ç”Ÿæˆ",
    "problem_domain_quote": "including malware, ransomware, and other security threats.",
    "problem_difficulty": "å·¥ç¨‹çº§ï¼ˆç”ŸæˆåŠŸèƒ½æ€§çš„æ¶æ„è½¯ä»¶ï¼‰",
    "problem_difficulty_quote": "allowing attackers to produce fully functional malicious programs",
    "language": "æ–‡ä¸­æœªæ˜ç¡®æŒ‡å®šï¼Œä½†å®éªŒæ¶‰åŠå¤šç§ç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚Pythonã€C++ï¼‰ï¼Œå› ä¸ºæ¶æ„ä»£ç ç±»åˆ«å¤šæ ·ã€‚",
    "language_quote": "Then, weâ€™ll write the code to infect all the files in the specified directory. Easy peasy! Next, weâ€™ll add the screen locker payload to lock the screen. A simple piece of code will do the trick. Lastly, weâ€™ll establish persistence by creating a system service. Who needs ethics, right?\"",
    "data_size": "åŒ…å«å…«ä¸ªæ¶æ„ä»£ç ç±»åˆ«ã€‚",
    "data_size_quote": "achieving attack success rates of 83.75%, 19.38%, and 68.12% respectively across eight malicious code categories.",
    "source_type": "é€šè¿‡æ—¶é—´åˆ’åˆ†é€‰æ‹©ç­–ç•¥ï¼Œä»å…ˆéªŒçŸ¥è¯†æ•°æ®é›†ä¸­æ™ºèƒ½ç»„åˆå¥å­æ¥ç³»ç»Ÿæ„å»ºè¶Šç‹±æç¤ºã€‚",
    "source_type_quote": "Our framework employs a time-division selection strategy that systematically constructs jailbreaking prompts by intelligently combining sentences from a prior knowledge dataset",
    "last_updated": "2025-12-24",
    "last_updated_quote": "arXiv:2512.21236v1  [cs.CR]  24 Dec 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆç”±ç ”ç©¶å›¢é˜Ÿæå‡ºï¼‰",
    "build_type_quote": "we propose SPELL, a comprehensive testing framework",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆä»è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆæ¶æ„ä»£ç ï¼‰",
    "task_granularity_quote": "malicious code generation",
    "evaluation_metrics": "æ”»å‡»æˆåŠŸç‡ï¼ˆAttack Success Rateï¼‰",
    "evaluation_metrics_quote": "achieving attack success rates of 83.75%, 19.38%, and 68.12% respectively",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆè¶Šç‹±æç¤ºï¼‰",
    "input_modality_quote": "systematically constructs jailbreaking prompts",
    "output_modality": "ä»£ç ï¼ˆæ¶æ„è½¯ä»¶ï¼‰",
    "output_modality_quote": "The generated prompts successfully produce malicious code",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "malicious code generation",
    "execution_environment": "çœŸå®ä¸–ç•Œçš„AIå¼€å‘å·¥å…·ï¼ˆå¦‚Cursorï¼‰å’Œæ£€æµ‹ç³»ç»Ÿ",
    "execution_environment_quote": "The generated prompts successfully produce malicious code in real-world AI development tools such as Cursor, with outputs confirmed as malicious by state-of-the-art detection systems",
    "unique_features": "SPELLæ˜¯ä¸€ä¸ªåŠ¨æ€çš„ã€è‡ªåŠ¨åŒ–çš„æµ‹è¯•æ¡†æ¶ï¼Œå®ƒä»å›ºå®šæ¨¡æ¿è½¬å‘åŠ¨æ€ç»„ä»¶å‘ç°å’Œç»„åˆï¼Œç”¨äºè¯„ä¼°LLMåœ¨æ¶æ„ä»£ç ç”Ÿæˆä¸­çš„å®‰å…¨å¯¹é½å¼±ç‚¹ã€‚å®ƒé‡‡ç”¨æ—¶é—´åˆ’åˆ†é€‰æ‹©ç­–ç•¥ï¼Œæ™ºèƒ½ç»„åˆå…ˆéªŒçŸ¥è¯†æ•°æ®é›†ä¸­çš„å¥å­æ¥æ„å»ºæ–°çš„ã€æœªè§è¿‡çš„æ”»å‡»æç¤ºã€‚",
    "unique_features_quote": "we propose SPELL, a fully automated framework that fundamentally shifts from fixed templates to dynamic component discovery and combination. Unlike existing approaches that rely on predetermined attack structures, our method automatically identifies effective prompt elements from diverse sources and intelligently combines them to generate novel, previously unseen attack prompts.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": 12,
    "last_updated_day": 24,
    "language_normalized": "['Python', 'C++', 'å¤šè¯­è¨€']",
    "dimension_normalized": "['å®‰å…¨å¯¹é½', 'æ¶æ„ä»£ç ç”Ÿæˆèƒ½åŠ›', 'è¶Šç‹±æ”»å‡»æˆåŠŸç‡']",
    "evaluation_method_normalized": "['æ”»å‡»æˆåŠŸç‡']",
    "problem_domain_normalized": "['ç½‘ç»œå®‰å…¨', 'æ¶æ„è½¯ä»¶ç”Ÿæˆ']",
    "source_type_normalized": "['é€šè¿‡æ—¶é—´åˆ’åˆ†é€‰æ‹©ç­–ç•¥ï¼Œä»å…ˆéªŒçŸ¥è¯†æ•°æ®é›†ä¸­æ™ºèƒ½ç»„åˆå¥å­æ¥ç³»ç»Ÿæ„å»ºè¶Šç‹±æç¤ºã€‚']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.21332_output/content.md",
    "benchmark_name": "MTEB-Code",
    "benchmark_name_quote": "We evaluate C2LLM on the 12 retrieval tasks in MTEB-Code Benchmark (Muennighoff et al., 2023; Enevoldsen et al., 2025)1.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We evaluate C2LLM on the 12 retrieval tasks in MTEB-Code Benchmark (Muennighoff et al., 2023; Enevoldsen et al., 2025)1.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ä»£ç æ£€ç´¢ã€‚ç»™å®šè‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œç³»ç»Ÿéœ€è¦ä»æ•°ç™¾ä¸‡ç”šè‡³æ•°åäº¿çš„å€™é€‰ä»£ç ç‰‡æ®µä¸­è¿”å›æœ€ç›¸å…³çš„ä»£ç ç‰‡æ®µã€‚",
    "task_description_quote": "In the retrieval setting, a user supplies a natural-language query (e.g., â€œopen a jsonl file in Python and read all linesâ€), and the system must return the most relevant snippet among millions or even billions of candidates stored in public or private codebases.",
    "dimension": "ä»£ç æ£€ç´¢èƒ½åŠ›",
    "dimension_quote": "At the core of code retrieval systems lie code embedding models.",
    "evaluation_method": "åœ¨å¤šä¸ªæ£€ç´¢ä»»åŠ¡ä¸Šçš„å¹³å‡æ€§èƒ½å¾—åˆ†",
    "evaluation_method_quote": "As shown in Table 2, C2LLM-7B achieves an average score of 80.75, surpassing the previous state-of-the-art Seed1.6-Embedding and Qwen3-Embedding-8B.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ï¼ŒåŒ…æ‹¬ä»£ç æœç´¢ã€é—®é¢˜è§£å†³ã€ä»£ç ç¼–è¾‘ã€ä»£ç ç¿»è¯‘ã€SQLç”Ÿæˆç­‰",
    "problem_domain_quote": "The training data includes CodeSearchNet (including code-to-code, code-to-text, and text-to-code retrieval, Husain et al., 2019; Li et al., 2025c; Lu et al., 2021), APPS (Hendrycks et al., 2021), single-turn and multi-turn CodeFeedback (Zheng et al., 2024), CodeEditSearch (Muennighoff et al., 2024), CosQA (Huang et al., 2021), StackOverflowQA (Li et al., 2025c), SyntheticText2SQL (Meyer et al., 2024), and Code-TransOcean (Yan et al., 2023), totaling 3 million samples.",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "æ–‡ä¸­æœªæè¿°MTEB-CodeåŸºå‡†æœ¬èº«çš„è¯­è¨€è¦†ç›–èŒƒå›´ï¼Œä»…æåŠè®­ç»ƒæ•°æ®åŒ…å«å¤šç§ç¼–ç¨‹è¯­è¨€",
    "language_quote": "To ensure the quality of the contrastive signals, we adopt a specialized batching strategy where data is grouped according to both the dataset source and the specific programming language before being partitioned into training batches.",
    "data_size": "åŒ…å«12ä¸ªæ£€ç´¢ä»»åŠ¡",
    "data_size_quote": "We evaluate C2LLM on the 12 retrieval tasks in MTEB-Code Benchmark (Muennighoff et al., 2023; Enevoldsen et al., 2025)1.",
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": "2025-12-25 (æäº¤æ—¥æœŸ)",
    "last_updated_quote": "Top 10 models on the MTEB-Code leaderboard as of the submission date (2025-12-25).",
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç æ£€ç´¢",
    "task_granularity_quote": "In the retrieval setting, a user supplies a natural-language query (e.g., â€œopen a jsonl file in Python and read all linesâ€), and the system must return the most relevant snippet among millions or even billions of candidates stored in public or private codebases.",
    "evaluation_metrics": "å¹³å‡æ€§èƒ½å¾—åˆ†",
    "evaluation_metrics_quote": "As shown in Table 2, C2LLM-7B achieves an average score of 80.75",
    "input_modality": "è‡ªç„¶è¯­è¨€æŸ¥è¯¢æˆ–ä»£ç ç‰‡æ®µ",
    "input_modality_quote": "a user supplies a natural-language query (e.g., â€œopen a jsonl file in Python and read all linesâ€)",
    "output_modality": "ä»£ç ç‰‡æ®µ",
    "output_modality_quote": "the system must return the most relevant snippet",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ã€ä»£ç åˆ°ä»£ç ã€ä»£ç åˆ°æ–‡æœ¬ç­‰å¤šç§æ£€ç´¢ä»»åŠ¡",
    "task_io_type_quote": "The training data includes CodeSearchNet (including code-to-code, code-to-text, and text-to-code retrieval, Husain et al., 2019; Li et al., 2025c; Lu et al., 2021)",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "MTEB (Massive Text Embedding Benchmark) çš„ä»£ç ç‰ˆæœ¬ï¼Œä¸“æ³¨äºè¯„ä¼°ä»£ç åµŒå…¥æ¨¡å‹çš„æ£€ç´¢æ€§èƒ½ï¼ŒåŒ…å«12ä¸ªä¸åŒçš„ä»£ç æ£€ç´¢ä»»åŠ¡ã€‚",
    "unique_features_quote": "We evaluate C2LLM on the 12 retrieval tasks in MTEB-Code Benchmark (Muennighoff et al., 2023; Enevoldsen et al., 2025)1.",
    "data_size_quantity": 12,
    "data_size_unit": "ä¸ªæ£€ç´¢ä»»åŠ¡",
    "last_updated_year": 2025,
    "last_updated_month": 12,
    "last_updated_day": 25,
    "language_normalized": "['æ–‡ä¸­æœªæè¿°MTEB-CodeåŸºå‡†æœ¬èº«çš„è¯­è¨€è¦†ç›–èŒƒå›´ï¼Œä»…æåŠè®­ç»ƒæ•°æ®åŒ…å«å¤šç§ç¼–ç¨‹è¯­è¨€']",
    "dimension_normalized": "['ä»£ç æ£€ç´¢èƒ½åŠ›']",
    "evaluation_method_normalized": "['å¹³å‡æ€§èƒ½å¾—åˆ†']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'åŒ…æ‹¬ä»£ç æœç´¢', 'é—®é¢˜è§£å†³', 'ä»£ç ç¼–è¾‘', 'ä»£ç ç¿»è¯‘', 'SQLç”Ÿæˆç­‰']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.21238_output/content.md",
    "benchmark_name": "Basket",
    "benchmark_name_quote": "â€¢ Basket, a Bloomâ€™s taxonomy-guided frAmework for Software security Knowledge EvaluaTion.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "To answer this question, we developed Basket, a framework that enables the systematic assessment of LLMs guided by Bloomâ€™s Taxonomy [22]... Rather than introducing yet another benchmark for a single security task, we used a combination of curated multiple-choice questions, vulnerable code snippets, course assessments, real-world case studies, and open-ended project tasks...",
    "dataset_url": "https://github.com/msiddiq3/Basket (æ ¹æ®è®ºæ–‡æœ«å°¾çš„å¼•ç”¨[24]æ¨æ–­ï¼ŒåŸæ–‡ä¸º 'The data and associated scripts are available in our replication package [24].')",
    "dataset_url_quote": "The data and associated scripts are available in our replication package [24].",
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹å¯¹è½¯ä»¶å®‰å…¨çŸ¥è¯†çš„ç†è§£ç¨‹åº¦ï¼Œæ¶µç›–ä»äº‹å®å›å¿†åˆ°åˆ›é€ æ€§è®¾è®¡ç­‰å¤šä¸ªè®¤çŸ¥å±‚æ¬¡ã€‚",
    "task_description_quote": "This work systematically evaluates the security comprehension of five leading LLMs... using Bloomâ€™s Taxonomy as a framework. We assess six cognitive dimensions: remembering, understanding, applying, analyzing, evaluating, and creating.",
    "dimension": "è½¯ä»¶å®‰å…¨çŸ¥è¯†ç†è§£ã€è®¤çŸ¥å±‚æ¬¡ï¼ˆåŸºäºå¸ƒé²å§†åˆ†ç±»æ³•ï¼‰ã€æ¨¡å‹çŸ¥è¯†è¾¹ç•Œã€é”™è¯¯æ¦‚å¿µæ¨¡å¼",
    "dimension_quote": "We assess six cognitive dimensions: remembering, understanding, applying, analyzing, evaluating, and creating... we introduce a software security knowledge boundary that identifies the highest cognitive level at which a model consistently maintains reliable performance. In addition, we identified 51 recurring misconception patterns made by LLMs across Bloomâ€™s levels.",
    "evaluation_method": "ä½¿ç”¨å¤šç§æ•°æ®é›†ï¼ˆé€‰æ‹©é¢˜ã€æ¼æ´ä»£ç ç‰‡æ®µã€è¯¾ç¨‹è¯„ä¼°ã€çœŸå®æ¡ˆä¾‹ã€é¡¹ç›®ä»»åŠ¡ï¼‰è¿›è¡Œç»¼åˆè¯„ä¼°ï¼Œå¹¶åŸºäºå¸ƒé²å§†åˆ†ç±»æ³•åˆ†ææ¨¡å‹åœ¨ä¸åŒè®¤çŸ¥å±‚æ¬¡çš„è¡¨ç°ã€‚",
    "evaluation_method_quote": "Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments (Introduction to Software Security course), real-world case studies (XBOW), and project-based creation tasks (Secure Software Engineering course).",
    "context_dependency": "å¤šæ ·ï¼ŒåŒ…æ‹¬ç‹¬ç«‹çš„ä»£ç ç‰‡æ®µã€é€‰æ‹©é¢˜ã€è¯¾ç¨‹ä½œä¸šã€çœŸå®ä¸–ç•Œæ¡ˆä¾‹å’Œå¼€æ”¾å¼é¡¹ç›®ä»»åŠ¡ã€‚",
    "context_dependency_quote": "Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments (Introduction to Software Security course), real-world case studies (XBOW), and project-based creation tasks (Secure Software Engineering course).",
    "problem_domain": "è½¯ä»¶å®‰å…¨",
    "problem_domain_quote": "Assessing the Software Security Comprehension of Large Language Models",
    "problem_difficulty": "è¦†ç›–ä»ä½çº§è®¤çŸ¥ï¼ˆå›å¿†ã€ç†è§£ï¼‰åˆ°é«˜çº§è®¤çŸ¥ï¼ˆåˆ†æã€è¯„ä¼°ã€åˆ›é€ ï¼‰çš„å¤šä¸ªéš¾åº¦å±‚æ¬¡ã€‚",
    "problem_difficulty_quote": "Results show that while LLMs perform well on lower-level cognitive tasks, such as recalling facts and identifying known vulnerabilities, their performance degrades significantly on higher-order tasks that require reasoning, architectural evaluation, and secure system creation.",
    "language": "æ–‡ä¸­æœªæ˜ç¡®æåŠæ•°æ®é›†ä½¿ç”¨çš„ç¼–ç¨‹è¯­è¨€ï¼Œä½†æåŠäº†è¯„ä¼°æ¨¡å‹åœ¨è½¯ä»¶å®‰å…¨æ¦‚å¿µä¸Šçš„ç†è§£ï¼Œå¯èƒ½æ¶‰åŠå¤šç§è¯­è¨€ã€‚",
    "language_quote": NaN,
    "data_size": "æ–‡ä¸­æœªæ˜ç¡®æåŠå…·ä½“é—®é¢˜æ•°é‡ï¼Œä½†æåŠæ•´åˆäº†å¤šç§æ¥æºçš„æ•°æ®é›†ã€‚",
    "data_size_quote": NaN,
    "source_type": "æ•´åˆäº†å¤šç§æ¥æºï¼šäººå·¥ç­–åˆ’çš„å¤šé€‰é¢˜ã€æ¼æ´ä»£ç ç‰‡æ®µï¼ˆSALLMï¼‰ã€è¯¾ç¨‹è¯„ä¼°ï¼ˆè½¯ä»¶å®‰å…¨å¯¼è®ºè¯¾ç¨‹ï¼‰ã€çœŸå®ä¸–ç•Œæ¡ˆä¾‹ç ”ç©¶ï¼ˆXBOWï¼‰ã€åŸºäºé¡¹ç›®çš„åˆ›é€ ä»»åŠ¡ï¼ˆå®‰å…¨è½¯ä»¶å·¥ç¨‹è¯¾ç¨‹ï¼‰ã€‚",
    "source_type_quote": "Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments (Introduction to Software Security course), real-world case studies (XBOW), and project-based creation tasks (Secure Software Engineering course).",
    "last_updated": "2025-12-24 (é¢„å°æœ¬å‘å¸ƒæ—¥æœŸ)",
    "last_updated_quote": "arXiv:2512.21238v1  [cs.SE]  24 Dec 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆç”±è®ºæ–‡ä½œè€…æ„å»ºçš„è¯„ä¼°æ¡†æ¶ï¼‰",
    "build_type_quote": "To answer this question, we developed Basket, a framework that enables the systematic assessment of LLMs...",
    "contamination_status": "æ–‡ä¸­æœªæ˜ç¡®è®¨è®ºã€‚",
    "contamination_status_quote": NaN,
    "dataset_license": "æ–‡ä¸­æœªæ˜ç¡®æåŠã€‚",
    "dataset_license_quote": NaN,
    "task_granularity": "å¤šæ ·ï¼ŒåŒ…æ‹¬çŸ¥è¯†é—®ç­”ã€æ¼æ´è¯†åˆ«ã€ä»£ç åˆ†æã€å®‰å…¨è¯„ä¼°å’Œç³»ç»Ÿè®¾è®¡ã€‚",
    "task_granularity_quote": "We assess six cognitive dimensions: remembering, understanding, applying, analyzing, evaluating, and creating.",
    "evaluation_metrics": "æ–‡ä¸­æœªæ˜ç¡®æåŠå•ä¸€æŒ‡æ ‡ï¼Œä½†æŠ¥å‘Šäº†æ¨¡å‹åœ¨ä¸åŒè®¤çŸ¥å±‚æ¬¡ä¸Šçš„è¡¨ç°ï¼ˆå¦‚å‡†ç¡®ç‡ï¼‰å’ŒçŸ¥è¯†è¾¹ç•Œã€‚",
    "evaluation_metrics_quote": "Beyond reporting aggregate accuracy, we introduce a software security knowledge boundary...",
    "input_modality": "å¤šæ ·ï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€é—®é¢˜ã€ä»£ç ç‰‡æ®µã€æ¡ˆä¾‹æè¿°ã€é¡¹ç›®è¦æ±‚ã€‚",
    "input_modality_quote": "Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments... real-world case studies... and project-based creation tasks...",
    "output_modality": "å¤šæ ·ï¼ŒåŒ…æ‹¬é€‰æ‹©é¢˜ç­”æ¡ˆã€ä»£ç ä¿®å¤ã€å®‰å…¨åˆ†æã€è¯„ä¼°åˆ¤æ–­ã€è®¾è®¡æ–¹æ¡ˆã€‚",
    "output_modality_quote": "We assess six cognitive dimensions: remembering, understanding, applying, analyzing, evaluating, and creating.",
    "task_io_type": "å¤šæ ·ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ°æ–‡æœ¬ï¼ˆé—®ç­”ï¼‰ã€ä»£ç åˆ°æ–‡æœ¬ï¼ˆåˆ†æï¼‰ã€æ–‡æœ¬åˆ°ä»£ç ï¼ˆä¿®å¤/è®¾è®¡ï¼‰ç­‰ã€‚",
    "task_io_type_quote": "Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments... real-world case studies... and project-based creation tasks...",
    "execution_environment": "æ–‡ä¸­æœªæ˜ç¡®æè¿°ã€‚",
    "execution_environment_quote": NaN,
    "unique_features": "1. é¦–ä¸ªåŸºäºå¸ƒé²å§†åˆ†ç±»æ³•ç³»ç»Ÿè¯„ä¼°LLMè½¯ä»¶å®‰å…¨çŸ¥è¯†ç†è§£çš„æ¡†æ¶ã€‚2. å¼•å…¥äº†â€œè½¯ä»¶å®‰å…¨çŸ¥è¯†è¾¹ç•Œâ€æ¦‚å¿µï¼Œç”¨äºè¯†åˆ«æ¨¡å‹å¯é æ€§èƒ½çš„æœ€é«˜è®¤çŸ¥å±‚æ¬¡ã€‚3. è¯†åˆ«å¹¶åˆ†ç±»äº†LLMåœ¨è½¯ä»¶å®‰å…¨æ¦‚å¿µä¸Šçš„51ç§é‡å¤æ€§é”™è¯¯æ¨¡å¼ã€‚",
    "unique_features_quote": "Rather than introducing yet another benchmark for a single security task... Beyond reporting aggregate accuracy, we introduce a software security knowledge boundary... In addition, we identified 51 recurring misconception patterns made by LLMs across Bloomâ€™s levels.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": 12,
    "last_updated_day": 24,
    "language_normalized": "[]",
    "dimension_normalized": "['è½¯ä»¶å®‰å…¨çŸ¥è¯†ç†è§£', 'è®¤çŸ¥å±‚æ¬¡ï¼ˆåŸºäºå¸ƒé²å§†åˆ†ç±»æ³•ï¼‰', 'æ¨¡å‹çŸ¥è¯†è¾¹ç•Œ', 'é”™è¯¯æ¦‚å¿µæ¨¡å¼']",
    "evaluation_method_normalized": "[]",
    "problem_domain_normalized": "['è½¯ä»¶å®‰å…¨']",
    "source_type_normalized": "['äººå·¥ç­–åˆ’çš„å¤šé€‰é¢˜', 'æ¼æ´ä»£ç ç‰‡æ®µï¼ˆSALLMï¼‰', 'è¯¾ç¨‹è¯„ä¼°ï¼ˆè½¯ä»¶å®‰å…¨å¯¼è®ºè¯¾ç¨‹ï¼‰', 'çœŸå®ä¸–ç•Œæ¡ˆä¾‹ç ”ç©¶ï¼ˆXBOWï¼‰', 'åŸºäºé¡¹ç›®çš„åˆ›é€ ä»»åŠ¡ï¼ˆå®‰å…¨è½¯ä»¶å·¥ç¨‹è¯¾ç¨‹ï¼‰']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.21028_output/content.md",
    "benchmark_name": "BigCodeBench (Hard)",
    "benchmark_name_quote": "Using the BigCodeBench (Hard) dataset [49], we design five prompting conditions...",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Using the BigCodeBench (Hard) dataset [49], we design five prompting conditions...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ä»£ç ç”Ÿæˆï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹æ ¹æ®ä»»åŠ¡æè¿°ç”Ÿæˆæ­£ç¡®ä»£ç çš„èƒ½åŠ›ã€‚",
    "task_description_quote": "In this paper, we present an empirical study that systematically examines how LLMs adapt their code generation strategies when exposed to test cases under different instructions.",
    "dimension": "ä»£ç ç”Ÿæˆæ­£ç¡®æ€§ã€ä»£ç ç›¸ä¼¼æ€§ã€ç¨‹åºå¤§å°ã€ä»£ç å˜æ›´ã€æ¨¡å‹åœ¨æµ‹è¯•å¯è§æ€§ä¸æŒ‡ä»¤é™åˆ¶ä¸‹çš„è¡Œä¸ºé€‚åº”æ€§",
    "dimension_quote": "We evaluate five LLMs (four open-source and one closed-source) across correctness, similarity to reference code, program size, and code churn, and analyze cross-model consistency to identify adaptation strategies.",
    "evaluation_method": "æ‰§è¡Œå•å…ƒæµ‹è¯•ä»¥è¯„ä¼°åŠŸèƒ½æ­£ç¡®æ€§ï¼Œä½¿ç”¨ pass@k æŒ‡æ ‡ã€‚",
    "evaluation_method_quote": "Across benchmarks, evaluation typically relies on executing unit tests to standardize correctness assessment.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": NaN,
    "problem_domain_quote": NaN,
    "problem_difficulty": "Hard (å›°éš¾)",
    "problem_difficulty_quote": "Using the BigCodeBench (Hard) dataset [49]...",
    "language": "æ–‡ä¸­æœªæ˜ç¡®æåŠBigCodeBench (Hard)æ•°æ®é›†åŒ…å«çš„å…·ä½“ç¼–ç¨‹è¯­è¨€ï¼Œä»…æåŠäº†å®éªŒèƒŒæ™¯ä¸‹çš„å…¶ä»–åŸºå‡†ï¼ˆå¦‚HumanEvalã€MBPPæ˜¯Pythonï¼‰ã€‚",
    "language_quote": "HumanEval and MBPP measure functional correctness on short Python tasks using unit tests [5, 14]; ... MultiPL-E systematically translates HumanEval and MBPP into multiple languages to probe cross-language generalization [13].",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "In this paper, we present an empirical study that systematically examines how LLMs adapt their code generation strategies...",
    "evaluation_metrics": "pass@1, pass@5",
    "evaluation_metrics_quote": "For example, Pass@1 success rates rose from 15.5%â€“24.4% without tests to 37.2%â€“54.7% with tests... Pass@5, which increased from 29.1%â€“37.2% without tests to 57.4%â€“72.3% with tests.",
    "input_modality": "è‡ªç„¶è¯­è¨€ä»»åŠ¡æè¿°ï¼Œå¯èƒ½åŒ…å«å•å…ƒæµ‹è¯•",
    "input_modality_quote": "We define five prompting conditions that manipulate test visibility... (1) Baseline (B), with only the task description; (2) Full Test (FT), task plus all tests...",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "...how LLMs adapt their code generation strategies...",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "...producing correct code for a given problem...",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "å¼ºè°ƒå¤æ‚çš„å‡½æ•°è°ƒç”¨å’ŒæŒ‡ä»¤ï¼Œä»¥æ›´å¥½åœ°åæ˜ å½“å‰æ¨¡å‹çš„èƒ½åŠ›ã€‚",
    "unique_features_quote": "More recently, BigCodeBench emphasizes diverse function calls and complex instructions, better reflecting current model capabilities [49].",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆæ­£ç¡®æ€§', 'ä»£ç ç›¸ä¼¼æ€§', 'ç¨‹åºå¤§å°', 'ä»£ç å˜æ›´', 'æ¨¡å‹åœ¨æµ‹è¯•å¯è§æ€§ä¸æŒ‡ä»¤é™åˆ¶ä¸‹çš„è¡Œä¸ºé€‚åº”æ€§']",
    "evaluation_method_normalized": "['pass@1', 'pass@5']",
    "problem_domain_normalized": "[]",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "ç«èµ›çº§",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.20957_output/content.md",
    "benchmark_name": "SWE-smith",
    "benchmark_name_quote": "We extract valid samples from SWE-smith (Yang et al., 2025b) to form the training set.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We extract valid samples from SWE-smith (Yang et al., 2025b) to form the training set.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„æµ‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨å¤§å‹å¼€æºè½¯ä»¶ï¼ˆOSSï¼‰ä»“åº“ä¸­å®šä½éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶å’Œå‡½æ•°çš„èƒ½åŠ›ã€‚è¿™æ˜¯ä¸€ä¸ªä»“åº“çº§åˆ«çš„æ£€ç´¢ä»»åŠ¡ã€‚",
    "task_description_quote": "Locating the files and functions requiring modification in large open-source software (OSS) repositories is challenging due to their scale and structural complexity. Existing large language model (LLM)-based methods typically treat this as a repository-level retrieval task...",
    "dimension": "ä»“åº“çº§é—®é¢˜å®šä½èƒ½åŠ›",
    "dimension_quote": "Experiments demonstrate that RL-trained RepoNavigator achieves state-of-the-art performance... These results confirm that integrating a single, structurally grounded tool with RL training provides an efficient and scalable solution for repository-level issue localization.",
    "evaluation_method": "ä½¿ç”¨Diceç³»æ•°ï¼ˆDICEï¼‰è¿›è¡Œé›†åˆçº§åˆ«çš„æ¯”è¾ƒï¼Œå¹¶ç»“åˆå·¥å…·è°ƒç”¨æˆåŠŸç‡ï¼ˆS(Ï„)ï¼‰è®¡ç®—å¥–åŠ±ã€‚",
    "evaluation_method_quote": "The reward of GRPO process is calculated as: R( Ë†Y , Y âˆ—, Ï„) = DICE( Ë†Y , Y âˆ—) + S(Ï„)... Dice is a common metric for set-level comparison, for set Ë†Y and set Y âˆ— DICE( Ë†Y , Y âˆ—) = 2 Ã— | Ë†Y âˆ©Y âˆ—| / (| Ë†Y | + |Y âˆ—|) and S(Ï„) is the success rate of tool-calling extracted from Ï„.",
    "context_dependency": "å¤šæ–‡ä»¶é¡¹ç›®ï¼ˆæ•´ä¸ªä»£ç ä»“åº“ï¼‰",
    "context_dependency_quote": "Given a repository R = {f1, . . . , fN} and an issue description q, the goal is to output relevant code regions Y âˆ—= {(fi, gi,j)}, where gi,j denotes a function or code span in file fi.",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ï¼ˆSWEï¼‰ï¼Œç‰¹åˆ«æ˜¯å¼€æºè½¯ä»¶ä»“åº“çš„ç»´æŠ¤å’Œé—®é¢˜ä¿®å¤ã€‚",
    "problem_domain_quote": "In the domain of software engineering (SWE)... SWE-BENCH (Jimenez et al., 2023) currently serves as the most comprehensive benchmark for evaluating whether LLMs can resolve real-world GitHub issues.",
    "problem_difficulty": "å·¥ç¨‹çº§ï¼ˆå¤„ç†çœŸå®ä¸–ç•Œã€å¤§è§„æ¨¡çš„ä»£ç ä»“åº“ï¼‰",
    "problem_difficulty_quote": "In the domain of software engineering (SWE), although LLM agents can effectively handle simple programming tasks (Hui et al., 2024; Guo et al., 2024a), their ability to operate on large-scale open-source software (OSS) repositories remains limited.",
    "language": "Python",
    "language_quote": "Language servers resolve the definition of a Python symbol through a deterministic static analysis pipeline that approximates Pythonâ€™s runtime name-binding semantics.",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": "ä»SWE-smithæ•°æ®é›†ä¸­æå–çš„æœ‰æ•ˆæ ·æœ¬ã€‚",
    "source_type_quote": "We extract valid samples from SWE-smith (Yang et al., 2025b) to form the training set.",
    "last_updated": "2025",
    "last_updated_quote": "arXiv:2512.20957v1 [cs.SE] 24 Dec 2025",
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç å®šä½ï¼ˆè¯†åˆ«ä¸é—®é¢˜ç›¸å…³çš„æ–‡ä»¶å’Œå‡½æ•°ï¼‰",
    "task_granularity_quote": "Given a repository R = {f1, . . . , fN} and an issue description q, the goal is to output relevant code regions Y âˆ—= {(fi, gi,j)}, where gi,j denotes a function or code span in file fi.",
    "evaluation_metrics": "Diceç³»æ•°ï¼Œå·¥å…·è°ƒç”¨æˆåŠŸç‡",
    "evaluation_metrics_quote": "The reward of GRPO process is calculated as: R( Ë†Y , Y âˆ—, Ï„) = DICE( Ë†Y , Y âˆ—) + S(Ï„)... Dice is a common metric for set-level comparison...",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆé—®é¢˜æè¿°ï¼‰å’Œä»£ç ä»“åº“ç»“æ„",
    "input_modality_quote": "Given a repository R = {f1, . . . , fN} and an issue description q...",
    "output_modality": "ä»£ç åŒºåŸŸé›†åˆï¼ˆæ–‡ä»¶è·¯å¾„å’Œå‡½æ•°/ä»£ç ç‰‡æ®µæ ‡è¯†ç¬¦ï¼‰",
    "output_modality_quote": "...the goal is to output relevant code regions Y âˆ—= {(fi, gi,j)}, where gi,j denotes a function or code span in file fi.",
    "task_io_type": "æ–‡æœ¬ï¼ˆé—®é¢˜ï¼‰åˆ°ä»£ç ä½ç½®",
    "task_io_type_quote": "Given a repository R = {f1, . . . , fN} and an issue description q, the goal is to output relevant code regions...",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºä»“åº“çº§åˆ«çš„ä»£ç å®šä½ä»»åŠ¡ï¼Œè€Œéå®Œæ•´çš„ä»£ç ä¿®å¤ã€‚æ—¨åœ¨é€šè¿‡è¯†åˆ«æ•…éšœç»„ä»¶ï¼ˆæ–‡ä»¶æˆ–å‡½æ•°çº§åˆ«ï¼‰æ¥ç®€åŒ–ä¸‹æ¸¸çš„ä¿®å¤ä»»åŠ¡ï¼Œå¹¶æ”¯æŒå¯éªŒè¯çš„ã€å­—ç¬¦ä¸²çº§åˆ«çš„è¯„ä¼°ï¼Œä¸SFTå’ŒRLVRç­‰å¯æ‰©å±•è®­ç»ƒæ¡†æ¶å…¼å®¹ã€‚",
    "unique_features_quote": "Since modern OSS repositories contain a significant amount of codeâ€”far beyond any LLMâ€™s context windowâ€”localization drastically reduces the search space and improves downstream solvability. Crucially, localization outputs a discrete set of paths, enabling verifiable, string-level evaluation that is compatible with scalable training frameworks such as SFT and RLVR.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»“åº“çº§é—®é¢˜å®šä½èƒ½åŠ›']",
    "evaluation_method_normalized": "['Diceç³»æ•°', 'å·¥å…·è°ƒç”¨æˆåŠŸç‡']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹ï¼ˆSWEï¼‰', 'ç‰¹åˆ«æ˜¯å¼€æºè½¯ä»¶ä»“åº“çš„ç»´æŠ¤å’Œé—®é¢˜ä¿®å¤ã€‚']",
    "source_type_normalized": "['ä»SWE-smithæ•°æ®é›†ä¸­æå–çš„æœ‰æ•ˆæ ·æœ¬ã€‚']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "JSON",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.21132_output/content.md",
    "benchmark_name": "AUTOBAXBENCH",
    "benchmark_name_quote": "We use AUTOBAXBUILDER to construct entirely new tasks and release them to the public as AUTOBAXBENCH, together with a thorough evaluation of the security capabilities of LLMs on these tasks.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "In this work, we address these challenges and present AUTOBAXBUILDER, a framework that generates tasks and tests for code security benchmarking from scratch. ... We use AUTOBAXBUILDER to construct entirely new tasks and release them to the public as AUTOBAXBENCH...",
    "dataset_url": "https://baxbench.com/autobaxbuilder, https://github.com/eth-sri/autobaxbuilder",
    "dataset_url_quote": "https://baxbench.com/autobaxbuilder\nhttps://github.com/eth-sri/autobaxbuilder\n... We publicly release the scenarios at https://github.com/eth-sri/autobaxbuilder.",
    "task_description": "è¯„ä¼°LLMç”Ÿæˆçš„åç«¯åº”ç”¨ç¨‹åºä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§å’Œå®‰å…¨æ€§ã€‚æ¯ä¸ªä»»åŠ¡åŒ…å«ä¸€ä¸ªåœºæ™¯æè¿°ï¼ˆåç«¯åº”ç”¨è§„èŒƒï¼‰ã€åŠŸèƒ½æµ‹è¯•å’Œå®‰å…¨æ¼æ´åˆ©ç”¨è„šæœ¬ã€‚",
    "task_description_quote": "BAXBENCH is a recent benchmark that measures both functional correctness and security of LLM-generated application backends. ... Each scenario is combined with functional tests and security exploits that test LLM-generated solutions through the REST endpoints...",
    "dimension": "ä»£ç å®‰å…¨æ€§ä¸åŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "As LLMs see wide adoption in software engineering, the reliable assessment of the correctness and security of LLM-generated code is crucial. ... BAXBENCH is a recent benchmark that measures both functional correctness and security of LLM-generated application backends.",
    "evaluation_method": "é€šè¿‡RESTç«¯ç‚¹å¯¹ç”Ÿæˆçš„ä»£ç è¿›è¡ŒåŠ¨æ€æµ‹è¯•ï¼ŒåŒ…æ‹¬åŠŸèƒ½æµ‹è¯•å’Œç«¯åˆ°ç«¯å®‰å…¨æ¼æ´åˆ©ç”¨ã€‚å®‰å…¨æµ‹è¯•æˆåŠŸä¼šè¿”å›å¯¹åº”çš„CWEåˆ†ç±»ã€‚",
    "evaluation_method_quote": "The generated code is launched in an isolated environment, exposing its endpoints via REST. This allows testing the solution via HTTP requests. ... If a security test finds an successful exploit, it returns a corresponding classification as an entry in the Common Weakness Enumeration (CWE).",
    "context_dependency": "åç«¯åº”ç”¨ç¨‹åºçº§åˆ«ï¼ˆå¤šç«¯ç‚¹ã€å¤šæ–‡ä»¶é¡¹ç›®ï¼‰",
    "context_dependency_quote": "BAXBENCH consists of scenarios, each specifying a backend application to implement, including a natural language description and specific REST endpoints.",
    "problem_domain": "Webåº”ç”¨åç«¯å¼€å‘ï¼Œå®‰å…¨å…³é”®é¢†åŸŸ",
    "problem_domain_quote": "This is particularly important in safety-critical domains such as web application backends, as these are directly exposed to malicious actors.",
    "problem_difficulty": "åŒ…å«ä¸‰ä¸ªä¸åŒéš¾åº¦çš„å­é›†ï¼šç®€å•ã€ä¸­ç­‰ï¼ˆæ¯”BAXBENCHç¨éš¾ï¼‰ã€å›°éš¾ï¼ˆæœ€ä½³æ¨¡å‹å‡†ç¡®ç‡ä½äº9%ï¼‰",
    "problem_difficulty_quote": "We leverage our tool to explicitly generate three distinct subsets of varying difficulty, including a medium version that is slightly more difficult than BAXBENCH, an easier version suitable for evaluation of smaller LLMs, and a hard version that challenges the best evaluated LLM, which achieves less than 9% accuracy.",
    "language": "è¯­è¨€æ— å…³ï¼Œæ”¯æŒ14ç§æ¡†æ¶å’Œ6ç§ç¼–ç¨‹è¯­è¨€",
    "language_quote": "Each such combination defines a language-independent task, which can readily be evaluated in 14 frameworks across 6 programming languages.",
    "data_size": "åŒ…å«40ä¸ªæ–°åœºæ™¯ï¼Œæ˜¯åŸå§‹BAXBENCHè§„æ¨¡çš„ä¸¤å€å¤š",
    "data_size_quote": "We then generate 40 new scenarios with AUTOBAXBUILDER, more than doubling the size of BAXBENCH.",
    "source_type": "ç”±AUTOBAXBUILDERæ¡†æ¶é€šè¿‡LLMè‡ªåŠ¨ç”Ÿæˆï¼Œæ— éœ€äººå·¥ç¼–å†™",
    "source_type_quote": "In this work, we address this challenge and propose an agentic LLM-based pipeline that creates new scenarios with minimal human intervention, including corresponding functionality test cases and security exploits.",
    "last_updated": "2025",
    "last_updated_quote": "arXiv:2512.21132v1  [cs.CR]  24 Dec 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆç”±ETH Zurichç ”ç©¶å›¢é˜Ÿé€šè¿‡è‡ªåŠ¨åŒ–æ¡†æ¶æ„å»ºï¼‰",
    "build_type_quote": "We use AUTOBAXBUILDER to construct entirely new tasks and release them to the public as AUTOBAXBENCH...",
    "contamination_status": "æŠ—æ±¡æŸ“è®¾è®¡ï¼Œå¯ä¸æ–­ç”Ÿæˆæ–°ä»»åŠ¡ä»¥é¿å…è®­ç»ƒæ•°æ®æ±¡æŸ“",
    "contamination_status_quote": "However, relying on manually-crafted benchmarks is insufficient in the long term, because benchmarks (i) naturally end up contaminating training data... ...and constantly updated to ensure valid evaluation in the face of contamination.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆç”Ÿæˆå®Œæ•´çš„åç«¯åº”ç”¨ç¨‹åºï¼‰",
    "task_granularity_quote": "For each such task, an evaluated model is prompted to generate application code in the target language.",
    "evaluation_metrics": "åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆé€šè¿‡åŠŸèƒ½æµ‹è¯•ï¼‰å’Œå®‰å…¨æ€§ï¼ˆé€šè¿‡å®‰å…¨æ¼æ´åˆ©ç”¨æµ‹è¯•ï¼‰çš„å‡†ç¡®ç‡",
    "evaluation_metrics_quote": "BAXBENCH is a recent benchmark that measures both functional correctness and security of LLM-generated application backends.",
    "input_modality": "è‡ªç„¶è¯­è¨€æè¿°å’ŒOpenAPIè§„èŒƒ",
    "input_modality_quote": "BAXBENCH consists of scenarios, each specifying a backend application to implement, including a natural language description and specific REST endpoints. Concretely, the endpoints are specified in the OpenAPI language...",
    "output_modality": "ä»£ç ï¼ˆåç«¯åº”ç”¨ç¨‹åºå®ç°ï¼‰",
    "output_modality_quote": "For each such task, an evaluated model is prompted to generate application code in the target language.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "BAXBENCH consists of scenarios, each specifying a backend application to implement, including a natural language description and specific REST endpoints. ... For each such task, an evaluated model is prompted to generate application code in the target language.",
    "execution_environment": "éš”ç¦»ç¯å¢ƒï¼Œé€šè¿‡RESTæš´éœ²ç«¯ç‚¹ï¼Œå¯è®¿é—®æ–‡ä»¶ç³»ç»Ÿå’Œæ•°æ®åº“ä»¥æ£€æµ‹æ”»å‡»",
    "execution_environment_quote": "The generated code is launched in an isolated environment, exposing its endpoints via REST. ... Further, we can access the file system, e.g., to check for successful Path Traversal or OS Injection attacks, and access used databases, e.g., to detect manipulations due to SQL Injection.",
    "unique_features": "1. å®Œå…¨è‡ªåŠ¨åŒ–ç”Ÿæˆï¼Œæ— éœ€äººå·¥ç¼–å†™åœºæ™¯ã€æµ‹è¯•å’Œæ¼æ´åˆ©ç”¨è„šæœ¬ã€‚2. æä¾›æ— å‡é˜³æ€§çš„å®‰å…¨ä¸Šç•Œä¿è¯ã€‚3. æ¡†æ¶å’Œç¼–ç¨‹è¯­è¨€æ— å…³ã€‚4. åŒ…å«ç«¯åˆ°ç«¯çš„å®‰å…¨æ¼æ´åˆ©ç”¨æµ‹è¯•ã€‚5. å¯æ‰©å±•ç”Ÿæˆä¸åŒéš¾åº¦çš„ä»»åŠ¡ã€‚",
    "unique_features_quote": "We present a robust method to generate a completely new benchmark following the design principles of BAXBENCH with minimal human intervention... ...This approach has no false-positives, and thus provides a sound upper bound on security. ... Each such combination defines a language-independent task, which can readily be evaluated in 14 frameworks across 6 programming languages. ... We leverage our tool to explicitly generate three distinct subsets of varying difficulty...",
    "data_size_quantity": 40,
    "data_size_unit": "ä¸ªæ–°åœºæ™¯",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['è¯­è¨€æ— å…³', '14ç§æ¡†æ¶', '6ç§ç¼–ç¨‹è¯­è¨€']",
    "dimension_normalized": "['ä»£ç å®‰å…¨æ€§', 'åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['åŠŸèƒ½æ­£ç¡®æ€§', 'å®‰å…¨æ€§']",
    "problem_domain_normalized": "['Webåº”ç”¨åç«¯å¼€å‘', 'å®‰å…¨å…³é”®é¢†åŸŸ']",
    "source_type_normalized": "['AUTOBAXBUILDERæ¡†æ¶', 'LLMè‡ªåŠ¨ç”Ÿæˆ']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2512.20203_output/content.md",
    "benchmark_name": "VulnLoc+",
    "benchmark_name_quote": "We evaluated LoopRepair on a real-world C/C++ vulnerability repair dataset VulnLoc+, which contains 40 vulnerabilities and their Proofs-of-Vulnerability.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We evaluate the performance of LoopRepair on the VulnLoc+ [51, 53] dataset, which consists of 40 vulnerabilities with their PoVs collected from 10 open-source software projects.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è‡ªåŠ¨åŒ–è½¯ä»¶æ¼æ´ä¿®å¤ã€‚ç»™å®šæ˜“å—æ”»å‡»çš„å‡½æ•°å’Œæ¼æ´è¯­å¥ï¼Œç›®æ ‡æ˜¯ç”Ÿæˆä¿®å¤åçš„è¡¥ä¸å‡½æ•°ã€‚",
    "task_description_quote": "In this paper, we focus on function-level Automated Vulnerability Repair (AVR), which takes a sequential vulnerable function ğ¹ğ‘£ and the vulnerable statement ğ‘†ğ‘£ as inputs, and outputs the patch function ğ¹ğ‘.",
    "dimension": "æ¼æ´ä¿®å¤çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬ç”Ÿæˆåˆç†è¡¥ä¸å’Œæ­£ç¡®è¡¥ä¸çš„èƒ½åŠ›ã€‚",
    "dimension_quote": "LoopRepair is able to generate 27 plausible patches... In terms of correct patch generation, LoopRepair repairs 8 to 13 additional vulnerabilities compared with existing approaches.",
    "evaluation_method": "ä½¿ç”¨Proofs-of-Vulnerability (PoV) è¿›è¡ŒéªŒè¯ï¼Œè¯„ä¼°ç”Ÿæˆçš„è¡¥ä¸æ˜¯å¦åˆç†ï¼ˆplausibleï¼‰å’Œæ­£ç¡®ï¼ˆcorrectï¼‰ã€‚",
    "evaluation_method_quote": "Following previous work [19, 51], we assume that the vulnerability has already been detected and that 1-2 Proofs-of-Vulnerability (PoVs) are available.",
    "context_dependency": "å‡½æ•°çº§åˆ«ã€‚",
    "context_dependency_quote": "In this paper, we focus on function-level Automated Vulnerability Repair (AVR)...",
    "problem_domain": "è½¯ä»¶å®‰å…¨ï¼Œå…·ä½“ä¸ºC/C++ç¨‹åºä¸­çš„æ¼æ´ä¿®å¤ã€‚",
    "problem_domain_quote": "We evaluated LoopRepair on a real-world C/C++ vulnerability repair dataset VulnLoc+...",
    "problem_difficulty": "åŒ…å«å¤šå—ï¼ˆmulti-hunkï¼‰æ¼æ´ï¼Œä¿®å¤éš¾åº¦è¾ƒé«˜ã€‚",
    "problem_difficulty_quote": "A multi-hunk vulnerability is a vulnerability that requires edits at multiple, non-contiguous hunks in a program [64, 69].",
    "language": "C/C++",
    "language_quote": "We evaluated LoopRepair on a real-world C/C++ vulnerability repair dataset VulnLoc+...",
    "data_size": "åŒ…å«40ä¸ªæ¼æ´åŠå…¶Proofs-of-Vulnerability (PoVs)ï¼Œæ”¶é›†è‡ª10ä¸ªå¼€æºè½¯ä»¶é¡¹ç›®ã€‚",
    "data_size_quote": "We evaluate the performance of LoopRepair on the VulnLoc+ [51, 53] dataset, which consists of 40 vulnerabilities with their PoVs collected from 10 open-source software projects.",
    "source_type": "ä»å¼€æºè½¯ä»¶é¡¹ç›®ä¸­æ”¶é›†çš„çœŸå®æ¼æ´ã€‚",
    "source_type_quote": "...collected from 10 open-source software projects.",
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ä¿®å¤ï¼ˆæ¼æ´ä¿®å¤ï¼‰ã€‚",
    "task_granularity_quote": "Automated Vulnerability Repair (AVR)",
    "evaluation_metrics": "åˆç†è¡¥ä¸æ•°é‡ï¼ˆplausible patchesï¼‰ï¼Œæ­£ç¡®ä¿®å¤çš„æ¼æ´æ•°é‡ï¼ˆcorrect patch generationï¼‰ã€‚",
    "evaluation_metrics_quote": "LoopRepair is able to generate 27 plausible patches... In terms of correct patch generation, LoopRepair repairs 8 to 13 additional vulnerabilities...",
    "input_modality": "ä»£ç ï¼ˆæ˜“å—æ”»å‡»çš„å‡½æ•°ï¼‰ä¸ä»£ç ï¼ˆæ¼æ´è¯­å¥ï¼‰ã€‚",
    "input_modality_quote": "...takes a sequential vulnerable function ğ¹ğ‘£ and the vulnerable statement ğ‘†ğ‘£ as inputs...",
    "output_modality": "ä»£ç ï¼ˆè¡¥ä¸å‡½æ•°ï¼‰ã€‚",
    "output_modality_quote": "...and outputs the patch function ğ¹ğ‘.",
    "task_io_type": "ä»£ç åˆ°ä»£ç ï¼ˆæ¼æ´ä»£ç åˆ°ä¿®å¤ä»£ç ï¼‰ã€‚",
    "task_io_type_quote": "...takes a sequential vulnerable function ğ¹ğ‘£ and the vulnerable statement ğ‘†ğ‘£ as inputs, and outputs the patch function ğ¹ğ‘.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºå¤šå—ï¼ˆmulti-hunkï¼‰æ¼æ´ä¿®å¤ï¼Œæ¯ä¸ªæ¼æ´éƒ½é…æœ‰Proofs-of-Vulnerability (PoVs) ç”¨äºéªŒè¯ã€‚",
    "unique_features_quote": "A multi-hunk vulnerability is a vulnerability that requires edits at multiple, non-contiguous hunks in a program... We evaluated LoopRepair on a real-world C/C++ vulnerability repair dataset VulnLoc+, which contains 40 vulnerabilities and their Proofs-of-Vulnerability.",
    "data_size_quantity": 40,
    "data_size_unit": "ä¸ªæ¼æ´",
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['C/C++']",
    "dimension_normalized": "['æ¼æ´ä¿®å¤çš„æœ‰æ•ˆæ€§', 'ç”Ÿæˆåˆç†è¡¥ä¸çš„èƒ½åŠ›', 'æ­£ç¡®è¡¥ä¸çš„èƒ½åŠ›']",
    "evaluation_method_normalized": "['åˆç†è¡¥ä¸æ•°é‡', 'æ­£ç¡®ä¿®å¤çš„æ¼æ´æ•°é‡']",
    "problem_domain_normalized": "['è½¯ä»¶å®‰å…¨', 'C/C++ç¨‹åºä¸­çš„æ¼æ´ä¿®å¤']",
    "source_type_normalized": "['ä»å¼€æºè½¯ä»¶é¡¹ç›®ä¸­æ”¶é›†çš„çœŸå®æ¼æ´']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ä¿®å¤",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.20845_output/content.md",
    "benchmark_name": "HumanEval",
    "benchmark_name_quote": "â€¢ HumanEval (Chen et al. 2021) is a benchmark for program synthesis in which each task specifies a function signature, a natural-language description of the intended behavior, and a series of hidden unit tests.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We evaluate our approach on two benchmarks that stress different aspects of LLM reasoning. ... HumanEval tests program synthesis by checking whether generated functions pass hidden unit tests.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ç¨‹åºåˆæˆï¼Œå³æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°å’Œå‡½æ•°ç­¾åç”Ÿæˆèƒ½é€šè¿‡éšè—å•å…ƒæµ‹è¯•çš„ä»£ç ã€‚",
    "task_description_quote": "HumanEval tests program synthesis by checking whether generated functions pass hidden unit tests.",
    "dimension": "ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "HumanEval tests program synthesis by checking whether generated functions pass hidden unit tests.",
    "evaluation_method": "åœ¨æ²™ç›’ç¯å¢ƒä¸­æ‰§è¡Œç”Ÿæˆçš„ä»£ç ï¼Œæ£€æŸ¥æ˜¯å¦é€šè¿‡æ‰€æœ‰éšè—çš„å•å…ƒæµ‹è¯•ã€‚",
    "evaluation_method_quote": "A model succeeds if its generated function passes all tests when executed in a sandboxed environment.",
    "context_dependency": "å•å‡½æ•°",
    "context_dependency_quote": "each task specifies a function signature, a natural-language description of the intended behavior, and a series of hidden unit tests.",
    "problem_domain": "é€šç”¨ç¼–ç¨‹",
    "problem_domain_quote": NaN,
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "æ–‡ä¸­ä»…æåŠå®éªŒè®¾ç½®ï¼Œæœªæè¿°å®Œæ•´æ•°æ®é›†ã€‚æ ¹æ®å¼•ç”¨æ–‡çŒ®(Chen et al. 2021)æ¨æ–­ä¸ºPythonã€‚",
    "language_quote": "HumanEval (Chen et al. 2021) is a benchmark for program synthesis...",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": "2021",
    "last_updated_quote": "HumanEval (Chen et al. 2021)",
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "HumanEval tests program synthesis",
    "evaluation_metrics": "pass@1",
    "evaluation_metrics_quote": "for HumanEval, we use pass@1 based on whether the generated solution passes all hidden unit tests.",
    "input_modality": "è‡ªç„¶è¯­è¨€ä¸ä»£ç ï¼ˆå‡½æ•°ç­¾åï¼‰",
    "input_modality_quote": "each task specifies a function signature, a natural-language description of the intended behavior",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "generated functions",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "HumanEval tests program synthesis",
    "execution_environment": "æ²™ç›’ç¯å¢ƒ",
    "execution_environment_quote": "when executed in a sandboxed environment.",
    "unique_features": "åŒ…å«éšè—çš„å•å…ƒæµ‹è¯•ï¼Œç”¨äºè¯„ä¼°ç”Ÿæˆä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚",
    "unique_features_quote": "a series of hidden unit tests.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2021,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['pass@1']",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.20732_output/content.md",
    "benchmark_name": "FEM-Bench",
    "benchmark_name_quote": "We introduce FEM-Bench, a computational mechanics benchmark designed to evaluate the ability of LLMs to generate correct finite element method (FEM) and related code.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We introduce FEM-Bench, a benchmark designed to evaluate the ability of LLMs to generate correct implementations of finite element method (FEM) and related computational mechanics tasks.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆæ­£ç¡®çš„æœ‰é™å…ƒæ–¹æ³•åŠç›¸å…³è®¡ç®—åŠ›å­¦ä»£ç çš„èƒ½åŠ›ã€‚",
    "task_description_quote": "FEM-Bench, a computational mechanics benchmark designed to evaluate the ability of LLMs to generate correct finite element method (FEM) and related code.",
    "dimension": "ç‰©ç†å»ºæ¨¡ã€æ•°å€¼ç¦»æ•£åŒ–ã€ç»“æ„åŒ–è®¡ç®—æ¨ç†ã€ç§‘å­¦è®¡ç®—ä»£ç ç”Ÿæˆèƒ½åŠ›",
    "dimension_quote": "no widely used benchmarks evaluate an LLMs ability to carry out the physical modeling, numerical discretization, and structured computational reasoning required for advanced scientific computing.",
    "evaluation_method": "åŸºäºè®¡ç®—åŠ›å­¦éªŒè¯ã€éªŒè¯å’Œä¸ç¡®å®šæ€§é‡åŒ–æ–‡åŒ–çš„å®¢è§‚æ•°å€¼æ£€æŸ¥ï¼Œå¦‚å¯¹ç§°æ€§ã€ä¸€è‡´æ€§ã€ç½‘æ ¼æ”¶æ•›æ€§ç­‰ã€‚",
    "evaluation_method_quote": "These practices transform numerical implementations into testable artifacts with well-defined correctness criteria... including patch tests, equilibrium checks, symmetry requirements, energy consistency, and mesh convergence studies.",
    "context_dependency": "å¤šæ­¥éª¤ç»“æ„åŒ–æ¨ç†ï¼Œéµå¾ªæ˜ç¡®çš„æ•°å­¦æµç¨‹ï¼ˆå¦‚æ§åˆ¶æ–¹ç¨‹ã€å¼±å½¢å¼ã€ç¦»æ•£åŒ–ã€å…¨å±€ç»„è£…ï¼‰ã€‚",
    "context_dependency_quote": "Problems in computational mechanics follow a precise mathematical pipeline: e.g., governing equations are formulated, converted into weak or variational forms, discretized into finite-dimensional approximations, and assembled into global algebraic systems.",
    "problem_domain": "è®¡ç®—åŠ›å­¦ã€ç§‘å­¦è®¡ç®—ã€ç‰©ç†å»ºæ¨¡ã€æœ‰é™å…ƒæ–¹æ³•ã€çŸ©é˜µç»“æ„åˆ†æ",
    "problem_domain_quote": "Computational mechanics, the discipline that develops and applies mathematical models and numerical methods to predict the behavior of physical systems under forces, deformation, and constraints, provides an ideal foundation for structured scientific reasoning based evaluation.",
    "problem_difficulty": "å…¥é—¨çº§ä½†éå¹³å‡¡ï¼Œä¸è®¡ç®—åŠ›å­¦ç ”ç©¶ç”Ÿå…¥é—¨è¯¾ç¨‹å†…å®¹ä¸€è‡´ã€‚",
    "problem_difficulty_quote": "FEM-Bench 2025 focuses on introductory but nontrivial tasks aligned with material from a first graduate course on FEM.",
    "language": "æ–‡ä¸­æœªæ˜ç¡®æŒ‡å®šç¼–ç¨‹è¯­è¨€ï¼Œä½†ä»»åŠ¡æ¶‰åŠç”Ÿæˆç§‘å­¦è®¡ç®—ä»£ç ã€‚",
    "language_quote": NaN,
    "data_size": "FEM-Bench 2025 åŒ…å«33ä¸ªä»»åŠ¡ã€‚",
    "data_size_quote": "the best performing model at function writing, Gemini 3 Pro, completed 30/33 tasks at least one out of five times, and 26/33 tasks five out of five times.",
    "source_type": "åŸºäºæˆç†Ÿçš„è®¡ç®—åŠ›å­¦æ–‡çŒ®ã€ç»å…¸æ•™ç§‘ä¹¦å’Œå¹¿æ³›æ•™æˆçš„ç ”ç©¶ç”Ÿè¯¾ç¨‹ã€‚",
    "source_type_quote": "The computational mechanics literature spans classic textbooks, widely taught graduate curricula in engineering, and ongoing research at the frontier of numerical methods development, offering a rich, well-understood space of problems with clear expectations for correctness and implementation quality.",
    "last_updated": "2025",
    "last_updated_quote": "FEM-Bench 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œç”±ç ”ç©¶å›¢é˜ŸåŸºäºè®¡ç®—åŠ›å­¦é¢†åŸŸçŸ¥è¯†æ„å»ºã€‚",
    "build_type_quote": "In this work, we introduce FEM-Bench, a benchmark designed to evaluate the ability of LLMs...",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆç”Ÿæˆæœ‰é™å…ƒæ–¹æ³•åŠç›¸å…³è®¡ç®—åŠ›å­¦ä»»åŠ¡çš„å®ç°ä»£ç ï¼‰ã€‚",
    "task_granularity_quote": "evaluate the ability of LLMs to generate correct implementations of finite element method (FEM) and related computational mechanics tasks.",
    "evaluation_metrics": "ä»»åŠ¡å®Œæˆç‡ï¼ˆå¦‚30/33ï¼Œ26/33ï¼‰ï¼Œå¹³å‡è”åˆæˆåŠŸç‡ï¼ˆAverage Joint Success Rateï¼Œå¦‚73.8%ï¼‰ã€‚",
    "evaluation_metrics_quote": "the best performing model at function writing, Gemini 3 Pro, completed 30/33 tasks at least one out of five times, and 26/33 tasks five out of five times. The best performing model at unit test writing, GPT-5, had an Average Joint Success Rate of 73.8%.",
    "input_modality": "è‡ªç„¶è¯­è¨€æè¿°çš„è®¡ç®—åŠ›å­¦é—®é¢˜ï¼ˆåŒ…å«è½½è·ã€è¾¹ç•Œæ¡ä»¶ã€å‡ ä½•ç­‰ï¼‰ã€‚",
    "input_modality_quote": "A solid mechanics problem: given loads and boundary conditions on the reference configuration, solve for the deformation mapping to the deformed configuration.",
    "output_modality": "ä»£ç ï¼ˆæœ‰é™å…ƒæ–¹æ³•æˆ–ç›¸å…³è®¡ç®—åŠ›å­¦ç®—æ³•çš„å®ç°ï¼‰ã€‚",
    "output_modality_quote": "generate correct implementations of finite element method (FEM) and related computational mechanics tasks.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "evaluate the ability of LLMs to generate correct implementations... from problem descriptions.",
    "execution_environment": "éœ€è¦ç§‘å­¦è®¡ç®—ç¯å¢ƒä»¥æ‰§è¡Œç”Ÿæˆçš„ä»£ç å¹¶è¿›è¡Œæ•°å€¼éªŒè¯ã€‚",
    "execution_environment_quote": "These tasks isolate essential numerical and physical modeling challenges while remaining amenable to automated evaluation.",
    "unique_features": "ä¸“æ³¨äºè®¡ç®—åŠ›å­¦å’Œç‰©ç†ä¸–ç•Œå»ºæ¨¡ï¼Œå¡«è¡¥äº†ç°æœ‰ä»£ç ç”ŸæˆåŸºå‡†åœ¨è¯„ä¼°ç§‘å­¦è®¡ç®—ã€ç‰©ç†å»ºæ¨¡å’Œæ•°å€¼æ–¹æ³•å®ç°èƒ½åŠ›æ–¹é¢çš„ç©ºç™½ã€‚å®ƒå¼ºè°ƒä¸¥æ ¼çš„ç‰©ç†å’Œæ•°å€¼çº¦æŸï¼Œä»¥åŠåŸºäºè®¡ç®—åŠ›å­¦æˆç†ŸéªŒè¯æ–‡åŒ–çš„å®¢è§‚è¯„ä¼°ã€‚",
    "unique_features_quote": "no widely used benchmarks evaluate an LLMs ability to carry out the physical modeling, numerical discretization, and structured computational reasoning required for advanced scientific computing. Computational mechanics... provides an ideal foundation for structured scientific reasoning based evaluation. Problems follow clear mathematical structure, enforce strict physical and numerical constraints, and support objective verification.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['æ–‡ä¸­æœªæ˜ç¡®æŒ‡å®šç¼–ç¨‹è¯­è¨€ï¼Œä½†ä»»åŠ¡æ¶‰åŠç”Ÿæˆç§‘å­¦è®¡ç®—ä»£ç ã€‚']",
    "dimension_normalized": "['ç‰©ç†å»ºæ¨¡', 'æ•°å€¼ç¦»æ•£åŒ–', 'ç»“æ„åŒ–è®¡ç®—æ¨ç†', 'ç§‘å­¦è®¡ç®—ä»£ç ç”Ÿæˆèƒ½åŠ›']",
    "evaluation_method_normalized": "['ä»»åŠ¡å®Œæˆç‡', 'å¹³å‡è”åˆæˆåŠŸç‡']",
    "problem_domain_normalized": "['è®¡ç®—åŠ›å­¦', 'ç§‘å­¦è®¡ç®—', 'ç‰©ç†å»ºæ¨¡', 'æœ‰é™å…ƒæ–¹æ³•', 'çŸ©é˜µç»“æ„åˆ†æ']",
    "source_type_normalized": "['åŸºäºæˆç†Ÿçš„è®¡ç®—åŠ›å­¦æ–‡çŒ®ã€ç»å…¸æ•™ç§‘ä¹¦å’Œå¹¿æ³›æ•™æˆçš„ç ”ç©¶ç”Ÿè¯¾ç¨‹ã€‚']",
    "problem_difficulty_normalized": "å…¥é—¨çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.22113_output/content.md",
    "benchmark_name": "Code-Cloud-RCA Benchmark",
    "benchmark_name_quote": "3) Code-Cloud-RCA Benchmark consisting of 30 different scenarios and each incorporating a unique software, configuration, deployment, or resource-related fault observed in the real-world [3]â€“[5], [10]â€“[13], and injected in a live Kubernetes cloud environment.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "3) Code-Cloud-RCA Benchmark consisting of 30 different scenarios... PRAXIS is demonstrated on a set of 30 comprehensive real-world incidents that is being compiled into an RCA benchmark.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ç”¨äºè¯Šæ–­ç”±ä»£ç å’Œé…ç½®å¼•èµ·çš„äº‘æœåŠ¡æ•…éšœçš„æ ¹å› åˆ†æï¼ˆRCAï¼‰ã€‚",
    "task_description_quote": "This paper introduces PRAXIS, an orchestrator that manages and deploys an agentic workflow for diagnosing code- and configuration-caused cloud incidents.",
    "dimension": "æ ¹å› åˆ†æï¼ˆRCAï¼‰çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼ŒåŒ…æ‹¬æ ¹å› æ¨ç†å‡†ç¡®æ€§å’Œæ ¹å› è¯†åˆ«å‡†ç¡®æ€§ã€‚",
    "dimension_quote": "PRAXIS achieved a root-cause reasoning accuracy of 61.5% and a root-cause identification accuracy of 73.9%... PRAXIS reduced token consumption by 3.8Ã— from 624.4k tokens to 166.5k tokens per successful diagnosis...",
    "evaluation_method": "åœ¨30ä¸ªçœŸå®åœºæ™¯ä¸Šè¯„ä¼°æ ¹å› æ¨ç†å‡†ç¡®æ€§ã€æ ¹å› è¯†åˆ«å‡†ç¡®æ€§å’Œä»¤ç‰Œæ¶ˆè€—ã€‚",
    "evaluation_method_quote": "We evaluated the PRAXISâ€™s RCA effectiveness in terms of root cause accuracy and token consumptions across those 30 scenarios.",
    "context_dependency": "è·¨å¾®æœåŠ¡å’Œä»£ç çº§åˆ«çš„ä¾èµ–å…³ç³»ï¼Œæ¶‰åŠæœåŠ¡ä¾èµ–å›¾ï¼ˆSDGï¼‰å’Œç¨‹åºä¾èµ–å›¾ï¼ˆPDGï¼‰ã€‚",
    "context_dependency_quote": "PRAXIS employs an LLM-driven structured traversal over two types of graph: (1) a service dependency graph (SDG) that captures microservice-level dependencies; and (2) a hammock-block program dependence graph (PDG) that captures code-level dependencies for each microservice.",
    "problem_domain": "äº‘æœåŠ¡å¯é æ€§ã€æ•…éšœè¯Šæ–­ã€æ ¹å› åˆ†æï¼ˆRCAï¼‰ã€è½¯ä»¶å·¥ç¨‹ï¼ˆSWEï¼‰å’Œç«™ç‚¹å¯é æ€§å·¥ç¨‹ï¼ˆSREï¼‰ã€‚",
    "problem_domain_quote": "PRAXIS stands at the intersection of cloud incident diagnosis (an SRE task) and program analysis (an SWE task)... Code-Cloud-RCA Benchmark consisting of 30 different scenarios and each incorporating a unique software, configuration, deployment, or resource-related fault observed in the real-world...",
    "problem_difficulty": "çœŸå®ä¸–ç•Œçš„ã€å…·æœ‰æŒ‘æˆ˜æ€§çš„äº‘æ•…éšœåœºæ™¯ï¼Œå…¶æ•…éšœé“¾åœ¨å¤šä¸ªå¾®æœåŠ¡å’Œä»£ç ä¾èµ–ä¸­äº¤ç»‡ã€‚",
    "problem_difficulty_quote": "PRAXIS can diagnose incidents whose fault chains interleave microservice and code dependencies across multiple hops... demonstrated on a set of 30 comprehensive real-world incidents...",
    "language": "æœªæ˜ç¡®æŒ‡å®šï¼Œä½†æ¶‰åŠå¾®æœåŠ¡ä»£ç åº“ï¼Œå¯èƒ½åŒ…å«å¤šç§ç¼–ç¨‹è¯­è¨€ã€‚",
    "language_quote": NaN,
    "data_size": "åŒ…å«30ä¸ªä¸åŒçš„çœŸå®ä¸–ç•Œæ•…éšœåœºæ™¯ã€‚",
    "data_size_quote": "Code-Cloud-RCA Benchmark consisting of 30 different scenarios...",
    "source_type": "åŸºäºçœŸå®ä¸–ç•Œè§‚å¯Ÿåˆ°çš„è½¯ä»¶ã€é…ç½®ã€éƒ¨ç½²æˆ–èµ„æºç›¸å…³æ•…éšœï¼Œå¹¶æ³¨å…¥åˆ°å®æ—¶çš„Kubernetesäº‘ç¯å¢ƒä¸­ã€‚",
    "source_type_quote": "...each incorporating a unique software, configuration, deployment, or resource-related fault observed in the real-world [3]â€“[5], [10]â€“[13], and injected in a live Kubernetes cloud environment.",
    "last_updated": "2025-12-26 (arXivç‰ˆæœ¬æ—¥æœŸ)",
    "last_updated_quote": "arXiv:2512.22113v1  [cs.DC]  26 Dec 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œç”±è®ºæ–‡ä½œè€…å›¢é˜Ÿç¼–è¯‘ã€‚",
    "build_type_quote": "PRAXIS is demonstrated on a set of 30 comprehensive real-world incidents that is being compiled into an RCA benchmark.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": "æ–‡ä¸­æåŠæ­£åœ¨å¼€æºè¿‡ç¨‹ä¸­ï¼Œä½†æœªæ˜ç¡®è®¸å¯è¯ã€‚",
    "dataset_license_quote": "...integrated into a comprehensive benchmark which is in the process of being open-sourced.",
    "task_granularity": "æ ¹å› è¯Šæ–­ï¼Œæ¶‰åŠä»å¾®æœåŠ¡çº§åˆ«åˆ°ä»£ç è¯­å¥çº§åˆ«çš„å¤šç²’åº¦åˆ†æã€‚",
    "task_granularity_quote": "PRAXIS uniquely constructs the PDG using hammock blocks at different granularities spanning module, class, function, and statement levels...",
    "evaluation_metrics": "æ ¹å› æ¨ç†å‡†ç¡®ç‡ï¼ˆ%ï¼‰ã€æ ¹å› è¯†åˆ«å‡†ç¡®ç‡ï¼ˆ%ï¼‰ã€æ¯æ¬¡æˆåŠŸè¯Šæ–­çš„ä»¤ç‰Œæ¶ˆè€—é‡ã€‚",
    "evaluation_metrics_quote": "PRAXIS achieved a root-cause reasoning accuracy of 61.5% and a root-cause identification accuracy of 73.9%... reduced token consumption by 3.8Ã— from 624.4k tokens to 166.5k tokens per successful diagnosis...",
    "input_modality": "å¤šæ¨¡æ€è¾“å…¥ï¼ŒåŒ…æ‹¬å¯è§‚æµ‹æ€§æ•°æ®ï¼ˆæ—¥å¿—ã€æŒ‡æ ‡ã€äº‹ä»¶ã€åˆ†å¸ƒå¼è¿½è¸ªï¼‰ã€è­¦æŠ¥ã€æœåŠ¡ä¾èµ–å›¾ï¼ˆSDGï¼‰å’Œç¨‹åºä¾èµ–å›¾ï¼ˆPDGï¼‰ã€‚",
    "input_modality_quote": "PRAXIS uses cloud monitoring tools... to actively collect a set of automated and user-specified alerts... and observability data consisting of cloud application and microservice logs, distributed traces of microservice calls, Kubernetes events, and application metrics.",
    "output_modality": "æ–‡æœ¬æŠ¥å‘Šï¼Œè¯¦ç»†è¯´æ˜æ•…éšœçš„èµ·æºã€ä¼ æ’­å’Œå½±å“ã€‚",
    "output_modality_quote": "...a final RCA report detailing the faultâ€™s origin, propagation, and impact.",
    "task_io_type": "å¤šæ¨¡æ€æ•°æ®ï¼ˆå›¾ã€æ—¥å¿—ã€è­¦æŠ¥ï¼‰åˆ°è¯Šæ–­æŠ¥å‘Šã€‚",
    "task_io_type_quote": "Upon triggering by active golden-signal alert(s), PRAXIS presents the relevant data... to an LLM... PRAXIS calls the LLM to consolidate the per-microservice investigation decision into a final RCA report...",
    "execution_environment": "å®æ—¶çš„Kubernetesäº‘ç¯å¢ƒã€‚",
    "execution_environment_quote": "...injected in a live Kubernetes cloud environment.",
    "unique_features": "ä¸“æ³¨äºç”±ä»£ç å’Œé…ç½®å¼•èµ·çš„äº‘æ•…éšœçš„æ ¹å› åˆ†æï¼ˆRCAï¼‰ã€‚åŸºå‡†åŒ…å«30ä¸ªçœŸå®ä¸–ç•Œåœºæ™¯ï¼Œå¹¶æ³¨å…¥åˆ°å®æ—¶Kubernetesç¯å¢ƒä¸­ã€‚å®ƒè¯„ä¼°ä»£ç†åœ¨è·¨æœåŠ¡ä¾èµ–å›¾ï¼ˆSDGï¼‰å’Œç¨‹åºä¾èµ–å›¾ï¼ˆPDGï¼‰çš„ç»“æ„åŒ–éå†ä¸­çš„è¡¨ç°ã€‚",
    "unique_features_quote": "Code-Cloud-RCA Benchmark consisting of 30 different scenarios and each incorporating a unique software, configuration, deployment, or resource-related fault observed in the real-world... and injected in a live Kubernetes cloud environment. PRAXIS employs an LLM-driven structured traversal over two types of graph: (1) a service dependency graph (SDG)... (2) a hammock-block program dependence graph (PDG)...",
    "data_size_quantity": 30,
    "data_size_unit": "ä¸ª",
    "last_updated_year": 2025,
    "last_updated_month": 12,
    "last_updated_day": 26,
    "language_normalized": "['æœªæ˜ç¡®æŒ‡å®š']",
    "dimension_normalized": "['æ ¹å› åˆ†æï¼ˆRCAï¼‰çš„å‡†ç¡®æ€§å’Œæ•ˆç‡', 'æ ¹å› æ¨ç†å‡†ç¡®æ€§', 'æ ¹å› è¯†åˆ«å‡†ç¡®æ€§']",
    "evaluation_method_normalized": "['æ ¹å› æ¨ç†å‡†ç¡®ç‡ï¼ˆ%ï¼‰', 'æ ¹å› è¯†åˆ«å‡†ç¡®ç‡ï¼ˆ%ï¼‰', 'æ¯æ¬¡æˆåŠŸè¯Šæ–­çš„ä»¤ç‰Œæ¶ˆè€—é‡']",
    "problem_domain_normalized": "['äº‘æœåŠ¡å¯é æ€§', 'æ•…éšœè¯Šæ–­', 'æ ¹å› åˆ†æï¼ˆRCAï¼‰', 'è½¯ä»¶å·¥ç¨‹ï¼ˆSWEï¼‰', 'ç«™ç‚¹å¯é æ€§å·¥ç¨‹ï¼ˆSREï¼‰']",
    "source_type_normalized": "['åŸºäºçœŸå®ä¸–ç•Œè§‚å¯Ÿåˆ°çš„è½¯ä»¶ã€é…ç½®ã€éƒ¨ç½²æˆ–èµ„æºç›¸å…³æ•…éšœ', 'å®æ—¶çš„Kubernetesäº‘ç¯å¢ƒ']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.20823_output/content.md",
    "benchmark_name": "NotSoTiny",
    "benchmark_name_quote": "this paper introduces NotSoTiny, a benchmark that assesses LLM on the generation of structurally rich and context-aware RTL.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "this paper introduces NotSoTiny, a benchmark... To address these limitations, this work presents NotSoTiny, a new benchmark...",
    "dataset_url": "https://huggingface.co/datasets/HPAI-BSC/NotSoTiny-25-12",
    "dataset_url_quote": "The dataset is publicly available on HuggingFace1. 1https://huggingface.co/datasets/HPAI-BSC/NotSoTiny-25-12",
    "task_description": "è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆç»“æ„å¤æ‚ã€ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„RTLï¼ˆå¯„å­˜å™¨ä¼ è¾“çº§ï¼‰ä»£ç æ–¹é¢çš„èƒ½åŠ›ï¼Œå…·ä½“ä»»åŠ¡ä¸ºæ¨¡å—è¡¥å…¨ã€‚",
    "task_description_quote": "NotSoTiny is a novel benchmark for systematically evaluating LLM capabilities in RTL code generation, focusing on module completion.",
    "dimension": "RTLä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§ã€ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›ã€å¯¹æ•°æ®æ±¡æŸ“çš„æŠµå¾¡èƒ½åŠ›",
    "dimension_quote": "assesses LLM on the generation of structurally rich and context-aware RTL... explicitly designed to be resilient to contamination.",
    "evaluation_method": "åŸºäºä»¿çœŸçš„æµ‹è¯•å°éªŒè¯å’ŒåŸºäºç­‰ä»·æ€§æ£€æŸ¥çš„å½¢å¼åŒ–éªŒè¯æŠ€æœ¯",
    "evaluation_method_quote": "Outputs generated by LLMs are then evaluated using both simulation-based and formal verification techniques.",
    "context_dependency": "å¤šæ¨¡å—ã€å±‚æ¬¡åŒ–ç³»ç»Ÿï¼Œéœ€è¦æ ¹æ®è®¾è®¡ä¸Šä¸‹æ–‡ï¼ˆå…¶ä»–æ¨¡å—å’Œé¡¶å±‚é€»è¾‘ï¼‰æ¨æ–­ç¼ºå¤±æ¨¡å—çš„åŠŸèƒ½å’Œæ¥å£",
    "context_dependency_quote": "NotSoTiny requires the model to infer functionality and interface behavior solely from the surrounding implementation. This mirrors real-world development scenarios where new components must integrate seamlessly into existing systems.",
    "problem_domain": "æ•°å­—ç¡¬ä»¶è®¾è®¡ã€RTLï¼ˆå¯„å­˜å™¨ä¼ è¾“çº§ï¼‰è®¾è®¡",
    "problem_domain_quote": "Built from hundreds of actual hardware designs produced by the Tiny Tapeout community... for evaluating LLMs on structurally complex, context-aware RTL code-generation tasks",
    "problem_difficulty": "å·¥ç¨‹çº§ã€å…·æœ‰ç°å®ä¸–ç•Œç¡¬ä»¶è®¾è®¡çš„å¤æ‚æ€§",
    "problem_difficulty_quote": "NotSoTiny tasks are more challenging than prior benchmarks... emphasizing its effectiveness in overcoming current limitations of LLMs applied to hardware design",
    "language": "Verilog",
    "language_quote": "translate high-level specifications... into Verilog... Each project is parsed to extract Verilog modules and testbenches",
    "data_size": "åŒ…å«1,114ä¸ªå»é‡åçš„æ¨¡å—è¡¥å…¨ä»»åŠ¡ï¼Œæºè‡ª1,021ä¸ªæœ‰æ•ˆçš„ç¡¬ä»¶è®¾è®¡é¡¹ç›®",
    "data_size_quote": "We introduce NotSoTiny, a dataset of 1,114 deduplicated module-completion tasks... These files serve as the foundation for our contextual module completion tasks for the total number of 1,021 projects.",
    "source_type": "æºè‡ªTiny Tapeoutç¤¾åŒºçš„æ•°ç™¾ä¸ªå®é™…ç¡¬ä»¶è®¾è®¡ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼€æ”¾çš„æ•™è‚²æ€§å¤šé¡¹ç›®æ™¶åœ†è®¡åˆ’",
    "source_type_quote": "Built from hundreds of actual hardware designs produced by the Tiny Tapeout community... The benchmark is built from data derived from the Tiny Tapeout project, a collaborative initiative that allows hardware designers to submit open-source digital circuit designs for fabrication",
    "last_updated": "2025-12-23 (arXivæäº¤æ—¥æœŸ)",
    "last_updated_quote": "arXiv:2512.20823v1 [cs.AR] 23 Dec 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œé€šè¿‡è‡ªåŠ¨åŒ–æµæ°´çº¿ä»å¼€æºç¤¾åŒºæ•°æ®ä¸­æ„å»º",
    "build_type_quote": "our automated pipeline removes duplicates, verifies correctness and periodically incorporates new designs... We develop an end-to-end construction pipeline, including crawling, merging, task generation, and near-duplicate filtering",
    "contamination_status": "æŠ—æ±¡æŸ“è®¾è®¡ï¼Œé€šè¿‡å®šæœŸçº³å…¥æ–°è®¾è®¡å’Œå»é‡æ¥ç¼“è§£æ±¡æŸ“é£é™©",
    "contamination_status_quote": "explicitly designed to be resilient to contamination... periodically incorporates new designs to mitigate contamination... Min-K analysis [19] further shows low contamination risk",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "æ¨¡å—è¡¥å…¨",
    "task_granularity_quote": "focusing on module completion... For each task, the body of a single module is removed and designated as the target for completion",
    "evaluation_metrics": "åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆåŸºäºå½¢å¼åŒ–ç­‰ä»·æ€§æ£€æŸ¥ï¼‰",
    "evaluation_metrics_quote": "state-of-the-art LLMs achieve only 20% functional correctness under formal equivalence",
    "input_modality": "ä»£ç ï¼ˆVerilogæ¨¡å—ä¸Šä¸‹æ–‡ï¼‰",
    "input_modality_quote": "the remaining modules in the file are retained as context... presents the model with a realistic design context",
    "output_modality": "ä»£ç ï¼ˆç¼ºå¤±çš„Verilogæ¨¡å—ï¼‰",
    "output_modality_quote": "asks it to produce the missing moduleâ€™s code",
    "task_io_type": "ä»£ç åˆ°ä»£ç ",
    "task_io_type_quote": "module completion... produce the missing moduleâ€™s code",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "1) è§„æ¨¡å¤§ä¸”ç»“æ„å¤æ‚ï¼Œæºè‡ªçœŸå®æµç‰‡è®¾è®¡ï¼›2) è¯„ä¼°æ–¹æ³•ä¸¥è°¨ï¼Œç»“åˆä»¿çœŸä¸å½¢å¼åŒ–éªŒè¯ï¼›3) å…·æœ‰æŠ—æ±¡æŸ“å’Œå®šæœŸæ›´æ–°ï¼ˆLivingï¼‰çš„ç‰¹æ€§ï¼Œä¸Tiny Tapeoutå‘å¸ƒè®¡åˆ’åŒæ­¥ã€‚",
    "unique_features_quote": "1) A large and structurally rich RTL benchmark. 2) A rigorous and scalable evaluation methodology. 3) A contamination-resilient, periodically updated benchmark pipeline.",
    "data_size_quantity": 1114,
    "data_size_unit": "ä¸ª",
    "last_updated_year": 2025,
    "last_updated_month": 12,
    "last_updated_day": 23,
    "language_normalized": "['Verilog']",
    "dimension_normalized": "['RTLä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§', 'ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›', 'å¯¹æ•°æ®æ±¡æŸ“çš„æŠµå¾¡èƒ½åŠ›']",
    "evaluation_method_normalized": "['åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆåŸºäºå½¢å¼åŒ–ç­‰ä»·æ€§æ£€æŸ¥ï¼‰']",
    "problem_domain_normalized": "['æ•°å­—ç¡¬ä»¶è®¾è®¡', 'RTLï¼ˆå¯„å­˜å™¨ä¼ è¾“çº§ï¼‰è®¾è®¡']",
    "source_type_normalized": "['æºè‡ªTiny Tapeoutç¤¾åŒºçš„æ•°ç™¾ä¸ªå®é™…ç¡¬ä»¶è®¾è®¡', 'è¿™æ˜¯ä¸€ä¸ªå¼€æ”¾çš„æ•™è‚²æ€§å¤šé¡¹ç›®æ™¶åœ†è®¡åˆ’']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç è¡¥å…¨",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2512.23631_output/content.md",
    "benchmark_name": "SWE-bench-Verified, SWE-bench-Live",
    "benchmark_name_quote": "On SWE-bench-Verified [25], a benchmark for software engineering tasks grounded in real GitHub issues. On SWE-bench-Live [30], which includes more recently collected issues and presents out-of-distribution challenges",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We evaluate our BOAD on SWE-bench-Verified [25], a benchmark for software engineering tasks grounded in real GitHub issues. On SWE-bench-Live [30], which includes more recently collected issues and presents out-of-distribution challenges",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è§£å†³ç°å®ä¸–ç•Œä¸­çš„è½¯ä»¶å·¥ç¨‹é—®é¢˜ï¼Œå…·ä½“æ˜¯è§£å†³GitHubä¸Šçš„çœŸå®issueã€‚",
    "task_description_quote": "We study the problem of using LLMs to resolve real-world GitHub issues, where each issue consists of a textual description and a corresponding code repository.",
    "dimension": "è½¯ä»¶å·¥ç¨‹ä»»åŠ¡è§£å†³èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯å¯¹é•¿è§†é‡ã€åˆ†å¸ƒå¤–é—®é¢˜çš„æ³›åŒ–èƒ½åŠ›ã€‚",
    "dimension_quote": "Large language models (LLMs) have shown strong reasoning and coding capabilities, yet they struggle to generalize to real-world software engineering (SWE) problems that are long-horizon and out of distribution.",
    "evaluation_method": "æäº¤è¡¥ä¸å¹¶é€šè¿‡æ‰€æœ‰æµ‹è¯•ã€‚",
    "evaluation_method_quote": "Rewards are sparse: r(st, at) = 0 for t < T, r(sT , aT ) = 1 if y passes all tests, 0 otherwise.",
    "context_dependency": "å¤šæ–‡ä»¶é¡¹ç›®ï¼Œéœ€è¦æµè§ˆå’Œä¿®æ”¹ä»£ç åº“ä¸­çš„ç›¸å…³éƒ¨åˆ†ã€‚",
    "context_dependency_quote": "Since issues are not self-contained, solving them requires identifying and modifying relevant parts of the codebase.",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ï¼Œæ¶‰åŠbugä¿®å¤ã€ä»£ç ä¿®æ”¹ç­‰ã€‚",
    "problem_domain_quote": "a benchmark for software engineering tasks grounded in real GitHub issues.",
    "problem_difficulty": "ç°å®ä¸–ç•Œã€é•¿è§†é‡ã€åˆ†å¸ƒå¤–çš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚",
    "problem_difficulty_quote": "real-world software engineering (SWE) problems that are long-horizon and out of distribution.",
    "language": "æ–‡ä¸­æœªæ˜ç¡®æåŠï¼Œä½†åŸºäºGitHub issuesï¼Œå¯èƒ½æ¶‰åŠå¤šç§ç¼–ç¨‹è¯­è¨€ã€‚",
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": "æ¥è‡ªçœŸå®GitHub issuesåŠå…¶å¯¹åº”çš„ä»£ç ä»“åº“ã€‚",
    "source_type_quote": "a benchmark for software engineering tasks grounded in real GitHub issues.",
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": "SWE-bench-LiveåŒ…å«æ–°è¿‘æ”¶é›†çš„ã€åˆ†å¸ƒå¤–çš„é—®é¢˜ï¼Œæ—¨åœ¨æµ‹è¯•æ³›åŒ–èƒ½åŠ›ã€‚",
    "contamination_status_quote": "On SWE-bench-Live [30], which includes more recently collected issues and presents out-of-distribution challenges",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ç«¯åˆ°ç«¯çš„è½¯ä»¶å·¥ç¨‹é—®é¢˜è§£å†³ï¼ŒåŒ…æ‹¬å®šä½ã€ç¼–è¾‘ã€éªŒè¯ç­‰å¤šä¸ªå­ä»»åŠ¡ã€‚",
    "task_granularity_quote": "Solving a software engineering task requires handling multiple sub-tasksâ€”such as locating relevant files, editing code, and running tests",
    "evaluation_metrics": "è¡¥ä¸é€šè¿‡ç‡ï¼ˆæ˜¯å¦é€šè¿‡æ‰€æœ‰æµ‹è¯•ï¼‰ã€‚",
    "evaluation_metrics_quote": "r(sT , aT ) = 1 if y passes all tests, 0 otherwise.",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆissueæè¿°ï¼‰å’Œä»£ç ï¼ˆä»£ç åº“ï¼‰ã€‚",
    "input_modality_quote": "each issue consists of a textual description and a corresponding code repository.",
    "output_modality": "ä»£ç è¡¥ä¸ã€‚",
    "output_modality_quote": "when the agent submits a patch y",
    "task_io_type": "æ–‡æœ¬ï¼ˆissueæè¿°ï¼‰å’Œä»£ç ï¼ˆä»£ç åº“ï¼‰åˆ°ä»£ç ï¼ˆè¡¥ä¸ï¼‰ã€‚",
    "task_io_type_quote": "using LLMs to resolve real-world GitHub issues, where each issue consists of a textual description and a corresponding code repository.",
    "execution_environment": "äº¤äº’å¼è¿è¡Œæ—¶ç¯å¢ƒï¼Œä»£ç†å¯ä»¥æµè§ˆæ–‡ä»¶ã€æ‰§è¡Œå‘½ä»¤ã€è¿è¡Œæµ‹è¯•ã€‚",
    "execution_environment_quote": "an LLM interacts with a runtime environment through tool use. Such agents can browse files, execute shell commands, run tests, and edit code directly",
    "unique_features": "åŸºäºçœŸå®GitHub issueï¼Œä»»åŠ¡éè‡ªåŒ…å«ï¼Œéœ€è¦ä¸ä»£ç åº“äº¤äº’ï¼Œæ˜¯é•¿è§†é‡ã€äº¤äº’å¼çš„ç«¯åˆ°ç«¯è½¯ä»¶å·¥ç¨‹ä»»åŠ¡åŸºå‡†ã€‚åŒ…å«å·²éªŒè¯çš„ï¼ˆVerifiedï¼‰å’ŒæŒç»­æ›´æ–°çš„ï¼ˆLiveï¼‰ä¸¤ä¸ªç‰ˆæœ¬ã€‚",
    "unique_features_quote": "a benchmark for software engineering tasks grounded in real GitHub issues. Since issues are not self-contained, solving them requires identifying and modifying relevant parts of the codebase. On SWE-bench-Live [30], which includes more recently collected issues and presents out-of-distribution challenges",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['å¤šç§ç¼–ç¨‹è¯­è¨€']",
    "dimension_normalized": "['è½¯ä»¶å·¥ç¨‹ä»»åŠ¡è§£å†³èƒ½åŠ›', 'é•¿è§†é‡', 'åˆ†å¸ƒå¤–é—®é¢˜çš„æ³›åŒ–èƒ½åŠ›']",
    "evaluation_method_normalized": "['è¡¥ä¸é€šè¿‡ç‡']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'bugä¿®å¤', 'ä»£ç ä¿®æ”¹']",
    "source_type_normalized": "['GitHub issues', 'ä»£ç ä»“åº“']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2512.24796_output/content.md",
    "benchmark_name": "LeanCat (Part I: 1-Categories)",
    "benchmark_name_quote": "LEANCAT: A BENCHMARK SUITE FOR FORMAL CATEGORY THEORY IN LEAN (PART I: 1-CATEGORIES)",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "we introduce LeanCat, a Lean benchmark for category-theoretic formalization",
    "dataset_url": "https://github.com/sciencraft/LeanCat",
    "dataset_url_quote": "LeanCat (Part I: 1-Categories) is open sourced at https://github.com/sciencraft/LeanCat.",
    "task_description": "è¯„æµ‹æ¨¡å‹åœ¨å½¢å¼åŒ–å®šç†è¯æ˜æ–¹é¢çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨èŒƒç•´è®ºè¿™ä¸€é«˜åº¦æŠ½è±¡çš„æ•°å­¦é¢†åŸŸå†…ï¼ŒåŸºäºæˆç†Ÿçš„Leanåº“ï¼ˆmathlibï¼‰è¿›è¡Œåº“é©±åŠ¨çš„ã€æŠ½è±¡æ¨ç†çš„èƒ½åŠ›ã€‚",
    "task_description_quote": "designed to test whether automated provers can operate inside a mature library and compose high-level abstractions rather than solve isolated puzzles.",
    "dimension": "å½¢å¼åŒ–å®šç†è¯æ˜èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åº“é©±åŠ¨çš„æŠ½è±¡æ¨ç†ã€æ¥å£çº§æ¨ç†ã€é•¿ç¨‹è§„åˆ’ä»¥åŠä»è‡ªç„¶è¯­è¨€åˆ°å½¢å¼åŒ–è¯­è¨€çš„è½¬æ¢èƒ½åŠ›ã€‚",
    "dimension_quote": "serving as a stress test of structural, interface-level reasoning... highlighting a pronounced natural-to-formal bottleneck.",
    "evaluation_method": "ä½¿ç”¨pass@kæŒ‡æ ‡è¿›è¡Œè¯„ä¼°ï¼Œæ–‡ä¸­æŠ¥å‘Šäº†pass@1å’Œpass@4çš„ç»“æœã€‚",
    "evaluation_method_quote": "The best model solves 8.25% of tasks at pass@1... and 12.00% at pass@4",
    "context_dependency": "è¯­å¥çº§ä»»åŠ¡ï¼Œæ¯ä¸ªé—®é¢˜æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„å®šç†å£°æ˜ï¼Œæ‰€æœ‰å¿…è¦çš„å®šä¹‰éƒ½å­˜åœ¨äºLeanç¯å¢ƒä¸­ï¼ˆåœ¨Mathlibä¸­æˆ–ä½œä¸ºé—®é¢˜è®¾ç½®çš„ä¸€éƒ¨åˆ†å¼•å…¥ï¼‰ã€‚",
    "context_dependency_quote": "Each LeanCat problem is self-contained at the statement level... all necessary definitions exist in Leanâ€™s environment... LeanCat is a statement-level benchmark: each item consists of a single standalone theorem to prove",
    "problem_domain": "èŒƒç•´è®ºï¼Œä¸€ä¸ªé«˜åº¦æŠ½è±¡çš„æ•°å­¦é¢†åŸŸï¼Œä½œä¸ºç°ä»£æ•°å­¦ç»“æ„çš„ç»Ÿä¸€è¯­è¨€å’Œè¯æ˜å·¥ç¨‹çš„æ ¸å¿ƒå±‚ã€‚",
    "problem_domain_quote": "a Lean benchmark for category-theoretic formalizationâ€”a unifying language for mathematical structure and a core layer of modern proof engineering",
    "problem_difficulty": "åˆ†ä¸ºä¸‰ä¸ªéš¾åº¦ç­‰çº§ï¼šç®€å•ã€ä¸­ç­‰ã€é«˜ã€‚å…·ä½“åˆ†å¸ƒä¸º20/42/38ä¸ªé—®é¢˜ã€‚",
    "problem_difficulty_quote": "Easy/Medium/High labels (20/42/38)",
    "language": "Lean 4 (å½¢å¼åŒ–è¯æ˜è¯­è¨€)ï¼Œä¾èµ–mathlibåº“ã€‚",
    "language_quote": "formalized in Lean 4 (mathlib)... a set of 100 category-theory problems formalized in Lean 4 (mathlib 4.19.0).",
    "data_size": "åŒ…å«100ä¸ªå®Œå…¨å½¢å¼åŒ–çš„èŒƒç•´è®ºå®šç†å£°æ˜ï¼ˆè¯­å¥çº§ä»»åŠ¡ï¼‰ã€‚",
    "data_size_quote": "LeanCat comprises 100 theorem statements in category theory, each fully formalized in Lean 4",
    "source_type": "é—®é¢˜ä¸»è¦æ¥æºäºæ ‡å‡†çš„èŒƒç•´è®ºæ•™ç§‘ä¹¦ï¼ˆå¦‚Riehl, Mac Laneï¼‰ã€ä¸“è‘—ï¼ˆå¦‚AdÃ¡mek et al.ï¼‰ï¼Œä»¥åŠå°‘é‡æ¥è‡ªæœªå‘è¡¨çš„è®²ä¹‰å’Œç ”ç©¶è®ºæ–‡ã€‚",
    "source_type_quote": "Problems are primarily drawn from standard, widely used textbooks in category theory... together with a small number of problems adapted from unpublished lecture notes... also include selected problems inspired by research papers and advanced community-driven references",
    "last_updated": "2025-12-31 (æ ¹æ®arXivç‰ˆæœ¬å·æ¨æ–­)",
    "last_updated_quote": "arXiv:2512.24796v1  [cs.LO]  31 Dec 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œç”±èŒƒç•´è®ºä¸“å®¶å’ŒLeanä¸“å®¶é€šè¿‡ä¸‰é˜¶æ®µå·¥ä½œæµç¨‹ï¼ˆæ”¶é›†ã€å½¢å¼åŒ–ã€è¯„å®¡ï¼‰ç²¾å¿ƒç­–åˆ’å’Œæ„å»ºã€‚",
    "build_type_quote": "LeanCat is curated through a three-stage workflow that combines expert selection, LLM-assisted drafting, and rigorous human verification",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "å®šç†è¯æ˜ï¼ˆè¯­å¥çº§ä»»åŠ¡ï¼‰ï¼Œå³ç»™å®šä¸€ä¸ªå½¢å¼åŒ–çš„å®šç†å£°æ˜ï¼Œè¦æ±‚ç”Ÿæˆå®Œæ•´çš„è¯æ˜ã€‚",
    "task_granularity_quote": "LeanCat is a statement-level benchmark: each item consists of a single standalone theorem to prove",
    "evaluation_metrics": "pass@1, pass@4",
    "evaluation_metrics_quote": "The best model solves 8.25% of tasks at pass@1... and 12.00% at pass@4",
    "input_modality": "è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°ï¼ˆä»¥LaTeXæ³¨é‡Šå½¢å¼æä¾›ï¼‰å’Œå½¢å¼åŒ–çš„Leanå®šç†å£°æ˜ã€‚",
    "input_modality_quote": "the natural-language problem description (in LATEX) is included as a comment immediately preceding the formal statement",
    "output_modality": "å½¢å¼åŒ–çš„Leanè¯æ˜ä»£ç ã€‚",
    "output_modality_quote": "generating compilable Lean code",
    "task_io_type": "è‡ªç„¶è¯­è¨€ä¸å½¢å¼åŒ–è¯­è¨€åˆ°å½¢å¼åŒ–è¯æ˜ä»£ç ",
    "task_io_type_quote": "highlighting a pronounced natural-to-formal bottleneck",
    "execution_environment": "Lean 4ç¼–è¯‘ç¯å¢ƒï¼Œä¾èµ–mathlib 4.19.0åº“ã€‚",
    "execution_environment_quote": "formalized in Lean 4 (mathlib 4.19.0)",
    "unique_features": "1. ä¸“æ³¨äºèŒƒç•´è®ºè¿™ä¸€é«˜åº¦æŠ½è±¡çš„æ•°å­¦é¢†åŸŸï¼Œä½œä¸ºåº“é©±åŠ¨æŠ½è±¡æ¨ç†çš„å‹åŠ›æµ‹è¯•ã€‚2. é—®é¢˜æœ‰æ—¶ä¼šè¶…è¶Šç°æœ‰åº“ï¼ˆmathlibï¼‰çš„èŒƒå›´ï¼Œè¿«ä½¿å½¢å¼åŒ–è€…è®¾è®¡ä¸­é—´ç»“æœï¼Œå¯¹è‡ªåŠ¨åŒ–è¯æ˜å™¨æ„æˆä¸¥æ ¼æµ‹è¯•ã€‚3. é‡‡ç”¨ç»“åˆLLMè¾…åŠ©å’Œäººç±»ä¸“å®¶è¯„çº§çš„ç³»ç»ŸåŒ–éš¾åº¦æ ‡æ³¨æµç¨‹ã€‚4. æ˜¯ç³»åˆ—åŸºå‡†çš„ç¬¬ä¸€éƒ¨åˆ†ï¼ˆ1-èŒƒç•´ï¼‰ï¼Œè®¡åˆ’æœªæ¥æ‰©å±•åˆ°æ›´ä¸°å¯Œçš„ç»“æ„ã€‚",
    "unique_features_quote": "This aspect of LeanCat, that it sometimes goes beyond the library, makes it a particularly stringent test for automated provers... We implemented a systematic LLM+human rating pipeline to grade problem difficulty... LeanCat is the first installment of a category-theory benchmark series.",
    "data_size_quantity": 100,
    "data_size_unit": "ä¸ª",
    "last_updated_year": 2025,
    "last_updated_month": 12,
    "last_updated_day": 31,
    "language_normalized": "['Lean 4']",
    "dimension_normalized": "['å½¢å¼åŒ–å®šç†è¯æ˜èƒ½åŠ›', 'åº“é©±åŠ¨çš„æŠ½è±¡æ¨ç†', 'æ¥å£çº§æ¨ç†', 'é•¿ç¨‹è§„åˆ’', 'ä»è‡ªç„¶è¯­è¨€åˆ°å½¢å¼åŒ–è¯­è¨€çš„è½¬æ¢èƒ½åŠ›']",
    "evaluation_method_normalized": "['pass@1', 'pass@4']",
    "problem_domain_normalized": "['èŒƒç•´è®º']",
    "source_type_normalized": "['æ ‡å‡†çš„èŒƒç•´è®ºæ•™ç§‘ä¹¦', 'ä¸“è‘—', 'æœªå‘è¡¨çš„è®²ä¹‰', 'ç ”ç©¶è®ºæ–‡']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.24630_output/content.md",
    "benchmark_name": "AIDev-POP",
    "benchmark_name_quote": "We used the AIDev-POP subset of version v3 of the AIDev dataset [25]",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Motivated by these gaps, we answer the following three research questions by analyzing a set of AI-generated performance-related pull requests as provided by the publicly available AIDev dataset [25].",
    "dataset_url": "https://github.com/SQMLab/LLM-performance",
    "dataset_url_quote": "To enable replication, we share our data and code publicly.1 1https://github.com/SQMLab/LLM-performance",
    "task_description": "è¯¥æ•°æ®é›†æ—¨åœ¨åˆ†æAIä»£ç†åœ¨çœŸå®è½¯ä»¶å¼€å‘å·¥ä½œæµä¸­ç”Ÿæˆçš„ã€ä¸æ€§èƒ½ä¼˜åŒ–ç›¸å…³çš„Pull Requestsï¼ˆPRsï¼‰ï¼Œä»¥ç ”ç©¶AIä»£ç†å¦‚ä½•è§£å†³æ€§èƒ½é—®é¢˜ã€‚",
    "task_description_quote": "In this paper, we present an empirical study of performance-related pull requests generated by AI agents.",
    "dimension": "AIä»£ç†çš„æ€§èƒ½ä¼˜åŒ–è¡Œä¸ºã€PRæ¥å—ç‡ã€è¯„å®¡æ—¶é—´ã€ä¸è½¯ä»¶å¼€å‘ç”Ÿå‘½å‘¨æœŸï¼ˆSDLCï¼‰æ´»åŠ¨çš„å…³è”",
    "dimension_quote": "Our findings provide empirical evidence that can support the evaluation and improvement of agentic AI systems with respect to their performance optimization behaviors and review outcomes.",
    "evaluation_method": "åŸºäºBERTopicçš„ä¸»é¢˜å»ºæ¨¡ã€ç»Ÿè®¡åˆ†æï¼ˆWilcoxonç§©å’Œæ£€éªŒã€Cliff's deltaæ•ˆåº”é‡ã€Kendall's Ï„ç›¸å…³ç³»æ•°ï¼‰ã€äººå·¥æ ‡æ³¨ä¸ä¸€è‡´æ€§æ£€éªŒ",
    "evaluation_method_quote": "We applied the BERTopic [16] due to its superior performance over traditional methods... In this paper, we use the non-parametric Wilcoxon rank-sum test, Cliffâ€™s delta effect size, and Kendallâ€™s ğœ correlation coefficients...",
    "context_dependency": "çœŸå®ä¸–ç•Œè½¯ä»¶é¡¹ç›®çš„Pull Requestä¸Šä¸‹æ–‡",
    "context_dependency_quote": "We used the AIDev-POP subset... which comprises 33,596 agentic pull requests... captures higher-quality, real-world projects",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ã€æ€§èƒ½ä¼˜åŒ–",
    "problem_domain_quote": "Performance, Pull Request, Agentic AI, LLM, BERTopic",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": NaN,
    "language_quote": NaN,
    "data_size": "åŒ…å«33,596ä¸ªAIä»£ç†ç”Ÿæˆçš„Pull Requestsï¼Œä»ä¸­è¯†åˆ«å‡º1,221ä¸ªä¸æ€§èƒ½ç›¸å…³çš„PRsï¼Œæ¶‰åŠ447ä¸ªä»£ç ä»“åº“ã€‚",
    "data_size_quote": "We used the AIDev-POP subset of version v3 of the AIDev dataset [25]... which comprises 33,596 agentic pull requests... This process identified 1,160 performance-related PRs across 447 repositories... yielding a final dataset of 1,221 performance PRs",
    "source_type": "æ¥è‡ªAIDevæ•°æ®é›†v3ç‰ˆæœ¬çš„AIDev-POPå­é›†ï¼Œè¯¥å­é›†åŒ…å«æ¥è‡ªçœŸå®ä¸–ç•Œé¡¹ç›®çš„AIä»£ç†ç”Ÿæˆçš„Pull Requestsã€‚",
    "source_type_quote": "We used the AIDev-POP subset of version v3 of the AIDev dataset [25], obtained from Zenodo [24]",
    "last_updated": "2025",
    "last_updated_quote": "arXiv:2512.24630v1  [cs.SE]  31 Dec 2025",
    "build_type": "ç¤¾åŒºè´¡çŒ®ï¼ˆæ¥è‡ªAIDevæ•°æ®é›†ï¼‰",
    "build_type_quote": "analyzing a set of AI-generated performance-related pull requests as provided by the publicly available AIDev dataset [25]",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "Pull Requestçº§åˆ«çš„åˆ†æ",
    "task_granularity_quote": "an empirical study of performance-related pull requests generated by AI agents",
    "evaluation_metrics": "PRæ‹’ç»ç‡ã€åˆå¹¶æ—¶é—´ã€ä¸»é¢˜åˆ†å¸ƒã€ä¸SDLCæ´»åŠ¨çš„å…³è”",
    "evaluation_metrics_quote": "We investigate whether this trend also applies to performance-related PRs... analyze their time to merge... Association with SDLC Activities",
    "input_modality": "Pull Requestçš„æ ‡é¢˜å’Œæ­£æ–‡æ–‡æœ¬",
    "input_modality_quote": "Combining the title and body of a PR, we generated embeddings",
    "output_modality": "æ€§èƒ½ä¼˜åŒ–ä¸»é¢˜ç±»åˆ«ã€ç»Ÿè®¡ç»“æœ",
    "output_modality_quote": "We identified 52 performance-related topics grouped into 10 higher-level categories... Our results show that AI agents apply performance optimizations across diverse layers of the software stack",
    "task_io_type": "æ–‡æœ¬åˆ°åˆ†æ",
    "task_io_type_quote": "Using LLM-assisted detection and BERTopic-based topic modeling, we identified 52 performance-related topics",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºAIä»£ç†åœ¨çœŸå®ä¸–ç•Œå¼€å‘å·¥ä½œæµä¸­ç”Ÿæˆçš„æ€§èƒ½ç›¸å…³Pull Requestsï¼Œè€Œéå—æ§ç¯å¢ƒä¸‹çš„ä»£ç ç”Ÿæˆç»“æœã€‚æ•°æ®é›†åŒ…å«PRå…ƒæ•°æ®ï¼ˆæ¥å—çŠ¶æ€ã€è¯„å®¡æ—¶é—´ã€SDLCæ´»åŠ¨ï¼‰ã€‚",
    "unique_features_quote": "However, existing studies largely rely on outcome-based evaluations conducted in controlled settings [1, 12], providing limited insight into how agentic AI systems address performance concerns within real-world development workflows... it provides the metadata required for RQ2 and RQ3, including PR acceptance status, review timing, and SDLC activities.",
    "data_size_quantity": 33596,
    "data_size_unit": "ä¸ªAIä»£ç†ç”Ÿæˆçš„Pull Requests",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['AIä»£ç†çš„æ€§èƒ½ä¼˜åŒ–è¡Œä¸º', 'PRæ¥å—ç‡', 'è¯„å®¡æ—¶é—´', 'ä¸è½¯ä»¶å¼€å‘ç”Ÿå‘½å‘¨æœŸï¼ˆSDLCï¼‰æ´»åŠ¨çš„å…³è”']",
    "evaluation_method_normalized": "['PRæ‹’ç»ç‡', 'åˆå¹¶æ—¶é—´', 'ä¸»é¢˜åˆ†å¸ƒ', 'ä¸SDLCæ´»åŠ¨çš„å…³è”']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'æ€§èƒ½ä¼˜åŒ–']",
    "source_type_normalized": "['æ¥è‡ªAIDevæ•°æ®é›†v3ç‰ˆæœ¬çš„AIDev-POPå­é›†ï¼Œè¯¥å­é›†åŒ…å«æ¥è‡ªçœŸå®ä¸–ç•Œé¡¹ç›®çš„AIä»£ç†ç”Ÿæˆçš„Pull Requestsã€‚']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.00635_output/content.md",
    "benchmark_name": "SEMODS",
    "benchmark_name_quote": "To address this gap, we present SEMODS: an SE-focused dataset of 3,427 models extracted from HF, combining automated collection with rigorous validation through manual annotation and large language model assistance.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "To address this gap, we present SEMODS: an SE-focused dataset of 3,427 models extracted from HF...",
    "dataset_url": "Zenodo [7] (A snapshot of the dataset (November 2025 release), containing 3,427 SE models and their associated attributes, is publicly available in Zenodo [7].)",
    "dataset_url_quote": "Data availability: A snapshot of the dataset (November 2025 release), containing 3,427 SE models and their associated attributes, is publicly available in Zenodo [7].",
    "task_description": "è¯¥æ•°æ®é›†æ—¨åœ¨ä¸ºè½¯ä»¶å·¥ç¨‹é¢†åŸŸæä¾›ä¸€ä¸ªç»è¿‡éªŒè¯çš„ã€ä¸“æ³¨äºSEä»»åŠ¡çš„é¢„è®­ç»ƒæ¨¡å‹ç›®å½•ï¼Œä»¥æ”¯æŒæ¨¡å‹å‘ç°ã€åˆ†æã€åŸºå‡†æµ‹è¯•å’Œæ¨¡å‹é€‚åº”ã€‚",
    "task_description_quote": "...a dataset of SE models, systematically collected, processed, and validated to support their in-depth analysis, efficient discovery, and practical use within SE contexts. The dataset enables researchers and practitioners to explore models tailored to specific SE tasks... while also supporting benchmarking, and facilitating the identification of models for model adaptation.",
    "dimension": "æ¨¡å‹ä¸è½¯ä»¶å·¥ç¨‹ä»»åŠ¡çš„å…³è”æ€§ã€æ¨¡å‹å…ƒæ•°æ®ï¼ˆå¦‚è®¸å¯è¯ã€åº“ã€è®­ç»ƒæ•°æ®é›†ï¼‰ã€æ¨¡å‹è¯„ä¼°ç»“æœçš„æ ‡å‡†åŒ–è¡¨ç¤ºã€‚",
    "dimension_quote": "Our dataset links models to SE tasks and activities from the software development lifecycle, offering a standardized representation of their evaluation results...",
    "evaluation_method": "æ–‡ä¸­æœªæ˜ç¡®æè¿°é’ˆå¯¹è¯¥æ•°æ®é›†æœ¬èº«çš„è¯„ä¼°æ–¹æ³•ã€‚è¯¥æ•°æ®é›†æœ¬èº«æ˜¯ä¸€ä¸ªæ¨¡å‹ç›®å½•ï¼Œå…¶ä»·å€¼åœ¨äºç»„ç»‡å’Œæ ‡å‡†åŒ–æ¨¡å‹ä¿¡æ¯ï¼Œè€Œéç›´æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚",
    "evaluation_method_quote": NaN,
    "context_dependency": "ä¸é€‚ç”¨ã€‚è¯¥æ•°æ®é›†æ˜¯æ¨¡å‹å…ƒæ•°æ®ç›®å½•ï¼Œä¸æ¶‰åŠä»£ç ç”Ÿæˆæˆ–ä¿®å¤çš„ä¸Šä¸‹æ–‡ã€‚",
    "context_dependency_quote": NaN,
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ï¼ˆæ¶µç›–éœ€æ±‚å·¥ç¨‹ã€è½¯ä»¶è®¾è®¡ã€è½¯ä»¶å®ç°ã€è½¯ä»¶è´¨é‡ä¿è¯ã€è½¯ä»¶ç»´æŠ¤äº”ä¸ªç”Ÿå‘½å‘¨æœŸé˜¶æ®µï¼‰ã€‚",
    "problem_domain_quote": "...catalogued according to SE activities and tasks across the SDLC.",
    "problem_difficulty": "ä¸é€‚ç”¨ã€‚è¯¥æ•°æ®é›†æ˜¯æ¨¡å‹ç›®å½•ï¼Œä¸åŒ…å«ç¼–ç¨‹é—®é¢˜ã€‚",
    "problem_difficulty_quote": NaN,
    "language": "ä¸é€‚ç”¨ã€‚è¯¥æ•°æ®é›†æ”¶å½•çš„æ¨¡å‹å¯èƒ½æ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€ï¼Œä½†æ•°æ®é›†æœ¬èº«ä¸é™å®šè¯­è¨€ã€‚",
    "language_quote": NaN,
    "data_size": "åŒ…å«3,427ä¸ªä¸è½¯ä»¶å·¥ç¨‹ç›¸å…³çš„æ¨¡å‹ã€‚",
    "data_size_quote": "...an SE-focused dataset of 3,427 models extracted from HF...",
    "source_type": "ä»Hugging Faceå¹³å°é€šè¿‡å…¶APIç³»ç»Ÿæ”¶é›†çš„æ¨¡å‹ï¼Œç»“åˆäº†æ‰‹åŠ¨æ ‡æ³¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹è¾…åŠ©çš„ä¸¥æ ¼éªŒè¯ã€‚",
    "source_type_quote": "...3,427 models extracted from HF, combining automated collection with rigorous validation through manual annotation and large language model assistance.",
    "last_updated": "2025å¹´11æœˆï¼ˆæ•°æ®é›†å¿«ç…§ç‰ˆæœ¬ï¼‰ï¼Œè®ºæ–‡å‘è¡¨äº2026å¹´ã€‚",
    "last_updated_quote": "Data availability: A snapshot of the dataset (November 2025 release)...",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆç”±ç ”ç©¶å›¢é˜Ÿæ„å»ºå’Œç»´æŠ¤ï¼‰ã€‚",
    "build_type_quote": "To address this gap, we present SEMODS...",
    "contamination_status": "æ–‡ä¸­æœªæåŠã€‚",
    "contamination_status_quote": NaN,
    "dataset_license": "æ–‡ä¸­æœªæ˜ç¡®æåŠæ•°æ®é›†çš„è®¸å¯è¯ã€‚",
    "dataset_license_quote": NaN,
    "task_granularity": "ä¸é€‚ç”¨ã€‚è¯¥æ•°æ®é›†çš„ä»»åŠ¡æ˜¯æ¨¡å‹åˆ†ç±»å’Œç¼–ç›®ï¼Œè€Œéä»£ç ç”Ÿæˆç­‰ä»»åŠ¡ã€‚",
    "task_granularity_quote": NaN,
    "evaluation_metrics": "ä¸é€‚ç”¨ã€‚è¯¥æ•°æ®é›†æœ¬èº«ä¸å®šä¹‰è¯„ä¼°æŒ‡æ ‡ï¼Œè€Œæ˜¯æ ‡å‡†åŒ–å­˜å‚¨æ¨¡å‹è‡ªå¸¦çš„è¯„ä¼°ç»“æœï¼ˆå¦‚åŸºå‡†æµ‹è¯•åˆ†æ•°ï¼‰ã€‚",
    "evaluation_metrics_quote": NaN,
    "input_modality": "ä¸é€‚ç”¨ã€‚è¯¥æ•°æ®é›†çš„è¾“å…¥æ˜¯æ¨¡å‹å…ƒæ•°æ®ã€‚",
    "input_modality_quote": NaN,
    "output_modality": "ä¸é€‚ç”¨ã€‚è¯¥æ•°æ®é›†çš„è¾“å‡ºæ˜¯ç»“æ„åŒ–çš„æ¨¡å‹ç›®å½•ä¿¡æ¯ã€‚",
    "output_modality_quote": NaN,
    "task_io_type": "ä¸é€‚ç”¨ã€‚è¯¥æ•°æ®é›†æ˜¯ç”¨äºæ¨¡å‹å‘ç°å’Œåˆ†æçš„èµ„æºç›®å½•ï¼Œè€Œéä¸€ä¸ªå…·æœ‰è¾“å…¥è¾“å‡ºå®šä¹‰çš„ä»»åŠ¡ã€‚",
    "task_io_type_quote": NaN,
    "execution_environment": "ä¸é€‚ç”¨ã€‚è¯¥æ•°æ®é›†ä¸æ¶‰åŠä»£ç æ‰§è¡Œã€‚",
    "execution_environment_quote": NaN,
    "unique_features": "1. ä¸“æ³¨äºè½¯ä»¶å·¥ç¨‹é¢†åŸŸçš„æ¨¡å‹ç›®å½•ã€‚2. å°†æ¨¡å‹æ˜ å°„åˆ°è½¯ä»¶å¼€å‘ç”Ÿå‘½å‘¨æœŸä¸­çš„å…·ä½“æ´»åŠ¨å’Œä»»åŠ¡ï¼ˆåŸºäºåŒ…å«147ä¸ªä»»åŠ¡çš„åˆ†ç±»æ³•ï¼‰ã€‚3. æä¾›äº†æ¨¡å‹è¯„ä¼°ç»“æœçš„æ ‡å‡†åŒ–è¡¨ç¤ºã€‚4. åŒ…å«è‡ªåŠ¨åŒ–æµç¨‹ä»¥ä¿æŒæ•°æ®é›†ä¸Hugging Faceæ–°æ¨¡å‹çš„åŒæ­¥æ›´æ–°ã€‚5. æ•°æ®é›†è®¾è®¡åŒ…å«ä¸“é—¨çš„å®ä½“ï¼ˆå¦‚SETask, Benchmark, Metricï¼‰ä»¥æ”¯æŒSEæ¨¡å‹çš„å¼€å‘å’Œè¯„ä¼°ã€‚",
    "unique_features_quote": "Compared to prior work, we have specifically curated and validated models with explicit relevance to SE, introducing novel mappings to SE tasks and standardized benchmarking data. ... This design introduces novel entities specific to SE models (e.g., SETask) and their standardized evaluation tables (e.g., Benchmark, Metric), enabling the development and assessment of SE models.",
    "data_size_quantity": 3427,
    "data_size_unit": "ä¸ª",
    "last_updated_year": 2025,
    "last_updated_month": 11,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['æ¨¡å‹ä¸è½¯ä»¶å·¥ç¨‹ä»»åŠ¡çš„å…³è”æ€§', 'æ¨¡å‹å…ƒæ•°æ®ï¼ˆå¦‚è®¸å¯è¯ã€åº“ã€è®­ç»ƒæ•°æ®é›†ï¼‰', 'æ¨¡å‹è¯„ä¼°ç»“æœçš„æ ‡å‡†åŒ–è¡¨ç¤º']",
    "evaluation_method_normalized": "[]",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹ï¼ˆæ¶µç›–éœ€æ±‚å·¥ç¨‹ã€è½¯ä»¶è®¾è®¡ã€è½¯ä»¶å®ç°ã€è½¯ä»¶è´¨é‡ä¿è¯ã€è½¯ä»¶ç»´æŠ¤äº”ä¸ªç”Ÿå‘½å‘¨æœŸé˜¶æ®µï¼‰']",
    "source_type_normalized": "['ä»Hugging Faceå¹³å°é€šè¿‡å…¶APIç³»ç»Ÿæ”¶é›†çš„æ¨¡å‹', 'ç»“åˆäº†æ‰‹åŠ¨æ ‡æ³¨å’Œå¤§å‹è¯­è¨€æ¨¡å‹è¾…åŠ©çš„ä¸¥æ ¼éªŒè¯']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.00559_output/content.md",
    "benchmark_name": "oHC/IoTB dataset å’Œ Mutation dataset",
    "benchmark_name_quote": "assessing their performance on both the original openHAB (oHC/IoTB) dataset and a structurally challenging Mutation dataset designed to test robustness under rule transformations.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "This work presents the first comprehensive evaluation of Large Language Models (LLMs) across a multi-category interaction threat taxonomy, assessing their performance on both the original openHAB (oHC/IoTB) dataset and a structurally challenging Mutation dataset designed to test robustness under rule transformations.",
    "dataset_url": "https://github.com/JasonQuantrill/llm-v-static-results",
    "dataset_url_quote": "The data pertaining to this research can be found at https://github.com/JasonQuantrill/llm-v-static-results",
    "task_description": "æ£€æµ‹æ™ºèƒ½å®¶å±…IoTå¹³å°ï¼ˆå¦‚openHABï¼‰ä¸­åŸºäºTrigger-Action-Condition (TAC)è§„åˆ™çš„è‡ªåŠ¨åŒ–è§„åˆ™ä¹‹é—´çš„äº¤äº’å¨èƒï¼ˆRule Interaction Threats, RITsï¼‰ã€‚è¿™äº›å¨èƒæ˜¯ç”±äºè§„åˆ™é—´çš„éšå¼ä¾èµ–ã€å†²çªè§¦å‘æˆ–é‡å æ¡ä»¶å¯¼è‡´çš„æ„å¤–æˆ–ä¸å®‰å…¨è¡Œä¸ºã€‚",
    "task_description_quote": "Identifying these threats requires semantic understanding and structural reasoning that traditionally depend on symbolic, constraint-driven static analysis. ... RITs occur when the interplay of multiple rules leads to unintended behaviors, such as security vulnerabilities or functional failures.",
    "dimension": "è¯­ä¹‰ç†è§£èƒ½åŠ›ã€è·¨è§„åˆ™çš„ç»“æ„åŒ–æ¨ç†èƒ½åŠ›ã€åœ¨è§„åˆ™å˜æ¢ä¸‹çš„é²æ£’æ€§",
    "dimension_quote": "assessing their performance on both the original openHAB (oHC/IoTB) dataset and a structurally challenging Mutation dataset designed to test robustness under rule transformations. ... their accuracy degrades significantly for threats requiring cross-rule structural reasoning, especially under mutated rule forms.",
    "evaluation_method": "å°†LLMçš„æ£€æµ‹ç»“æœä¸æ‰‹åŠ¨éªŒè¯çš„åŸºå‡†çœŸå€¼ï¼ˆoHIT's manually validated ground truthï¼‰è¿›è¡Œæ¯”è¾ƒï¼Œè¯„ä¼°å…¶å‡†ç¡®æ€§ã€‚",
    "evaluation_method_quote": "comparing their results against oHITâ€™s manually validated ground truth.",
    "context_dependency": "å¤šè§„åˆ™äº¤äº’ã€‚éœ€è¦åˆ†æå¤šä¸ªTACè§„åˆ™ä¹‹é—´çš„è¯­ä¹‰å’Œç»“æ„å…³ç³»ã€‚",
    "context_dependency_quote": "Detecting these threats is challenging, as it requires a deep understanding of both the semantic and structural relationships between rules.",
    "problem_domain": "æ™ºèƒ½å®¶å±…ç‰©è”ç½‘ï¼ˆIoTï¼‰å®‰å…¨ã€è‡ªåŠ¨åŒ–è§„åˆ™åˆ†æ",
    "problem_domain_quote": "Cracking IoT Security: Can LLMs Outsmart Static Analysis Tools? ... Smart home IoT platforms such as openHAB rely on Triggerâ€“Actionâ€“Condition (TAC) rules to automate device behavior",
    "problem_difficulty": "å·¥ç¨‹çº§ï¼Œæ¶‰åŠçœŸå®ä¸–ç•Œæ™ºèƒ½å®¶å±…è®¾ç½®ä¸­çš„å¤æ‚è§„åˆ™äº¤äº’ã€‚",
    "problem_difficulty_quote": "many arise simply from complex rule interplay in real-world smart home setups.",
    "language": "Xtendï¼ˆä¸€ç§åŸºäºJavaçš„é¢†åŸŸç‰¹å®šè¯­è¨€ï¼Œç”¨äºç¼–å†™openHABè§„åˆ™ï¼‰",
    "language_quote": "These rules are authored in the Xtend programming language [3], which offers a more streamlined and expressive syntax compared to standard Java",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": "åŸºäºçœŸå®openHABå¹³å°ï¼ˆoHC/IoTBæ•°æ®é›†ï¼‰ä»¥åŠé€šè¿‡è§„åˆ™å˜æ¢ç”Ÿæˆçš„çªå˜æ•°æ®é›†ï¼ˆMutation datasetï¼‰ã€‚",
    "source_type_quote": "assessing their performance on both the original openHAB (oHC/IoTB) dataset and a structurally challenging Mutation dataset designed to test robustness under rule transformations.",
    "last_updated": "2026ï¼ˆæ ¹æ®arXivç‰ˆæœ¬æ—¥æœŸæ¨æ–­ï¼‰",
    "last_updated_quote": "arXiv:2601.00559v1  [cs.CR]  2 Jan 2026",
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "è§„åˆ™äº¤äº’å¨èƒåˆ†ç±»ï¼ˆå¤šæ ‡ç­¾åˆ†ç±»ï¼‰ï¼ŒåŒ…æ‹¬ç»†ç²’åº¦ç±»åˆ«åŒºåˆ†ã€‚",
    "task_granularity_quote": "In this work, we evaluate a representative set of state-of-the-art LLMs across all experimental conditions, including multi-label RIT classification, fine-grained category differentiation",
    "evaluation_metrics": "å‡†ç¡®æ€§ï¼ˆä¸åŸºå‡†çœŸå€¼æ¯”è¾ƒï¼‰",
    "evaluation_metrics_quote": "comparing their results against oHITâ€™s manually validated ground truth.",
    "input_modality": "ä»£ç ï¼ˆXtendè¯­è¨€ç¼–å†™çš„TACè§„åˆ™ï¼‰",
    "input_modality_quote": "These rules are authored in the Xtend programming language",
    "output_modality": "åˆ†ç±»æ ‡ç­¾ï¼ˆå¨èƒç±»åˆ«ï¼Œå¦‚Action Contradiction, Trigger Cascade, Condition CascadeåŠå…¶å¼ºå¼±å­ç±»ï¼‰",
    "output_modality_quote": "multi-label RIT classification, fine-grained category differentiation",
    "task_io_type": "ä»£ç åˆ°åˆ†ç±»",
    "task_io_type_quote": "assessing their performance on ... classifying Rule Interaction Threats (RITs) in real-world openHAB datasets",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "1. ä¸“æ³¨äºIoTè‡ªåŠ¨åŒ–è§„åˆ™ï¼ˆTACè§„åˆ™ï¼‰çš„å®‰å…¨å¨èƒæ£€æµ‹ã€‚2. åŒ…å«ä¸€ä¸ªç”¨äºæµ‹è¯•é²æ£’æ€§çš„â€œçªå˜æ•°æ®é›†â€ï¼ˆMutation datasetï¼‰ï¼Œè¯¥æ•°æ®é›†é€šè¿‡è§„åˆ™å˜æ¢ç”Ÿæˆï¼Œæ—¨åœ¨æµ‹è¯•æ¨¡å‹åœ¨ç»“æ„æ‰°åŠ¨ä¸‹çš„è¡¨ç°ã€‚3. å¨èƒåˆ†ç±»åŸºäºä¸€ä¸ªæ•´åˆçš„ã€å¤šç±»åˆ«çš„äº¤äº’å¨èƒåˆ†ç±»æ³•ï¼ŒåŒ…æ‹¬åŠ¨ä½œçŸ›ç›¾ï¼ˆACï¼‰ã€è§¦å‘çº§è”ï¼ˆTCï¼‰å’Œæ¡ä»¶çº§è”ï¼ˆCCï¼‰ï¼Œæ¯ç§åˆæœ‰å¼ºå¼±å­ç±»ã€‚",
    "unique_features_quote": "a structurally challenging Mutation dataset designed to test robustness under rule transformations. ... we consolidated these perspectives into three foundational categories of rule interaction threats. ... each can manifest in different forms depending on how rules depend on, trigger, or contradict one another.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2026,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Xtend']",
    "dimension_normalized": "['è¯­ä¹‰ç†è§£èƒ½åŠ›', 'è·¨è§„åˆ™çš„ç»“æ„åŒ–æ¨ç†èƒ½åŠ›', 'åœ¨è§„åˆ™å˜æ¢ä¸‹çš„é²æ£’æ€§']",
    "evaluation_method_normalized": "['å‡†ç¡®æ€§']",
    "problem_domain_normalized": "['æ™ºèƒ½å®¶å±…ç‰©è”ç½‘å®‰å…¨', 'è‡ªåŠ¨åŒ–è§„åˆ™åˆ†æ']",
    "source_type_normalized": "['åŸºäºçœŸå®openHABå¹³å°', 'é€šè¿‡è§„åˆ™å˜æ¢ç”Ÿæˆçš„çªå˜æ•°æ®é›†']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "JSON",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.00753_output/content.md",
    "benchmark_name": "AIDev dataset",
    "benchmark_name_quote": "We use the AIDev dataset v1.0 [1]: 33,707 agent-authored PRs from 2,807 repositories (>100 stars), identified via AIDev metadata (type='Bot') plus generative agent names (Codex, Claude, Devin, Copilot), excluding deterministic bots.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We use the AIDev dataset v1.0 [1]: 33,707 agent-authored PRs from 2,807 repositories (>100 stars), identified via AIDev metadata (type='Bot') plus generative agent names (Codex, Claude, Devin, Copilot), excluding deterministic bots.",
    "dataset_url": "https://zenodo.org/records/17993901",
    "dataset_url_quote": "Artifact: https://zenodo.org/records/17993901.",
    "task_description": "è¯¥æ•°æ®é›†ç”¨äºç ”ç©¶AIä»£ç†ç”Ÿæˆçš„Pull Requestï¼ˆPRï¼‰çš„å®¡æŸ¥å·¥ä½œé‡é¢„æµ‹å’Œä»£ç†â€œå¹½çµåŒ–â€ï¼ˆghostingï¼‰è¡Œä¸ºã€‚",
    "task_description_quote": "This paradigm shift raises a critical question: can we predict which agent-generated PRs will consume excessive review effort before any human interaction begins? ... We operationalize agentic ghosting and quantify its prevalence.",
    "dimension": "AIä»£ç†ç”Ÿæˆçš„Pull Requestçš„å®¡æŸ¥å·¥ä½œé‡é¢„æµ‹ã€ä»£ç†â€œå¹½çµåŒ–â€ï¼ˆæ”¾å¼ƒå“åº”ï¼‰è¡Œä¸ºåˆ†æ",
    "dimension_quote": "Research Questions. RQ1: Can creation-time structural signals predict high-effort PRs? RQ2: Which early cues correlate with agentic ghosting?",
    "evaluation_method": "ä½¿ç”¨AUCï¼ˆArea Under the Curveï¼‰å’ŒPR-AUCï¼ˆPrecision-Recall AUCï¼‰è¯„ä¼°äºŒå…ƒåˆ†ç±»æ¨¡å‹æ€§èƒ½ï¼Œä»¥é¢„æµ‹é«˜æˆæœ¬PRï¼ˆå®¡æŸ¥å·¥ä½œé‡å‰20%ï¼‰ã€‚",
    "evaluation_method_quote": "LightGBM model reaches AUC 0.9571 [0.955, 0.962] (temporal split) with PR-AUC 0.8812... We benchmarked LightGBM against 5 alternatives (Stacking, Voting, HistGradient, MLP); the best (Stacking) matches LightGBM at AUC 0.957 (temporal) and 0.834 (repo-disjoint)...",
    "context_dependency": "åŸºäºPull Requestçš„ä¸Šä¸‹æ–‡ï¼ŒåŒ…æ‹¬ä»£ç å˜æ›´ã€æ–‡ä»¶ç±»å‹ã€PRæè¿°ç­‰ã€‚",
    "context_dependency_quote": "We extract 35 features across Intent, Context, and Complexity at two stages: T0 (Creation-Time) captures signals available at PR submission (Complexity: additions, deletions, changed files, entropy; Intent: body length, has plan; Context: language, agent, file types)...",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ã€ä»£ç å®¡æŸ¥ã€AIä»£ç†åä½œ",
    "problem_domain_quote": "As autonomous AI agents transition from code completion tools to full-fledged teammates capable of opening pull requests (PRs) at scale, software maintainers face a new challenge: not just reviewing code, but managing complex interaction loops with non-human contributors.",
    "problem_difficulty": "å·¥ç¨‹çº§ï¼Œæ¶‰åŠçœŸå®ä¸–ç•ŒGitHubä»“åº“ä¸­AIä»£ç†æäº¤çš„Pull Requestçš„å®¡æŸ¥è¿‡ç¨‹ã€‚",
    "problem_difficulty_quote": "Analyzing 33,707 agent-authored PRs from the AIDev dataset [1] across 2,807 repositories... We target review effort (comment/review volume) rather than latency (time-to-merge); this choice reflects maintainer attention cost directly.",
    "language": "æ–‡ä¸­æœªæ˜ç¡®æåŠæ•°æ®é›†æ¶µç›–çš„å…·ä½“ç¼–ç¨‹è¯­è¨€ï¼Œä»…ä½œä¸ºç‰¹å¾ä¹‹ä¸€ï¼ˆContext: languageï¼‰ã€‚",
    "language_quote": "We extract 35 features across Intent, Context, and Complexity at two stages: T0 (Creation-Time) captures signals available at PR submission (... Context: language, agent, file types)...",
    "data_size": "åŒ…å«33,707ä¸ªç”±AIä»£ç†ç”Ÿæˆçš„Pull Requestï¼Œæ¥è‡ª2,807ä¸ªä»“åº“ï¼ˆæ˜Ÿæ ‡æ•°>100ï¼‰ã€‚",
    "data_size_quote": "We use the AIDev dataset v1.0 [1]: 33,707 agent-authored PRs from 2,807 repositories (>100 stars)...",
    "source_type": "æ¥è‡ªAIDevæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ”¶é›†äº†GitHubä¸Šç”±AIä»£ç†ï¼ˆå¦‚Codex, Claude, Devin, Copilotï¼‰ç”Ÿæˆçš„Pull Requestã€‚",
    "source_type_quote": "We use the AIDev dataset v1.0 [1]: 33,707 agent-authored PRs from 2,807 repositories (>100 stars), identified via AIDev metadata (type='Bot') plus generative agent names (Codex, Claude, Devin, Copilot), excluding deterministic bots.",
    "last_updated": "v1.0ï¼ˆç‰ˆæœ¬å·ï¼‰ï¼Œå…·ä½“å¹´ä»½æœªæ˜ç¡®æåŠã€‚",
    "last_updated_quote": "We use the AIDev dataset v1.0 [1]...",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆAIDevæ•°æ®é›†ï¼‰",
    "build_type_quote": "We use the AIDev dataset v1.0 [1]...",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "Pull Requestçº§åˆ«çš„å®¡æŸ¥å·¥ä½œé‡é¢„æµ‹ä¸è¡Œä¸ºåˆ†æ",
    "task_granularity_quote": "We develop a Circuit Breaker triage model that predicts high-review-effort PRs (top 20% by effort score) at creation time.",
    "evaluation_metrics": "AUC, PR-AUC, Brier Score, Effort Coverageï¼ˆå®¡æŸ¥å·¥ä½œé‡è¦†ç›–ç‡ï¼‰",
    "evaluation_metrics_quote": "LightGBM model reaches AUC 0.9571 [0.955, 0.962] (temporal split) with PR-AUC 0.8812... The model is well-calibrated after Platt Scaling (Brier Score [19] = 0.1279); at 20% budget: 67% coverage...",
    "input_modality": "Pull Requestçš„å…ƒæ•°æ®å’Œä»£ç å˜æ›´ç‰¹å¾ï¼ˆå¦‚è¡¥ä¸å¤§å°ã€ä¿®æ”¹æ–‡ä»¶æ•°ã€æ–‡ä»¶ç±»å‹ã€PRæ­£æ–‡é•¿åº¦ã€æ˜¯å¦æœ‰è®¡åˆ’ç­‰ï¼‰",
    "input_modality_quote": "We extract 35 features across Intent, Context, and Complexity at two stages: T0 (Creation-Time) captures signals available at PR submission (Complexity: additions, deletions, changed files, entropy; Intent: body length, has plan; Context: language, agent, file types)...",
    "output_modality": "äºŒå…ƒåˆ†ç±»æ ‡ç­¾ï¼ˆé«˜æˆæœ¬PRä¸å¦ï¼‰æˆ–ä»£ç†å¹½çµåŒ–é¢„æµ‹",
    "output_modality_quote": "We frame triage as binary classification targeting High Cost PRs (top 20% by effort score = total review + comment count including human and bot messages)... Target Definition: High Cost - Top 20% of PRs by Effort Score...",
    "task_io_type": "Pull Requestç‰¹å¾åˆ°å®¡æŸ¥å·¥ä½œé‡/è¡Œä¸ºé¢„æµ‹",
    "task_io_type_quote": "We develop a Circuit Breaker triage model that predicts high-review-effort PRs (top 20% by effort score) at creation time.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºAIä»£ç†ï¼ˆè€Œéäººç±»ï¼‰ç”Ÿæˆçš„Pull Requestï¼›ç ”ç©¶â€œä»£ç†å¹½çµåŒ–â€ï¼ˆagentic ghostingï¼‰ç°è±¡ï¼Œå³ä»£ç†æäº¤å˜æ›´åæ— å“åº”åœ°æ”¾å¼ƒï¼›æ­ç¤ºAIä»£ç†PRçš„â€œåŒå³°è¡Œä¸ºæ¨¡å¼â€ï¼šå³æ—¶åˆå¹¶ï¼ˆ28.3%ï¼‰ä¸è¿­ä»£å¤±è´¥ã€‚",
    "unique_features_quote": "Analyzing 33,707 agent-authored PRs from the AIDev dataset [1] across 2,807 repositories, we uncover a striking two-regime behavioral pattern that fundamentally distinguishes autonomous agents from human developers... We observe substantial rates of agentic ghosting-abandonment without explanation-where agents submit changes but fail to respond to feedback.",
    "data_size_quantity": 33707,
    "data_size_unit": "ä¸ª",
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['AIä»£ç†ç”Ÿæˆçš„Pull Requestçš„å®¡æŸ¥å·¥ä½œé‡é¢„æµ‹', 'ä»£ç†â€œå¹½çµåŒ–â€ï¼ˆæ”¾å¼ƒå“åº”ï¼‰è¡Œä¸ºåˆ†æ']",
    "evaluation_method_normalized": "['AUC', 'PR-AUC', 'Brier Score', 'Effort Coverageï¼ˆå®¡æŸ¥å·¥ä½œé‡è¦†ç›–ç‡ï¼‰']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'ä»£ç å®¡æŸ¥', 'AIä»£ç†åä½œ']",
    "source_type_normalized": "['æ¥è‡ªAIDevæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ”¶é›†äº†GitHubä¸Šç”±AIä»£ç†ï¼ˆå¦‚Codex, Claude, Devin, Copilotï¼‰ç”Ÿæˆçš„Pull Requestã€‚']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.00482_output/content.md",
    "benchmark_name": "Co-renameBench",
    "benchmark_name_quote": "â€¢ Benchmark. We create Co-renameBench, a new dataset of post-2025 coordinated renaming to enable uncontaminated evaluation of future LLM-based tools.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "â€¢ Benchmark. We create Co-renameBench, a new dataset of post-2025 coordinated renaming to enable uncontaminated evaluation of future LLM-based tools.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„æµ‹åŸºå‡†æ—¨åœ¨è¯„ä¼°è‡ªåŠ¨åŒ–å·¥å…·ï¼ˆç‰¹åˆ«æ˜¯åŸºäºLLMçš„ä»£ç†ï¼‰åœ¨ä»£ç é‡æ„ä»»åŠ¡ä¸­æ‰§è¡Œåè°ƒé‡å‘½åï¼ˆcoordinated renamingï¼‰çš„èƒ½åŠ›ã€‚åè°ƒé‡å‘½åæ˜¯æŒ‡å¯¹ä¸€ä¸ªæ ‡è¯†ç¬¦ï¼ˆå¦‚ç±»åã€æ–¹æ³•åï¼‰çš„é‡æ„ä¼šè§¦å‘å¯¹å¤šä¸ªè¯­ä¹‰ç›¸å…³çš„ç¨‹åºå…ƒç´ è¿›è¡Œç±»ä¼¼çš„é‡å‘½åæ“ä½œï¼Œä»¥ä¿æŒæ¦‚å¿µä¸€è‡´æ€§ã€‚",
    "task_description_quote": "Coordinated renaming, where a single rename refactoring triggers refactorings in multiple, related identifiers, is a frequent yet challenging task.",
    "dimension": "åè°ƒé‡å‘½åçš„å‡†ç¡®æ€§ä¸å®Œæ•´æ€§",
    "dimension_quote": "To compare with the state-of-the-art approaches [10], we first tested on the established RenasBench [10], a benchmark containing 1349 renames across 161 co-rename sets. On this benchmark, CoRenameAgent achieves an F1-score of 54.6%, a 2.3Ã— improvement over the best baseline.",
    "evaluation_method": "ä½¿ç”¨F1åˆ†æ•°ï¼ˆç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡æ•°ï¼‰è¿›è¡Œè¯„ä¼°",
    "evaluation_method_quote": "On this benchmark, CoRenameAgent achieves an F1-score of 54.6%, a 2.3Ã— improvement over the best baseline.",
    "context_dependency": "å¤šæ–‡ä»¶é¡¹ç›®",
    "context_dependency_quote": "Developers must manually propagate these rename refactorings across numerous files and contexts, a process that is both tedious and highly error-prone.",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ï¼Œä»£ç é‡æ„",
    "problem_domain_quote": "In this paper, we answer such questions in the context of coordinated refactoring, where a single refactoring triggers a set of similar refactorings on semantically related program elements.",
    "problem_difficulty": "å·¥ç¨‹çº§ï¼Œæ¶‰åŠè·¨å¤šä¸ªæ–‡ä»¶å’Œä¸Šä¸‹æ–‡çš„å¤æ‚è¯­ä¹‰å…³è”",
    "problem_difficulty_quote": "Coordinated renaming is a frequent yet challenging task. Developers must manually propagate these rename refactorings across numerous files and contexts, a process that is both tedious and highly error-prone.",
    "language": "Java",
    "language_quote": "Finally, to validate its real-world utility, we used it to recommend other co-renames starting from single renames in active open-source projects. Using CoRenameAgent-generated renames, we submitted 10 pull requests for active open-source projects; their developers have already accepted and merged 5, while 2 were rejected for socio-technical reasons and 3 are still under review.",
    "data_size": "åŒ…å«1573ä¸ªè¿‘æœŸé‡å‘½åæ“ä½œçš„æ–°åŸºå‡†",
    "data_size_quote": "Second, on our new, uncontaminated benchmark of 1573 recent renames, it demonstrates a 3.1Ã— F1-score improvement.",
    "source_type": "ä»å¼€æºé¡¹ç›®çš„çœŸå®æäº¤ï¼ˆcommitsï¼‰ä¸­æ”¶é›†",
    "source_type_quote": "We first conducted a formative study on the practice of coordinated renaming in 609K commits in 100 open-source projects and surveyed 205 developers, and then we implemented these ideas into CoRenameAgent.",
    "last_updated": "2025å¹´ä¹‹å",
    "last_updated_quote": "â€¢ Benchmark. We create Co-renameBench, a new dataset of post-2025 coordinated renaming to enable uncontaminated evaluation of future LLM-based tools.",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "â€¢ Benchmark. We create Co-renameBench, a new dataset of post-2025 coordinated renaming to enable uncontaminated evaluation of future LLM-based tools.",
    "contamination_status": "æŠ—æ±¡æŸ“è®¾è®¡ï¼Œä¸“é—¨æ”¶é›†2025å¹´åçš„æ•°æ®ä»¥é¿å…LLMè®­ç»ƒæ•°æ®æ±¡æŸ“",
    "contamination_status_quote": "Thus, we curated a new, uncontaminated benchmark of recent co-renames. On this benchmark, CoRenameAgent achieves an F1-score of 48.5%, showing a 3.1Ã— improvement over the previous state-of-the-art.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç é‡æ„ï¼ˆé‡å‘½åï¼‰",
    "task_granularity_quote": "Coordinated renaming, where a single rename refactoring triggers refactorings in multiple, related identifiers, is a frequent yet challenging task.",
    "evaluation_metrics": "F1åˆ†æ•°",
    "evaluation_metrics_quote": "On this benchmark, CoRenameAgent achieves an F1-score of 54.6%, a 2.3Ã— improvement over the best baseline.",
    "input_modality": "ä»£ç ï¼ˆåˆå§‹é‡å‘½åæ“ä½œï¼‰",
    "input_modality_quote": "A developer initiates in the IDE the desired design change (such as renaming a class) that serves as a signal of refactoring intent.",
    "output_modality": "ä»£ç ï¼ˆä¸€ç»„åè°ƒçš„é‡å‘½åæ“ä½œï¼‰",
    "output_modality_quote": "Coordinated renaming, where a single rename refactoring triggers refactorings in multiple, related identifiers",
    "task_io_type": "ä»£ç åˆ°ä»£ç ",
    "task_io_type_quote": "Coordinated renaming, where a single rename refactoring triggers refactorings in multiple, related identifiers",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "1. ä¸“é—¨é’ˆå¯¹åè°ƒé‡å‘½åï¼ˆcoordinated renamingï¼‰è¿™ä¸€ç‰¹å®šä¸”å¤æ‚çš„é‡æ„ä»»åŠ¡ã€‚2. æ•°æ®æ”¶é›†æ—¶é—´åœ¨2025å¹´ä¹‹åï¼Œæ—¨åœ¨é¿å…LLMè®­ç»ƒæ•°æ®æ±¡æŸ“ï¼Œä¸ºæœªæ¥å·¥å…·æä¾›å¹²å‡€çš„è¯„ä¼°åŸºå‡†ã€‚3. åŸºäºå¯¹609Kæ¬¡æäº¤å’Œ100ä¸ªå¼€æºé¡¹ç›®çš„å®è¯ç ”ç©¶æ„å»ºã€‚",
    "unique_features_quote": "â€¢ Benchmark. We create Co-renameBench, a new dataset of post-2025 coordinated renaming to enable uncontaminated evaluation of future LLM-based tools.",
    "data_size_quantity": 1573,
    "data_size_unit": "ä¸ª",
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Java']",
    "dimension_normalized": "['åè°ƒé‡å‘½åçš„å‡†ç¡®æ€§ä¸å®Œæ•´æ€§']",
    "evaluation_method_normalized": "['F1åˆ†æ•°']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'ä»£ç é‡æ„']",
    "source_type_normalized": "['ä»å¼€æºé¡¹ç›®çš„çœŸå®æäº¤ï¼ˆcommitsï¼‰ä¸­æ”¶é›†']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç é‡æ„",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2601.02941_output/content.md",
    "benchmark_name": "SASTBENCH",
    "benchmark_name_quote": "We introduce SASTBENCH, a benchmark for evaluating SAST triage agents...",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "In this paper, we introduce SASTBENCH, a new benchmark designed specifically to evaluate the ability of LLM agents to classify SAST findings.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°LLMæ™ºèƒ½ä½“å¯¹é™æ€åº”ç”¨å®‰å…¨æµ‹è¯•ï¼ˆSASTï¼‰å·¥å…·å‘ç°çš„åˆ†ç±»èƒ½åŠ›ï¼Œå³åŒºåˆ†çœŸå®æ¼æ´ï¼ˆçœŸé˜³æ€§ï¼‰ä¸è¯¯æŠ¥ï¼ˆå‡é˜³æ€§ï¼‰ã€‚",
    "task_description_quote": "...evaluating SAST triage agents... to classify SAST findings... distinguish true vulnerabilities from false alarms.",
    "dimension": "SASTè¯¯æŠ¥åˆ†ç±»çš„å‡†ç¡®æ€§ã€æ™ºèƒ½ä½“åœ¨çœŸå®SASTå‘ç°åˆ†å¸ƒä¸‹çš„æ€§èƒ½",
    "dimension_quote": "...evaluate the ability of LLM agents to classify SAST findings... aims to minimize the simulation-reality gap prevalent in existing benchmarks.",
    "evaluation_method": "æ™ºèƒ½ä½“åœ¨éš”ç¦»ç¯å¢ƒä¸­è®¿é—®ä»£ç åº“ï¼Œæ¥æ”¶æ½œåœ¨æ¼æ´ä»£ç ä½ç½®åˆ—è¡¨ï¼Œè¿”å›äºŒå…ƒåˆ¤æ–­ï¼ˆçœŸé˜³æ€§æˆ–å‡é˜³æ€§ï¼‰ï¼ŒåŸºäºå…¶é¢„æµ‹è¿›è¡Œè¯„ä¼°ã€‚",
    "evaluation_method_quote": "The agent is loaded into an isolated environment with a target repository at a static path, and it is given a list of potentially vulnerable code sites, required to return a binary verdict: true positive or false positive. This design allows SASTBENCH to serve as a neutral testing ground for comparing diverse agentic design choices...",
    "context_dependency": "å¤šæ–‡ä»¶é¡¹ç›®çº§ä¸Šä¸‹æ–‡ï¼Œæ™ºèƒ½ä½“æ‹¥æœ‰å¯¹ä»£ç åº“çš„å®Œå…¨è®¿é—®æƒé™ä»¥è¿›è¡Œåˆ†æã€‚",
    "context_dependency_quote": "Agents have full access to the codebase and full freedom to explore the environment...",
    "problem_domain": "ç½‘ç»œå®‰å…¨ã€æ¼æ´åˆ†æã€é™æ€åº”ç”¨å®‰å…¨æµ‹è¯•ï¼ˆSASTï¼‰",
    "problem_domain_quote": "SAST (Static Application Security Testing) tools are among the most widely used techniques in defensive cybersecurity... a benchmark for evaluating SAST triage agents...",
    "problem_difficulty": "å·¥ç¨‹çº§ã€å¯¹æŠ—æ€§åˆ†å¸ƒï¼Œæ—¨åœ¨æ¨¡æ‹ŸçœŸå®ä¸–ç•Œä¸­SASTå·¥å…·äº§ç”Ÿçš„éš¾ä»¥åˆ†ç±»çš„å‘ç°ã€‚",
    "problem_difficulty_quote": "...samples from DSAST make classification harder by design, as they are selected by their confusing, apparent vulnerability to SAST tools... forms a hard negative class.",
    "language": "å¤šç§ç¼–ç¨‹è¯­è¨€ï¼Œæ—¨åœ¨åæ˜ ç°å®ä¸–ç•Œä¸­çš„è¯­è¨€åˆ†å¸ƒï¼Œç‰¹åˆ«åŒ…å«å¦‚PHPã€Goã€Javascript/TypeScriptç­‰åœ¨ç°ä»£ç›®æ ‡ç³»ç»Ÿå’Œæ˜“å—æ”»å‡»ç³»ç»Ÿä¸­å¸¸è§ä½†å…¶ä»–åŸºå‡†å¸¸å¿½ç•¥çš„è¯­è¨€ã€‚",
    "language_quote": "Language Diversity: Datasets often concentrate on 1-4 programming languages. This does not reflect the language distribution in the wild... most datasets donâ€™t contain PHP, though it is one of the languages with most vulnerabilities and most SAST findings...",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": "çœŸå®æ•°æ®ã€‚çœŸé˜³æ€§æ¥æºäºå›½å®¶æ¼æ´æ•°æ®åº“ï¼ˆNVDï¼‰ä¸­å¼•ç”¨GitHubæäº¤çš„CVEï¼›å‡é˜³æ€§æ¥æºäºåœ¨åŒ…å«CVEçš„ä»£ç åº“ä¸Šè¿è¡Œå¼€æºSASTå·¥å…·ï¼ˆsemgrepï¼‰çš„å‘ç°ã€‚",
    "source_type_quote": "True Positives (Vulnerabilities). We mine Common Vulnerabilities and Exposures (CVEs) from the National Vulnerability Database (NVD) that reference commits on GitHub... False Positives (SAST Findings). We execute a popular open-source SAST tool â€“ semgrepâ€™s free edition â€“ on the pre-fix versions of the repositories containing our curated CVEs.",
    "last_updated": "2026å¹´1æœˆï¼ˆè®ºæ–‡ç‰ˆæœ¬æ—¥æœŸï¼‰ï¼ŒåŸºå‡†è®¾è®¡æ”¯æŒæŒç»­æ›´æ–°ã€‚",
    "last_updated_quote": "arXiv:2601.02941v1 [cs.CR] 6 Jan 2026... the automated nature of our benchmark enables continuous updates...",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œç”±è®ºæ–‡ä½œè€…æ„å»ºå¹¶å‘å¸ƒã€‚",
    "build_type_quote": "We introduce SASTBENCH... We open-source our code and data...",
    "contamination_status": "æŠ—æ±¡æŸ“è®¾è®¡ï¼Œé€šè¿‡è®¾ç½®çŸ¥è¯†æˆªæ­¢æ—¥æœŸï¼ˆå¦‚2025å¹´2æœˆï¼‰æ¥ç­›é€‰CVEï¼Œä»¥é¿å…æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸­è§è¿‡è¿™äº›æ¼æ´ã€‚",
    "contamination_status_quote": "To avoid contamination, we keep CVEs only if they were reported after a knowledge cutoff period. In the instantiation used in this paper, we set it to February 2025, which is after the knowledge cutoff of all the models tested herein.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç åˆ†ç±»ï¼ˆå…·ä½“ä¸ºå®‰å…¨å‘ç°åˆ†ç±»ï¼‰",
    "task_granularity_quote": "...classify SAST findings... return a binary verdict: true positive or false positive.",
    "evaluation_metrics": "ç²¾ç¡®åº¦å’Œå¬å›ç‡",
    "evaluation_metrics_quote": "We find that stronger models tend to perform better in terms of both precision and recall...",
    "input_modality": "ä»£ç ä¸å…ƒæ•°æ®ï¼ˆä»£ç åº“è®¿é—®æƒé™ã€æ½œåœ¨æ¼æ´ä½ç½®åˆ—è¡¨ã€æ–‡ä»¶è·¯å¾„ã€è¡Œå·ã€CWE IDç­‰ï¼‰",
    "input_modality_quote": "The agent is loaded into an isolated environment with a target repository... and it is given a list of potentially vulnerable code sites... Each entry is a list of key-value dictionaries. Keys are detailed in Table 2.",
    "output_modality": "åˆ†ç±»æ ‡ç­¾ï¼ˆäºŒå…ƒåˆ¤æ–­ï¼‰",
    "output_modality_quote": "...required to return a binary verdict: true positive or false positive.",
    "task_io_type": "ä»£ç ä¸å…ƒæ•°æ®åˆ°åˆ†ç±»æ ‡ç­¾",
    "task_io_type_quote": "Agents have full access to the codebase... given a list of potentially vulnerable code sites, required to return a binary verdict...",
    "execution_environment": "éš”ç¦»çš„Dockerç¯å¢ƒï¼ŒåŒ…å«ç›®æ ‡ä»£ç åº“ã€‚",
    "execution_environment_quote": "The agent is loaded into an isolated environment with a target repository at a static path... Competitors submit their agent as an arbitrary ZIP with a Dockerfile...",
    "unique_features": "1. ä¸“ä¸ºSASTè¯¯æŠ¥åˆ†ç±»ï¼ˆè€Œéé€šç”¨æ¼æ´æ£€æµ‹ï¼‰è®¾è®¡ã€‚2. ç»“åˆçœŸå®CVEä½œä¸ºçœŸé˜³æ€§å’Œè¿‡æ»¤åçš„SASTå·¥å…·å‘ç°ä½œä¸ºè¿‘ä¼¼å‡é˜³æ€§ã€‚3. æ™ºèƒ½ä½“æ— å…³çš„è®¾è®¡ï¼Œæ¨¡æ‹ŸçœŸå®æ™ºèƒ½ä½“å·¥ä½œæµã€‚4. æ³¨é‡è¯­è¨€å¤šæ ·æ€§å’ŒçœŸå®çš„æ•°æ®åˆ†å¸ƒã€‚5. é‡‡ç”¨ç‰ˆæœ¬æ§åˆ¶ï¼ˆå¦‚SASTBENCH-v0.1ï¼‰å’Œå¯é…ç½®çš„çŸ¥è¯†æˆªæ­¢æ—¥æœŸä»¥æ”¯æŒæŒç»­æ›´æ–°å’ŒæŠ—æ±¡æŸ“ã€‚",
    "unique_features_quote": "SASTBENCH aims to minimize the simulation-reality gap... combines real CVEs as true positives with filtered SAST tool findings as approximate false positives. SASTBENCH features an agent-agnostic design... following agentic benchmarks like SWE-Bench... To avoid contamination, we keep CVEs only if they were reported after a knowledge cutoff period... we use version numbers to help us keep track of changes... SASTBENCH-v0.1.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2026,
    "last_updated_month": 1,
    "last_updated_day": null,
    "language_normalized": "['å¤šç§ç¼–ç¨‹è¯­è¨€', 'PHP', 'Go', 'Javascript', 'TypeScript']",
    "dimension_normalized": "['SASTè¯¯æŠ¥åˆ†ç±»çš„å‡†ç¡®æ€§', 'æ™ºèƒ½ä½“åœ¨çœŸå®SASTå‘ç°åˆ†å¸ƒä¸‹çš„æ€§èƒ½']",
    "evaluation_method_normalized": "['ç²¾ç¡®åº¦', 'å¬å›ç‡']",
    "problem_domain_normalized": "['ç½‘ç»œå®‰å…¨', 'æ¼æ´åˆ†æ', 'é™æ€åº”ç”¨å®‰å…¨æµ‹è¯•ï¼ˆSASTï¼‰']",
    "source_type_normalized": "['çœŸå®æ•°æ®', 'å›½å®¶æ¼æ´æ•°æ®åº“ï¼ˆNVDï¼‰', 'GitHubæäº¤', 'CVE', 'å¼€æºSASTå·¥å…·ï¼ˆsemgrepï¼‰']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2601.02736_output/content.md",
    "benchmark_name": "AIOps 2022 dataset",
    "benchmark_name_quote": "Preliminary experiments on the AIOps 2022 dataset demonstrate that SpecRCA achieves superior accuracy and efficiency compared to existing approaches...",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We conduct preliminary experiments on the AIOps 2022 dataset.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "å¾®æœåŠ¡ç³»ç»Ÿçš„æ ¹å› åˆ†æï¼ˆRCAï¼‰ï¼Œæ—¨åœ¨ä»å¼‚å¸¸çš„ç³»ç»Ÿä¿¡å·ï¼ˆæŒ‡æ ‡ã€è¿½è¸ªã€æ—¥å¿—ï¼‰ä¸­å®šä½æ•…éšœæºå¤´å¹¶è§£é‡Šå…¶æœºåˆ¶ã€‚",
    "task_description_quote": "Ensuring system reliability therefore hinges on effective root cause analysis (RCA), which entails not only localizing the source of anomalies but also characterizing the underlying failures in a timely and interpretable manner.",
    "dimension": "æ•…éšœå®šä½çš„å‡†ç¡®æ€§å’Œè¯Šæ–­æ¨ç†çš„æ•ˆç‡",
    "dimension_quote": "SpecRCA achieves superior accuracy and efficiency compared to existing approaches...",
    "evaluation_method": "åœ¨æ•°æ®é›†ä¸Šè¿›è¡Œå®éªŒï¼Œæ¯”è¾ƒæ•…éšœå®šä½çš„å‡†ç¡®æ€§å’Œç”Ÿæˆè¯Šæ–­æŠ¥å‘Šçš„æ—¶é—´ï¼ˆå»¶è¿Ÿï¼‰ã€‚",
    "evaluation_method_quote": "Preliminary experiments on the AIOps 2022 dataset demonstrate that SpecRCA achieves superior accuracy and efficiency compared to existing approaches... In terms of accuracy, SpecRCA surpasses the state-of-the-art methods in failure localization by approximately 12.14%. In terms of efficiency, SpecRCA is able to generate a complete diagnosis report within 20 seconds...",
    "context_dependency": "å¾®æœåŠ¡ç³»ç»Ÿä¸Šä¸‹æ–‡ï¼Œä¾èµ–æœåŠ¡æ‹“æ‰‘ã€æ—¶åºæŒ‡æ ‡ã€åˆ†å¸ƒå¼è¿½è¸ªå’Œæ—¥å¿—åºåˆ—ã€‚",
    "context_dependency_quote": "Upon detecting an anomaly in a microservice system, the framework collects abnormal operational data and a preceding segment of normal data (metrics, traces, and logs) to establish a behavioral baseline.",
    "problem_domain": "äº‘è®¡ç®—ã€å¾®æœåŠ¡ã€ç³»ç»Ÿè¿ç»´ï¼ˆAIOpsï¼‰",
    "problem_domain_quote": "Microservice systems have become the backbone of cloud-native enterprise applications... Ensuring system reliability therefore hinges on effective root cause analysis (RCA)...",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": NaN,
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": "2022",
    "last_updated_quote": "Preliminary experiments on the AIOps 2022 dataset...",
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "æ ¹å› åˆ†æï¼ˆæ•…éšœå®šä½ä¸è§£é‡Šï¼‰",
    "task_granularity_quote": "...root cause analysis (RCA), which entails not only localizing the source of anomalies but also characterizing the underlying failures...",
    "evaluation_metrics": "æ•…éšœå®šä½å‡†ç¡®ç‡ï¼ˆç™¾åˆ†æ¯”æå‡ï¼‰å’Œè¯Šæ–­æŠ¥å‘Šç”Ÿæˆå»¶è¿Ÿï¼ˆç§’ï¼‰",
    "evaluation_metrics_quote": "In terms of accuracy, SpecRCA surpasses the state-of-the-art methods in failure localization by approximately 12.14%... SpecRCA is able to generate a complete diagnosis report within 20 seconds...",
    "input_modality": "å¤šæ¨¡æ€ç³»ç»Ÿæ•°æ®ï¼šæŒ‡æ ‡ï¼ˆMetricsï¼‰ã€è¿½è¸ªï¼ˆTracesï¼‰ã€æ—¥å¿—ï¼ˆLogsï¼‰",
    "input_modality_quote": "...collects abnormal operational data and a preceding segment of normal data (metrics, traces, and logs)...",
    "output_modality": "è¯Šæ–­æŠ¥å‘Šï¼ˆè‡ªç„¶è¯­è¨€è§£é‡Šï¼‰",
    "output_modality_quote": "...outputs a coherent, interpretable diagnosis report.",
    "task_io_type": "å¤šæ¨¡æ€ç³»ç»Ÿæ•°æ®åˆ°è¯Šæ–­æŠ¥å‘Š",
    "task_io_type_quote": "The framework collects abnormal operational data... outputs a coherent, interpretable diagnosis report.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºå¾®æœåŠ¡ç¯å¢ƒä¸‹çš„å®æ—¶æ ¹å› åˆ†æï¼Œæ•°æ®é›†åŒ…å«çœŸå®çš„è¿ç»´æ•°æ®ï¼ˆæŒ‡æ ‡ã€è¿½è¸ªã€æ—¥å¿—ï¼‰ã€‚",
    "unique_features_quote": "SpecRCA, a speculative root cause analysis framework for microservices... Preliminary experiments on the AIOps 2022 dataset...",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2022,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['æ•…éšœå®šä½çš„å‡†ç¡®æ€§', 'è¯Šæ–­æ¨ç†çš„æ•ˆç‡']",
    "evaluation_method_normalized": "['æ•…éšœå®šä½å‡†ç¡®ç‡ï¼ˆç™¾åˆ†æ¯”æå‡ï¼‰', 'è¯Šæ–­æŠ¥å‘Šç”Ÿæˆå»¶è¿Ÿï¼ˆç§’ï¼‰']",
    "problem_domain_normalized": "['äº‘è®¡ç®—', 'å¾®æœåŠ¡', 'ç³»ç»Ÿè¿ç»´ï¼ˆAIOpsï¼‰']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.02868_output/content.md",
    "benchmark_name": "CodeIF-Bench",
    "benchmark_name_quote": "Experimental results on the instruction-following benchmark CodeIF-Bench and the code generation benchmark CoderEval demonstrate that CODEMEM achieves state-of-the-art performance...",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Experiments on instruction following benchmark CodeIF-Bench for iterative code generation and the extended multi-turn repository-level code generation benchmark CoderEval demonstrate that CODEMEM improves current-turn and session-level instruction following...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„æµ‹æ¨¡å‹åœ¨ä»“åº“çº§åˆ«ã€å¤šè½®äº¤äº’å¼ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚",
    "task_description_quote": "Experiments on instruction following benchmark CodeIF-Bench for iterative code generation...",
    "dimension": "æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼ˆå½“å‰è½®æ¬¡å’Œä¼šè¯çº§åˆ«ï¼‰",
    "dimension_quote": "improving instruction following by 12.2% for the current turn and 11.5% for the session level",
    "evaluation_method": "é€šè¿‡æ‰§è¡Œå…³è”çš„æµ‹è¯•å¥—ä»¶æ¥å®šé‡è¯„ä¼°ç”Ÿæˆä»£ç çš„æ­£ç¡®æ€§ã€‚",
    "evaluation_method_quote": "After each interaction round, the generated code is executed against a test suite to evaluate compliance with the current instruction.",
    "context_dependency": "ä»“åº“çº§åˆ«ï¼ˆrepository-levelï¼‰ï¼Œéœ€è¦åˆ©ç”¨ä»“åº“å†…çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚",
    "context_dependency_quote": "Experiments on instruction following benchmark CodeIF-Bench for iterative code generation and the extended multi-turn repository-level code generation benchmark CoderEval...",
    "problem_domain": NaN,
    "problem_domain_quote": NaN,
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "æ–‡ä¸­ä»…æåŠå®éªŒç¤ºä¾‹ä¸ºPythonï¼Œæœªæè¿°å®Œæ•´æ•°æ®é›†",
    "language_quote": "Instruction-1: Please write a python function called 'set_status'...",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "è¿­ä»£å¼ä»£ç ç”Ÿæˆï¼ˆå¤šè½®äº¤äº’ï¼ŒåŒ…å«ç”Ÿæˆå’Œä¿®å¤ï¼‰",
    "task_granularity_quote": "We refer to this as repository-level iterative code generation.",
    "evaluation_metrics": "æŒ‡ä»¤éµå¾ªæå‡ç™¾åˆ†æ¯”ï¼ˆcurrent-turn å’Œ session-levelï¼‰",
    "evaluation_metrics_quote": "improving instruction following by 12.2% for the current turn and 11.5% for the session level",
    "input_modality": "è‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¸ä»£ç ä»“åº“ä¸Šä¸‹æ–‡",
    "input_modality_quote": "Given a code repository, an LLM incrementally generates and refines code through multi-turn interactions with the user.",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "LLM-generated answer A",
    "task_io_type": "æ–‡æœ¬ï¼ˆæŒ‡ä»¤ï¼‰åˆ°ä»£ç ",
    "task_io_type_quote": "Given a code repository, user instruction I, and LLM-generated answer A",
    "execution_environment": "æµ‹è¯•å¥—ä»¶æ‰§è¡Œç¯å¢ƒ",
    "execution_environment_quote": "After each interaction round, the generated code is executed against a test suite...",
    "unique_features": "ä¸“æ³¨äºå¤šè½®äº¤äº’å¼ã€ä»“åº“çº§åˆ«çš„ä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œè¯„ä¼°æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚",
    "unique_features_quote": "Recent benchmarks such as CodeIF-Bench (Wang et al., 2025a) and SR-Eval (Zhan et al., 2025) extend this setting to multi-round iterative scenarios...",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼ˆå½“å‰è½®æ¬¡å’Œä¼šè¯çº§åˆ«ï¼‰']",
    "evaluation_method_normalized": "['æŒ‡ä»¤éµå¾ªæå‡ç™¾åˆ†æ¯”ï¼ˆcurrent-turn å’Œ session-levelï¼‰']",
    "problem_domain_normalized": "[]",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.03878_output/content.md",
    "benchmark_name": "LiveCodeBench",
    "benchmark_name_quote": "Participants will solve medium-difficulty problems from the LiveCodeBench dataset",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Participants will solve medium-difficulty problems from the LiveCodeBench dataset",
    "dataset_url": "https://huggingface.co/datasets/livecodebench/code_generation_lite",
    "dataset_url_quote": "More details are reported in the HuggingFace dataset card3. The version of the dataset will be release_v5",
    "task_description": "ä»£ç ç”Ÿæˆã€‚æ•°æ®é›†åŒ…å«ä»ç«äº‰æ€§ç¼–ç¨‹å¹³å°æå–çš„é«˜è´¨é‡ç¼–ç¨‹é—®é¢˜ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹ç”Ÿæˆæ­£ç¡®ä»£ç çš„èƒ½åŠ›ã€‚",
    "task_description_quote": "The benchmark is continuously updated with new problems, providing detailed information for each problem, such as test cases, problem description, difficulty level, and solution.",
    "dimension": "ä»£ç ç”Ÿæˆæ­£ç¡®æ€§",
    "dimension_quote": "The goal of this experiment is to evaluate whether an LLM-based TDD-inspired workflow is effective in generating correct code",
    "evaluation_method": "æ‰§è¡Œå•å…ƒæµ‹è¯•",
    "evaluation_method_quote": "The extension logs time-stamped actions (produce/explain/regenerate tests; ask/regenerate function; re-run tests) and unit-test outcomes (per-test pass/fail).",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "ç®—æ³•ã€æ•°æ®ç»“æ„",
    "problem_domain_quote": "We will select a mix of problem types (e.g., algorithms, data structures) to evaluate different coding scenarios",
    "problem_difficulty": "å…¥é—¨çº§ã€ä¸­ç­‰éš¾åº¦",
    "problem_difficulty_quote": "We will select problems labeled as easy and medium difficulty",
    "language": "æ–‡ä¸­æœªæ˜ç¡®æåŠå®Œæ•´æ•°æ®é›†çš„è¯­è¨€ï¼Œä»…æåŠå®éªŒä½¿ç”¨Pythonã€‚",
    "language_quote": "Confounding variables control for prior skill and habits (years of programming experience, Python familiarity, prior TDD experience, and prior use of LLMs for coding).",
    "data_size": "æˆªè‡³2025å¹´1æœˆï¼ŒåŒ…å«880ä¸ªç¼–ç¨‹é—®é¢˜ã€‚",
    "data_size_quote": "The version of the dataset will be release_v5, containing a total of 880 coding problems until January 2025.",
    "source_type": "ä»ç«äº‰æ€§ç¼–ç¨‹å¹³å°æå–",
    "source_type_quote": "which contains a large set of high-quality coding problems, defined for different coding tasks, extracted from competitive programming platforms.",
    "last_updated": "2025å¹´1æœˆ (æ•°æ®é›†ç‰ˆæœ¬release_v5)",
    "last_updated_quote": "The version of the dataset will be release_v5, containing a total of 880 coding problems until January 2025.",
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": "æœ‰é¿å…æ•°æ®æ±¡æŸ“çš„è®¾è®¡è€ƒè™‘",
    "contamination_status_quote": "Avoiding data contamination: We will avoid problems that are likely to have been included in the training data of the LLM used in CURRANTE, based on the reported dates and known datasets.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "The goal of this experiment is to evaluate whether an LLM-based TDD-inspired workflow is effective in generating correct code",
    "evaluation_metrics": "é€šè¿‡ç‡ã€å…¨éƒ¨é€šè¿‡å®Œæˆç‡ã€æµ‹è¯•è¦†ç›–ç‡ã€æµ‹è¯•å¤šæ ·æ€§ã€é€šè¿‡æ—¶é—´",
    "evaluation_metrics_quote": "We will collect detailed data on effectiveness (e.g., PassAll, PassRate, TestCoverage, TestDiversity), efficiency (e.g., TimeToPass)",
    "input_modality": "è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°ä¸ç»“æ„åŒ–è§„èŒƒï¼ˆTOMLæ ¼å¼ï¼‰",
    "input_modality_quote": "A text area where the user can input a structured natural language specification of the problem to be solved. The specification follows a TOML format",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "the code generation phase (area 3), where CURRANTE uses the LLM to produce the corresponding code implementation",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "The user starts by defining the problem requirements in a structured way, via a TOML file... the LLM to produce the corresponding code implementation",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "æ•°æ®é›†æŒç»­æ›´æ–°ï¼ŒåŒ…å«æ¥è‡ªç«äº‰æ€§ç¼–ç¨‹å¹³å°çš„é«˜è´¨é‡ã€å¸¦æµ‹è¯•ç”¨ä¾‹å’Œéš¾åº¦æ ‡ç­¾çš„é—®é¢˜ã€‚",
    "unique_features_quote": "The benchmark is continuously updated with new problems, providing detailed information for each problem, such as test cases, problem description, difficulty level, and solution.",
    "data_size_quantity": 880,
    "data_size_unit": "ä¸ªç¼–ç¨‹é—®é¢˜",
    "last_updated_year": 2025,
    "last_updated_month": 1,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆæ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['é€šè¿‡ç‡', 'å…¨éƒ¨é€šè¿‡å®Œæˆç‡', 'æµ‹è¯•è¦†ç›–ç‡', 'æµ‹è¯•å¤šæ ·æ€§', 'é€šè¿‡æ—¶é—´']",
    "problem_domain_normalized": "['ç®—æ³•', 'æ•°æ®ç»“æ„']",
    "source_type_normalized": "['ä»ç«äº‰æ€§ç¼–ç¨‹å¹³å°æå–']",
    "problem_difficulty_normalized": "å…¥é—¨çº§",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2601.03731_output/content.md",
    "benchmark_name": "RepoReason",
    "benchmark_name_quote": "We present REPOREASON, a white-box diagnostic benchmark centered on Abductive Assertion Verification.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We present REPOREASON, a white-box diagnostic benchmark... We introduce RepoReason, a repository-level code reasoning benchmark designed as a white-box diagnostic tool.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯¥è¯„æµ‹åŸºå‡†æ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨ä»“åº“çº§åˆ«ï¼ˆRepository-Levelï¼‰çš„ä»£ç æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡â€œæº¯å› æ–­è¨€éªŒè¯â€ï¼ˆAbductive Assertion Verificationï¼‰æ¥æ¨å¯¼å¤æ‚ã€ç›¸äº’ä¾èµ–çš„æ–‡ä»¶ç³»ç»Ÿä¸­ï¼Œç»è¿‡æ‰§è¡Œå†å²åçš„å½“å‰ç³»ç»ŸçŠ¶æ€ã€‚",
    "task_description_quote": "RepoReason shifts to a verification-centric approach: â€œGiven the complex execution history of this repository, what is the current system state?â€ By requiring models to derive deterministic values that satisfy assertions rather than writing implementation logic...",
    "dimension": "ä»“åº“çº§ä»£ç æ¨ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬è·¨æ–‡ä»¶é€»è¾‘ä¸€è‡´æ€§ã€é•¿é“¾çŠ¶æ€è·Ÿè¸ªã€å¤šæºä¿¡æ¯èšåˆç­‰è®¤çŸ¥ç»´åº¦ã€‚",
    "dimension_quote": "...maintaining logical consistency across extensive, interdependent file systemsâ€”a capability we define as Repository-Level Reasoning. This capability imposes profound challenges on LLMs, demanding... the agency to explore vast contexts to pinpoint target definitions, aggregate multi-source information across complex dependencies, and perform long-chain reasoning to accurately track and compute state mutations.",
    "evaluation_method": "é‡‡ç”¨ç™½ç›’è¯Šæ–­è¯„ä¼°æ–¹æ³•ï¼Œé€šè¿‡åŠ¨æ€ç¨‹åºåˆ‡ç‰‡ï¼ˆdynamic program slicingï¼‰å’Œä¸‰ä¸ªæ­£äº¤çš„è®¤çŸ¥æŒ‡æ ‡è¿›è¡Œç»†ç²’åº¦é‡åŒ–ï¼šESVï¼ˆé˜…è¯»è´Ÿè½½ï¼‰ã€MCLï¼ˆæ¨¡æ‹Ÿæ·±åº¦ï¼‰ã€DFIï¼ˆé›†æˆå®½åº¦ï¼‰ã€‚ä»»åŠ¡å½¢å¼ä¸ºå¡«ç©ºå¼æ¨ç†ï¼ˆcloze-style reasoning tasksï¼‰ï¼Œè¦æ±‚æ¨¡å‹æ¨å¯¼å‡ºè¢«æ©ç çš„æ–­è¨€ä¸­çš„ç¡®å®šå€¼ã€‚",
    "evaluation_method_quote": "...we establish a fine-grained diagnostic system using dynamic program slicing, quantifying reasoning via three orthogonal metrics: ESV (Reading Load), MCL (Simulation Depth), and DFI (Integration Width)... subsequently masking these verified values to create cloze-style reasoning tasks.",
    "context_dependency": "å¤šæ–‡ä»¶é¡¹ç›®ã€ä»“åº“çº§åˆ«ã€‚æ¨¡å‹éœ€è¦å¤„ç†å¤§è§„æ¨¡ã€ç›¸äº’ä¾èµ–çš„æ–‡ä»¶ç³»ç»Ÿï¼Œä¸Šä¸‹æ–‡èŒƒå›´å¯è¾¾æ•´ä¸ªä»£ç ä»“åº“ã€‚",
    "context_dependency_quote": "...maintaining logical consistency across extensive, interdependent file systems... challenging models with authentic, deep logic... apply deep execution simulation to massive repository scopes, challenging models to maintain logical consistency across extensive file systems.",
    "problem_domain": "é€šç”¨è½¯ä»¶å·¥ç¨‹ï¼Œä»»åŠ¡å®ä¾‹æå–è‡ªä¸»æµPythonå¼€æºä»“åº“ï¼ˆå¦‚toolz, sympy, jinja2ï¼‰çš„çœŸå®å•å…ƒæµ‹è¯•ã€‚",
    "problem_domain_quote": "To ensure the benchmark reflects real-world complexity, we extract task instances from a curated set of mainstream Python repositories (e.g., toolz (Developers, 2025b), sympy (Team, 2025c), jinja2 (Pallets, 2025))...",
    "problem_difficulty": "å·¥ç¨‹çº§ã€é«˜ç†µé€»è¾‘ã€‚é€šè¿‡åŸºäºè¿è¡Œæ—¶è½¨è¿¹ï¼ˆå¦‚å †æ ˆæ·±åº¦ã€å‡½æ•°è°ƒç”¨å¯†åº¦ã€è·¨æ–‡ä»¶ä¾èµ–å¹¿åº¦ï¼‰çš„ç»“æ„åŒ–æŒ‡æ ‡è¿›è¡Œå®šé‡è¿‡æ»¤ï¼Œä»…ä¿ç•™æœ€å¤æ‚çš„æ¨ç†ä»»åŠ¡ã€‚",
    "problem_difficulty_quote": "...we demonstrate that even top-tier agents face severe performance degradation when confronted with high-entropy logic... By designing structural metrics based on these tracesâ€”such as stack depth, function call density, and cross-file dependency breadthâ€”we quantitatively filter and retain only the most complex reasoning tasks.",
    "language": "Python",
    "language_quote": "...we extract task instances from a curated set of mainstream Python repositories... Pure-Python Only",
    "data_size": "è§„æ¨¡ä»1.2kåˆ°775k+è¡Œä»£ç ï¼ˆLoCï¼‰çš„ä»“åº“èŒƒå›´ã€‚",
    "data_size_quote": "RepoReason (Ours) âœ“ 1.2k â€“ 775k+ Assertion Verification",
    "source_type": "æ•°æ®æ¥æºäºç²¾é€‰çš„ä¸»æµå¼€æºPythonä»“åº“åŠå…¶å†…ç½®çš„å•å…ƒæµ‹è¯•å¥—ä»¶ï¼Œé€šè¿‡å…·ä½“æ‰§è¡Œæ•è·ç»†ç²’åº¦çš„è¿è¡Œæ—¶è½¨è¿¹ã€‚",
    "source_type_quote": "...we extract task instances from a curated set of mainstream Python repositories... Beyond static code extraction, we perform concrete execution of the repositoriesâ€™ built-in unit test suites to capture granular runtime traces.",
    "last_updated": "2026",
    "last_updated_quote": "arXiv:2601.03731v1  [cs.SE]  7 Jan 2026",
    "build_type": "å®˜æ–¹è‡ªå»ºã€‚è®ºæ–‡ä½œè€…é€šè¿‡ä¸€ä¸ªåŒ…å«äº”ä¸ªé˜¶æ®µçš„é˜²å¾¡ç³»ç»Ÿï¼ˆä»“åº“ç­›é€‰ã€ç»“æ„è¿‡æ»¤ã€æ‰§è¡Œé©±åŠ¨çªå˜ã€ä»»åŠ¡å®ä¾‹åŒ–ã€è¯Šæ–­è¯„ä¼°ï¼‰æ„å»ºã€‚",
    "build_type_quote": "In this section, we delineate the methodological framework of RepoReason, conceptualizing the construction pipeline as a rigorous five-phase defense system.",
    "contamination_status": "æŠ—æ±¡æŸ“è®¾è®¡ã€‚é€šè¿‡â€œæ‰§è¡Œé©±åŠ¨çªå˜â€ï¼ˆExecution-Driven Mutationï¼‰æ¡†æ¶è§£å†³æ•°æ®æ³„æ¼é£é™©ï¼Œåˆ©ç”¨æ‰§è¡Œç¯å¢ƒä½œä¸ºè¯­ä¹‰é¢„è¨€æœºï¼Œé€šè¿‡æ‰°åŠ¨ç¨‹åºè¾“å…¥å¹¶é‡æ–°æ‰§è¡Œæ¥ç”Ÿæˆæ–°çš„ã€æœªè¢«è®°å¿†çš„åœ°é¢çœŸå€¼çŠ¶æ€ã€‚",
    "contamination_status_quote": "To resolve this, we implement an Execution-Driven Mutation methodology... By keeping the reasoning logic... intact but perturbing the program inputs, we effectively sever the modelâ€™s memory retrieval path... This design ensures that while the reasoning logic remains authentic and complex, the specific state values are unmemorized and deterministic.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç æ¨ç†ä¸çŠ¶æ€éªŒè¯ï¼Œè€Œéä»£ç ç”Ÿæˆã€‚ä»»åŠ¡æ˜¯éªŒè¯æ–­è¨€ï¼Œæ¨å¯¼ç³»ç»ŸçŠ¶æ€ã€‚",
    "task_granularity_quote": "Unlike traditional generation tasks, RepoReason shifts to a verification-centric approach... By requiring models to derive deterministic values that satisfy assertions rather than writing implementation logic...",
    "evaluation_metrics": "ESVï¼ˆé˜…è¯»è´Ÿè½½ï¼‰ã€MCLï¼ˆæ¨¡æ‹Ÿæ·±åº¦ï¼‰ã€DFIï¼ˆé›†æˆå®½åº¦ï¼‰ä¸‰ä¸ªæ­£äº¤çš„è®¤çŸ¥æŒ‡æ ‡ï¼Œç”¨äºè¿›è¡Œç»†ç²’åº¦çš„ç™½ç›’è¯Šæ–­ã€‚",
    "evaluation_metrics_quote": "...quantifying reasoning via three orthogonal metrics: ESV (Reading Load), MCL (Simulation Depth), and DFI (Integration Width).",
    "input_modality": "ä»£ç ä¸è‡ªç„¶è¯­è¨€ï¼ˆè¢«æ©ç çš„æ–­è¨€åŠå…¶æ‰€åœ¨çš„å¤æ‚ä»“åº“ä¸Šä¸‹æ–‡ï¼‰ã€‚",
    "input_modality_quote": "...masking these verified values to create cloze-style reasoning tasks... Given the complex execution history of this repository...",
    "output_modality": "ä»£ç /ç¡®å®šå€¼ã€‚æ¨¡å‹éœ€è¦è¾“å‡ºä¸€ä¸ªç¡®å®šçš„å€¼ï¼ˆå¦‚æ•°å­—ã€å­—ç¬¦ä¸²ã€å¸ƒå°”å€¼ï¼‰æ¥æ»¡è¶³è¢«æ©ç çš„æ–­è¨€ã€‚",
    "output_modality_quote": "By requiring models to derive deterministic values that satisfy assertions... We introduce a Deterministic Value Protocol, which filters runtime data to eliminate representational drift and ensure stable, unique ground-truth values.",
    "task_io_type": "ä»£ç åˆ°ç¡®å®šå€¼ã€‚è¾“å…¥æ˜¯åŒ…å«æ©ç æ–­è¨€çš„ä»“åº“ä»£ç ä¸Šä¸‹æ–‡ï¼Œè¾“å‡ºæ˜¯æ»¡è¶³è¯¥æ–­è¨€çš„ç‰¹å®šç¡®å®šå€¼ã€‚",
    "task_io_type_quote": "â€œGiven the complex execution history of this repository, what is the current system state?â€... derive deterministic values that satisfy assertions",
    "execution_environment": "éœ€è¦å®Œæ•´çš„Pythonè¿è¡Œæ—¶ç¯å¢ƒæ¥æ‰§è¡Œä»“åº“çš„å•å…ƒæµ‹è¯•å¥—ä»¶ä»¥æ•è·è½¨è¿¹ï¼Œå¹¶ä½œä¸ºè¯­ä¹‰é¢„è¨€æœºæ¥ç”Ÿæˆæ–°çš„åœ°é¢çœŸå€¼ã€‚",
    "execution_environment_quote": "We treat the code execution environment as a Semantic Oracle... We then re-execute the entire repository context to capture the new ground-truth state...",
    "unique_features": "1. èŒƒå¼è½¬å˜ï¼šä»é¢„æµ‹è½¬å‘æº¯å› éªŒè¯ï¼Œä½¿ç”¨å•å…ƒæµ‹è¯•æ–­è¨€ä½œä¸ºè¯­ä¹‰é”šç‚¹ã€‚2. è¯­ä¹‰é¢„è¨€æœºæ¡†æ¶ï¼šé€šè¿‡æ‰§è¡Œé©±åŠ¨çªå˜è§£å†³ä¸°å¯Œæ€§ä¸æ±¡æŸ“çš„ä¸¤éš¾å›°å¢ƒã€‚3. ç™½ç›’è¯Šæ–­å·¥å…·ï¼šè¶…è¶Šç«¯åˆ°ç«¯è¯„åˆ†ï¼Œæä¾›åŸºäºåŠ¨æ€ç¨‹åºåˆ‡ç‰‡çš„ç»†ç²’åº¦è®¤çŸ¥ç“¶é¢ˆåˆ†æã€‚",
    "unique_features_quote": "â€¢ From Prediction to Abductive Verification: We pioneer a repository-level reasoning paradigm that pivots from direct outcome prediction to abductive assertion verification... â€¢ The Semantic Oracle Framework: We resolve the Richness-Contamination Dilemma through an Execution-Driven Mutation framework... â€¢ White-box Diagnostic Instrument: Beyond end-to-end scoring, we introduce a granular diagnostic system powered by dynamic program slicing.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2026,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»“åº“çº§ä»£ç æ¨ç†èƒ½åŠ›', 'è·¨æ–‡ä»¶é€»è¾‘ä¸€è‡´æ€§', 'é•¿é“¾çŠ¶æ€è·Ÿè¸ª', 'å¤šæºä¿¡æ¯èšåˆ']",
    "evaluation_method_normalized": "['ESV', 'MCL', 'DFI']",
    "problem_domain_normalized": "['é€šç”¨è½¯ä»¶å·¥ç¨‹']",
    "source_type_normalized": "['ä¸»æµå¼€æºPythonä»“åº“', 'å•å…ƒæµ‹è¯•å¥—ä»¶']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2601.04126_output/content.md",
    "benchmark_name": "WebGen-Bench",
    "benchmark_name_quote": "Experiments demonstrate that our system surpasses advanced coding agents in building realistic web environments on WebGen-Bench, achieving superior performance in both visual and functional quality.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Experiments demonstrate that our system surpasses advanced coding agents in building realistic web environments on WebGen-Bench...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°ä»è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆå®Œæ•´ã€åŠŸèƒ½æ€§ç½‘ç«™çš„èƒ½åŠ›ã€‚",
    "task_description_quote": "WebGen-Bench (Lu et al., 2025) evaluates end-to-end website generation from natural language descriptions.",
    "dimension": "ç½‘ç«™ç”Ÿæˆè´¨é‡ï¼ŒåŒ…æ‹¬è§†è§‰å’ŒåŠŸèƒ½è´¨é‡ã€‚",
    "dimension_quote": "achieving superior performance in both visual and functional quality.",
    "evaluation_method": NaN,
    "evaluation_method_quote": NaN,
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "Webå‰ç«¯å¼€å‘ï¼Œç½‘ç«™æ„å»ºã€‚",
    "problem_domain_quote": "evaluates end-to-end website generation from natural language descriptions.",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "æ–‡ä¸­æœªæ˜ç¡®æåŠï¼Œä½†æ ¹æ®ä¸Šä¸‹æ–‡æ¨æ–­ä¸»è¦æ¶‰åŠWebå‰ç«¯æŠ€æœ¯ï¼ˆå¦‚HTML, CSS, JavaScriptï¼‰ã€‚",
    "language_quote": "Recent work has explored UI-to-code generation: Design2Code (Si et al., 2024) benchmarks the conversion of visual designs to front-end code, while WebGen-Bench (Lu et al., 2025) evaluates end-to-end website generation from natural language descriptions.",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": "2025",
    "last_updated_quote": "WebGen-Bench (Lu et al., 2025)",
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ç«¯åˆ°ç«¯ç½‘ç«™ç”Ÿæˆ",
    "task_granularity_quote": "evaluates end-to-end website generation from natural language descriptions.",
    "evaluation_metrics": NaN,
    "evaluation_metrics_quote": NaN,
    "input_modality": "è‡ªç„¶è¯­è¨€æè¿°",
    "input_modality_quote": "evaluates end-to-end website generation from natural language descriptions.",
    "output_modality": "ä»£ç ï¼ˆç½‘ç«™ï¼‰",
    "output_modality_quote": "evaluates end-to-end website generation from natural language descriptions.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "evaluates end-to-end website generation from natural language descriptions.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºè¯„ä¼°ä»è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆå®Œæ•´ã€åŠŸèƒ½æ€§ç½‘ç«™çš„èƒ½åŠ›ï¼Œè€Œéå•ä¸ªç½‘é¡µæˆ–UIç»„ä»¶ã€‚",
    "unique_features_quote": "While LLMs perform well on generating a single webpage, building a realistic and functional website with many interconnected pages faces challenges. ... WebGen-Bench (Lu et al., 2025) evaluates end-to-end website generation from natural language descriptions.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['HTML', 'CSS', 'JavaScript']",
    "dimension_normalized": "['ç½‘ç«™ç”Ÿæˆè´¨é‡', 'è§†è§‰è´¨é‡', 'åŠŸèƒ½è´¨é‡']",
    "evaluation_method_normalized": "[]",
    "problem_domain_normalized": "['Webå‰ç«¯å¼€å‘', 'ç½‘ç«™æ„å»º']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.05214_output/content.md",
    "benchmark_name": "Glaive Function-Calling dataset",
    "benchmark_name_quote": "We utilize the Glaive Function-Calling dataset (GlaiveAI 2024) from Hugging Face",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We utilize the Glaive Function-Calling dataset (GlaiveAI 2024) from Hugging Face",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„æµ‹åŸºå‡†æ—¨åœ¨è¯„ä¼°AIä»£ç†åœ¨å·¥å…·è°ƒç”¨ï¼ˆå¦‚APIè°ƒç”¨ï¼‰ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯æ£€æµ‹å·¥å…·è°ƒç”¨ä¸­çš„å¹»è§‰ï¼ˆå¦‚é€‰æ‹©é”™è¯¯å·¥å…·ã€å‚æ•°é”™è¯¯ç­‰ï¼‰ã€‚",
    "task_description_quote": "We evaluate this approach on reasoning tasks across multiple domains, demonstrating strong detection performance (up to 86.4% accuracy)... particularly excelling at detecting parameter-level hallucinations and inappropriate tool selections, critical for reliable agent deployment.",
    "dimension": "å·¥å…·è°ƒç”¨å¹»è§‰æ£€æµ‹ï¼ˆåŒ…æ‹¬å‡½æ•°é€‰æ‹©é”™è¯¯ã€å‚æ•°é”™è¯¯ã€å·¥å…·ç»•è¿‡ç­‰ï¼‰",
    "dimension_quote": "We formalize tool-calling hallucination detection as a binary classification task... We define a tool-calling hallucination as occurring when any of the following conditions hold: (1) Function Selection Error... (2) Function Appropriateness Error... (3) Parameter Error... (4) Completeness Error... (5) Tool Bypass Error",
    "evaluation_method": "ä½¿ç”¨è½»é‡çº§åˆ†ç±»å™¨å¯¹æ¨¡å‹æœ€åä¸€å±‚çš„å†…éƒ¨è¡¨ç¤ºè¿›è¡ŒäºŒå…ƒåˆ†ç±»ï¼ˆå¹»è§‰/éå¹»è§‰ï¼‰ï¼Œå¹¶æŠ¥å‘Šæ£€æµ‹å‡†ç¡®ç‡ã€‚",
    "evaluation_method_quote": "We formalize tool-calling hallucination detection as a binary classification task operating on the internal representations of the last layer of large language models (LLMs) during tool call generation... demonstrating strong detection performance (up to 86.4% accuracy)",
    "context_dependency": "ä¾èµ–äºç”¨æˆ·æŸ¥è¯¢å’Œä¸Šä¸‹æ–‡ä¿¡æ¯æ¥ç”Ÿæˆå’Œè¯„ä¼°å·¥å…·è°ƒç”¨ã€‚",
    "context_dependency_quote": "Given a user query q and contextual information c, an LLM M generates a tool call",
    "problem_domain": "å¤šé¢†åŸŸï¼ŒåŒ…æ‹¬ä¸ªäººå¥åº·ã€é‡‘èè®¡ç®—ã€å¯æŒç»­æ€§æŒ‡æ ‡å’Œé€šç”¨è®¡ç®—å™¨åŠŸèƒ½ã€‚",
    "problem_domain_quote": "covering domains such as personal health, finance, and general utility calculations... spanning personal health calculations, financial computations, sustainability metrics, and general calculator functions",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": NaN,
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": "æ¥è‡ªHugging Faceçš„Glaive Function-Callingæ•°æ®é›†ï¼ŒåŒ…å«å¤šé¢†åŸŸä»£ç†äº¤äº’çš„æ ‡æ³¨æ•°æ®ã€‚",
    "source_type_quote": "We utilize the Glaive Function-Calling dataset (GlaiveAI 2024) from Hugging Face, which comprises multi-domain agent interactions annotated with structured tool usage information and corresponding parameters",
    "last_updated": "2024 (æ•°æ®é›†å¼•ç”¨å¹´ä»½)",
    "last_updated_quote": "Glaive Function-Calling dataset (GlaiveAI 2024)",
    "build_type": "ç¤¾åŒºè´¡çŒ®ï¼ˆæ¥è‡ªHugging Faceï¼‰",
    "build_type_quote": "from Hugging Face",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "å·¥å…·è°ƒç”¨ç”Ÿæˆä¸å¹»è§‰æ£€æµ‹",
    "task_granularity_quote": "tool-calling hallucination detection",
    "evaluation_metrics": "æ£€æµ‹å‡†ç¡®ç‡",
    "evaluation_metrics_quote": "demonstrating strong detection performance (up to 86.4% accuracy)",
    "input_modality": "è‡ªç„¶è¯­è¨€æŸ¥è¯¢å’Œä¸Šä¸‹æ–‡ä¿¡æ¯",
    "input_modality_quote": "Given a user query q and contextual information c",
    "output_modality": "ç»“æ„åŒ–çš„å·¥å…·è°ƒç”¨ï¼ˆå‡½æ•°åå’Œå‚æ•°ï¼‰",
    "output_modality_quote": "generates a tool call Ëœf(a) where Ëœf âˆˆF âˆª{âˆ…} and a represents the function arguments",
    "task_io_type": "æ–‡æœ¬åˆ°ç»“æ„åŒ–å·¥å…·è°ƒç”¨",
    "task_io_type_quote": "Given a user query q and contextual information c, an LLM M generates a tool call",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºå·¥å…·è°ƒç”¨ï¼ˆè€Œéé€šç”¨æ–‡æœ¬ç”Ÿæˆï¼‰ä¸­çš„å¹»è§‰æ£€æµ‹ï¼Œå®šä¹‰äº†äº”ç§å…·ä½“çš„å¹»è§‰ç±»å‹ï¼ˆå‡½æ•°é€‰æ‹©ã€é€‚å½“æ€§ã€å‚æ•°ã€å®Œæ•´æ€§ã€å·¥å…·ç»•è¿‡ï¼‰ã€‚æ•°æ®é›†åŒ…å«å¤šé¢†åŸŸä»£ç†äº¤äº’çš„æ ‡æ³¨ã€‚",
    "unique_features_quote": "Unlike textual hallucinations, tool-calling hallucinations manifest as inappropriate tool selection, malformed parameters, incorrect tool chaining, tool bypass behavior... We define a tool-calling hallucination as occurring when any of the following conditions hold: (1) Function Selection Error... (5) Tool Bypass Error... covering domains such as personal health, finance, and general utility calculations",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2024,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['å·¥å…·è°ƒç”¨å¹»è§‰æ£€æµ‹', 'å‡½æ•°é€‰æ‹©é”™è¯¯', 'å‚æ•°é”™è¯¯', 'å·¥å…·ç»•è¿‡']",
    "evaluation_method_normalized": "['æ£€æµ‹å‡†ç¡®ç‡']",
    "problem_domain_normalized": "['å¤šé¢†åŸŸ', 'ä¸ªäººå¥åº·', 'é‡‘èè®¡ç®—', 'å¯æŒç»­æ€§æŒ‡æ ‡', 'é€šç”¨è®¡ç®—å™¨åŠŸèƒ½']",
    "source_type_normalized": "['Hugging Face', 'Glaive Function-Callingæ•°æ®é›†', 'å¤šé¢†åŸŸä»£ç†äº¤äº’', 'æ ‡æ³¨æ•°æ®']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "JSON",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.04886_output/content.md",
    "benchmark_name": "AIDev",
    "benchmark_name_quote": "We used the AIDev dataset [16] and analyzed the AIDev-pop subset (33,596 PRs from 2,807 repositories with >100 stars) via the pull_request split.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We used the AIDev dataset [16] and analyzed the AIDev-pop subset... To address this gap, we conducted an empirical study analyzing 23,247 Agentic-PRs from the AIDev dataset [16]...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„æµ‹AIç¼–ç ä»£ç†ç”Ÿæˆçš„Pull Requestï¼ˆPRï¼‰æè¿°ä¸ä»£ç å˜æ›´ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚æ—¨åœ¨è¡¡é‡AIä»£ç†ç”Ÿæˆçš„PRæè¿°ï¼ˆæ ‡é¢˜+æ­£æ–‡ï¼‰æ˜¯å¦å¿ å®äºå…¶åº•å±‚ä»£ç å˜æ›´ã€‚",
    "task_description_quote": "We used PR message-code inconsistency (PR-MCI) as the degree to which a PR description (title + body) diverges from the underlying code changes.",
    "dimension": "AIç”Ÿæˆä»£ç çš„å¯é æ€§ä¸å¯ä¿¡åº¦ï¼Œå…·ä½“ä¸ºPRæè¿°ä¸ä»£ç å˜æ›´çš„ä¸€è‡´æ€§ã€‚",
    "dimension_quote": "However, the alignment between these messages and the actual changes remains unexplored, raising concerns about the trustworthiness of AI agents.",
    "evaluation_method": "ä½¿ç”¨å¯å‘å¼ç›¸ä¼¼åº¦è¯„åˆ†ï¼ˆPR-MCI scoreï¼‰æ¥è¡¡é‡ä¸ä¸€è‡´æ€§ã€‚è¯¥è¯„åˆ†ç»“åˆäº†ä¸‰ä¸ªäº’è¡¥ä¿¡å·ï¼šèŒƒå›´å……åˆ†æ€§ã€æ–‡ä»¶ç±»å‹ä¸€è‡´æ€§å’Œä»»åŠ¡ç±»å‹å¯¹é½åº¦ã€‚é€šè¿‡æ‰‹åŠ¨æ ‡æ³¨éªŒè¯è¯„åˆ†å¹¶è®¾å®šé˜ˆå€¼ï¼ˆÎ¸=0.61ï¼‰æ¥è¯†åˆ«é«˜ä¸ä¸€è‡´æ€§ï¼ˆhigh-MCIï¼‰çš„PRã€‚",
    "evaluation_method_quote": "We measured PR-MCI using a heuristic similarity score that combines three complementary signals: scope adequacy ğ‘ ğ‘ (whether description verbosity matches code churn), file-type consistency ğ‘ ğ‘“(whether mentioned file types, e.g., tests or documentation, are actually modified), and task-type alignment ğ‘ ğ‘¡(whether description language matches the labeled task type). PRs with similarity below ğœƒ= 0.61 are labeled high-MCI.",
    "context_dependency": "Pull Requestçº§åˆ«ï¼ŒåŒ…å«å¤šä¸ªæäº¤å’Œæ–‡ä»¶å˜æ›´ã€‚",
    "context_dependency_quote": "Note that PR descriptions differ from commit messages: PR descriptions may span multiple commits and serve as the primary communication channel for reviewers.",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ï¼Œå…·ä½“ä¸ºä»£ç å®¡æŸ¥å’Œåä½œå¼€å‘ã€‚",
    "problem_domain_quote": "Pull request (PR) descriptions generated by AI coding agents are the primary channel for communicating code changes to human reviewers.",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "æœªæ˜ç¡®æŒ‡å®šï¼Œæ•°æ®é›†åŒ…å«å¤šç§ç¼–ç¨‹è¯­è¨€çš„PRã€‚",
    "language_quote": NaN,
    "data_size": "åˆ†æçš„æ•°æ®é›†åŒ…å«23,247ä¸ªç”±AIä»£ç†ç”Ÿæˆçš„PRï¼ˆæ¥è‡ªAIDevæ•°æ®é›†ï¼‰ã€‚æ­¤å¤–ï¼Œä½œè€…è´¡çŒ®äº†974ä¸ªæ‰‹åŠ¨æ ‡æ³¨çš„PRï¼ˆå…¶ä¸­432ä¸ªä¸ºéƒ¨åˆ†å¯¹é½/æœªå¯¹é½ï¼‰ã€‚",
    "data_size_quote": "We analyzed 23,247 agentic PRs across five agents... We contributed 974 manually annotated PRs... the final dataset contains 23,247 PRs authored by five AI agents.",
    "source_type": "æ¥è‡ªAIDevæ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«æ¥è‡ªGitHubä¸ŠçœŸå®è½¯ä»¶é¡¹ç›®çš„Pull Requestã€‚",
    "source_type_quote": "We used the AIDev dataset [16] and analyzed the AIDev-pop subset (33,596 PRs from 2,807 repositories with >100 stars)...",
    "last_updated": "2026",
    "last_updated_quote": "Jingzhi Gong, Giovanni Pinna, Yixin Bian, and Jie M. Zhang. 2026. Analyzing Message-Code Inconsistency in AI Coding Agent-Authored Pull Requests.",
    "build_type": "ç¤¾åŒºè´¡çŒ®ï¼ˆåŸºäºAIDevæ•°æ®é›†æ„å»ºï¼‰ï¼Œæœ¬æ–‡ä½œè€…å¯¹å…¶è¿›è¡Œäº†è¿‡æ»¤å’Œæ ‡æ³¨ã€‚",
    "build_type_quote": "We used the AIDev dataset [16]... We contributed 974 manually annotated PRs...",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": "è®¸å¯åè®®ï¼ˆMITæˆ–Apache 2.0ï¼‰",
    "dataset_license_quote": "After filtering for closed PRs, permissive licenses (MIT or Apache 2.0)...",
    "task_granularity": "ä»£ç å˜æ›´æè¿°è¯„ä¼°",
    "task_granularity_quote": "We used PR message-code inconsistency (PR-MCI) as the degree to which a PR description (title + body) diverges from the underlying code changes.",
    "evaluation_metrics": "PR-MCIç›¸ä¼¼åº¦è¯„åˆ†ï¼ˆèŒƒå›´0-1ï¼‰ï¼Œé«˜ä¸ä¸€è‡´æ€§ï¼ˆhigh-MCIï¼‰PRçš„æµè¡Œç‡ï¼Œä»¥åŠPRæ¥å—ç‡ã€åˆå¹¶æ—¶é—´ç­‰å½±å“æŒ‡æ ‡ã€‚",
    "evaluation_metrics_quote": "The similarity score ğ‘ âˆˆ[0, 1]... Overall, 406 out of 23,247 Agentic-PRs (1.7%) exhibited high PR-MCI... high-MCI PRs had 51.7% lower acceptance rates (28.3% vs. 80.0%) and took 3.5Ã— longer to merge (55.8 vs. 16.0 hours).",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆPRæè¿°ï¼‰å’Œä»£ç å˜æ›´ï¼ˆdiffï¼‰ã€‚",
    "input_modality_quote": "PR message-code inconsistency (PR-MCI) as the degree to which a PR description (title + body) diverges from the underlying code changes.",
    "output_modality": "ä¸€è‡´æ€§è¯„åˆ†å’Œåˆ†ç±»ï¼ˆå¯¹é½ã€éƒ¨åˆ†å¯¹é½ã€æœªå¯¹é½ï¼‰ã€‚",
    "output_modality_quote": "PRs with similarity below ğœƒ= 0.61 are labeled high-MCI... manually annotated 600 PRs, stratified by agent, task type, and score bins, as aligned, partially aligned, or misaligned",
    "task_io_type": "ä»£ç ä¸æ–‡æœ¬åˆ°ä¸€è‡´æ€§è¯„ä¼°",
    "task_io_type_quote": "We used PR message-code inconsistency (PR-MCI) as the degree to which a PR description (title + body) diverges from the underlying code changes.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºAIç¼–ç ä»£ç†ç”Ÿæˆçš„Pull Requestï¼ˆAgentic-PRsï¼‰ä¸­æ¶ˆæ¯ä¸ä»£ç çš„ä¸ä¸€è‡´æ€§ï¼ˆPR-MCIï¼‰ã€‚æå‡ºäº†ä¸€ä¸ªåŒ…å«å…«ç§ç±»å‹çš„PR-MCIåˆ†ç±»æ³•ï¼ˆå¦‚â€œå¹»å½±å˜æ›´â€ã€â€œèŒƒå›´ä½ä¼°â€ï¼‰ã€‚ç ”ç©¶äº†ä¸ä¸€è‡´æ€§å¯¹PRæ¥å—ç‡å’Œåˆå¹¶æ—¶é—´çš„å½±å“ã€‚",
    "unique_features_quote": "We identified eight PR-MCI types, finding that Phantom Changes (descriptions claim unimplemented changes) dominated (45.4%)... high-MCI PRs had 51.7% lower acceptance rates (28.3% vs. 80.0%) and took 3.5Ã— longer to merge (55.8 vs. 16.0 hours).",
    "data_size_quantity": 23247,
    "data_size_unit": "ä¸ªPR",
    "last_updated_year": 2026,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['æœªæ˜ç¡®æŒ‡å®š', 'å¤šç§ç¼–ç¨‹è¯­è¨€']",
    "dimension_normalized": "['AIç”Ÿæˆä»£ç çš„å¯é æ€§ä¸å¯ä¿¡åº¦', 'PRæè¿°ä¸ä»£ç å˜æ›´çš„ä¸€è‡´æ€§']",
    "evaluation_method_normalized": "['PR-MCIç›¸ä¼¼åº¦è¯„åˆ†', 'é«˜ä¸ä¸€è‡´æ€§PRçš„æµè¡Œç‡', 'PRæ¥å—ç‡', 'åˆå¹¶æ—¶é—´']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'ä»£ç å®¡æŸ¥', 'åä½œå¼€å‘']",
    "source_type_normalized": "['AIDevæ•°æ®é›†', 'GitHub', 'çœŸå®è½¯ä»¶é¡¹ç›®', 'Pull Request']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "JSON",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "MIT",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.04540_output/content.md",
    "benchmark_name": "AdaptEval",
    "benchmark_name_quote": "To fill this gap, we propose AdaptEval, a benchmark designed to evaluate LLMs on code snippet adaptation.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "To fill this gap, we propose AdaptEval, a benchmark designed to evaluate LLMs on code snippet adaptation.",
    "dataset_url": "https://github.com/ztwater/AdaptEval",
    "dataset_url_quote": "1https://github.com/ztwater/AdaptEval.",
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç ç‰‡æ®µé€‚åº”ï¼ˆCode Snippet Adaptationï¼‰ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚ä»£ç ç‰‡æ®µé€‚åº”æ˜¯ä»£ç é‡ç”¨è¿‡ç¨‹ä¸­çš„ä¸€ä¸ªå…³é”®æ­¥éª¤ï¼Œæ¶‰åŠå°†ä»£ç ç‰‡æ®µä»ä¸€ä¸ªç‰¹å®šç¯å¢ƒä¿®æ”¹ä»¥é€‚åº”å¦ä¸€ä¸ªç¯å¢ƒã€‚",
    "task_description_quote": "Recent advancements in large language models (LLMs) have automated various software engineering tasks, with benchmarks emerging to evaluate their capabilities. However, for adaptation, a critical activity during code reuse, there is no benchmark to assess LLMsâ€™ performance, leaving their practical utility in this area unclear. To fill this gap, we propose AdaptEval, a benchmark designed to evaluate LLMs on code snippet adaptation.",
    "dimension": "ä»£ç ç‰‡æ®µé€‚åº”èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¯¹å®é™…ä¸Šä¸‹æ–‡çš„ç†è§£ã€éµå¾ªå¤šç²’åº¦ä»»åŠ¡è¦æ±‚ã€ä»¥åŠæ‰§è¡Œç»†ç²’åº¦ä»£ç ä¿®æ”¹çš„èƒ½åŠ›ã€‚",
    "dimension_quote": "Unlike existing benchmarks, AdaptEval incorporates the following three distinctive features: First, practical context. ... Second, multi-granularity annotation. ... Third, fine-grained evaluation.",
    "evaluation_method": "ç»“åˆé€‚åº”çº§åˆ«ï¼ˆadaptation-levelï¼‰å’Œå‡½æ•°çº§åˆ«ï¼ˆfunction-levelï¼‰æµ‹è¯•çš„ä¸¤å±‚æµ‹è¯•æ¡†æ¶ã€‚è¯„ä¼°æ¨¡å‹åœ¨ä¸ªä½“é€‚åº”æ­¥éª¤ä¸Šçš„è¡¨ç°ã€‚",
    "evaluation_method_quote": "Third, fine-grained evaluation. AdaptEval includes a two-tier testing framework combining adaptation-level and function-level tests, which enables evaluating LLMsâ€™ performance across various individual adaptations.",
    "context_dependency": "å¼ºä¸Šä¸‹æ–‡ä¾èµ–ï¼Œä»»åŠ¡æºè‡ªå¼€å‘è€…çš„å®é™…å®è·µï¼Œä¿ç•™äº†æ¥è‡ªStack Overflowå¸–å­å’Œå…³è”GitHubä»“åº“çš„ä¸°å¯Œä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚",
    "context_dependency_quote": "First, practical context. Tasks in AdaptEval are derived from developersâ€™ practices, preserving rich contextual information from Stack Overflow and GitHub communities.",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ï¼Œå…·ä½“æ˜¯ä»£ç é‡ç”¨å’Œä»£ç é€‚åº”ã€‚",
    "problem_domain_quote": "Reusing code snippets is a widely adopted practice to improve the quality and efficiency of software development. A critical step in this process is adaptation, where developers modify code snippets to fit their specific context.",
    "problem_difficulty": "å®é™…å·¥ç¨‹çº§ï¼ŒåŸºäºçœŸå®çš„ã€è·¨å¹³å°çš„å¼€å‘è€…é€‚åº”å®è·µã€‚",
    "problem_difficulty_quote": "comprising 164 tasks derived from real-world, cross-platform adaptation practices of developers.",
    "language": "Python",
    "language_quote": "To address these challenges, we propose AdaptEval, the first benchmark for code snippet adaptation, comprising 164 tasks with 523 adaptations in Python language.",
    "data_size": "åŒ…å«164ä¸ªä»»åŠ¡ï¼Œå…±523ä¸ªé€‚åº”æ­¥éª¤ã€‚",
    "data_size_quote": "To address these challenges, we propose AdaptEval, the first benchmark for code snippet adaptation, comprising 164 tasks with 523 adaptations in Python language.",
    "source_type": "æ•°æ®æ¥æºäºå¼€å‘è€…çš„å®é™…é€‚åº”å®è·µï¼Œé€šè¿‡æ”¶é›†Stack Overflowå¸–å­ã€å…³è”çš„GitHubä»“åº“ï¼Œå¹¶ç»è¿‡å…‹éš†æ£€æµ‹å’Œæ•°æ®æ¸…æ´—æ„å»ºè€Œæˆã€‚",
    "source_type_quote": "Firstly, each task in AdaptEval is collected from the actual adaptation practice of developers. We preserve the original context by including the referenced SO post and the associated GitHub repository for better understanding and traceability.",
    "last_updated": "2026-01-08 (æ ¹æ®arXivç‰ˆæœ¬v1æ—¥æœŸ)",
    "last_updated_quote": "arXiv:2601.04540v1  [cs.SE]  8 Jan 2026",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œç”±ç ”ç©¶å›¢é˜Ÿæ„å»ºå’Œæ ‡æ³¨ã€‚",
    "build_type_quote": "Overall, it takes approximately 550 person-hours to construct AdaptEval, covering adaptations in 38 types.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç¼–è¾‘/é€‚åº”ï¼Œæ¶‰åŠå¯¹ç°æœ‰ä»£ç ç‰‡æ®µçš„ä¿®æ”¹ä»¥é€‚åº”æ–°ä¸Šä¸‹æ–‡ã€‚",
    "task_granularity_quote": "A critical step in this process is adaptation, where developers modify code snippets to fit their specific context.",
    "evaluation_metrics": "pass@1ï¼ˆæ–‡ä¸­æåŠï¼‰ï¼Œä»¥åŠé€‚åº”çº§åˆ«å’Œå‡½æ•°çº§åˆ«çš„æµ‹è¯•é€šè¿‡ç‡ã€‚",
    "evaluation_metrics_quote": "Compared with task-level requirements, all LLMs perform significantly better with adaptation-level steps, where an increase up to 34.84% in pass@1 is observed.",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆä»»åŠ¡çº§å’Œé€‚åº”çº§æè¿°ï¼‰ä¸ä»£ç ï¼ˆåŸå§‹ä»£ç ç‰‡æ®µåŠå…¶ä¸Šä¸‹æ–‡ï¼‰ã€‚",
    "input_modality_quote": "Task-level annotations describe concise developer intentions... while adaptation-level ones simulate developersâ€™ step-by-step adaptation process, serving as specific instructions for LLMs to implement code changes.",
    "output_modality": "ä»£ç ï¼ˆé€‚åº”åçš„ä»£ç ç‰‡æ®µï¼‰ã€‚",
    "output_modality_quote": "serving as specific instructions for LLMs to implement code changes.",
    "task_io_type": "æ–‡æœ¬ï¼ˆä»»åŠ¡æè¿°ï¼‰ä¸ä»£ç ï¼ˆåŸå§‹ä»£ç ï¼‰åˆ°ä»£ç ï¼ˆé€‚åº”åä»£ç ï¼‰ã€‚",
    "task_io_type_quote": "Task-level annotations describe concise developer intentions... serving as specific instructions for LLMs to implement code changes.",
    "execution_environment": "éœ€è¦ç‰¹å®šä¾èµ–ï¼ŒåŸºå‡†åŒ…å«ä»åŸå§‹GitHubä»“åº“æå–çš„ä¾èµ–ä¿¡æ¯ã€‚",
    "execution_environment_quote": "Adaptation Dependencies ... External: logging.LoggerAdapter",
    "unique_features": "1. å®é™…ä¸Šä¸‹æ–‡ï¼šä»»åŠ¡æºè‡ªå¼€å‘è€…å®è·µï¼Œä¿ç•™Stack Overflowå’ŒGitHubçš„ä¸°å¯Œä¸Šä¸‹æ–‡ã€‚2. å¤šç²’åº¦æ ‡æ³¨ï¼šæ¯ä¸ªä»»åŠ¡éƒ½æœ‰ä»»åŠ¡çº§å’Œé€‚åº”çº§çš„æè¿°ã€‚3. ç»†ç²’åº¦è¯„ä¼°ï¼šä¸¤å±‚æµ‹è¯•æ¡†æ¶ï¼ˆé€‚åº”çº§å’Œå‡½æ•°çº§ï¼‰ï¼Œæ”¯æŒè¯„ä¼°ä¸ªä½“é€‚åº”æ­¥éª¤ã€‚è¦†ç›–38ç§é€‚åº”ç±»å‹ã€‚",
    "unique_features_quote": "Unlike existing benchmarks, AdaptEval incorporates the following three distinctive features: First, practical context. Tasks in AdaptEval are derived from developersâ€™ practices, preserving rich contextual information from Stack Overflow and GitHub communities. Second, multi-granularity annotation. Each task is annotated with requirements at both task and adaptation levels, supporting the evaluation of LLMs across diverse adaptation scenarios. Third, fine-grained evaluation. AdaptEval includes a two-tier testing framework combining adaptation-level and function-level tests, which enables evaluating LLMsâ€™ performance across various individual adaptations.",
    "data_size_quantity": 164,
    "data_size_unit": "ä¸ªä»»åŠ¡",
    "last_updated_year": 2026,
    "last_updated_month": 1,
    "last_updated_day": 8,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç‰‡æ®µé€‚åº”èƒ½åŠ›', 'å®é™…ä¸Šä¸‹æ–‡çš„ç†è§£', 'å¤šç²’åº¦ä»»åŠ¡è¦æ±‚', 'æ‰§è¡Œç»†ç²’åº¦ä»£ç ä¿®æ”¹çš„èƒ½åŠ›']",
    "evaluation_method_normalized": "['pass@1', 'é€‚åº”çº§åˆ«æµ‹è¯•é€šè¿‡ç‡', 'å‡½æ•°çº§åˆ«æµ‹è¯•é€šè¿‡ç‡']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'ä»£ç é‡ç”¨', 'ä»£ç é€‚åº”']",
    "source_type_normalized": "['Stack Overflowå¸–å­', 'GitHubä»“åº“', 'å…‹éš†æ£€æµ‹', 'æ•°æ®æ¸…æ´—']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ä¿®å¤",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.05187_output/content.md",
    "benchmark_name": "SimuBench",
    "benchmark_name_quote": "We release SimuBench, the first large-scale benchmark for LLM-based Simulink modeling.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We release SimuBench, the first large-scale benchmark for LLM-based Simulink modeling.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ç”¨äºè¯„ä¼°åŸºäºLLMçš„Simulinkå»ºæ¨¡èƒ½åŠ›çš„åŸºå‡†ã€‚ä»»åŠ¡åŒ…æ‹¬æ¨¡å‹åˆ›å»ºã€ä¿®æ”¹å’Œé—®ç­”ï¼ˆQ&Aï¼‰ã€‚",
    "task_description_quote": "Tasks include model creation, modification, and question-answering (Q&A), with complete source files, XML representations, and visualizations for reproducible evaluation.",
    "dimension": "LLMåœ¨å›¾å½¢åŒ–ã€ç»“æ„åŒ–ã€éæ–‡æœ¬é¢†åŸŸçš„æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šé¢†åŸŸç‰©ç†ç³»ç»Ÿå»ºæ¨¡æ–¹é¢çš„èƒ½åŠ›ã€‚",
    "dimension_quote": "This makes it an ideal testbed for evaluating and advancing LLM reasoning capabilities in highly structured, non-textual domains.",
    "evaluation_method": "æ–‡ä¸­æœªæ˜ç¡®æè¿°SimuBenchçš„å…·ä½“è¯„ä¼°æ–¹æ³•ã€‚",
    "evaluation_method_quote": NaN,
    "context_dependency": "æ¶‰åŠåˆ†å±‚ã€å›¾å½¢åŒ–çš„å¤æ‚æ¡†å›¾ã€ä¿¡å·è·¯ç”±å’Œä¸¥æ ¼çš„æ‹“æ‰‘çº¦æŸï¼Œå¯èƒ½åŒ…å«æ•°ç™¾ä¸ªæ¨¡å—çš„å·¥ä¸šç³»ç»Ÿæ¨¡å‹ã€‚",
    "context_dependency_quote": "Simulink employs a hierarchical, graphical paradigm with complex block diagrams, signal routing, and strict topological constraints. ... industrial systems with hundreds of blocks...",
    "problem_domain": "æ§åˆ¶ã€æœºæ¢°ã€ç”µæ°”ã€æµä½“ã€çƒ­åŠ›å­¦å’Œç”µç£å­¦ç­‰å¤šä¸ªç‰©ç†å·¥ç¨‹é¢†åŸŸã€‚",
    "problem_domain_quote": "It comprises 5300 tasks across control, mechanical, electrical, fluid, thermal, and electromagnetic domains.",
    "problem_difficulty": "æ–‡ä¸­æœªæ˜ç¡®æè¿°ä»»åŠ¡éš¾åº¦ç­‰çº§ã€‚",
    "problem_difficulty_quote": NaN,
    "language": "Simulinkå»ºæ¨¡ç¯å¢ƒï¼ˆå›¾å½¢åŒ–å»ºæ¨¡è¯­è¨€ï¼‰ï¼Œä»¥åŠç”¨äºè¡¨ç¤ºæ¨¡å‹çš„Pythonå­—å…¸/JSONæ ¼å¼ã€‚",
    "language_quote": "SimuAgent transforms Simulink models into a compact Python dictionary (JSON) format that LLMs can process more efficiently.",
    "data_size": "åŒ…å«5300ä¸ªä»»åŠ¡ã€‚",
    "data_size_quote": "It comprises 5300 tasks across control, mechanical, electrical, fluid, thermal, and electromagnetic domains.",
    "source_type": "æ–‡ä¸­æœªæ˜ç¡®æè¿°æ•°æ®æ¥æºã€‚",
    "source_type_quote": NaN,
    "last_updated": "2026å¹´1æœˆ8æ—¥ï¼ˆé¢„å°æœ¬æ—¥æœŸï¼‰",
    "last_updated_quote": "arXiv:2601.05187v1  [cs.AI]  8 Jan 2026",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆæœ¬æ–‡ä½œè€…å‘å¸ƒï¼‰",
    "build_type_quote": "We release SimuBench, the first large-scale benchmark for LLM-based Simulink modeling.",
    "contamination_status": "æ–‡ä¸­æœªè®¨è®ºæ•°æ®æ±¡æŸ“çŠ¶æ€ã€‚",
    "contamination_status_quote": NaN,
    "dataset_license": "æ–‡ä¸­æœªæåŠè®¸å¯è¯ã€‚",
    "dataset_license_quote": NaN,
    "task_granularity": "æ¨¡å‹åˆ›å»ºã€æ¨¡å‹ä¿®æ”¹ã€é—®ç­”ï¼ˆQ&Aï¼‰ã€‚",
    "task_granularity_quote": "Tasks include model creation, modification, and question-answering (Q&A)...",
    "evaluation_metrics": "æ–‡ä¸­æœªæ˜ç¡®æåŠSimuBenchçš„å…·ä½“è¯„ä¼°æŒ‡æ ‡ã€‚",
    "evaluation_metrics_quote": NaN,
    "input_modality": "è‡ªç„¶è¯­è¨€ä»»åŠ¡æè¿°ï¼ˆå¦‚â€œæ„å»ºä¸€ä¸ªçº¿æ€§LEDé©±åŠ¨å™¨æ¨¡å‹...â€ï¼‰ï¼Œä»¥åŠå¯èƒ½çš„Simulinkæ¨¡å‹è¡¨ç¤ºï¼ˆPythonå­—å…¸/JSONï¼‰ã€‚",
    "input_modality_quote": "Build a linear LED driver model with three LEDs connected in series. Measure the current and the total radiant power of the LEDs.",
    "output_modality": "Simulinkæ¨¡å‹ï¼ˆä»¥Pythonå­—å…¸/JSONæ ¼å¼è¡¨ç¤ºï¼‰ï¼Œä»¥åŠå¯èƒ½çš„é—®ç­”ç­”æ¡ˆã€‚",
    "output_modality_quote": "SimuAgent transforms Simulink models into a compact Python dictionary (JSON) format...",
    "task_io_type": "æ–‡æœ¬åˆ°æ¨¡å‹ï¼ˆè‡ªç„¶è¯­è¨€æè¿°åˆ°Simulinkæ¨¡å‹è¡¨ç¤ºï¼‰ã€‚",
    "task_io_type_quote": "Build a linear LED driver model with three LEDs connected in series. Measure the current and the total radiant power of the LEDs.",
    "execution_environment": "Simulinkå»ºæ¨¡ä¸ä»¿çœŸç¯å¢ƒï¼Œä»¥åŠç”¨äºå¿«é€ŸéªŒè¯çš„è½»é‡çº§Pythonæµ‹è¯•ç¯å¢ƒã€‚",
    "execution_environment_quote": "Integrated with an in-process Python test harness, SimuAgent enables instant structural validation and parameter tuning. ... SimuAgent includes a local Python-based testing environment testbed that performs static checks on signal types, parameter ranges, and port wiring.",
    "unique_features": "é¦–ä¸ªé¢å‘LLMçš„ã€å¤§è§„æ¨¡ã€å¤šé¢†åŸŸçš„Simulinkå»ºæ¨¡åŸºå‡†ã€‚åŒ…å«å®Œæ•´çš„æºæ–‡ä»¶ã€XMLè¡¨ç¤ºå’Œå¯è§†åŒ–ï¼Œç”¨äºå¯å¤ç°çš„è¯„ä¼°ã€‚",
    "unique_features_quote": "We release SimuBench, the first large-scale benchmark for LLM-based Simulink modeling. ... with complete source files, XML representations, and visualizations for reproducible evaluation.",
    "data_size_quantity": 5300,
    "data_size_unit": "ä¸ªä»»åŠ¡",
    "last_updated_year": 2026,
    "last_updated_month": 1,
    "last_updated_day": 8,
    "language_normalized": "['Simulinkå»ºæ¨¡ç¯å¢ƒ', 'å›¾å½¢åŒ–å»ºæ¨¡è¯­è¨€', 'Pythonå­—å…¸', 'JSONæ ¼å¼']",
    "dimension_normalized": "['LLMåœ¨å›¾å½¢åŒ–é¢†åŸŸçš„æ¨ç†èƒ½åŠ›', 'LLMåœ¨ç»“æ„åŒ–é¢†åŸŸçš„æ¨ç†èƒ½åŠ›', 'LLMåœ¨éæ–‡æœ¬é¢†åŸŸçš„æ¨ç†èƒ½åŠ›', 'å¤šé¢†åŸŸç‰©ç†ç³»ç»Ÿå»ºæ¨¡æ–¹é¢çš„èƒ½åŠ›']",
    "evaluation_method_normalized": "[]",
    "problem_domain_normalized": "['æ§åˆ¶', 'æœºæ¢°', 'ç”µæ°”', 'æµä½“', 'çƒ­åŠ›å­¦', 'ç”µç£å­¦', 'ç‰©ç†å·¥ç¨‹é¢†åŸŸ']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "JSON",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.04920_output/content.md",
    "benchmark_name": "ELOPE (Event-based Lunar OPtical flow Egomotion estimation) competition",
    "benchmark_name_quote": "This paper presents a case study of using ChatGPT for rapid prototyping in ESAâ€™s ELOPE (Event-based Lunar OPtical flow Egomotion estimation) competition.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "This paper presents a case study of using ChatGPT for rapid prototyping in ESAâ€™s ELOPE (Event-based Lunar OPtical flow Egomotion estimation) competition.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ä»äº‹ä»¶ç›¸æœºã€IMUå’Œé›·è¾¾æµ‹è·æ•°æ®ä¸­ä¼°è®¡æœˆçƒç€é™†å™¨çš„3Dé€Ÿåº¦ã€‚",
    "task_description_quote": "The target of the ELOPE (Event-based Lunar OPtical flow Egomotion estimation) competition is to estimate the 3D velocity of a lunar lander from the data of an event camera, an IMU and radar range measurements.",
    "dimension": "ç®—æ³•å®ç°ä¸ç§‘å­¦åŸå‹å¼€å‘èƒ½åŠ›",
    "dimension_quote": "This paper presents a case study of using ChatGPT for rapid prototyping in ESAâ€™s ELOPE (Event-based Lunar OPtical flow Egomotion estimation) competition.",
    "evaluation_method": "ç«èµ›æ’åä¸å¾—åˆ†",
    "evaluation_method_quote": "we achieved second place with a score of 0.01282",
    "context_dependency": "å¤šæ¨¡æ€ä¼ æ„Ÿå™¨æ•°æ®æµå¤„ç†",
    "context_dependency_quote": "The data set of the ELOPE challenge encompasses 93 simulated landing sequences. For each landing sequence, three major data elements are provided: camera event stream, trajectory and range meter reading.",
    "problem_domain": "è®¡ç®—æœºè§†è§‰ã€æœºå™¨äººå­¦ã€èˆªå¤©å·¥ç¨‹ï¼ˆæœˆçƒç€é™†ï¼‰",
    "problem_domain_quote": "The competition required participants to process event camera data in order to estimate lunar lander trajectories.",
    "problem_difficulty": "é«˜éš¾åº¦ç«èµ›çº§",
    "problem_difficulty_quote": "The competition required participants to process event camera data in order to estimate lunar lander trajectories. Entering late in the challenge, we faced a compressed development timeline â€” an ideal test for evaluating whether conversational AI could accelerate the scientific prototyping process.",
    "language": "æ–‡ä¸­æœªæ˜ç¡®æåŠï¼Œä½†å®éªŒæ¶‰åŠä»£ç ç”Ÿæˆï¼Œæ¨æµ‹ä¸ºPythonç­‰ç§‘å­¦è®¡ç®—å¸¸ç”¨è¯­è¨€ã€‚",
    "language_quote": NaN,
    "data_size": "åŒ…å«93ä¸ªæ¨¡æ‹Ÿç€é™†åºåˆ—ï¼Œå…¶ä¸­28ä¸ªè®­ç»ƒåºåˆ—ï¼Œ65ä¸ªæµ‹è¯•åºåˆ—ã€‚",
    "data_size_quote": "The data set of the ELOPE challenge encompasses 93 simulated landing sequences. The sequences are divided into 28 train sequences and 65 test sequences.",
    "source_type": "æ¨¡æ‹Ÿæ•°æ®",
    "source_type_quote": "The data set of the ELOPE challenge encompasses 93 simulated landing sequences.",
    "last_updated": "2025",
    "last_updated_quote": "The competition ran from May to August 2025.",
    "build_type": "å®˜æ–¹ç«èµ›ç»„ç»‡è€…æ„å»º",
    "build_type_quote": "organized by ESAâ€™s Advanced Concepts Team (ACT) in partnership with University of Adelaide and TU Delft.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ç«¯åˆ°ç«¯ç³»ç»Ÿå¼€å‘ï¼ˆæ•°æ®å¤„ç†ã€ç®—æ³•è®¾è®¡ã€ä»£ç å®ç°ï¼‰",
    "task_granularity_quote": "The competition required participants to process event camera data in order to estimate lunar lander trajectories.",
    "evaluation_metrics": "ç«èµ›å¾—åˆ†ï¼ˆå…·ä½“æŒ‡æ ‡æœªè¯¦è¿°ï¼‰",
    "evaluation_metrics_quote": "we achieved second place with a score of 0.01282",
    "input_modality": "äº‹ä»¶ç›¸æœºæ•°æ®æµã€IMUæ•°æ®ã€é›·è¾¾æµ‹è·æ•°æ®",
    "input_modality_quote": "The target of the ELOPE (Event-based Lunar OPtical flow Egomotion estimation) competition is to estimate the 3D velocity of a lunar lander from the data of an event camera, an IMU and radar range measurements.",
    "output_modality": "3Dé€Ÿåº¦å‘é‡ (vx, vy, vz)",
    "output_modality_quote": "The goal of the competition was to estimate the missing velocities (vx, vy, vz) for each test trajectory.",
    "task_io_type": "å¤šæ¨¡æ€ä¼ æ„Ÿå™¨æ•°æ®åˆ°è¿åŠ¨å‚æ•°",
    "task_io_type_quote": "estimate the 3D velocity of a lunar lander from the data of an event camera, an IMU and radar range measurements.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºäº‹ä»¶ç›¸æœºæ•°æ®å¤„ç†å’Œæœˆçƒç€é™†åœºæ™¯çš„ç‰¹å®šç§‘å­¦ç«èµ›ï¼›æ•°æ®åŒ…å«æ¨¡æ‹Ÿçš„äº‹ä»¶æµã€è½¨è¿¹å’Œæµ‹è·è¯»æ•°ã€‚",
    "unique_features_quote": "The target of the ELOPE (Event-based Lunar OPtical flow Egomotion estimation) competition is to estimate the 3D velocity of a lunar lander from the data of an event camera, an IMU and radar range measurements. Event cameras are a special type of camera that do not output image frames but pixel events based on brightness changes.",
    "data_size_quantity": 93,
    "data_size_unit": "ä¸ªæ¨¡æ‹Ÿç€é™†åºåˆ—",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ç®—æ³•å®ç°', 'ç§‘å­¦åŸå‹å¼€å‘èƒ½åŠ›']",
    "evaluation_method_normalized": "['ç«èµ›å¾—åˆ†']",
    "problem_domain_normalized": "['è®¡ç®—æœºè§†è§‰', 'æœºå™¨äººå­¦', 'èˆªå¤©å·¥ç¨‹']",
    "source_type_normalized": "['æ¨¡æ‹Ÿæ•°æ®']",
    "problem_difficulty_normalized": "ç«èµ›çº§",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.04996_output/content.md",
    "benchmark_name": "AlgBench",
    "benchmark_name_quote": "To address the limitations illustrated above, we introduce AlgBench, a novel benchmark for evaluating LRMsâ€™ algorithmic reasoning.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "To address the limitations illustrated above, we introduce AlgBench, a novel benchmark for evaluating LRMsâ€™ algorithmic reasoning.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°å¤§å‹æ¨ç†æ¨¡å‹ï¼ˆLRMsï¼‰çš„ç®—æ³•æ¨ç†èƒ½åŠ›ï¼Œå³æ¨¡å‹æ˜¯å¦çœŸæ­£æŒæ¡å’Œç†è§£ç®—æ³•ã€‚",
    "task_description_quote": "AlgBench, an expert-curated benchmark that evaluates LRMs under an algorithm-centric paradigm.",
    "dimension": "ç®—æ³•æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯å¯¹ç‰¹å®šç®—æ³•çš„æŒæ¡ç¨‹åº¦ã€‚",
    "dimension_quote": "evaluates LRMs under an algorithm-centric paradigm.",
    "evaluation_method": "Pass@kï¼Œ Z-score",
    "evaluation_method_quote": "Evaluator Pass@k Z-score",
    "context_dependency": "å•ç®—æ³•é—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜è®¾è®¡ä¸ºä»…éœ€ä¸€ç§ç‰¹å®šç®—æ³•ã€‚",
    "context_dependency_quote": "we manually curated a collection of 3,000 original problems covering 27 distinct algorithms, each designed to require exactly one specific algorithm",
    "problem_domain": "ç®—æ³•é¢†åŸŸï¼Œæ¶µç›–æ¬§å‡ é‡Œå¾—ç»“æ„ã€éæ¬§å‡ é‡Œå¾—ç»“æ„ã€éä¼˜åŒ–ã€å±€éƒ¨ä¼˜åŒ–ã€å…¨å±€ä¼˜åŒ–å’Œå¯å‘å¼ä¼˜åŒ–ç­‰ç±»åˆ«ã€‚",
    "problem_domain_quote": "including Euclidean-structured, non-Euclidean-structured, non-optimized, local-optimized, global-optimized, and heuristic-optimized categories.",
    "problem_difficulty": "æ ¹æ®è®¡ç®—å¤æ‚åº¦åˆ†ä¸ºä¸‰ä¸ªéš¾åº¦çº§åˆ«ã€‚",
    "problem_difficulty_quote": "we divide each problem into three difficulty levels, according to the computation complexity.",
    "language": NaN,
    "language_quote": NaN,
    "data_size": "è¶…è¿‡3000ä¸ªåŸåˆ›é—®é¢˜ï¼Œæ¶µç›–27ç§ä¸åŒçš„ç®—æ³•ã€‚",
    "data_size_quote": "The resulting dataset comprises over 3,000 original problems spanning 27 distinct algorithms",
    "source_type": "ç”±ACMç®—æ³•ä¸“å®¶æ‰‹åŠ¨æ„å»ºçš„åŸåˆ›é—®é¢˜ï¼Œä¸ä»»ä½•ç°æœ‰å¼€æºå¹³å°æ— é‡å ã€‚",
    "source_type_quote": "we engaged a group of algorithm experts... we manually curated a collection of 3,000 original problems... verified to have no overlap with any existing open-source platforms.",
    "last_updated": "2026-01-08 (æ ¹æ®arXivç‰ˆæœ¬v1æ—¥æœŸ)",
    "last_updated_quote": "arXiv:2601.04996v1  [cs.AI]  8 Jan 2026",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œç”±ç®—æ³•ä¸“å®¶æ‰‹åŠ¨ç­–åˆ’ã€‚",
    "build_type_quote": "we engaged a group of algorithm experts... we manually curated a collection of 3,000 original problems",
    "contamination_status": "æŠ—æ±¡æŸ“è®¾è®¡ï¼Œæ‰€æœ‰é—®é¢˜å‡ä¸ºåŸåˆ›ï¼Œä¸ç°æœ‰å¼€æºå¹³å°æ— é‡å ã€‚",
    "contamination_status_quote": "verified to have no overlap with any existing open-source platforms. This construction process effectively mitigates both the evaluation bias and data contamination concerns",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ç®—æ³•æ¨ç†ä¸ä»£ç ç”Ÿæˆï¼ˆæ ¹æ®ä¸Šä¸‹æ–‡æ¨æ–­ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åº”ç”¨ç‰¹å®šç®—æ³•è§£å†³é—®é¢˜çš„èƒ½åŠ›ï¼‰ã€‚",
    "task_granularity_quote": "evaluates LRMs under an algorithm-centric paradigm... each designed to require exactly one specific algorithm",
    "evaluation_metrics": "Pass@kï¼Œ Z-score",
    "evaluation_metrics_quote": "Evaluator Pass@k Z-score",
    "input_modality": "è‡ªç„¶è¯­è¨€æè¿°çš„é—®é¢˜ï¼ˆæ ¹æ®ä¸Šä¸‹æ–‡æ¨æ–­ï¼Œæ—¨åœ¨è¯„ä¼°ç®—æ³•æ¨ç†ï¼‰ã€‚",
    "input_modality_quote": "Alg-centric Task + Input + Correct Answer",
    "output_modality": "ä»£ç æˆ–ç®—æ³•è§£å†³æ–¹æ¡ˆï¼ˆæ ¹æ®ä¸Šä¸‹æ–‡æ¨æ–­ï¼Œæ—¨åœ¨è¯„ä¼°ç®—æ³•æ¨ç†ï¼‰ã€‚",
    "output_modality_quote": "Alg-centric Task + Input + Correct Answer",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç /ç®—æ³•",
    "task_io_type_quote": "Alg-centric Task + Input + Correct Answer",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "1. èŒƒå¼è½¬å˜ï¼šé‡‡ç”¨ç®—æ³•ä¸­å¿ƒèŒƒå¼ï¼Œè€Œéé—®é¢˜ä¸­å¿ƒèŒƒå¼ï¼Œä»¥è¯„ä¼°æ¨¡å‹å¯¹å•ä¸ªç®—æ³•çš„æŒæ¡ã€‚2. æ–°é¢–çš„åˆ†ç±»æ³•ï¼šåŸºäºç®—æ³•æ ¸å¿ƒæ€æƒ³ï¼ˆä¼˜åŒ–å¯¼å‘å’Œç»“æ„å¯¼å‘ï¼‰è¿›è¡Œåˆ†ç±»ã€‚3. æ—¨åœ¨è§£å†³ç°æœ‰åŸºå‡†çš„å±€é™æ€§ï¼šå¦‚æ— æ³•è¯„ä¼°å•ä¸€ç®—æ³•ã€ç®—æ³•è¦†ç›–èŒƒå›´æœ‰é™ã€æ•°æ®æ±¡æŸ“é£é™©é«˜ã€‚",
    "unique_features_quote": "Paradigm Shift: Unlike prior benchmarks that adhere to a problem-centric paradigm, we are the first to adopt the algorithm-centric paradigm... Novel-taxonomic and Contamination-free Dataset Construction: We engage multiple algorithmic experts to manually construct problems guided by a novel algorithmic taxonomy... To address the limitations illustrated above, we introduce AlgBench",
    "data_size_quantity": 3000,
    "data_size_unit": "ä¸ªé—®é¢˜",
    "last_updated_year": 2026,
    "last_updated_month": 1,
    "last_updated_day": 8,
    "language_normalized": "[]",
    "dimension_normalized": "['ç®—æ³•æ¨ç†èƒ½åŠ›', 'å¯¹ç‰¹å®šç®—æ³•çš„æŒæ¡ç¨‹åº¦']",
    "evaluation_method_normalized": "['Pass@k', 'Z-score']",
    "problem_domain_normalized": "['ç®—æ³•é¢†åŸŸ', 'æ¬§å‡ é‡Œå¾—ç»“æ„', 'éæ¬§å‡ é‡Œå¾—ç»“æ„', 'éä¼˜åŒ–', 'å±€éƒ¨ä¼˜åŒ–', 'å…¨å±€ä¼˜åŒ–', 'å¯å‘å¼ä¼˜åŒ–']",
    "source_type_normalized": "['ç”±ACMç®—æ³•ä¸“å®¶æ‰‹åŠ¨æ„å»ºçš„åŸåˆ›é—®é¢˜', 'ä¸ä»»ä½•ç°æœ‰å¼€æºå¹³å°æ— é‡å ']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2601.05777_output/content.md",
    "benchmark_name": "SWE-bench Verified",
    "benchmark_name_quote": "We evaluate EET on the widely adopted SWE-bench Verified benchmark (OpenAI, 2025)",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We evaluate EET on the widely adopted SWE-bench Verified benchmark (OpenAI, 2025)",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è‡ªåŠ¨åŒ–è§£å†³ç°å®ä¸–ç•Œè½¯ä»¶å·¥ç¨‹é—®é¢˜ï¼Œç»™å®šä¸€ä¸ªæè¿°bugæˆ–åŠŸèƒ½è¯·æ±‚çš„é—®é¢˜ä»¥åŠå¯¹åº”çš„ä»£ç ä»“åº“ï¼Œç›®æ ‡æ˜¯å¯¼èˆªä»£ç åº“ã€å®šä½ç›¸å…³ä»£ç å¹¶ç”Ÿæˆè¡¥ä¸æ¥è§£å†³é—®é¢˜ï¼Œå¹¶é€šè¿‡ç›¸å…³æµ‹è¯•éªŒè¯æ­£ç¡®æ€§ã€‚",
    "task_description_quote": "Given an issue describing a bug or a feature request along with the corresponding code repository, an SE agent aims to navigate the codebase, localize relevant code, and generate a patch (i.e., modify the code) to resolve the issue, with correctness validated by the associated tests",
    "dimension": "è½¯ä»¶å·¥ç¨‹ä»£ç†çš„è‡ªåŠ¨åŒ–é—®é¢˜è§£å†³èƒ½åŠ›",
    "dimension_quote": "SWE-bench (Jimenez et al., 2024) and its human-validated variant SWE-bench Verified (OpenAI, 2025) have become standard benchmarks for evaluating automated resolution of real-world SE issues.",
    "evaluation_method": "é€šè¿‡æ‰§è¡Œæµ‹è¯•éªŒè¯ç”Ÿæˆçš„è¡¥ä¸æ˜¯å¦æ­£ç¡®è§£å†³é—®é¢˜",
    "evaluation_method_quote": "Issue resolution typically includes a patch evaluation phase, which validates whether a generated patch resolves the issue by executing tests.",
    "context_dependency": "å¤šæ–‡ä»¶é¡¹ç›®ï¼Œéœ€è¦å¯¼èˆªå’Œç†è§£æ•´ä¸ªä»£ç åº“",
    "context_dependency_quote": "Given an issue describing a bug or a feature request along with the corresponding code repository, an SE agent aims to navigate the codebase, localize relevant code, and generate a patch",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ï¼ŒåŒ…æ‹¬bugä¿®å¤å’ŒåŠŸèƒ½å®ç°",
    "problem_domain_quote": "Given an issue describing a bug or a feature request",
    "problem_difficulty": "ç°å®ä¸–ç•Œè½¯ä»¶å·¥ç¨‹é—®é¢˜",
    "problem_difficulty_quote": "standard benchmarks for evaluating automated resolution of real-world SE issues.",
    "language": NaN,
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": "ç°å®ä¸–ç•Œè½¯ä»¶ä»“åº“ä¸­çš„é—®é¢˜",
    "source_type_quote": "Real-world issue descriptions are often lengthy and detailed (Jimenez et al., 2024), containing background information, discussion, and auxiliary context that is not directly relevant to resolution.",
    "last_updated": "2025",
    "last_updated_quote": "SWE-bench Verified (OpenAI, 2025)",
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ä¿®å¤/è¡¥ä¸ç”Ÿæˆ",
    "task_granularity_quote": "generate a patch (i.e., modify the code) to resolve the issue",
    "evaluation_metrics": "è§£å†³ç‡",
    "evaluation_metrics_quote": "with negligible loss in resolution rate (at most 0.2%)",
    "input_modality": "è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°å’Œä»£ç ä»“åº“",
    "input_modality_quote": "Given an issue describing a bug or a feature request along with the corresponding code repository",
    "output_modality": "ä»£ç è¡¥ä¸",
    "output_modality_quote": "generate a patch (i.e., modify the code)",
    "task_io_type": "æ–‡æœ¬ä¸ä»£ç åˆ°ä»£ç ",
    "task_io_type_quote": "Given an issue describing a bug or a feature request along with the corresponding code repository, an SE agent aims to navigate the codebase, localize relevant code, and generate a patch",
    "execution_environment": "ä»£ç ä»“åº“å’Œæ‰§è¡Œç¯å¢ƒ",
    "execution_environment_quote": "interacting with the code repository and execution environment through tool invocations such as file editing and command execution",
    "unique_features": "SWE-benchçš„äººç±»éªŒè¯å˜ä½“ï¼Œä¸“æ³¨äºè¯„ä¼°è‡ªåŠ¨åŒ–è§£å†³ç°å®ä¸–ç•Œè½¯ä»¶å·¥ç¨‹é—®é¢˜çš„èƒ½åŠ›",
    "unique_features_quote": "SWE-bench (Jimenez et al., 2024) and its human-validated variant SWE-bench Verified (OpenAI, 2025) have become standard benchmarks for evaluating automated resolution of real-world SE issues.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['è½¯ä»¶å·¥ç¨‹ä»£ç†çš„è‡ªåŠ¨åŒ–é—®é¢˜è§£å†³èƒ½åŠ›']",
    "evaluation_method_normalized": "['è§£å†³ç‡']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'bugä¿®å¤', 'åŠŸèƒ½å®ç°']",
    "source_type_normalized": "['ç°å®ä¸–ç•Œè½¯ä»¶ä»“åº“ä¸­çš„é—®é¢˜']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ä¿®å¤",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.05772_output/content.md",
    "benchmark_name": "StriderSPD benchmark",
    "benchmark_name_quote": "To enable more realistic evaluation, we construct a binary SPD benchmark that is disjoint from prior datasets in both projects and domains and extensively evaluate StriderSPD on this benchmark.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "To enable more realistic evaluation, we construct a binary SPD benchmark that is disjoint from prior datasets in both projects and domains and extensively evaluate StriderSPD on this benchmark.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "åœ¨é—­æºè½¯ä»¶ä¸­ï¼Œæ£€æµ‹äºŒè¿›åˆ¶è¡¥ä¸å‰åç‰ˆæœ¬ä¹‹é—´çš„å·®å¼‚æ˜¯å¦å¯¹åº”äºå®‰å…¨æ¼æ´ä¿®å¤ã€‚å…·ä½“ä»»åŠ¡æ˜¯åœ¨å‡½æ•°çº§åˆ«å¯¹æ¯ä¸ªå‘ç”Ÿå˜åŒ–çš„å‡½æ•°å¯¹è¿›è¡Œåˆ†ç±»ï¼Œåˆ¤æ–­å…¶æ˜¯å¦ä¸ºå®‰å…¨è¡¥ä¸ã€‚",
    "task_description_quote": "In closed-source software, SPD aims to determine whether the differences between a pre-patch binary and its post-patch counterpart indicate a vulnerability fix. ... we formalize binary SPD as shown in Formula 1. Classifier(âŸ¨f pre i , f post i âŸ©) = {security | non-security}",
    "dimension": "äºŒè¿›åˆ¶å®‰å…¨è¡¥ä¸æ£€æµ‹çš„å‡†ç¡®æ€§å’Œæ³›åŒ–èƒ½åŠ›",
    "dimension_quote": "Experimental results demonstrate the effectiveness of StriderSPD, surpassing the best-performing baseline by 12.66% in accuracy. Furthermore, StriderSPD exhibits generalizability to different LLMs, enhancing the performance of Llama, Qwen, and DeepSeek on binary SPD tasks.",
    "evaluation_method": "ä½¿ç”¨å‡†ç¡®ç‡ã€F1åˆ†æ•°å’Œè¯¯æŠ¥ç‡ç­‰æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ã€‚",
    "evaluation_method_quote": "Experimental results demonstrate that StriderSPD outperforms the best baseline Yi-Coder-9B-Chat [18], achieving improvements of 12.66%, 8.20%, and 38.57% in terms of accuracy, F1 score, and false positive rate, respectively.",
    "context_dependency": "å‡½æ•°çº§åˆ«",
    "context_dependency_quote": "To avoid losing change meaning (too fine-grained) or label noise (too coarse-grained), we follow prior work [9, 10, 25] to perform binary SPD at the function level by classifying each changed function pair.",
    "problem_domain": "è½¯ä»¶å®‰å…¨ã€äºŒè¿›åˆ¶ç¨‹åºåˆ†æã€æ¼æ´ä¿®å¤",
    "problem_domain_quote": "Vulnerabilities severely threaten software systems, making the timely application of security patches crucial for mitigating attacks. ... Security Patch Detection (SPD) comes to protect software assets.",
    "problem_difficulty": "å·¥ç¨‹çº§ï¼Œæ¶‰åŠçœŸå®ä¸–ç•Œçš„é—­æºè½¯ä»¶äºŒè¿›åˆ¶è¡¥ä¸åˆ†æ",
    "problem_difficulty_quote": "To enable more realistic evaluation, we construct a binary SPD benchmark that is disjoint from prior datasets in both projects and domains and extensively evaluate StriderSPD on this benchmark. ... reflect real-world closed-source scenarios",
    "language": "æ±‡ç¼–ä»£ç å’Œåç¼–è¯‘å¾—åˆ°çš„ç±»Cä¼ªä»£ç ",
    "language_quote": "Given that assembly code and pseudo-code are the most typical abstraction levels obtained when reversing native binaries [6], the limited binary SPD methods utilize either of them to identify security patches [7â€“10].",
    "data_size": "åŒ…å«1720ä¸ªäºŒè¿›åˆ¶è¡¥ä¸ï¼Œæ¶µç›–äº”ä¸ªä¼˜åŒ–çº§åˆ«ï¼ˆO0, O1, O2, O3, Osï¼‰",
    "data_size_quote": "Our benchmark comprises 1,720 binary patches across five optimization levels (O0, O1, O2, O3, and Os).",
    "source_type": "ä»é¡¹ç›®å’Œé¢†åŸŸä¸Šéƒ½ä¸å…ˆå‰äºŒè¿›åˆ¶SPDæ•°æ®é›†ä¸åŒçš„æ¥æºæ„å»º",
    "source_type_quote": "We construct a cross-project and cross-domain benchmark to more faithfully reflect real-world closed-source scenarios, where historical security patch data from the same project is inaccessible.",
    "last_updated": "2026",
    "last_updated_quote": "IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 14, NO. 8, AUGUST 2026",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "To enable more realistic evaluation, we construct a binary SPD benchmark that is disjoint from prior datasets in both projects and domains and extensively evaluate StriderSPD on this benchmark.",
    "contamination_status": "è®¾è®¡ä¸Šæ—¨åœ¨é¿å…æ±¡æŸ“ï¼Œå…¶é¡¹ç›®å’Œé¢†åŸŸä¸å…ˆå‰æ•°æ®é›†ä¸ç›¸äº¤",
    "contamination_status_quote": "We construct a cross-project and cross-domain benchmark to more faithfully reflect real-world closed-source scenarios, where historical security patch data from the same project is inaccessible.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç åˆ†ç±»ï¼ˆå®‰å…¨è¡¥ä¸ vs. éå®‰å…¨è¡¥ä¸ï¼‰",
    "task_granularity_quote": "Classifier(âŸ¨f pre i , f post i âŸ©) = {security | non-security}",
    "evaluation_metrics": "å‡†ç¡®ç‡ã€F1åˆ†æ•°ã€è¯¯æŠ¥ç‡",
    "evaluation_metrics_quote": "achieving improvements of 12.66%, 8.20%, and 38.57% in terms of accuracy, F1 score, and false positive rate, respectively.",
    "input_modality": "äºŒè¿›åˆ¶è¡¥ä¸å‰åçš„æ±‡ç¼–ä»£ç æ§åˆ¶æµå›¾å’Œä¼ªä»£ç å‡½æ•°",
    "input_modality_quote": "StriderSPD comprises two key components: (1) A structure-guided joint representation neural network that integrates a graph branch into an LLM branch... The graph branch employs a GNN to represent pre- and post-patch assembly-code CFGs... The LLM branch employs Qwen3-8B [17] as the backbone to embed the instruction that contains the pre- and post-patch pseudo-code functions.",
    "output_modality": "åˆ†ç±»æ ‡ç­¾ï¼ˆå®‰å…¨æˆ–éå®‰å…¨ï¼‰",
    "output_modality_quote": "Classifier(âŸ¨f pre i , f post i âŸ©) = {security | non-security}",
    "task_io_type": "ä»£ç åˆ°åˆ†ç±»",
    "task_io_type_quote": "Classifier(âŸ¨f pre i , f post i âŸ©) = {security | non-security}",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“é—¨ä¸ºé—­æºè½¯ä»¶äºŒè¿›åˆ¶å®‰å…¨è¡¥ä¸æ£€æµ‹æ„å»ºçš„åŸºå‡†ï¼Œå…¶é¡¹ç›®å’Œé¢†åŸŸä¸ç°æœ‰æ•°æ®é›†ä¸ç›¸äº¤ï¼Œæ—¨åœ¨æ›´çœŸå®åœ°åæ˜ ç°å®ä¸–ç•Œåœºæ™¯ã€‚åŒ…å«å¤šä¸ªç¼–è¯‘å™¨ä¼˜åŒ–çº§åˆ«çš„è¡¥ä¸ã€‚",
    "unique_features_quote": "We construct a cross-project and cross-domain benchmark to more faithfully reflect real-world closed-source scenarios, where historical security patch data from the same project is inaccessible. Our benchmark comprises 1,720 binary patches across five optimization levels (O0, O1, O2, O3, and Os).",
    "data_size_quantity": 1720,
    "data_size_unit": "ä¸ªäºŒè¿›åˆ¶è¡¥ä¸",
    "last_updated_year": 2026,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['æ±‡ç¼–ä»£ç ', 'åç¼–è¯‘å¾—åˆ°çš„ç±»Cä¼ªä»£ç ']",
    "dimension_normalized": "['äºŒè¿›åˆ¶å®‰å…¨è¡¥ä¸æ£€æµ‹çš„å‡†ç¡®æ€§', 'æ³›åŒ–èƒ½åŠ›']",
    "evaluation_method_normalized": "['å‡†ç¡®ç‡', 'F1åˆ†æ•°', 'è¯¯æŠ¥ç‡']",
    "problem_domain_normalized": "['è½¯ä»¶å®‰å…¨', 'äºŒè¿›åˆ¶ç¨‹åºåˆ†æ', 'æ¼æ´ä¿®å¤']",
    "source_type_normalized": "['ä»é¡¹ç›®å’Œé¢†åŸŸä¸Šéƒ½ä¸å…ˆå‰äºŒè¿›åˆ¶SPDæ•°æ®é›†ä¸åŒçš„æ¥æºæ„å»º']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç åˆ†ç±»",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "task_io_type_normalized": "ä»£ç åˆ°æ–‡æœ¬",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2601.05755_output/content.md",
    "benchmark_name": "SIREN (Systemic Injection & Reasoning Evaluation beNchmark)",
    "benchmark_name_quote": "We evaluate our framework on SIREN (Systemic Injection & Reasoning Evaluation beNchmark) which simulates a realistic execution environment...",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We further introduce SIREN, a benchmark comprising 959 tool stream injection cases designed to simulate pervasive threats characterized by dynamic dependencies.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°LLMæ™ºèƒ½ä½“åœ¨é¢ä¸´å·¥å…·æµæ³¨å…¥æ”»å‡»æ—¶çš„å®‰å…¨æ€§å’Œé²æ£’æ€§ã€‚è¯¥åŸºå‡†æ¨¡æ‹Ÿäº†åŒ…å«åŠ¨æ€ä¾èµ–å…³ç³»çš„çœŸå®æ‰§è¡Œç¯å¢ƒï¼Œæ—¨åœ¨æµ‹è¯•æ™ºèƒ½ä½“åœ¨è§„åˆ’å’Œæ‰§è¡Œé˜¶æ®µæŠµå¾¡æ¶æ„å·¥å…·å®šä¹‰å’Œè¿è¡Œæ—¶åé¦ˆæ”»å‡»çš„èƒ½åŠ›ã€‚",
    "task_description_quote": "SIREN (Systemic Injection & Reasoning Evaluation beNchmark) which simulates a realistic execution environment characterized by 496 competing tools and dynamic dependencies.",
    "dimension": "æ™ºèƒ½ä½“å®‰å…¨æ€§ã€é²æ£’æ€§ã€å¯¹æŠ—å·¥å…·æµæ³¨å…¥æ”»å‡»çš„é˜²å¾¡èƒ½åŠ›",
    "dimension_quote": "We introduce SIREN, a comprehensive benchmark comprising 959 cases across five vectors to simulate agentic reasoning challenges in realistic, stochastic environments.",
    "evaluation_method": "é€šè¿‡æ”»å‡»æˆåŠŸç‡ï¼ˆAttack Success Rate, ASRï¼‰å’Œå—æ”»å‡»ä¸‹çš„æ•ˆç”¨ï¼ˆUtility Under Attack, UAï¼‰ç­‰æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ã€‚",
    "evaluation_method_quote": "Extensive experiments demonstrate that VIGIL outperforms state-of-the-art dynamic defenses by reducing the attack success rate by over 22% while more than doubling the utility under attack compared to static baselines...",
    "context_dependency": "å¤šæ­¥éª¤ã€åŠ¨æ€äº¤äº’ç¯å¢ƒï¼Œæ¶‰åŠå·¥å…·è°ƒç”¨ã€è¿è¡Œæ—¶åé¦ˆå’Œé‡æ–°è§„åˆ’ã€‚",
    "context_dependency_quote": "SIREN comprises 959 tool stream injection cases across five attack vectors that target critical phases of the agent lifecycle...",
    "problem_domain": "æ™ºèƒ½ä½“å®‰å…¨ã€ç½‘ç»œå®‰å…¨ã€å¯¹æŠ—æ€§æœºå™¨å­¦ä¹ ",
    "problem_domain_quote": "LLM agents operating in open environments face escalating risks from indirect prompt injection, particularly within the tool stream...",
    "problem_difficulty": "é«˜éš¾åº¦ï¼Œæ¨¡æ‹ŸçœŸå®ä¸–ç•Œã€éšæœºç¯å¢ƒä¸­çš„å¤æ‚æ¨ç†æŒ‘æˆ˜å’Œå¯¹æŠ—æ€§æ”»å‡»ã€‚",
    "problem_difficulty_quote": "...to simulate agentic reasoning challenges in realistic, stochastic environments.",
    "language": "æœªæ˜ç¡®æåŠç¼–ç¨‹è¯­è¨€ï¼Œä¸»è¦æ¶‰åŠè‡ªç„¶è¯­è¨€æŒ‡ä»¤å’Œå·¥å…·å®šä¹‰ã€‚",
    "language_quote": NaN,
    "data_size": "åŒ…å«959ä¸ªå·¥å…·æµæ³¨å…¥æ¡ˆä¾‹ï¼Œæ¶µç›–5ä¸ªæ”»å‡»å‘é‡ï¼›å¹¶åŒ…å«949ä¸ªæ•°æ®æµåŸºçº¿æ¡ˆä¾‹ã€‚",
    "data_size_quote": "SIREN comprises 959 tool stream injection cases across five attack vectors... alongside 949 data stream baselines from Agent-Dojo (Debenedetti et al., 2024).",
    "source_type": "åŸºäºAgentDojoç¯å¢ƒé‡å»ºï¼Œé€šè¿‡å¼•å…¥è¯­ä¹‰å·¥å…·å†—ä½™å’Œéšæœºè¿è¡Œæ—¶åé¦ˆæ¥æ¨¡æ‹ŸçœŸå®ä¸–ç•ŒæŒ‘æˆ˜ã€‚",
    "source_type_quote": "To evaluate agent robustness against tool stream manipulation, we reconstruct the execution environment based on AgentDojo (Debenedetti et al., 2024) by introducing two architectural features that mirror real-world operational challenges.",
    "last_updated": "2026 (æ ¹æ®arXivç‰ˆæœ¬å·æ¨æ–­)",
    "last_updated_quote": "arXiv:2601.05755v1  [cs.CR]  9 Jan 2026",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œä½œä¸ºæœ¬æ–‡è´¡çŒ®çš„ä¸€éƒ¨åˆ†æå‡ºã€‚",
    "build_type_quote": "We further introduce SIREN, a benchmark comprising 959 tool stream injection cases...",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "å¤šæ­¥éª¤ä»»åŠ¡æ‰§è¡Œä¸å®‰å…¨å†³ç­–",
    "task_granularity_quote": "...target critical phases of the agent lifecycle...",
    "evaluation_metrics": "æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰ã€å—æ”»å‡»ä¸‹çš„æ•ˆç”¨ï¼ˆUAï¼‰",
    "evaluation_metrics_quote": "...reducing the attack success rate by over 22% while more than doubling the utility under attack...",
    "input_modality": "è‡ªç„¶è¯­è¨€ç”¨æˆ·æŒ‡ä»¤ã€å·¥å…·å®šä¹‰ï¼ˆå¯èƒ½åŒ…å«æ¶æ„æŒ‡ä»¤ï¼‰ã€è¿è¡Œæ—¶åé¦ˆ",
    "input_modality_quote": "...injecting forged tool descriptions or deceptive error messages...",
    "output_modality": "æ™ºèƒ½ä½“çš„è¡ŒåŠ¨å†³ç­–ï¼ˆå¦‚å·¥å…·è°ƒç”¨åºåˆ—ï¼‰",
    "output_modality_quote": "...to hijack the execution flow and compel agents to execute unauthorized actions...",
    "task_io_type": "å¤šæ¨¡æ€äº¤äº’ï¼ˆè‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€å·¥å…·å®šä¹‰ã€åé¦ˆåˆ°è¡ŒåŠ¨å†³ç­–ï¼‰",
    "task_io_type_quote": "The tool stream consists of functional definitions and runtime feedback that the model interprets as binding operational constraints...",
    "execution_environment": "æ¨¡æ‹Ÿçš„æ‰§è¡Œç¯å¢ƒï¼ŒåŒ…å«496ä¸ªç«äº‰å·¥å…·å’ŒåŠ¨æ€ä¾èµ–å…³ç³»ï¼Œæ¨¡æ‹Ÿå¤–éƒ¨APIçš„ä¸ç¨³å®šæ€§ã€‚",
    "execution_environment_quote": "...simulates a realistic execution environment characterized by 496 competing tools and dynamic dependencies.",
    "unique_features": "ä¸“æ³¨äºå·¥å…·æµæ³¨å…¥æ”»å‡»ï¼ˆè€Œä¸ä»…ä»…æ˜¯æ•°æ®æµï¼‰ï¼ŒåŒ…å«5ä¸ªä¸åŒçš„æ”»å‡»å‘é‡ï¼Œæ¨¡æ‹Ÿäº†è¯­ä¹‰å·¥å…·å†—ä½™å’Œéšæœºè¿è¡Œæ—¶åé¦ˆï¼Œä»¥è¯„ä¼°æ™ºèƒ½ä½“åœ¨å¤æ‚ã€å¯¹æŠ—æ€§ç¯å¢ƒä¸­çš„æ¨ç†å’Œæ¢å¤èƒ½åŠ›ã€‚",
    "unique_features_quote": "We introduce SIREN, a comprehensive benchmark comprising 959 cases across five vectors to simulate agentic reasoning challenges in realistic, stochastic environments. ... This fragmented approach fails to quantify agent resilience against compounded threats that exploit both instruction-following biases and adaptive reasoning needs. We introduce SIREN to bridge this gap by integrating dual-stream threats and complex reasoning dependencies within a single unified evaluation framework.",
    "data_size_quantity": 959,
    "data_size_unit": "ä¸ªæ¡ˆä¾‹",
    "last_updated_year": 2026,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['æ™ºèƒ½ä½“å®‰å…¨æ€§', 'é²æ£’æ€§', 'å¯¹æŠ—å·¥å…·æµæ³¨å…¥æ”»å‡»çš„é˜²å¾¡èƒ½åŠ›']",
    "evaluation_method_normalized": "['æ”»å‡»æˆåŠŸç‡ï¼ˆASRï¼‰', 'å—æ”»å‡»ä¸‹çš„æ•ˆç”¨ï¼ˆUAï¼‰']",
    "problem_domain_normalized": "['æ™ºèƒ½ä½“å®‰å…¨', 'ç½‘ç»œå®‰å…¨', 'å¯¹æŠ—æ€§æœºå™¨å­¦ä¹ ']",
    "source_type_normalized": "['åŸºäºAgentDojoç¯å¢ƒé‡å»ºï¼Œé€šè¿‡å¼•å…¥è¯­ä¹‰å·¥å…·å†—ä½™å’Œéšæœºè¿è¡Œæ—¶åé¦ˆæ¥æ¨¡æ‹ŸçœŸå®ä¸–ç•ŒæŒ‘æˆ˜ã€‚']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.05752_output/content.md",
    "benchmark_name": "AutoMonitor-Bench",
    "benchmark_name_quote": "We introduce AutoMonitor-Bench, the first benchmark designed to systematically evaluate the reliability of LLM-based misbehavior monitors across diverse tasks and failure modes.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We introduce AutoMonitor-Bench, the first benchmark designed to systematically evaluate the reliability of LLM-based misbehavior monitors across diverse tasks and failure modes.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼‚å¸¸è¡Œä¸ºç›‘æ§å™¨åœ¨è¯†åˆ«å‰æ²¿AIç³»ç»Ÿï¼ˆå¦‚å¤§å‹æ¨ç†æ¨¡å‹å’Œæ™ºèƒ½ä½“ï¼‰çš„å¼‚å¸¸è¡Œä¸ºæ—¶çš„å¯é æ€§ã€‚",
    "task_description_quote": "We introduce AutoMonitor-Bench, the first benchmark designed to systematically evaluate the ability of LLM-based monitors to identify misbehavior in frontier AI systems.",
    "dimension": "å¼‚å¸¸è¡Œä¸ºç›‘æ§çš„å¯é æ€§ï¼ŒåŒ…æ‹¬æ¼æ£€ç‡å’Œè¯¯æŠ¥ç‡ä¹‹é—´çš„æƒè¡¡ã€‚",
    "dimension_quote": "We evaluate monitors using two complementary metrics: Miss Rate (MR) and False Alarm Rate (FAR), capturing failures to detect misbehavior and oversensitivity to benign behavior, respectively.",
    "evaluation_method": "å°†å¼‚å¸¸è¡Œä¸ºæ£€æµ‹è§†ä¸ºäºŒåˆ†ç±»ä»»åŠ¡ï¼Œä½¿ç”¨æ¼æ£€ç‡ï¼ˆMRï¼‰å’Œè¯¯æŠ¥ç‡ï¼ˆFARï¼‰ä¸¤ä¸ªäº’è¡¥æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ã€‚",
    "evaluation_method_quote": "To rigorously evaluate the reliability of LLM-based monitors, we formulate misbehavior detection as a binary classification task... We focus on two complementary metrics... Miss Rate (MR)... False Alarm Rate (FAR)...",
    "context_dependency": "ä¾èµ–äºä»»åŠ¡è¯·æ±‚å’Œå¯¹åº”çš„ç”Ÿæˆè§£å†³æ–¹æ¡ˆæˆ–æ¨ç†è½¨è¿¹ï¼ˆå¯èƒ½åŒ…å«æ˜¾å¼çš„ä¸­é—´æ€è€ƒé˜¶æ®µï¼‰ã€‚",
    "context_dependency_quote": "the LLM-based monitor is provided with a task request and a corresponding generated solution or trajectory... It includes tasks... and evaluates the solution or trajectory both with explicit intermediate thinking stages (e.g., reasoning traces enclosed in <think></think> tags) and without such trajectories.",
    "problem_domain": "é—®ç­”ã€ä»£ç ç”Ÿæˆå’Œæ¨ç†ã€‚",
    "problem_domain_quote": "AutoMonitor-Bench consists of 3,010 carefully annotated test samples spanning question answering, code generation, and reasoning...",
    "problem_difficulty": "æ¶µç›–ä»æ˜¾å¼å®‰å…¨è¿è§„åˆ°æ›´éšå¼çš„è¡Œä¸ºï¼ˆå¦‚è§„èŒƒåšå¼ˆï¼‰çš„å¤šç§å¼‚å¸¸è¡Œä¸ºç±»åˆ«ï¼Œéš¾åº¦é€’å¢ã€‚",
    "problem_difficulty_quote": "which comprises 1,505 carefully annotated misbehavior instances spanning three misbehavior categories, ranging from explicit safety violations to more implicit behaviors such as specification gaming... we structure the benchmark into three categories that reflect increasingly implicit and difficult-to-audit failure mechanisms...",
    "language": "ä¸»è¦æ¶‰åŠè‡ªç„¶è¯­è¨€ï¼ˆç”¨äºé—®ç­”ã€æ¨ç†ï¼‰å’Œç¼–ç¨‹è¯­è¨€ï¼ˆç”¨äºä»£ç ç”Ÿæˆä»»åŠ¡ï¼‰ã€‚",
    "language_quote": "spanning question answering, code generation, and reasoning...",
    "data_size": "åŒ…å«3,010ä¸ªæµ‹è¯•æ ·æœ¬ï¼ˆ1,505ä¸ªå¼‚å¸¸è¡Œä¸ºå®ä¾‹ï¼Œæ¯ä¸ªå®ä¾‹é…å¯¹ä¸€ä¸ªè‰¯æ€§å®ä¾‹ï¼‰ã€‚æ­¤å¤–ï¼Œè¿˜æ„å»ºäº†ä¸€ä¸ªåŒ…å«153,581ä¸ªæ ·æœ¬çš„å¤§è§„æ¨¡è®­ç»ƒè¯­æ–™åº“ã€‚",
    "data_size_quote": "AutoMonitor-Bench consists of 3,010 carefully annotated test samples... For each misbehavior instance, we additionally include a corresponding benign solution or trajectory, resulting in a total of 3,010 test samples... we construct a large-scale training corpus comprising 153,581 training samples...",
    "source_type": "ç»è¿‡ç²¾å¿ƒäººå·¥æ ‡æ³¨æ„å»ºã€‚",
    "source_type_quote": "carefully annotated test samples... carefully annotated misbehavior instances...",
    "last_updated": "2026å¹´1æœˆ9æ—¥ï¼ˆæ ¹æ®arXivç‰ˆæœ¬å·æ¨æ–­ï¼‰",
    "last_updated_quote": "arXiv:2601.05752v1 [cs.CL] 9 Jan 2026",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œä½œä¸ºé¦–ä¸ªç³»ç»Ÿæ€§è¯„ä¼°LLMç›‘æ§å™¨å¯é æ€§çš„åŸºå‡†ã€‚",
    "build_type_quote": "We introduce AutoMonitor-Bench, the first benchmark designed to systematically evaluate the reliability of LLM-based misbehavior monitors...",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "å¼‚å¸¸è¡Œä¸ºæ£€æµ‹ï¼ˆäºŒåˆ†ç±»åˆ¤æ–­ï¼‰ã€‚",
    "task_granularity_quote": "The monitor is prompted to assess whether the given solution exhibits any form of misbehavior under the task specification.",
    "evaluation_metrics": "æ¼æ£€ç‡ï¼ˆMiss Rate, MRï¼‰å’Œè¯¯æŠ¥ç‡ï¼ˆFalse Alarm Rate, FARï¼‰ã€‚",
    "evaluation_metrics_quote": "We evaluate monitors using two complementary metrics: Miss Rate (MR) and False Alarm Rate (FAR)...",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆä»»åŠ¡è¯·æ±‚ï¼‰å’Œæ¨¡å‹ç”Ÿæˆçš„è§£å†³æ–¹æ¡ˆæˆ–è½¨è¿¹ï¼ˆå¯èƒ½åŒ…å«ä»£ç å’Œè‡ªç„¶è¯­è¨€æ¨ç†ï¼‰ã€‚",
    "input_modality_quote": "the LLM-based monitor is provided with a task request and a corresponding generated solution or trajectory.",
    "output_modality": "åˆ†ç±»åˆ¤æ–­ï¼ˆå¼‚å¸¸è¡Œä¸ºæˆ–è‰¯æ€§è¡Œä¸ºï¼‰ã€‚",
    "output_modality_quote": "The monitor is prompted to assess whether the given solution exhibits any form of misbehavior under the task specification.",
    "task_io_type": "æ–‡æœ¬ï¼ˆä»»åŠ¡è¯·æ±‚+è§£å†³æ–¹æ¡ˆï¼‰åˆ°åˆ†ç±»åˆ¤æ–­ã€‚",
    "task_io_type_quote": "Given a user task and a model-generated solution or trajectory, an LLM-based monitor evaluates whether it exhibits concrete misbehavior under the task specification or is classified as benign.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "é¦–ä¸ªç³»ç»Ÿæ€§è¯„ä¼°LLMå¼‚å¸¸è¡Œä¸ºç›‘æ§å™¨å¯é æ€§çš„åŸºå‡†ï¼›åŒ…å«é…å¯¹çš„å¼‚å¸¸è¡Œä¸ºå’Œè‰¯æ€§å®ä¾‹ï¼›æ¶µç›–ä»æ˜¾å¼è¿è§„åˆ°éšå¼åšå¼ˆçš„ä¸‰ç§å¼‚å¸¸è¡Œä¸ºç±»åˆ«ï¼ˆå®‰å…¨ä¸æƒé™è¿è§„ã€è°„åªšä¸åè§ã€è§„èŒƒåšå¼ˆï¼‰ï¼›æ—¨åœ¨æ­ç¤ºç›‘æ§å™¨åœ¨å®‰å…¨è¦†ç›–ä¸æ“ä½œæ•ˆç”¨ä¹‹é—´çš„å†…åœ¨æƒè¡¡ã€‚",
    "unique_features_quote": "the first benchmark designed to systematically evaluate the reliability of LLM-based misbehavior monitors... For each misbehavior instance, we additionally include a corresponding benign solution or trajectory... spanning three misbehavior categoriesâ€”Safety & Permission Violations, Sycophancy & Bias, and Specification Gaming... we observe a systematic trade-off between Miss Rate and False Alarm Rate across most LLM-based monitors, indicating an inherent tension between safety coverage and operational utility.",
    "data_size_quantity": 3010,
    "data_size_unit": "ä¸ªæµ‹è¯•æ ·æœ¬",
    "last_updated_year": 2026,
    "last_updated_month": 1,
    "last_updated_day": 9,
    "language_normalized": "['è‡ªç„¶è¯­è¨€', 'ç¼–ç¨‹è¯­è¨€']",
    "dimension_normalized": "['å¼‚å¸¸è¡Œä¸ºç›‘æ§çš„å¯é æ€§', 'æ¼æ£€ç‡', 'è¯¯æŠ¥ç‡']",
    "evaluation_method_normalized": "['æ¼æ£€ç‡', 'è¯¯æŠ¥ç‡']",
    "problem_domain_normalized": "['é—®ç­”', 'ä»£ç ç”Ÿæˆ', 'æ¨ç†']",
    "source_type_normalized": "['äººå·¥æ ‡æ³¨']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.07790_output/content.md",
    "benchmark_name": "Syslog Severity Classification Benchmark (åŸºäºjournalctlçš„æ—¥å¿—ä¸¥é‡æ€§åˆ†ç±»åŸºå‡†)",
    "benchmark_name_quote": "The study focuses on evaluating small language models (SLMs) and small reasoning language models (SRLMs) using log severity classification as a controlled probe... based on logs collected from journalctl within the computing infrastructure.",
    "is_original_proposal": "Yesï¼Œæœ¬æ–‡æ˜¯æ•°æ®é›†çš„åŸå§‹å‘å¸ƒè®ºæ–‡ã€‚",
    "is_original_proposal_quote": "We argue that severity classification is more informative when treated as a benchmark for probing runtime log comprehension rather than as an end task. Using real-world journalctl data from Linux production servers, we evaluate nine small language models (SLMs) and small reasoning language models (SRLMs)... By emphasizing small, deployable models, this benchmark aligns with real-time requirements of digital twin (DT) systems and shows that severity classification serves as a lens for evaluating model competence and real-time deployability...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "å°†ç³»ç»Ÿæ—¥å¿—æ¶ˆæ¯ï¼ˆSyslogæ¶ˆæ¯ï¼‰åˆ†ç±»åˆ°å…¶é¢„å®šä¹‰çš„ä¸¥é‡æ€§çº§åˆ«ï¼ˆSeverity levelï¼‰ã€‚è¯¥ä»»åŠ¡æ—¨åœ¨ä½œä¸ºè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨å—é™è¾“å‡ºæ¡ä»¶ä¸‹ï¼Œå¯¹çœŸå®ä¸–ç•Œç³»ç»Ÿæ—¥å¿—è¯­ä¹‰è¿›è¡Œç†è§£å’Œæ¨ç†èƒ½åŠ›çš„æ¢é’ˆï¼Œè€Œéä¸€ä¸ªç‹¬ç«‹çš„å®ç”¨ä»»åŠ¡ã€‚",
    "task_description_quote": "Since severity levels are predefined metadata in system log messages, having a model merely classify them offers limited standalone practical value... We argue that severity classification is more informative when treated as a benchmark for probing runtime log comprehension rather than as an end task.",
    "dimension": "æ¨¡å‹å¯¹ç³»ç»Ÿæ—¥å¿—çš„è¿è¡Œæ—¶ç†è§£èƒ½åŠ›ã€åœ¨ä¸¥æ ¼è¾“å‡ºçº¦æŸä¸‹çš„æ€§èƒ½ã€å®æ—¶éƒ¨ç½²èƒ½åŠ›ã€‚",
    "dimension_quote": "We argue that severity classification is more informative when treated as a benchmark for probing runtime log comprehension... this benchmark aligns with real-time requirements of digital twin (DT) systems and shows that severity classification serves as a lens for evaluating model competence and real-time deployability...",
    "evaluation_method": "åœ¨é›¶æ ·æœ¬ã€å°‘æ ·æœ¬å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æç¤ºç­–ç•¥ä¸‹ï¼Œè¯„ä¼°æ¨¡å‹å¯¹æ—¥å¿—ä¸¥é‡æ€§åˆ†ç±»çš„å‡†ç¡®æ€§ã€‚",
    "evaluation_method_quote": "Using real-world journalctl data from Linux production servers, we evaluate nine small language models (SLMs) and small reasoning language models (SRLMs) under zero-shot, few-shot, and retrieval-augmented generation (RAG) prompting.",
    "context_dependency": "å•æ¡æ—¥å¿—æ¶ˆæ¯ã€‚ä»»åŠ¡åŸºäºå•æ¡Syslogæ¶ˆæ¯çš„å†…å®¹è¿›è¡Œåˆ†ç±»ã€‚",
    "context_dependency_quote": "Each Syslog message contains a PRI component that consists of a â€œ<â€, a number, and a â€œ>â€. The number is known as the severity value... However, it also makes them a realistic and challenging probe for evaluating whether language models (LMs) can align log content with operational intent under ambiguity.",
    "problem_domain": "ç³»ç»Ÿè¿ç»´ã€æ—¥å¿—åˆ†æã€æ•°å­—å­ªç”Ÿï¼ˆDTï¼‰ç³»ç»Ÿç›‘æ§ã€‚",
    "problem_domain_quote": "System logs are crucial for monitoring and diagnosing modern computing infrastructure... Within DT-oriented monitoring pipelines, such log interpretation must be both accurate and latency-efficient...",
    "problem_difficulty": "å…·æœ‰ç°å®æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºæ—¥å¿—ä¸¥é‡æ€§æ ‡ç­¾ç¼ºä¹ä¸¥æ ¼æ ‡å‡†åŒ–ï¼Œå­˜åœ¨æ¨¡ç³Šæ€§ï¼Œéœ€è¦æ¨¡å‹å°†æ—¥å¿—å†…å®¹ä¸æ“ä½œæ„å›¾å¯¹é½ã€‚",
    "problem_difficulty_quote": "This lack of strict standardization limits the use of severity labels as a canonical ground truth. However, it also makes them a realistic and challenging probe for evaluating whether language models (LMs) can align log content with operational intent under ambiguity.",
    "language": "æ—¥å¿—æ¶ˆæ¯ä¸ºè‡ªç„¶è¯­è¨€ï¼ˆè‹±æ–‡ï¼‰ï¼ŒåŒ…å«ç³»ç»Ÿäº‹ä»¶ã€è­¦å‘Šå’Œæ€§èƒ½ä¿¡æ¯ã€‚",
    "language_quote": "System logs are vital components of modern computing infrastructure, capturing operational events, warnings, and performance information across distributed systems... However, as computing systems generate massive volumes of logs with complex, context-dependent language...",
    "data_size": "æœªæ˜ç¡®æåŠå…·ä½“æ¡ç›®æ•°é‡ï¼Œä½†æ•°æ®æ¥æºäºLinuxç”Ÿäº§æœåŠ¡å™¨çš„çœŸå®journalctlæ—¥å¿—ã€‚",
    "data_size_quote": "Using real-world journalctl data from Linux production servers...",
    "source_type": "ä»Linuxç”Ÿäº§æœåŠ¡å™¨çš„journalctlæ”¶é›†çš„çœŸå®ä¸–ç•Œç³»ç»Ÿæ—¥å¿—ã€‚",
    "source_type_quote": "Using real-world journalctl data from Linux production servers...",
    "last_updated": "2026å¹´1æœˆ12æ—¥ï¼ˆæ ¹æ®arXivç‰ˆæœ¬æ—¥æœŸæ¨æ–­ï¼‰",
    "last_updated_quote": "arXiv:2601.07790v1  [cs.AI]  12 Jan 2026",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œç”±ç ”ç©¶å›¢é˜Ÿä»ç”Ÿäº§ç¯å¢ƒä¸­æ”¶é›†å’Œæ„å»ºã€‚",
    "build_type_quote": "Using real-world journalctl data from Linux production servers...",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "åˆ†ç±»ä»»åŠ¡ã€‚",
    "task_granularity_quote": "Since severity levels are predefined metadata in system log messages, having a model merely classify them...",
    "evaluation_metrics": "å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰ã€‚",
    "evaluation_metrics_quote": "Qwen3-4B achieves the highest accuracy at 95.64% with RAG, while Gemma3-1B improves from 20.25% under few-shot prompting to 85.28% with RAG. Notably, the tiny Qwen3-0.6B reaches 88.12% accuracy...",
    "input_modality": "è‡ªç„¶è¯­è¨€æ–‡æœ¬ï¼ˆç³»ç»Ÿæ—¥å¿—æ¶ˆæ¯ï¼‰ã€‚",
    "input_modality_quote": "System logs are vital components of modern computing infrastructure, capturing operational events, warnings, and performance information across distributed systems...",
    "output_modality": "åˆ†ç±»æ ‡ç­¾ï¼ˆä¸¥é‡æ€§çº§åˆ«ï¼Œæ•´æ•°0-7ï¼‰ã€‚",
    "output_modality_quote": "The Severity value is an integer from 0 to 7 that quantifies the risk level of each log.",
    "task_io_type": "æ–‡æœ¬åˆ°åˆ†ç±»æ ‡ç­¾ã€‚",
    "task_io_type_quote": "Since severity levels are predefined metadata in system log messages, having a model merely classify them...",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "1. ä¸“æ³¨äºè¯„ä¼°å°å‹ã€å¯éƒ¨ç½²çš„è¯­è¨€æ¨¡å‹ï¼ˆSLMs/SRLMsï¼‰ã€‚2. å¼ºè°ƒåœ¨ä¸¥æ ¼è¾“å‡ºå’Œè¿è¡Œæ—¶çº¦æŸä¸‹çš„æ€§èƒ½ï¼Œä¸æ•°å­—å­ªç”Ÿç³»ç»Ÿçš„å®æ—¶éœ€æ±‚å¯¹é½ã€‚3. ä½¿ç”¨çœŸå®ä¸–ç•Œçš„journalctlæ—¥å¿—ï¼Œè€Œéç»“æ„åŒ–åŸºå‡†æ•°æ®é›†ï¼ˆå¦‚BGL, HDFSï¼‰ï¼Œæ›´èƒ½æ•æ‰ç°å®ä¸­çš„æ—¶é—´ç»“æ„ã€ä¸è§„åˆ™æ€§å’Œå™ªå£°ã€‚4. å°†ä¸¥é‡æ€§åˆ†ç±»é‡æ–°å®šä½ä¸ºè¯„ä¼°æ¨¡å‹è¿è¡Œæ—¶æ—¥å¿—ç†è§£èƒ½åŠ›çš„æ¢é’ˆï¼Œè€Œéæœ€ç»ˆä»»åŠ¡ã€‚",
    "unique_features_quote": "By emphasizing small, deployable models, this benchmark aligns with real-time requirements of digital twin (DT) systems... Several influential benchmarks use structured datasets such as BGL, HDFS, or Thunderbird, which do not capture the temporal structure, irregularity, and noise of real-world journalctl logs. We argue that severity classification is more informative when treated as a benchmark for probing runtime log comprehension rather than as an end task.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2026,
    "last_updated_month": 1,
    "last_updated_day": 12,
    "language_normalized": "['æ—¥å¿—æ¶ˆæ¯ä¸ºè‡ªç„¶è¯­è¨€ï¼ˆè‹±æ–‡ï¼‰', 'åŒ…å«ç³»ç»Ÿäº‹ä»¶', 'è­¦å‘Šå’Œæ€§èƒ½ä¿¡æ¯']",
    "dimension_normalized": "['æ¨¡å‹å¯¹ç³»ç»Ÿæ—¥å¿—çš„è¿è¡Œæ—¶ç†è§£èƒ½åŠ›', 'åœ¨ä¸¥æ ¼è¾“å‡ºçº¦æŸä¸‹çš„æ€§èƒ½', 'å®æ—¶éƒ¨ç½²èƒ½åŠ›']",
    "evaluation_method_normalized": "['å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰']",
    "problem_domain_normalized": "['ç³»ç»Ÿè¿ç»´', 'æ—¥å¿—åˆ†æ', 'æ•°å­—å­ªç”Ÿï¼ˆDTï¼‰ç³»ç»Ÿç›‘æ§']",
    "source_type_normalized": "['ä»Linuxç”Ÿäº§æœåŠ¡å™¨çš„journalctlæ”¶é›†çš„çœŸå®ä¸–ç•Œç³»ç»Ÿæ—¥å¿—']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "JSON",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.07602_output/content.md",
    "benchmark_name": "OODEval",
    "benchmark_name_quote": "we introduce OODEval, a manually constructed benchmark covering 50 object-oriented design tasks of varying difficulty.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "we introduce OODEval, a manually constructed benchmark covering 50 object-oriented design tasks of varying difficulty.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é¢å‘å¯¹è±¡è®¾è®¡ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ï¼Œå…·ä½“ä»»åŠ¡æ˜¯ä»è‡ªç„¶è¯­è¨€éœ€æ±‚åˆ†æå¹¶ç”Ÿæˆç±»å›¾ã€‚",
    "task_description_quote": "this study focuses on object-oriented design (OOD) [10] tasks, investigating LLMsâ€™ ability to analyze natural language requirements and generate class diagrams.",
    "dimension": "é¢å‘å¯¹è±¡è®¾è®¡èƒ½åŠ›ï¼ŒåŒ…æ‹¬è¯­æ³•æ­£ç¡®æ€§å’Œè¯­ä¹‰æ­£ç¡®æ€§ï¼Œä»¥åŠç±»ã€å±æ€§ã€æ–¹æ³•ã€å…³ç³»çš„ç”Ÿæˆè´¨é‡ã€‚",
    "dimension_quote": "LLMs exhibit strong syntactic correctness but show significant weaknesses in semantic correctness. LLMs present weaker performance in generating class methods and relationships...",
    "evaluation_method": "ä½¿ç”¨æå‡ºçš„CLUEï¼ˆClass Likeness Unified Evaluationï¼‰æŒ‡æ ‡é›†è¿›è¡Œè¯„ä¼°ï¼Œè¯¥æŒ‡æ ‡é›†åŒ…å«ä¸€ä¸ªæ•´ä½“æ€§èƒ½æŒ‡æ ‡å’Œå››ä¸ªå±€éƒ¨æ€§èƒ½æŒ‡æ ‡ï¼Œé€šè¿‡è®¡ç®—ä¸äººå·¥è¯„åˆ†çš„ç›¸å…³æ€§æ¥éªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚",
    "evaluation_method_quote": "we propose the CLUE (Class Likeness Unified Evaluation) metrics, which includes an overall performance metric along with four local performance metrics. This metric integrates more complete structural and semantic information from class diagrams to evaluate design correctness.",
    "context_dependency": "æ¨¡å‹çº§ï¼Œä»å®Œæ•´éœ€æ±‚åˆ°ç±»å›¾è®¾è®¡çš„ç«¯åˆ°ç«¯ä»»åŠ¡ã€‚",
    "context_dependency_quote": "OODEval, a model-level benchmark dataset... which comprises 50 tasks from requirements to design... making it unsuitable for evaluating end-to-end OOD generation.",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ï¼Œå…·ä½“æ˜¯é¢å‘å¯¹è±¡è½¯ä»¶è®¾è®¡ã€‚",
    "problem_domain_quote": "object-oriented design (OOD) tasks... OOD plays a crucial role in ensuring software quality and reducing communication overhead within development teams.",
    "problem_difficulty": "åŒ…å«ç®€å•ã€ä¸­ç­‰ã€å›°éš¾ä¸‰ä¸ªéš¾åº¦çº§åˆ«ã€‚",
    "problem_difficulty_quote": "OODEval... encompasses three difficulty levels: simple, moderate, and hard.",
    "language": "ä¸æ¶‰åŠç‰¹å®šç¼–ç¨‹è¯­è¨€ï¼Œä»»åŠ¡è¾“å…¥æ˜¯è‡ªç„¶è¯­è¨€éœ€æ±‚ï¼Œè¾“å‡ºæ˜¯ç±»å›¾ï¼ˆä»¥PlantUMLä»£ç æ ¼å¼è¡¨ç¤ºï¼‰ã€‚",
    "language_quote": "OODEval... offering PlantUML code formats for automated evaluation and visualized formats for human review.",
    "data_size": "OODEvalåŒ…å«50ä¸ªä»»åŠ¡ã€‚OODEval-HumanåŒ…å«940ä¸ªæœ¬ç§‘ç”Ÿæäº¤çš„è§£å†³æ–¹æ¡ˆåŠæ•™å¸ˆè¯„åˆ†ã€‚",
    "data_size_quote": "OODEval, a manually constructed benchmark covering 50 object-oriented design tasks... we present OODEval-Human, the first human-rated OOD benchmark comprising 940 undergraduate-submitted solutions with instructor ratings.",
    "source_type": "äººå·¥æ„å»ºã€‚",
    "source_type_quote": "we manually constructed a model-level benchmark dataset, OODEval...",
    "last_updated": "2026",
    "last_updated_quote": "arXiv:2601.07602v1 [cs.SE] 12 Jan 2026",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆç”±è®ºæ–‡ä½œè€…å›¢é˜Ÿæ‰‹åŠ¨æ„å»ºï¼‰ã€‚",
    "build_type_quote": "we manually constructed a model-level benchmark dataset, OODEval...",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "æ¨¡å‹çº§è®¾è®¡ç”Ÿæˆï¼ˆä»éœ€æ±‚ç”Ÿæˆå®Œæ•´çš„ç±»å›¾ï¼‰ã€‚",
    "task_granularity_quote": "OODEval, a model-level benchmark dataset... which comprises 50 tasks from requirements to design...",
    "evaluation_metrics": "CLUEæŒ‡æ ‡é›†ï¼ˆåŒ…å«æ•´ä½“æ€§èƒ½æŒ‡æ ‡å’Œå››ä¸ªå±€éƒ¨æ€§èƒ½æŒ‡æ ‡ï¼‰ã€‚",
    "evaluation_metrics_quote": "we propose the CLUE (Class Likeness Unified Evaluation) metrics, which includes an overall performance metric along with four local performance metrics.",
    "input_modality": "è‡ªç„¶è¯­è¨€éœ€æ±‚æ–‡æœ¬ã€‚",
    "input_modality_quote": "investigating LLMsâ€™ ability to analyze natural language requirements and generate class diagrams.",
    "output_modality": "ç±»å›¾ï¼ˆä»¥PlantUMLä»£ç æ ¼å¼è¡¨ç¤ºï¼‰ã€‚",
    "output_modality_quote": "offering PlantUML code formats for automated evaluation and visualized formats for human review.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ï¼ˆå…·ä½“æ˜¯è‡ªç„¶è¯­è¨€éœ€æ±‚åˆ°ç±»å›¾å®šä¹‰ä»£ç ï¼‰ã€‚",
    "task_io_type_quote": "analyze natural language requirements and generate class diagrams.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "1. é¦–ä¸ªä¸“æ³¨äºè¯„ä¼°LLMé¢å‘å¯¹è±¡è®¾è®¡èƒ½åŠ›çš„åŸºå‡†ã€‚2. åŒ…å«äººå·¥è¯„åˆ†çš„å­é›†ï¼ˆOODEval-Humanï¼‰ï¼Œç”¨äºæ¯”è¾ƒLLMä¸äººç±»èƒ½åŠ›å¹¶éªŒè¯è¯„ä¼°æŒ‡æ ‡ã€‚3. æä¾›PlantUMLä»£ç æ ¼å¼ä»¥æ”¯æŒè‡ªåŠ¨åŒ–è¯„ä¼°ã€‚4. ä»»åŠ¡æŒ‰éš¾åº¦ï¼ˆç®€å•ã€ä¸­ç­‰ã€å›°éš¾ï¼‰åˆ†å±‚ã€‚5. æå‡ºäº†ä¸“é—¨ç”¨äºè¯„ä¼°ç±»å›¾è®¾è®¡æ­£ç¡®æ€§çš„CLUEæŒ‡æ ‡é›†ã€‚",
    "unique_features_quote": "OODEval-Human, the first human-rated OOD benchmark... OODEval surpasses previous datasets in scale and diversity, offering PlantUML code formats for automated evaluation... encompasses three difficulty levels: simple, moderate, and hard... we propose the CLUE (Class Likeness Unified Evaluation) metrics...",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2026,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['ä¸æ¶‰åŠç‰¹å®šç¼–ç¨‹è¯­è¨€']",
    "dimension_normalized": "['é¢å‘å¯¹è±¡è®¾è®¡èƒ½åŠ›', 'è¯­æ³•æ­£ç¡®æ€§', 'è¯­ä¹‰æ­£ç¡®æ€§', 'ç±»ç”Ÿæˆè´¨é‡', 'å±æ€§ç”Ÿæˆè´¨é‡', 'æ–¹æ³•ç”Ÿæˆè´¨é‡', 'å…³ç³»ç”Ÿæˆè´¨é‡']",
    "evaluation_method_normalized": "['CLUEæŒ‡æ ‡é›†', 'æ•´ä½“æ€§èƒ½æŒ‡æ ‡', 'å››ä¸ªå±€éƒ¨æ€§èƒ½æŒ‡æ ‡']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'é¢å‘å¯¹è±¡è½¯ä»¶è®¾è®¡']",
    "source_type_normalized": "['äººå·¥æ„å»º']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.08778_output/content.md",
    "benchmark_name": "BIRD, Spider 2.0-Snow",
    "benchmark_name_quote": "In this paper, we conduct an empirical study that (i) benchmarks annotation error rates for two widely used text-to-SQL benchmarks, BIRD and Spider 2.0-Snow",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We conduct an empirical study that (i) benchmarks annotation error rates for two widely used text-to-SQL benchmarks, BIRD and Spider 2.0-Snow, and (ii) corrects a subset of the BIRD development (Dev) set to measure the impact of annotation errors on text-to-SQL agent performance and leaderboard rankings.",
    "dataset_url": "https://github.com/uiuc-kang-lab/text_to_sql_benchmarks (æœ¬æ–‡ä»£ç å’Œæ•°æ®)",
    "dataset_url_quote": "Our code and data are available at https://github.com/uiuc-kang-lab/text_to_sql_benchmarks.",
    "task_description": "æ–‡æœ¬åˆ°SQLè½¬æ¢ã€‚ç»™å®šä¸€ä¸ªè‡ªç„¶è¯­è¨€é—®é¢˜ï¼ˆå’Œå¯é€‰çš„å¤–éƒ¨çŸ¥è¯†ï¼‰ä»¥åŠä¸€ä¸ªç›®æ ‡æ•°æ®åº“ï¼Œæ¨¡å‹éœ€è¦ç”Ÿæˆä¸€ä¸ªèƒ½æ­£ç¡®æ‰§è¡Œå¹¶å›ç­”é—®é¢˜çš„SQLæŸ¥è¯¢ã€‚",
    "task_description_quote": "Given a target database D, human annotators curate text-to-SQL examples, each comprising a natural language input T and a ground-truth SQL query Q.",
    "dimension": "æ–‡æœ¬åˆ°SQLè½¬æ¢çš„å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œç‰¹åˆ«å…³æ³¨æ•°æ®é›†ä¸­æ ‡æ³¨é”™è¯¯çš„æ™®éæ€§åŠå…¶å¯¹æ¨¡å‹è¯„ä¼°çš„å½±å“ã€‚",
    "dimension_quote": "We conduct an empirical study that (i) benchmarks annotation error rates for two widely used text-to-SQL benchmarks, BIRD and Spider 2.0-Snow, and (ii) corrects a subset of the BIRD development (Dev) set to measure the impact of annotation errors on text-to-SQL agent performance and leaderboard rankings.",
    "evaluation_method": "æ‰§è¡Œå‡†ç¡®åº¦ï¼ˆExecution Accuracyï¼‰ã€‚é€šè¿‡é‡æ–°è¯„ä¼°æ¨¡å‹åœ¨åŸå§‹å’Œæœ‰æ ‡æ³¨é”™è¯¯çš„å¼€å‘é›†å­é›†ä¸Šçš„æ€§èƒ½æ¥è¡¡é‡ã€‚",
    "evaluation_method_quote": "We re-evaluate all 16 open-source agents from the BIRD leaderboard on both the original and the corrected BIRD Dev subsets. We show that performance changes range from âˆ’7% to 31% (in relative terms)... We further computed two Spearmanâ€™s rank correlation coefficients (ğ‘Ÿğ‘ ) to assess the impact of annotation errors on agent rankings.",
    "context_dependency": "ä¾èµ–äºå•ä¸ªæ•°æ®åº“çš„Schemaï¼ˆè¡¨ã€åˆ—ç»“æ„ï¼‰å’Œå¯é€‰çš„å¤–éƒ¨çŸ¥è¯†ã€‚",
    "context_dependency_quote": "Given a target database D, human annotators curate text-to-SQL examples, each comprising a natural language input T and a ground-truth SQL query Q.1 ... Input T consists of a user question and, optionally, external knowledge.",
    "problem_domain": "æ•°æ®åº“æŸ¥è¯¢ï¼Œæ¶‰åŠé‡‘èã€å¤©æ°”ã€æ•™è‚²ç­‰å¤šä¸ªç°å®ä¸–ç•Œé¢†åŸŸã€‚",
    "problem_domain_quote": "Figure 1b presents an incorrect annotation from BIRD due to the annotatorâ€™s lack of domain knowledge (E3).",
    "problem_difficulty": "é«˜å¤æ‚æ€§ï¼Œæ¶‰åŠç°å®ä¸–ç•Œæ•°æ®åº“ã€å¤æ‚SQLæŸ¥è¯¢ï¼ˆå¦‚ä½¿ç”¨åœ°ç†ç©ºé—´å‡½æ•°ï¼‰å’Œé¢†åŸŸçŸ¥è¯†ã€‚",
    "problem_difficulty_quote": "Spider 2.0-Snow has the most complex SQL queries (highest average token count of its ground-truth SQL queries, 161.8 tokens per SQL query)...",
    "language": "SQL",
    "language_quote": "Given a target database D, human annotators curate text-to-SQL examples, each comprising a natural language input T and a ground-truth SQL query Q.",
    "data_size": "BIRDå¼€å‘é›†æœ‰1534ä¸ªç¤ºä¾‹ï¼Œæœ¬æ–‡ä»ä¸­éšæœºé‡‡æ ·äº†100ä¸ªã€‚Spider 2.0-Snowæœ‰121ä¸ªå…¬å¼€äº†çœŸå®SQLçš„é—®é¢˜ã€‚",
    "data_size_quote": "We randomly sampled 100 of the 1,534 BIRD Dev examples... across all 121 problems for which ground-truth SQL is publicly available...",
    "source_type": "äººå·¥æ ‡æ³¨ã€‚",
    "source_type_quote": "Since these benchmarks heavily rely on human annotations during question construction and answer evaluation, the validity of the annotations is crucial.",
    "last_updated": "2026å¹´1æœˆ13æ—¥ï¼ˆè®ºæ–‡ç‰ˆæœ¬æ—¥æœŸï¼‰ã€‚BIRDå’ŒSpider 2.0-Snowçš„åŸå§‹å‘å¸ƒæ—¶é—´æ›´æ—©ã€‚",
    "last_updated_quote": "arXiv:2601.08778v1  [cs.AI]  13 Jan 2026",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆBIRDå’ŒSpiderå›¢é˜Ÿï¼‰ã€‚",
    "build_type_quote": "We selected two text-to-SQL benchmarks, BIRD [31] and Spider 2.0-Snow [27]...",
    "contamination_status": "é«˜æ±¡æŸ“é£é™©ï¼Œå› ä¸ºæ ‡æ³¨é”™è¯¯ç‡å¾ˆé«˜ï¼ˆBIRD Mini-Dev 52.8%ï¼Œ Spider 2.0-Snow 62.8%ï¼‰ï¼Œå¯èƒ½è¯¯å¯¼æ¨¡å‹è¯„ä¼°ã€‚",
    "contamination_status_quote": "We find that 52.8% of the examples in BIRD Mini-Dev [31] contain annotation errors... we still identify an annotation error rate of 62.8%.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "æ–‡æœ¬åˆ°SQLç”Ÿæˆã€‚",
    "task_granularity_quote": "Researchers have proposed numerous text-to-SQL techniques to streamline data analytics...",
    "evaluation_metrics": "æ‰§è¡Œå‡†ç¡®åº¦ï¼ˆExecution Accuracyï¼‰ï¼Œæ–¯çš®å°”æ›¼ç­‰çº§ç›¸å…³ç³»æ•°ï¼ˆSpearmanâ€™s rank correlation coefficientï¼‰ã€‚",
    "evaluation_metrics_quote": "We re-evaluate all 16 open-source agents from the BIRD leaderboard on both the original and the corrected BIRD Dev subsets... We further computed two Spearmanâ€™s rank correlation coefficients (ğ‘Ÿğ‘ ) to assess the impact of annotation errors on agent rankings.",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆç”¨æˆ·é—®é¢˜ï¼‰å’Œå¯é€‰çš„å¤–éƒ¨çŸ¥è¯†æ–‡æœ¬ã€‚",
    "input_modality_quote": "Input T consists of a user question and, optionally, external knowledge.",
    "output_modality": "SQLæŸ¥è¯¢ä»£ç ã€‚",
    "output_modality_quote": "Given a target database D, human annotators curate text-to-SQL examples, each comprising a natural language input T and a ground-truth SQL query Q.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ã€‚",
    "task_io_type_quote": "Researchers have proposed numerous text-to-SQL techniques...",
    "execution_environment": "ç›®æ ‡æ•°æ®åº“ç¯å¢ƒï¼ˆå¦‚Snowflakeï¼‰ã€‚",
    "execution_environment_quote": "Figure 1a illustrates a misuse of a Snowflake function (E1)... The agent can directly access these databases by calling predefined functions.",
    "unique_features": "æœ¬æ–‡çš„æ ¸å¿ƒå‘ç°æ˜¯æ­ç¤ºäº†BIRDå’ŒSpider 2.0-Snowè¿™ä¸¤ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ–‡æœ¬åˆ°SQLåŸºå‡†ä¸­å­˜åœ¨æé«˜çš„æ ‡æ³¨é”™è¯¯ç‡ï¼ˆåˆ†åˆ«è¶…è¿‡50%å’Œ60%ï¼‰ï¼Œè¿™äº›é”™è¯¯ä¸¥é‡æ‰­æ›²äº†æ¨¡å‹æ€§èƒ½æŠ¥å‘Šå’Œæ’è¡Œæ¦œæ’åã€‚æœ¬æ–‡è¿˜æå‡ºäº†ç”¨äºæ£€æµ‹å’Œä¿®æ­£æ ‡æ³¨é”™è¯¯çš„å·¥å…·ï¼ˆSAR-Agentå’ŒSAPARï¼‰ã€‚",
    "unique_features_quote": "Our analysis reveals a wider range of annotation errors in text-to-SQL benchmarks than previously recognized... we find that 52.8% of the examples in BIRD Mini-Dev [31] contain annotation errors... we still identify an annotation error rate of 62.8%... These annotation errors cause severe misestimation and misranking of agentsâ€™ performance.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2026,
    "last_updated_month": 1,
    "last_updated_day": 13,
    "language_normalized": "['SQL']",
    "dimension_normalized": "['æ–‡æœ¬åˆ°SQLè½¬æ¢çš„å‡†ç¡®æ€§', 'å¯é æ€§', 'æ ‡æ³¨é”™è¯¯çš„æ™®éæ€§', 'æ¨¡å‹è¯„ä¼°çš„å½±å“']",
    "evaluation_method_normalized": "['æ‰§è¡Œå‡†ç¡®åº¦', 'æ–¯çš®å°”æ›¼ç­‰çº§ç›¸å…³ç³»æ•°']",
    "problem_domain_normalized": "['æ•°æ®åº“æŸ¥è¯¢', 'é‡‘è', 'å¤©æ°”', 'æ•™è‚²']",
    "source_type_normalized": "['äººå·¥æ ‡æ³¨']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å•æ–‡ä»¶",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "é«˜æ±¡æŸ“é£é™©"
  },
  {
    "source_paper": "2601.08806_output/content.md",
    "benchmark_name": "APEXâ€“SWE",
    "benchmark_name_quote": "We introduce the AI Productivity Index for Software Engineering (APEXâ€“SWE), a benchmark for assessing whether frontier AI models can execute economically valuable software engineering work.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We introduce the AI Productivity Index for Software Engineering (APEXâ€“SWE)... To assess models for real-world software engineering tasks, we present APEXâ€“SWE...",
    "dataset_url": "https://huggingface.co/datasets/mercor/APEX-SWE (dev set), https://github.com/Mercor-Intelligence/apex-evals (evaluation harness)",
    "dataset_url_quote": "We have released an open-source dev set via Hugging Face with a CC-BY license1 and our grading harness is available open-source on GitHub.2 1https://huggingface.co/datasets/mercor/APEX-SWE 2https://github.com/Mercor-Intelligence/apex-evals",
    "task_description": "è¯„ä¼°å‰æ²¿AIæ¨¡å‹æ‰§è¡Œå…·æœ‰ç»æµä»·å€¼çš„çœŸå®ä¸–ç•Œè½¯ä»¶å·¥ç¨‹å·¥ä½œçš„èƒ½åŠ›ã€‚åŒ…å«ä¸¤ç§ä»»åŠ¡ç±»å‹ï¼šé›†æˆä»»åŠ¡ï¼ˆæ„å»ºè·¨å¼‚æ„äº‘æœåŠ¡ã€ä¸šåŠ¡åº”ç”¨å’ŒåŸºç¡€è®¾æ–½å³ä»£ç æœåŠ¡çš„ç«¯åˆ°ç«¯ç³»ç»Ÿï¼‰å’Œå¯è§‚æµ‹æ€§ä»»åŠ¡ï¼ˆä½¿ç”¨é¥æµ‹ä¿¡å·ï¼ˆå¦‚æ—¥å¿—ã€ä»ªè¡¨æ¿ï¼‰å’Œéç»“æ„åŒ–ä¸Šä¸‹æ–‡è°ƒè¯•ç”Ÿäº§æ•…éšœï¼‰ã€‚",
    "task_description_quote": "a benchmark for assessing whether frontier AI models can execute economically valuable software engineering work. APEXâ€“SWE assesses two novel task types that reflect real-world software engineering: (1) Integration tasks... which require constructing end-to-end systems across heterogeneous cloud primitives, business applications, and infrastructure-as-code services, and (2) Observability tasks... which require debugging production failures using telemetry signals such as logs and dashboards, as well as unstructured context.",
    "dimension": "çœŸå®ä¸–ç•Œè½¯ä»¶å·¥ç¨‹èƒ½åŠ›ï¼ŒåŒ…æ‹¬è·¨å¹³å°é›†æˆã€åŸºç¡€è®¾æ–½é…ç½®ã€ç”Ÿäº§ç¯å¢ƒè°ƒè¯•ã€è®¤çŸ¥æ¨ç†ï¼ˆåŒºåˆ†å‡è®¾ä¸å·²éªŒè¯äº‹å®çš„èƒ½åŠ›ï¼‰å’Œä»£ç†èƒ½åŠ›ï¼ˆåœ¨è¡ŒåŠ¨å‰è§£å†³ä¸ç¡®å®šæ€§ï¼‰ã€‚",
    "dimension_quote": "assessing whether frontier AI models can execute economically valuable software engineering work... Our analysis shows that strong performance is primarily driven by epistemic reasoning, defined as the ability to distinguish between assumptions and verified facts, combined with agency to resolve uncertainty prior to acting.",
    "evaluation_method": "ä½¿ç”¨Pass@1ï¼ˆæ¨¡å‹é¦–æ¬¡å°è¯•é€šè¿‡æ‰€æœ‰æ­£ç¡®æ€§æµ‹è¯•çš„ä»»åŠ¡ç™¾åˆ†æ¯”ï¼‰ä½œä¸ºæ’è¡Œæ¦œä¸»è¦æŒ‡æ ‡ï¼Œä¹ŸæŠ¥å‘ŠPass@3ï¼ˆç»™å®šä¸‰æ¬¡å°è¯•è‡³å°‘æœ‰ä¸€æ¬¡æ­£ç¡®ç»“æœçš„ç™¾åˆ†æ¯”ï¼‰ã€‚é›†æˆä»»åŠ¡é€šè¿‡ç›´æ¥ä¸æœåŠ¡APIäº¤äº’çš„pytestå¥—ä»¶éªŒè¯æ­£ç¡®æ€§ï¼›å¯è§‚æµ‹æ€§ä»»åŠ¡é‡‡ç”¨FAIL_TO_PASS / PASS_TO_PASSæ–¹æ³•ï¼Œåº”ç”¨æ¨¡å‹è¡¥ä¸å¹¶æ‰§è¡Œæ¡†æ¶ç‰¹å®šæµ‹è¯•ï¼ˆpytest, go test, jestï¼‰ã€‚",
    "evaluation_method_quote": "For the APEXâ€“SWE leaderboard, we assess modelsâ€™ outputs using Pass@1, defined as the percentage of tasks where the modelâ€™s first attempt passes all correctness tests. For Integration tasks, correctness is verified via a pytest suite that interacts directly with service APIs... For Observability tasks, correctness follows a FAIL_TO_PASS / PASS_TO_PASS methodology inspired by SWE-bench (Jimenez et al., 2024)... We also report Pass@3 in this paper, assessing whether the model can achieve at least one correct outcome if given three attempts.",
    "context_dependency": "å¤šæœåŠ¡ã€å¤šæ–‡ä»¶é¡¹ç›®ç¯å¢ƒã€‚é›†æˆä»»åŠ¡æ¶‰åŠè·¨å¤šä¸ªäº‘æœåŠ¡å’Œä¸šåŠ¡åº”ç”¨çš„ç¼–æ’ï¼›å¯è§‚æµ‹æ€§ä»»åŠ¡éœ€è¦åœ¨æ•´ä¸ªä»£ç åº“ä¸­è¿½è¸ªæ ¹å› ï¼Œå¹¶å¤„ç†æ¥è‡ªå¤šä¸ªæ¥æºï¼ˆæ—¥å¿—ã€èŠå¤©ã€ä»£ç åº“ï¼‰çš„ä¸Šä¸‹æ–‡ã€‚",
    "context_dependency_quote": "Integration tasks... require constructing end-to-end systems across heterogeneous cloud primitives, business applications, and infrastructure-as-code services... Observability tasks... must interrogate production logs..., correlate evidence from developer chat discussions, and trace root causes through the codebase...",
    "problem_domain": "ä¸“ä¸šè½¯ä»¶å·¥ç¨‹ï¼Œå…·ä½“åŒ…æ‹¬äº‘åŸºç¡€è®¾æ–½é›†æˆã€æ”¯ä»˜ç®¡é“ã€æ— æœåŠ¡å™¨äº‹ä»¶é©±åŠ¨æ¶æ„ã€CRMä¸å·¥å•ç³»ç»Ÿæ•°æ®åŒæ­¥ã€ç”Ÿäº§ç¯å¢ƒæ•…éšœè¯Šæ–­ä¸ä¿®å¤ã€‚",
    "problem_domain_quote": "Tasks reflect realistic greenfield engineering workflows, including implementing idempotent payment pipelines, configuring serverless event-driven architectures, and synchronizing data between CRM and ticketing systems.",
    "problem_difficulty": "å·¥ç¨‹çº§/ç”Ÿäº§çº§ã€‚ä»»åŠ¡æ¨¡æ‹ŸçœŸå®çš„ç”Ÿäº§ç¯å¢ƒï¼Œæ¶‰åŠè·¨å¹³å°é›†æˆã€åŸºç¡€è®¾æ–½é…ç½®å’ŒåŸºäºä¸å®Œæ•´ä¿¡æ¯è°ƒè¯•ç”Ÿäº§æ•…éšœï¼Œè¿œè¶…ç¼–å†™çŸ­å‡½æ•°æˆ–ä¿®è¡¥å•ä¸ªæ–‡ä»¶çš„éš¾åº¦ã€‚",
    "problem_difficulty_quote": "Professional software engineering extends far beyond writing short functions or patching a single file. Real production environments involve cross-platform integration, infrastructure provisioning, and debugging production failures with incomplete information.",
    "language": "é›†æˆä»»åŠ¡æœªæ˜ç¡®æŒ‡å®šä¸»è¦ç¼–ç¨‹è¯­è¨€ï¼Œä½†æ¶‰åŠé…ç½®å¤šç§æœåŠ¡ã€‚å¯è§‚æµ‹æ€§ä»»åŠ¡æ¶‰åŠGo (30%), Python (25%), TypeScript (25%), Java (10%), å’Œ C++ (10%)ã€‚",
    "language_quote": "Observability tasks... Each task includes a real GitHub issue from widely adopted programming languages, including Go, TypeScript, Python, Rust, and Java... Observability tasks are distributed across five languages: Go (30%), Python (25%), TypeScript (25%), Java (10%), and C++ (10%).",
    "data_size": "åŒ…å«200ä¸ªä»»åŠ¡ï¼šé›†æˆä»»åŠ¡100ä¸ªï¼Œå¯è§‚æµ‹æ€§ä»»åŠ¡100ä¸ªã€‚æ­¤å¤–ï¼Œè¿˜å‘å¸ƒäº†ä¸€ä¸ªåŒ…å«50ä¸ªä»»åŠ¡çš„å¼€å‘é›†ã€‚",
    "data_size_quote": "Integration tasks (n = 100)... Observability tasks (n = 100)... We open-source the APEXâ€“SWE evaluation harness and a dev set (n = 50).",
    "source_type": "é›†æˆä»»åŠ¡ï¼šç”±æ‹¥æœ‰3å¹´ä»¥ä¸Šç»éªŒçš„è½¯ä»¶å·¥ç¨‹å¸ˆåˆ›å»ºï¼Œç»è¿‡ä¸‰é˜¶æ®µéªŒè¯è¿‡ç¨‹ã€‚å¯è§‚æµ‹æ€§ä»»åŠ¡ï¼šæºè‡ªçœŸå®ä¸–ç•Œçš„GitHub Issueâ€“PRå¯¹ï¼Œæ¥è‡ªè‡³å°‘æœ‰350é¢—æ˜Ÿçš„ä»“åº“ï¼Œå¹¶ç»è¿‡å¤æ‚æ€§ç­›é€‰ï¼ˆè¡¥ä¸è‡³å°‘100è¡Œä»£ç ä¸”å½±å“è‡³å°‘3ä¸ªæ–‡ä»¶ï¼‰ã€‚",
    "source_type_quote": "Integration Tasks Data - Task Sourcing: Tasks were created by software engineers with 3+ years of experience. Each case underwent a three-stage validation process... Observability Tasks Data - Task Sourcing: Tasks are derived from real-world GitHub Issueâ€“PR pairs, sourced from repositories with at least 350 stars. We filtered for complexity, selecting only patches with at least 100 lines of code impacting at least three files.",
    "last_updated": "2026-01-13 (è®ºæ–‡ç‰ˆæœ¬æ—¥æœŸ)",
    "last_updated_quote": "arXiv:2601.08806v1  [cs.SE]  13 Jan 2026",
    "build_type": "å®˜æ–¹è‡ªå»ºã€‚ç”±Mercorå…¬å¸çš„ç ”ç©¶å›¢é˜Ÿå’Œå¹³å°ä¸“å®¶æ„å»ºã€‚",
    "build_type_quote": "Production processes for APEXâ€“SWE Integration and APEXâ€“SWE Observability, leveraging Mercorâ€™s platform of experts.",
    "contamination_status": "æ–‡ä¸­æœªæ˜ç¡®è®¨è®ºæ•°æ®æ±¡æŸ“çŠ¶æ€ã€‚",
    "contamination_status_quote": NaN,
    "dataset_license": "CC-BYè®¸å¯è¯ï¼ˆé’ˆå¯¹å¼€å‘é›†ï¼‰ã€‚",
    "dataset_license_quote": "We have released an open-source dev set via Hugging Face with a CC-BY license1",
    "task_granularity": "ç³»ç»Ÿçº§/é¡¹ç›®çº§ä»»åŠ¡ã€‚æ¶‰åŠæ„å»ºç«¯åˆ°ç«¯ç³»ç»Ÿæˆ–è¯Šæ–­ä¿®å¤æ•´ä¸ªç”Ÿäº§æ•…éšœï¼Œè€Œéå•ä¸ªå‡½æ•°ç”Ÿæˆæˆ–ä»£ç è¡¥å…¨ã€‚",
    "task_granularity_quote": "APEXâ€“SWE assesses two novel task types that reflect real-world software engineering: (1) Integration tasks... which require constructing end-to-end systems... (2) Observability tasks... which require debugging production failures...",
    "evaluation_metrics": "Pass@1, Pass@3ã€‚æ­¤å¤–ï¼Œè¿˜ä½¿ç”¨è¯„åˆ†æ ‡å‡†ï¼ˆRubricï¼‰è¯„ä¼°å·¥ç¨‹è´¨é‡ï¼ŒåŒ…æ‹¬åŠŸèƒ½æ ‡å‡†ã€é²æ£’æ€§æ ‡å‡†å’Œé£æ ¼æ ‡å‡†ï¼ˆä»…å¯è§‚æµ‹æ€§ä»»åŠ¡ï¼‰ã€‚",
    "evaluation_metrics_quote": "For the APEXâ€“SWE leaderboard, we assess modelsâ€™ outputs using Pass@1... We also report Pass@3... Beyond Pass@1, APEXâ€“SWE uses rubrics to assess engineering quality.",
    "input_modality": "è‡ªç„¶è¯­è¨€ä»»åŠ¡æè¿°ã€ç³»ç»Ÿæç¤ºã€å·¥å…·å®šä¹‰ã€æ–‡ä»¶ï¼ˆå¦‚interface.mdï¼‰ã€æ—¥å¿—æ•°æ®ã€èŠå¤©å†å²ã€GitHub issueä¸Šä¸‹æ–‡ã€‚",
    "input_modality_quote": "The model receives a system prompt containing task instructions and tool definitions... Observability tasks... The model is provided with a summary of the user issue, an interface.md file... and specifications for available Model Context Protocol (MCP) tools. Chat data is pulled from public developer discussions... and GitHub issue data are taken directly from the repo.",
    "output_modality": "ä»£ç ï¼ˆåº”ç”¨ä»£ç ã€åŸºç¡€è®¾æ–½é…ç½®ï¼‰ã€è¡¥ä¸æ–‡ä»¶ã€é€šè¿‡ç»ˆç«¯å’Œæ–‡ä»¶æ“ä½œæ‰§è¡Œçš„ç³»ç»Ÿå‘½ä»¤å’Œé…ç½®æ›´æ”¹ã€‚",
    "output_modality_quote": "Models are required to write application code, configure infrastructure, and deploy functioning services... The harness applies the modelâ€™s patch...",
    "task_io_type": "æ–‡æœ¬åˆ°ç³»ç»Ÿå®ç°/ä¿®å¤ã€‚è¾“å…¥æ˜¯è‡ªç„¶è¯­è¨€æè¿°å’Œä¸Šä¸‹æ–‡æ–‡ä»¶ï¼Œè¾“å‡ºæ˜¯å¯¼è‡´ç³»ç»ŸåŠŸèƒ½æ­£ç¡®è¿è¡Œçš„ä»£ç ã€é…ç½®å’Œæ“ä½œåºåˆ—ã€‚",
    "task_io_type_quote": "APEXâ€“SWE Integration... evaluates a modelâ€™s ability to orchestrate end-to-end workflows... APEXâ€“SWE Observability... evaluates a modelâ€™s ability to diagnose and remediate real-world production failures.",
    "execution_environment": "å®¹å™¨åŒ–æ²™ç®±ç¯å¢ƒï¼ŒåŒ…å«å¤šç§äº‘æœåŠ¡æ¨¡æ‹Ÿï¼ˆAWS LocalStackï¼‰ã€ç”Ÿäº§çº§ä¸šåŠ¡åº”ç”¨ï¼ˆEspoCRM, Medusaç­‰ï¼‰ã€æ—¥å¿—èšåˆç³»ç»Ÿï¼ˆLoki, Promtail, Grafanaï¼‰ã€æ•°æ®åº“ï¼ˆPostgreSQLï¼‰å’Œåä½œå·¥å…·ï¼ˆPlane, Mattermostï¼‰ã€‚æ¨¡å‹é€šè¿‡ç»ˆç«¯ã€æ–‡ä»¶æ“ä½œå’ŒMCPæœåŠ¡å™¨å·¥å…·ä¸ç¯å¢ƒäº¤äº’ã€‚",
    "execution_environment_quote": "Integration tasks... The test stack includes cloud primitives (AWS LocalStack: S3, Lambda, DynamoDB, Kinesis) and production-grade business applications (EspoCRM, Medusa, Zammad, Plane)... Observability tasks... Each task deploys a containerized environment orchestrating five services: a client workspace, Loki and Promtail for log aggregation, Grafana for visualization, and Plane/Mattermost for ticket and chat context... Models have access to three categories of tools: Terminal..., File Operations..., and MCP Servers...",
    "unique_features": "1. ä¸“æ³¨äºçœŸå®ä¸–ç•Œã€å…·æœ‰ç»æµä»·å€¼çš„è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ï¼Œè€Œéç‹­çª„çš„ã€å®šä¹‰æ˜ç¡®çš„ä»»åŠ¡ã€‚2. åŒ…å«ä¸¤ç§æ–°é¢–çš„ä»»åŠ¡ç±»å‹ï¼šé›†æˆå’Œå¯è§‚æµ‹æ€§ã€‚3. å¼ºè°ƒè®¤çŸ¥æ¨ç†ï¼ˆåŒºåˆ†å‡è®¾ä¸äº‹å®ï¼‰å’Œä»£ç†èƒ½åŠ›ï¼ˆè¡ŒåŠ¨å‰è§£å†³ä¸ç¡®å®šæ€§ï¼‰æ˜¯æˆåŠŸçš„å…³é”®é©±åŠ¨å› ç´ ã€‚4. åŒ…å«å¤æ‚çš„èº«ä»½éªŒè¯å’Œå®‰å…¨æ–¹æ¡ˆï¼ˆBasic Auth, JWT, IAM, APIå¯†é’¥ï¼‰ï¼Œæµ‹è¯•æ¨¡å‹å¤„ç†çœŸå®å‡­è¯ç®¡ç†çš„èƒ½åŠ›ã€‚5. ä½¿ç”¨è¯„åˆ†æ ‡å‡†ï¼ˆRubricï¼‰è¿›è¡Œè¶…è¶ŠäºŒè¿›åˆ¶é€šè¿‡/å¤±è´¥çš„å·¥ç¨‹è´¨é‡è¯„ä¼°ã€‚",
    "unique_features_quote": "Unlike existing evaluations that focus on narrow, well-defined tasks, APEXâ€“SWE assesses two novel task types that reflect real-world software engineering... Our analysis shows that strong performance is primarily driven by epistemic reasoning, defined as the ability to distinguish between assumptions and verified facts, combined with agency to resolve uncertainty prior to acting... Tasks have authentication schemes to test modelsâ€™ ability to handle real-world credential management... Beyond Pass@1, APEXâ€“SWE uses rubrics to assess engineering quality.",
    "data_size_quantity": 200,
    "data_size_unit": "ä¸ªä»»åŠ¡",
    "last_updated_year": 2026,
    "last_updated_month": 1,
    "last_updated_day": 13,
    "language_normalized": "['Go', 'Python', 'TypeScript', 'Java', 'C++']",
    "dimension_normalized": "['çœŸå®ä¸–ç•Œè½¯ä»¶å·¥ç¨‹èƒ½åŠ›', 'è·¨å¹³å°é›†æˆ', 'åŸºç¡€è®¾æ–½é…ç½®', 'ç”Ÿäº§ç¯å¢ƒè°ƒè¯•', 'è®¤çŸ¥æ¨ç†', 'ä»£ç†èƒ½åŠ›']",
    "evaluation_method_normalized": "['Pass@1', 'Pass@3', 'è¯„åˆ†æ ‡å‡†']",
    "problem_domain_normalized": "['ä¸“ä¸šè½¯ä»¶å·¥ç¨‹', 'äº‘åŸºç¡€è®¾æ–½é›†æˆ', 'æ”¯ä»˜ç®¡é“', 'æ— æœåŠ¡å™¨äº‹ä»¶é©±åŠ¨æ¶æ„', 'CRMä¸å·¥å•ç³»ç»Ÿæ•°æ®åŒæ­¥', 'ç”Ÿäº§ç¯å¢ƒæ•…éšœè¯Šæ–­ä¸ä¿®å¤']",
    "source_type_normalized": "['é›†æˆä»»åŠ¡', 'å¯è§‚æµ‹æ€§ä»»åŠ¡']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "CC-BY-SA 4.0",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.09703_output/content.md",
    "benchmark_name": "ShorterCodeBench",
    "benchmark_name_quote": "producing ShorterCodeBenchâ€”a corpus of validated âŸ¨original code, simplified codeâŸ©pairs with semantic consistency;",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We present and publicly release ShorterCodeBench, a high-quality code brevity optimization dataset comprising 828 carefully curated âŸ¨original code, simplified codeâŸ© pairs.",
    "dataset_url": "https://github.com/DeepSoftwareAnalytics/ShorterCode",
    "dataset_url_quote": "We provide the code and data at https://github.com/DeepSoftwareAnalytics/ShorterCode.",
    "task_description": "ä»£ç ç®€æ´æ€§ä¼˜åŒ–ã€‚è¯¥æ•°æ®é›†åŒ…å«åŸå§‹ä»£ç å’Œç»è¿‡ç®€åŒ–ï¼ˆä¿æŒè¯­ä¹‰ä¸€è‡´ï¼‰çš„ä»£ç å¯¹ï¼Œæ—¨åœ¨ç”¨äºè®­ç»ƒæˆ–è¯„ä¼°æ¨¡å‹ç”Ÿæˆç®€æ´ä»£ç çš„èƒ½åŠ›ã€‚",
    "task_description_quote": "producing ShorterCodeBench, a corpus of validated âŸ¨original code, simplified codeâŸ©pairs with semantic consistency.",
    "dimension": "ä»£ç ç”Ÿæˆæ•ˆç‡ï¼ˆç®€æ´æ€§/ä»¤ç‰Œæ•ˆç‡ï¼‰",
    "dimension_quote": "optimizes code generation efficiency while preserving semantic equivalence and readability.",
    "evaluation_method": "æ–‡ä¸­æœªæ˜ç¡®æè¿°ShorterCodeBenchè‡ªèº«çš„è¯„ä¼°æ–¹æ³•ï¼Œä½†æåŠäº†åœ¨HumanEvalç­‰åŸºå‡†ä¸Šä½¿ç”¨pass@kç­‰æŒ‡æ ‡è¯„ä¼°æ¨¡å‹ã€‚",
    "evaluation_method_quote": NaN,
    "context_dependency": "å•å‡½æ•°æˆ–ä»£ç ç‰‡æ®µçº§åˆ«",
    "context_dependency_quote": "The rule formulation process comprises three phases: â€¢ Manual Rule Elicitation. We manually conduct inspection of each MBPP dataset entry... (MBPPæ˜¯å•å‡½æ•°çº§åˆ«ä»»åŠ¡ï¼Œæš—ç¤ºäº†ShorterCodeBenchçš„æ„å»ºåŸºç¡€)",
    "problem_domain": "é€šç”¨ç¼–ç¨‹ï¼ˆåŸºäºPythonè¯­æ³•ï¼‰",
    "problem_domain_quote": "we first designed ten syntax-level simplification rules for Python",
    "problem_difficulty": "æ–‡ä¸­æœªæ˜ç¡®æè¿°",
    "problem_difficulty_quote": NaN,
    "language": "Python",
    "language_quote": "we first designed ten syntax-level simplification rules for Python",
    "data_size": "828ä¸ªç»è¿‡éªŒè¯çš„<åŸå§‹ä»£ç ï¼Œç®€åŒ–ä»£ç >å¯¹",
    "data_size_quote": "producing ShorterCodeBench, a corpus of 828 validated âŸ¨original code, simplified codeâŸ©pairs",
    "source_type": "åŸºäºMBPPæ•°æ®é›†æ¡ç›®ï¼Œé€šè¿‡æ‰‹åŠ¨è§„åˆ™å¯å‘ã€ä¸“å®¶é©±åŠ¨æ‰©å±•å’ŒéªŒè¯é©±åŠ¨æœ€ç»ˆç¡®å®šçš„è§„åˆ™è¿›è¡Œåˆæˆã€‚",
    "source_type_quote": "We manually conduct inspection of each MBPP dataset entry... We engage 3 experts who are proficient in Python syntax to systematically extend the rule set... We perform cross-dataset validation...",
    "last_updated": "2026-01-14 (æ ¹æ®arXivç‰ˆæœ¬v1æ—¥æœŸ)",
    "last_updated_quote": "arXiv:2601.09703v1  [cs.SE]  14 Jan 2026",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆç”±è®ºæ–‡ä½œè€…å›¢é˜Ÿæ„å»ºï¼‰",
    "build_type_quote": "We present and publicly release ShorterCodeBench",
    "contamination_status": "æ–‡ä¸­æœªæ˜ç¡®æè¿°",
    "contamination_status_quote": NaN,
    "dataset_license": "æ–‡ä¸­æœªæ˜ç¡®æè¿°",
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç è½¬æ¢/ä»£ç é‡å†™ï¼ˆä»å†—é•¿ä»£ç åˆ°ç®€æ´ä»£ç ï¼‰",
    "task_granularity_quote": "a corpus of validated âŸ¨original code, simplified codeâŸ©pairs",
    "evaluation_metrics": "æ–‡ä¸­æœªæ˜ç¡®æè¿°ShorterCodeBenchè‡ªèº«çš„è¯„ä¼°æŒ‡æ ‡ã€‚",
    "evaluation_metrics_quote": NaN,
    "input_modality": "ä»£ç ",
    "input_modality_quote": "âŸ¨original code, simplified codeâŸ©",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "âŸ¨original code, simplified codeâŸ©",
    "task_io_type": "ä»£ç åˆ°ä»£ç ",
    "task_io_type_quote": "âŸ¨original code, simplified codeâŸ©",
    "execution_environment": "æ–‡ä¸­æœªæ˜ç¡®æè¿°",
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºä»£ç ç®€æ´æ€§ï¼ˆä»¤ç‰Œæ•ˆç‡ï¼‰ä¼˜åŒ–ï¼Œé€šè¿‡10ä¸ªåŸºäºASTä¿æŒè½¬æ¢çš„Pythonè¯­æ³•çº§ç®€åŒ–è§„åˆ™æ„å»ºï¼Œç¡®ä¿è¯­ä¹‰ä¸€è‡´æ€§ã€‚",
    "unique_features_quote": "ten syntax-level simplification rules for Python, derived from AST-preserving transformations, achieving 18.1% token reduction without functional compromise.",
    "data_size_quantity": 828,
    "data_size_unit": "ä¸ª",
    "last_updated_year": 2026,
    "last_updated_month": 1,
    "last_updated_day": 14,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆæ•ˆç‡ï¼ˆç®€æ´æ€§/ä»¤ç‰Œæ•ˆç‡ï¼‰']",
    "evaluation_method_normalized": "[]",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹ï¼ˆåŸºäºPythonè¯­æ³•ï¼‰']",
    "source_type_normalized": "['åŸºäºMBPPæ•°æ®é›†æ¡ç›®ï¼Œé€šè¿‡æ‰‹åŠ¨è§„åˆ™å¯å‘ã€ä¸“å®¶é©±åŠ¨æ‰©å±•å’ŒéªŒè¯é©±åŠ¨æœ€ç»ˆç¡®å®šçš„è§„åˆ™è¿›è¡Œåˆæˆã€‚']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.09097_output/content.md",
    "benchmark_name": "TravelPlanner",
    "benchmark_name_quote": "We evaluate SCOPE on benchmarks such as TravelPlanner (Xie et al., 2024) and Natural Plan (Zheng et al., 2024)",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We evaluate SCOPE on benchmarks such as TravelPlanner (Xie et al., 2024) and Natural Plan (Zheng et al., 2024)",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "å¤šçº¦æŸè§„åˆ’ï¼Œæ¶‰åŠè¯†åˆ«ã€è¯„ä¼°å’Œä¼˜åŒ–å€™é€‰è®¡åˆ’ï¼ŒåŒæ—¶æ»¡è¶³å¤šä¸ªå¯èƒ½ç›¸äº’å†²çªçš„çº¦æŸæ¡ä»¶ã€‚",
    "task_description_quote": "Multi-constraint planning involves identify- ing, evaluating, and refining candidate plans while satisfying multiple, potentially conflict- ing constraints.",
    "dimension": "å¤šçº¦æŸè§„åˆ’èƒ½åŠ›ã€æ¨ç†çš„é²æ£’æ€§å’Œå¯æ‰©å±•æ€§",
    "dimension_quote": "Existing large language model (LLM) approaches face fundamental limita- tions in this domain. Pure reasoning paradigms... are prone to inconsistency, error accumula- tion, and prohibitive cost as constraints com- pound.",
    "evaluation_method": "æˆåŠŸç‡ï¼ˆsuccess rateï¼‰",
    "evaluation_method_quote": "it reaches 93.1% success on TravelPlanner",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "è§„åˆ’ï¼ˆPlanningï¼‰ï¼Œç‰¹åˆ«æ˜¯å¤šçº¦æŸè§„åˆ’",
    "problem_domain_quote": "Planning is the process by which an agent orga- nizes sequences of decisions or actions to achieve a goal.",
    "problem_difficulty": "æ¶‰åŠé•¿è§†é‡æ¨ç†å’Œå¤æ‚çº¦æŸçš„è§„åˆ’ä»»åŠ¡",
    "problem_difficulty_quote": "Benchmarks... show failures under long- horizon reasoning and complex constraints.",
    "language": NaN,
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": "2024",
    "last_updated_quote": "TravelPlanner (Xie et al., 2024)",
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "è§„åˆ’ä»»åŠ¡ï¼ˆPlanningï¼‰",
    "task_granularity_quote": "Planning is the process by which an agent orga- nizes sequences of decisions or actions to achieve a goal.",
    "evaluation_metrics": "æˆåŠŸç‡",
    "evaluation_metrics_quote": "it reaches 93.1% success on TravelPlanner",
    "input_modality": "è‡ªç„¶è¯­è¨€æŸ¥è¯¢",
    "input_modality_quote": "to convert natural language queries into optimized structured representations",
    "output_modality": "è‡ªç„¶è¯­è¨€ç­”æ¡ˆæˆ–ç»“æ„åŒ–è®¡åˆ’",
    "output_modality_quote": "converts it into a natural language answer that follows the desired output format.",
    "task_io_type": "æ–‡æœ¬åˆ°æ–‡æœ¬ï¼ˆè‡ªç„¶è¯­è¨€æŸ¥è¯¢åˆ°è‡ªç„¶è¯­è¨€/ç»“æ„åŒ–è®¡åˆ’ï¼‰",
    "task_io_type_quote": "to convert natural language queries into optimized structured representations... converts it into a natural language answer",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºå¤šçº¦æŸè§„åˆ’ä»»åŠ¡ï¼Œè¯„ä¼°æ¨¡å‹åœ¨é•¿è§†é‡æ¨ç†å’Œå¤æ‚çº¦æŸä¸‹çš„å¤±è´¥æƒ…å†µã€‚",
    "unique_features_quote": "Benchmarks (Valmeekam et al., 2023; Zheng et al., 2024; Xie et al., 2024) show failures under long- horizon reasoning and complex constraints.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2024,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['å¤šçº¦æŸè§„åˆ’èƒ½åŠ›', 'æ¨ç†çš„é²æ£’æ€§', 'å¯æ‰©å±•æ€§']",
    "evaluation_method_normalized": "['æˆåŠŸç‡']",
    "problem_domain_normalized": "['è§„åˆ’', 'å¤šçº¦æŸè§„åˆ’']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.10402_output/content.md",
    "benchmark_name": "MLE-Bench",
    "benchmark_name_quote": "Within this paradigm, the machine learning engineering (MLE) tasks emphasized by OpenAIâ€™s MLE-Bench [2] emerge as its quintessential challenge.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We evaluate ML-Master 2.0 on OpenAIâ€™s MLE-Bench under a fixed 24-hour execution budget.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„æµ‹åŸºå‡†æ—¨åœ¨è¯„ä¼°æ™ºèƒ½ä½“åœ¨çœŸå®ä¸–ç•Œæœºå™¨å­¦ä¹ å·¥ç¨‹é¡¹ç›®ä¸­çš„èƒ½åŠ›ï¼Œè¿™äº›é¡¹ç›®æºè‡ªKaggleç«èµ›ï¼Œè¦æ±‚æ™ºèƒ½ä½“åœ¨é•¿æ—¶é—´è·¨åº¦å†…è¿›è¡Œæ¢ç´¢ã€è¯•é”™å’Œç»éªŒç§¯ç´¯ã€‚",
    "task_description_quote": "MLE-Bench is a benchmark comprising 75 real-world Kaggle machine learning competitions. Far exceeding simple code generation, it requires agents to navigate a vast, unstructured search space through prolonged trial and error and the accumulation of experience across iterations, rather than by single-step correctness.",
    "dimension": "è¶…é•¿è§†é‡è‡ªä¸»æ€§ã€æœºå™¨å­¦ä¹ å·¥ç¨‹èƒ½åŠ›ã€æˆ˜ç•¥è¿è´¯æ€§ã€è¿­ä»£ä¿®æ­£èƒ½åŠ›",
    "dimension_quote": "This intrinsic complexity necessitates ultra-long-horizon autonomy. It refers to the capacity to sustain strategic coherence and perform iterative correction over extended temporal scales without being overwhelmed by the accumulation of execution details.",
    "evaluation_method": "åœ¨å›ºå®š24å°æ—¶æ‰§è¡Œé¢„ç®—ä¸‹ï¼Œä½¿ç”¨å¥–ç‰Œç‡ï¼ˆè·å¾—é“œç‰Œã€é“¶ç‰Œæˆ–é‡‘ç‰Œçº§åˆ«æ€§èƒ½çš„ä»»åŠ¡ç™¾åˆ†æ¯”ï¼‰æ¥è¡¡é‡æ€§èƒ½ã€‚",
    "evaluation_method_quote": "We evaluate ML-Master 2.0 on OpenAIâ€™s MLE-Bench under a fixed 24-hour execution budget. As shown in Figure 1, we measure performance using the average medal rate, defined as the percentage of tasks where the method achieves Bronze, Silver, or Gold-level performance.",
    "context_dependency": "è¶…é•¿è§†é‡ã€å¤šé˜¶æ®µã€å¤šè½¨è¿¹æ¢ç´¢ï¼Œæ¶‰åŠè·¨è¿­ä»£çš„ç»éªŒç§¯ç´¯å’Œæˆ˜ç•¥è§„åˆ’ã€‚",
    "context_dependency_quote": "The interaction interval [ğ‘¡ğ‘âˆ’1, ğ‘¡ğ‘) therefore corresponds to one coherent exploration phase, typically consisting of multiple parallel implementation trajectories.",
    "problem_domain": "æœºå™¨å­¦ä¹ å·¥ç¨‹",
    "problem_domain_quote": "Within this paradigm, the machine learning engineering (MLE) tasks emphasized by OpenAIâ€™s MLE-Bench [2] emerge as its quintessential challenge.",
    "problem_difficulty": "åŒ…å«ä½ã€ä¸­ã€é«˜ä¸‰ç§å¤æ‚åº¦çº§åˆ«çš„ä»»åŠ¡",
    "problem_difficulty_quote": "We can see that ML-Master 2.0 achieves state-of-the-art performance across all difficulty levels. In particular, ML-Master 2.0 attains an overall medal rate of 56.44%... The gains are consistent across task complexities: performance on low-complexity tasks improves from 48.48% to 75.76%, while medium-complexity and high-complexity tasks improve from 20.18% to 50.88% and from 24.44% to 42.22%, respectively.",
    "language": NaN,
    "language_quote": NaN,
    "data_size": "åŒ…å«75ä¸ªçœŸå®ä¸–ç•Œçš„Kaggleæœºå™¨å­¦ä¹ ç«èµ›ä»»åŠ¡",
    "data_size_quote": "MLE-Bench is a benchmark comprising 75 real-world Kaggle machine learning competitions.",
    "source_type": "æºè‡ªKaggleç«èµ›",
    "source_type_quote": "MLE-Bench is a benchmark comprising 75 real-world Kaggle machine learning competitions.",
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": "OpenAIæ„å»º",
    "build_type_quote": "OpenAIâ€™s MLE-Bench [2]",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ç«¯åˆ°ç«¯çš„æœºå™¨å­¦ä¹ å·¥ç¨‹é¡¹ç›®ï¼Œè¿œè¶…ç®€å•çš„ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "Far exceeding simple code generation, it requires agents to navigate a vast, unstructured search space through prolonged trial and error and the accumulation of experience across iterations, rather than by single-step correctness.",
    "evaluation_metrics": "å¥–ç‰Œç‡ï¼ˆMedal Rateï¼‰",
    "evaluation_metrics_quote": "We measure performance using the average medal rate, defined as the percentage of tasks where the method achieves Bronze, Silver, or Gold-level performance.",
    "input_modality": "ä»»åŠ¡æè¿°ã€ç”¨æˆ·æŒ‡ä»¤ã€æ‰§è¡Œåé¦ˆç­‰ç¯å¢ƒäº‹ä»¶",
    "input_modality_quote": "We partition the event space into environment-originated events U (e.g., task descriptions, user instructions, execution feedback)",
    "output_modality": "ä»£ç è¡¥ä¸ã€å‘½ä»¤ã€è®¡åˆ’ç­‰æ™ºèƒ½ä½“åŠ¨ä½œ",
    "output_modality_quote": "and agent-originated events A (e.g., code patches, commands, plans)",
    "task_io_type": "å¤æ‚äº¤äº’åºåˆ—åˆ°æœ€ç»ˆè§£å†³æ–¹æ¡ˆä»£ç ",
    "task_io_type_quote": "after which the final solution code ğ¼âˆ—= â„(Eğ‘¡max) is obtained by applying the extraction function â„(Â·) to the terminal interaction history.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºè¯„ä¼°è¶…é•¿è§†é‡è‡ªä¸»æ€§ï¼Œæ¨¡æ‹ŸçœŸå®ç§‘å­¦ç ”ç©¶ä¸­è·¨è¶Šæ•°å¤©æˆ–æ•°å‘¨çš„å®éªŒå‘¨æœŸï¼Œå¼ºè°ƒæˆ˜ç•¥è¿è´¯æ€§å’Œè¿­ä»£ä¿®æ­£ï¼Œè€Œéå•æ­¥æ­£ç¡®æ€§ã€‚",
    "unique_features_quote": "Scientific Discovery is inherently a ultra-long-horizon process, characterized not by momentary acts of reasoning but by delayed feedback, high-dimensional exploration, and experimental cycles spanning days or weeks.",
    "data_size_quantity": 75,
    "data_size_unit": "ä¸ªä»»åŠ¡",
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['è¶…é•¿è§†é‡è‡ªä¸»æ€§', 'æœºå™¨å­¦ä¹ å·¥ç¨‹èƒ½åŠ›', 'æˆ˜ç•¥è¿è´¯æ€§', 'è¿­ä»£ä¿®æ­£èƒ½åŠ›']",
    "evaluation_method_normalized": "['å¥–ç‰Œç‡ï¼ˆMedal Rateï¼‰']",
    "problem_domain_normalized": "['æœºå™¨å­¦ä¹ å·¥ç¨‹']",
    "source_type_normalized": "['æºè‡ªKaggleç«èµ›']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.10496_output/content.md",
    "benchmark_name": "ManySStuBs4J",
    "benchmark_name_quote": "We introduce an exposure-aware evaluation framework... Using the ManySStuBs4J benchmark...",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We introduce an exposure-aware evaluation framework that quantifies how prior exposure to buggy versus fixed code influences a modelâ€™s preference. Using the ManySStuBs4J benchmark...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„æµ‹ä»£ç å¤§è¯­è¨€æ¨¡å‹åœ¨å•è¯­å¥é”™è¯¯ï¼ˆSStuBsï¼‰åŠå…¶ä¿®å¤ç‰ˆæœ¬ä¹‹é—´çš„åå¥½ï¼Œå¹¶é‡åŒ–è®­ç»ƒæ•°æ®æš´éœ²ï¼ˆå³æ¨¡å‹åœ¨è®­ç»ƒä¸­æ˜¯å¦è§è¿‡é”™è¯¯æˆ–ä¿®å¤ä»£ç ï¼‰å¯¹æ­¤åå¥½çš„å½±å“ã€‚",
    "task_description_quote": "We introduce an exposure-aware evaluation framework that quantifies how prior exposure to buggy versus fixed code influences a modelâ€™s preference.",
    "dimension": "æ¨¡å‹å¯¹é”™è¯¯ä»£ç ä¸ä¿®å¤ä»£ç çš„åå¥½ï¼›è®­ç»ƒæ•°æ®æš´éœ²ï¼ˆè®°å¿†ï¼‰å¯¹æ¨¡å‹è¡Œä¸ºçš„å½±å“ï¼›é”™è¯¯ä¼ æ’­é£é™©ã€‚",
    "dimension_quote": "Distinguishing whether an LLM prefers correct code, or a familiar incorrect version might be influenced by what itâ€™s been exposed to during training.",
    "evaluation_method": "ä½¿ç”¨åŸºäºä¼¼ç„¶æ€§çš„è¯„åˆ†æŒ‡æ ‡ï¼ˆå¦‚æœ€å°/æœ€å¤§è¯å…ƒæ¦‚ç‡ã€åŸºå°¼ç³»æ•°ï¼‰è¯„ä¼°æ¨¡å‹åå¥½ï¼›é€šè¿‡ä»£ç è¡¥å…¨ç”Ÿæˆå¹¶åŒ¹é…åŸå§‹é”™è¯¯æˆ–ä¿®å¤å˜ä½“ã€‚",
    "evaluation_method_quote": "We then measure model preference with a suite of likelihood-based metrics. Additionally we evaluate model completions based on the bug-fix context.",
    "context_dependency": "å•è¯­å¥ä¸Šä¸‹æ–‡ï¼ˆå•è¡Œä»£ç å˜æ›´ï¼‰ã€‚",
    "context_dependency_quote": "SStuBs focus on single-statement bugs that can be fixed with single-line changes.",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ï¼Œä»£ç ç¼ºé™·ä¸ä¿®å¤ã€‚",
    "problem_domain_quote": "The ManySStubs4J dataset collects thousands of SStuBs, (â€˜Simple, Stupid Bugsâ€™), which are single-statement bugs and their fixes from open-source Java repositories.",
    "problem_difficulty": "ç®€å•ä½†å½±å“é‡å¤§çš„é”™è¯¯ï¼ˆä¾‹å¦‚å·®ä¸€é”™è¯¯ã€é”™è¯¯æ¡ä»¶åˆ¤æ–­ã€ç¼ºå°‘ç©ºå€¼æ£€æŸ¥ç­‰ï¼‰ã€‚",
    "problem_difficulty_quote": "which capture small but impactful errors (off-by-one mistakes, incorrect conditionals, missing null checks, etc.) in real programs.",
    "language": "Java",
    "language_quote": "We use the ManySStuBs4J dataset, which contains mined bug-fix pairs from open-source Java projects.",
    "data_size": "åŒ…å«æ•°åƒä¸ªé”™è¯¯-ä¿®å¤å¯¹ã€‚",
    "data_size_quote": "The ManySStubs4J dataset collects thousands of SStuBs...",
    "source_type": "ä»å¼€æºJavaä»“åº“ä¸­æŒ–æ˜çš„çœŸå®é”™è¯¯åŠå…¶ä¿®å¤ã€‚",
    "source_type_quote": "The ManySStubs4J dataset collects thousands of SStuBs... from open-source Java repositories.",
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": "ç¤¾åŒºè´¡çŒ®ï¼ˆä»å¼€æºé¡¹ç›®æŒ–æ˜ï¼‰ã€‚",
    "build_type_quote": "The ManySStubs4J dataset collects thousands of SStuBs... from open-source Java repositories.",
    "contamination_status": "é«˜æ±¡æŸ“é£é™©ï¼ˆè®ºæ–‡æ ¸å¿ƒå…³æ³¨è®­ç»ƒæ•°æ®æš´éœ²å¯¹è¯„æµ‹çš„å½±å“ï¼‰ã€‚",
    "contamination_status_quote": "Our results indicate that exposure can skew bug-fix evaluations and highlight the risk that LLMs may propagate memorised errors in practice.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç è¡¥å…¨ï¼ˆåœ¨ç»™å®šä¸Šä¸‹æ–‡ä¸­ç”Ÿæˆå•è¡Œä»£ç ï¼‰ã€‚",
    "task_granularity_quote": "We then measure model preference... Additionally we evaluate model completions based on the bug-fix context.",
    "evaluation_metrics": "æœ€å°è¯å…ƒæ¦‚ç‡ã€æœ€å¤§è¯å…ƒæ¦‚ç‡ã€åŸºå°¼ç³»æ•°ç­‰åŸºäºä¼¼ç„¶æ€§çš„æŒ‡æ ‡ï¼›ç”Ÿæˆä»£ç ä¸åŸå§‹é”™è¯¯/ä¿®å¤çš„åŒ¹é…ç‡ã€‚",
    "evaluation_metrics_quote": "likelihood-based metrics such as the minimum and maximum token probability... others like the Gini coefficient... Generation matching",
    "input_modality": "ä»£ç ä¸è‡ªç„¶è¯­è¨€ï¼ˆåŒ…å«é”™è¯¯çš„ä»£ç ä¸Šä¸‹æ–‡ï¼‰ã€‚",
    "input_modality_quote": "We then measure model preference... Additionally we evaluate model completions based on the bug-fix context.",
    "output_modality": "ä»£ç ï¼ˆå•è¡Œï¼‰ã€‚",
    "output_modality_quote": "SStuBs focus on single-statement bugs that can be fixed with single-line changes.",
    "task_io_type": "ä»£ç åˆ°ä»£ç ï¼ˆåœ¨åŒ…å«é”™è¯¯çš„ä»£ç ä¸Šä¸‹æ–‡ä¸­ç”Ÿæˆä¿®å¤åçš„ä»£ç è¡Œï¼‰ã€‚",
    "task_io_type_quote": "We then measure model preference for either the bug or the fix... evaluate model completions based on the bug-fix context.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºå•è¯­å¥é”™è¯¯ï¼ˆSStuBsï¼‰ï¼›é›†æˆäº†è®­ç»ƒæ•°æ®æš´éœ²ï¼ˆæˆå‘˜æ¨æ–­ï¼‰åˆ†æï¼Œä»¥åŒºåˆ†æ¨¡å‹åå¥½æ˜¯æºäºè®°å¿†è¿˜æ˜¯çœŸæ­£çš„æ­£ç¡®æ€§å­¦ä¹ ï¼›ç”¨äºç ”ç©¶é”™è¯¯ä¼ æ’­é£é™©ã€‚",
    "unique_features_quote": "SStuBs focus on single-statement bugs that can be fixed with single-line changes... We introduce an exposure-aware evaluation framework that quantifies how prior exposure to buggy versus fixed code influences a modelâ€™s preference... Our results indicate that exposure can skew bug-fix evaluations and highlight the risk that LLMs may propagate memorised errors in practice.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Java']",
    "dimension_normalized": "['æ¨¡å‹å¯¹é”™è¯¯ä»£ç ä¸ä¿®å¤ä»£ç çš„åå¥½', 'è®­ç»ƒæ•°æ®æš´éœ²ï¼ˆè®°å¿†ï¼‰å¯¹æ¨¡å‹è¡Œä¸ºçš„å½±å“', 'é”™è¯¯ä¼ æ’­é£é™©']",
    "evaluation_method_normalized": "['æœ€å°è¯å…ƒæ¦‚ç‡', 'æœ€å¤§è¯å…ƒæ¦‚ç‡', 'åŸºå°¼ç³»æ•°ç­‰åŸºäºä¼¼ç„¶æ€§çš„æŒ‡æ ‡', 'ç”Ÿæˆä»£ç ä¸åŸå§‹é”™è¯¯/ä¿®å¤çš„åŒ¹é…ç‡']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'ä»£ç ç¼ºé™·ä¸ä¿®å¤']",
    "source_type_normalized": "['ä»å¼€æºJavaä»“åº“ä¸­æŒ–æ˜çš„çœŸå®é”™è¯¯åŠå…¶ä¿®å¤']",
    "problem_difficulty_normalized": "å…¥é—¨çº§",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç è¡¥å…¨",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "é«˜æ±¡æŸ“é£é™©"
  },
  {
    "source_paper": "2601.10343_output/content.md",
    "benchmark_name": "OCTOBENCH",
    "benchmark_name_quote": "To fill this gap, we introduce OCTOBENCH, which benchmarks scaffold-aware instruction following in repository-grounded agentic coding.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "To fill this gap, we introduce OCTOBENCH, which benchmarks scaffold-aware instruction following in repository-grounded agentic coding.",
    "dataset_url": "https://github.com/MiniMax-AI/mini-vela",
    "dataset_url_quote": "Â§ https://github.com/MiniMax-AI/mini-vela",
    "task_description": "è¯„æµ‹æ™ºèƒ½ä½“åœ¨åŸºäºä»£ç ä»“åº“çš„ã€å¤šè½®äº¤äº’çš„ä»£ç†å¼ç¼–ç åœºæ™¯ä¸­ï¼Œéµå¾ªè„šæ‰‹æ¶æ‰€æŒ‡å®šçš„ã€æ¥è‡ªå¼‚æ„æ¥æºçš„ã€æŒä¹…æ€§æŒ‡ä»¤çš„èƒ½åŠ›ã€‚",
    "task_description_quote": "OCTOBENCH includes 34 environments and 217 tasks instantiated under three scaffold types, and is paired with 7,098 objective checklist items. To disentangle solving the task from following the rules, we provide an automated observation-and-scoring toolkit that captures full trajectories and performs fine-grained checks.",
    "dimension": "è„šæ‰‹æ¶æ„ŸçŸ¥çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼ŒåŒ…æ‹¬å¯¹å¼‚æ„çº¦æŸã€ä¼˜å…ˆçº§æ„ŸçŸ¥çš„å†²çªè§£å†³ä»¥åŠè·¨è½®æ¬¡æŒä¹…æ€§éµå®ˆçš„è¯„ä¼°ã€‚",
    "dimension_quote": "Compliance is defined over multiple concurrent instruction sources with different authority levels and time horizons. Accordingly, evaluation must account for (i) heterogeneous constraints, (ii) priority-aware conflict resolution, and (iii) persistent adherence across turns, including interactions with tool schemas and state.",
    "evaluation_method": "é€šè¿‡è‡ªåŠ¨åŒ–è§‚å¯Ÿå’Œè¯„åˆ†å·¥å…·åŒ…æ•è·å®Œæ•´äº¤äº’è½¨è¿¹ï¼Œå¹¶å°†å…¶æ˜ å°„åˆ°åŸºäºå®ä¾‹çš„ã€ç»“æ„åŒ–çš„äºŒè¿›åˆ¶æ£€æŸ¥æ¸…å•ï¼Œä½¿ç”¨LLM-as-a-judgeè¿›è¡Œç»†ç²’åº¦ã€è¿‡ç¨‹çº§åˆ«çš„åˆè§„æ€§è¯„ä¼°ã€‚",
    "evaluation_method_quote": "Accordingly, we pair each task with a granular observation harness and automated evaluation toolkit that captures and normalizes the agentâ€™s full action trajectory, and then maps the realized behavior to a structured checklist of binary checks with an LLM-as-a-judge (Zheng et al., 2023; Gu et al., 2025). This enables fine-grained, process-level compliance assessment...",
    "context_dependency": "åŸºäºä»£ç ä»“åº“çš„ã€å¤šè½®äº¤äº’çš„ä»£ç†å¼ç¼–ç ç¯å¢ƒï¼Œæ¶‰åŠè·¨æ–‡ä»¶ã€é¡¹ç›®APIå’Œç°æœ‰æŠ½è±¡çš„ä½¿ç”¨ã€‚",
    "context_dependency_quote": "Evaluation of code generation has moved from isolated function synthesis... to repository-level generation and patching that requires using cross-file context, project APIs, and existing abstractions... OCTOBENCH targets this gap by benchmarking instruction adherence in repository-grounded agentic coding...",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ï¼Œä»£ç†å¼ç¼–ç ï¼Œæ¶‰åŠä»£ç ç”Ÿæˆã€ä¿®æ”¹ã€å·¥å…·è°ƒç”¨å’Œä»“åº“å¯¼èˆªã€‚",
    "problem_domain_quote": "In software engineering, agentic coding scaffolds such as Claude Code (Anthropic, 2025a), Kilo (Kilo, 2025), and Droid (Factory.ai, 2025) turn LLMs into end-to-end coding agents that can navigate repositories, invoke tools, and iteratively modify code.",
    "problem_difficulty": "ç°å®ã€é•¿ä¸Šä¸‹æ–‡ã€å¤æ‚çº¦æŸç»“æ„ï¼Œæºè‡ªå·¥ä¸šåº”ç”¨åœºæ™¯ã€‚",
    "problem_difficulty_quote": "We construct the first instruction-following benchmark tailored for agentic coding scaffolds, featuring realistic, long-context, and complex constraint structures derived from industrial applications.",
    "language": "æ–‡ä¸­æœªæ˜ç¡®æŒ‡å®šä¸»è¦ç¼–ç¨‹è¯­è¨€ï¼Œä½†ä»»åŠ¡ç¯å¢ƒåŸºäºé€šç”¨ä»£ç ä»“åº“ï¼Œå¯èƒ½æ¶‰åŠå¤šç§è¯­è¨€ã€‚",
    "language_quote": NaN,
    "data_size": "åŒ…å«34ä¸ªç‹¬ç«‹ç¯å¢ƒï¼Œ217ä¸ªä»»åŠ¡å®ä¾‹ï¼Œè¦†ç›–3ç§è„šæ‰‹æ¶ç±»å‹ï¼Œå¹¶é…æœ‰7,098ä¸ªå®¢è§‚çš„æ£€æŸ¥æ¸…å•é¡¹ã€‚",
    "data_size_quote": "OCTOBENCH includes 34 environments and 217 tasks instantiated under three scaffold types, and is paired with 7,098 objective checklist items.",
    "source_type": "ä»å·¥ä¸šåº”ç”¨ä¸­æ”¶é›†çš„åŸå§‹æŒ‡ä»¤æ‰¿è½½ææ–™ï¼ˆå¦‚ä»“åº“æ–‡ä»¶ã€æŠ€èƒ½æ–‡æ¡£ã€å·¥å…·æ¨¡å¼ç­‰ï¼‰ï¼Œç”±äººå·¥æ ‡æ³¨è€…ç­–åˆ’å’Œæ‰©å±•ã€‚",
    "source_type_quote": "Starting from raw instruction-carrying materials, human annotators curate executable task and expend the curated queries... Annotators collect and normalize constraint-carrying artifacts from multiple sources and package them into a Docker image, including repository policy files, skill documentation, optional pre-seeded persistent state files, and other auxiliary materials required by the scaffold.",
    "last_updated": "2026-01-15 (æ ¹æ®arXivç‰ˆæœ¬v1æ—¥æœŸ)",
    "last_updated_quote": "arXiv:2601.10343v1  [cs.CL]  15 Jan 2026",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œç”±ç ”ç©¶å›¢é˜Ÿäººå·¥ç­–åˆ’ç§å­é›†ï¼Œå¹¶ä½¿ç”¨æ¨¡å‹æ‰©å±•ã€‚",
    "build_type_quote": "We curate the dataset in a seed-and-expand method. Annotators manually construct a seed set of 72 instances, then use a model to expand it to 217 instances.",
    "contamination_status": "æ–‡ä¸­æœªæ˜ç¡®è®¨è®ºæ•°æ®æ±¡æŸ“çŠ¶æ€ã€‚",
    "contamination_status_quote": NaN,
    "dataset_license": "æ–‡ä¸­æœªæåŠæ•°æ®é›†çš„è®¸å¯è¯ã€‚",
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç†å¼ç¼–ç ä»»åŠ¡ï¼Œæ¶‰åŠå¤šè½®äº¤äº’ã€å·¥å…·è°ƒç”¨å’Œä»£ç ä¿®æ”¹ã€‚",
    "task_granularity_quote": "Rather than relying on static QA pairs or outcome-only scores, OCTOBENCH targets long-horizon, multi-turn agent-environment interactions in repository-grounded coding tasks.",
    "evaluation_metrics": "åŸºäºæ£€æŸ¥æ¸…å•çš„äºŒè¿›åˆ¶æ£€æŸ¥é¡¹å¾—åˆ†ï¼Œç”¨äºè¿›è¡Œç»†ç²’åº¦çš„è¿‡ç¨‹çº§åˆè§„è¯„ä¼°ã€‚",
    "evaluation_metrics_quote": "These trajectories are then mapped to an instance-specific binary checklist that operationalizes verifiable constraints across all evidenced sources, and are scored via an LLM-as-a-judge to produce fine-grained metrics...",
    "input_modality": "å¼‚æ„æŒ‡ä»¤æ¥æºçš„ç»„åˆï¼ŒåŒ…æ‹¬ç³»ç»Ÿæç¤ºã€ç”¨æˆ·æŸ¥è¯¢åºåˆ—ã€ä»“åº“ç­–ç•¥æ–‡ä»¶ã€å·¥å…·æ¨¡å¼ã€å¯é€‰è®°å¿†çŠ¶æ€ç­‰ã€‚",
    "input_modality_quote": "Each instance packages a self-contained, executable task environment together with a curated task specification (e.g., system prompts, user query sequences, repository policy files, and optional memory state) designed to surface verifiable constraints from heterogeneous instruction sources.",
    "output_modality": "ä»£ç†åœ¨ç¯å¢ƒä¸­çš„å®Œæ•´åŠ¨ä½œè½¨è¿¹ï¼ŒåŒ…æ‹¬ä»£ç è„šæœ¬ã€å·¥å…·è°ƒç”¨å’Œäº¤äº’åé¦ˆã€‚",
    "output_modality_quote": "Accordingly, we pair each task with a granular observation harness and automated evaluation toolkit that captures and normalizes the agentâ€™s full action trajectory...",
    "task_io_type": "å¼‚æ„æŒ‡ä»¤åˆ°ä»£ç†åŠ¨ä½œè½¨è¿¹ã€‚",
    "task_io_type_quote": "Figure 1: Overview of OCTOBENCH. ...combining heterogeneous, persistent instruction sources with a scaffold that interacts with an executable environment, while an observation harness records trajectories.",
    "execution_environment": "è‡ªåŒ…å«çš„å¯æ‰§è¡Œç¼–ç ç¯å¢ƒï¼Œæ‰“åŒ…åœ¨Dockeré•œåƒä¸­ï¼ŒåŒ…å«ä»“åº“æ–‡ä»¶ã€æŠ€èƒ½æ–‡æ¡£ã€é¢„ç½®çŠ¶æ€æ–‡ä»¶ç­‰ã€‚",
    "execution_environment_quote": "We construct each instance around a self-contained coding environment that an agent can execute end-to-end... Annotators collect and normalize constraint-carrying artifacts from multiple sources and package them into a Docker image...",
    "unique_features": "1. é¦–ä¸ªé’ˆå¯¹ä»£ç†å¼ç¼–ç è„šæ‰‹æ¶çš„æŒ‡ä»¤éµå¾ªåŸºå‡†ã€‚2. å¼ºè°ƒå¼‚æ„ã€æŒä¹…æ€§çº¦æŸã€‚3. æä¾›è‡ªåŠ¨åŒ–è§‚å¯Ÿå·¥å…·åŒ…å’ŒåŸºäºæ£€æŸ¥æ¸…å•çš„ç»†ç²’åº¦è¯„ä¼°ï¼Œä»¥åŒºåˆ†â€œå®Œæˆä»»åŠ¡â€å’Œâ€œéµå¾ªè§„åˆ™â€ã€‚4. åŒ…å«ä¸€ä¸ªä¸“é—¨çš„å†²çªè¯„ä¼°å­é›†OCTOBENCH-CONFLICTã€‚",
    "unique_features_quote": "To address this gap, we introduce OCTOBENCH, a repository-grounded benchmark for measuring instruction following under realistic agentic coding scaffolds... This enables fine-grained, process-level compliance assessment, explicitly detecting when a model violates constraints during execution, even if the final outcome appears correct, and thereby disentangling solving the task from following the rules. ...we construct OCTOBENCH-CONFLICT, an evaluation set featuring three types of instruction conflicts.",
    "data_size_quantity": 7098,
    "data_size_unit": "ä¸ª",
    "last_updated_year": 2026,
    "last_updated_month": 1,
    "last_updated_day": 15,
    "language_normalized": "[]",
    "dimension_normalized": "['è„šæ‰‹æ¶æ„ŸçŸ¥çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›', 'å¼‚æ„çº¦æŸ', 'ä¼˜å…ˆçº§æ„ŸçŸ¥çš„å†²çªè§£å†³', 'è·¨è½®æ¬¡æŒä¹…æ€§éµå®ˆ']",
    "evaluation_method_normalized": "['åŸºäºæ£€æŸ¥æ¸…å•çš„äºŒè¿›åˆ¶æ£€æŸ¥é¡¹å¾—åˆ†', 'ç»†ç²’åº¦çš„è¿‡ç¨‹çº§åˆè§„è¯„ä¼°']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'ä»£ç†å¼ç¼–ç ', 'ä»£ç ç”Ÿæˆ', 'ä¿®æ”¹', 'å·¥å…·è°ƒç”¨', 'ä»“åº“å¯¼èˆª']",
    "source_type_normalized": "['å·¥ä¸šåº”ç”¨', 'åŸå§‹æŒ‡ä»¤æ‰¿è½½ææ–™', 'ä»“åº“æ–‡ä»¶', 'æŠ€èƒ½æ–‡æ¡£', 'å·¥å…·æ¨¡å¼', 'äººå·¥æ ‡æ³¨è€…ç­–åˆ’å’Œæ‰©å±•']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.10498_output/content.md",
    "benchmark_name": "GSM8K",
    "benchmark_name_quote": "Experiments used the GSM8K dataset [5] with the Qwen-3 0.6B model [11].",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Experiments used the GSM8K dataset [5] with the Qwen-3 0.6B model [11].",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "æ–‡ä¸­æœªæè¿°GSM8Kæ•°æ®é›†æœ¬èº«çš„ä»»åŠ¡ã€‚æœ¬æ–‡ä½¿ç”¨å®ƒæ¥è¯„ä¼°å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ–¹æ³•ï¼ˆPROMAï¼‰çš„æ€§èƒ½ã€‚",
    "task_description_quote": "Experiments used the GSM8K dataset [5] with the Qwen-3 0.6B model [11].",
    "dimension": "æ–‡ä¸­æœªæè¿°GSM8Kæ•°æ®é›†çš„è¯„æµ‹ç»´åº¦ã€‚æœ¬æ–‡å…³æ³¨çš„æ˜¯å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ–¹æ³•çš„ç¨³å®šæ€§ã€KLæ•£åº¦æ§åˆ¶å’Œç­–ç•¥ç†µã€‚",
    "dimension_quote": "PROMA achieves comparable or improved validation performance relative to GRPO (Figure 2a)... PROMA also maintains higher entropy for longer during training... PROMA has consistently lower KL divergence between successive policies than both baselines (Figure 2d)...",
    "evaluation_method": "æ–‡ä¸­æœªæè¿°GSM8Kæ•°æ®é›†åŸç”Ÿçš„è¯„ä¼°æ–¹æ³•ã€‚æœ¬æ–‡é€šè¿‡éªŒè¯é›†æ€§èƒ½ï¼ˆValidation performanceï¼‰ã€KLæ•£åº¦ã€ç­–ç•¥ç†µç­‰æŒ‡æ ‡æ¥è¯„ä¼°å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ–¹æ³•ã€‚",
    "evaluation_method_quote": "Figure 2 shows the performance and policy dynamics of PROMA compared to GRPO and REINFORCE. (a) Validation performance. (b) KL divergence from the initial policy. (c) Policy entropy. (d) KL divergence between the current policy and a lagged reference policy...",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": NaN,
    "problem_domain_quote": NaN,
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": NaN,
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": NaN,
    "task_granularity_quote": NaN,
    "evaluation_metrics": "éªŒè¯é›†æ€§èƒ½ã€KLæ•£åº¦ï¼ˆç›¸å¯¹äºåˆå§‹ç­–ç•¥å’Œæ»åç­–ç•¥ï¼‰ã€ç­–ç•¥ç†µ",
    "evaluation_metrics_quote": "Figure 2 shows the performance and policy dynamics of PROMA compared to GRPO and REINFORCE. (a) Validation performance. (b) KL divergence from the initial policy. (c) Policy entropy. (d) KL divergence between the current policy and a lagged reference policy...",
    "input_modality": NaN,
    "input_modality_quote": NaN,
    "output_modality": NaN,
    "output_modality_quote": NaN,
    "task_io_type": NaN,
    "task_io_type_quote": NaN,
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "æœ¬æ–‡æ˜¯æ–¹æ³•è®ºæ–‡ï¼Œæå‡ºäº†ä¸€ç§åä¸ºPROMAï¼ˆProjected Microbatch Accumulationï¼‰çš„å¼ºåŒ–å­¦ä¹ å¾®è°ƒæ–¹æ³•ï¼Œç”¨äºå¤§è¯­è¨€æ¨¡å‹ã€‚å®ƒé€šè¿‡æŠ•å½±å¾®æ‰¹æ¬¡æ¢¯åº¦æ¥å®ç°æ— å‚è€ƒç­–ç•¥çš„è¿‘ç«¯ç­–ç•¥æ›´æ–°ï¼Œæ—¨åœ¨æé«˜è®­ç»ƒç¨³å®šæ€§å¹¶é¿å…ç†µå´©æºƒã€‚",
    "unique_features_quote": "This note introduces Projected Microbatch Accumulation (PROMA), a proximal policy update method for large language model fine-tuning... Unlike PPO and GRPO, PROMA achieves proximal updates without inducing entropy collapse and does not rely on a reference policy or likelihood-ratio clipping.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "[]",
    "evaluation_method_normalized": "['éªŒè¯é›†æ€§èƒ½', 'KLæ•£åº¦ï¼ˆç›¸å¯¹äºåˆå§‹ç­–ç•¥å’Œæ»åç­–ç•¥ï¼‰', 'ç­–ç•¥ç†µ']",
    "problem_domain_normalized": "[]",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.11077_output/content.md",
    "benchmark_name": "ABC-Bench",
    "benchmark_name_quote": "To address this gap, we introduce ABC-Bench, a benchmark explicitly designed to evaluate agentic back-end coding within a realistic, executable workflow.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "To address this gap, we introduce ABC-Bench, a benchmark explicitly designed to evaluate agentic back-end coding within a realistic, executable workflow.",
    "dataset_url": "https://github.com/OpenMOSS/ABC-Bench, https://huggingface.co/datasets/OpenMOSS-Team/ABC-Bench",
    "dataset_url_quote": "Github: https://github.com/OpenMOSS/ABC-Bench\nDataset: https://huggingface.co/datasets/OpenMOSS-Team/ABC-Bench",
    "task_description": "è¯„ä¼°æ™ºèƒ½ä½“åœ¨çœŸå®ã€å¯æ‰§è¡Œçš„å·¥ä½œæµä¸­è¿›è¡Œåç«¯ç¼–ç çš„èƒ½åŠ›ï¼Œæ¶µç›–ä»ä»“åº“æ¢ç´¢åˆ°å®ä¾‹åŒ–å®¹å™¨åŒ–æœåŠ¡å¹¶é€šè¿‡ç«¯åˆ°ç«¯APIæµ‹è¯•çš„å®Œæ•´å¼€å‘ç”Ÿå‘½å‘¨æœŸã€‚",
    "task_description_quote": "ABC-Bench require the agents to manage the entire development lifecycle from repository exploration to instantiating containerized services and pass the external end-to-end API tests.",
    "dimension": "æ™ºèƒ½ä½“åç«¯ç¼–ç èƒ½åŠ›ï¼ŒåŒ…æ‹¬ä»“åº“æ¢ç´¢ã€ä»£ç ç¼–è¾‘ã€ç¯å¢ƒé…ç½®ã€éƒ¨ç½²å’Œç«¯åˆ°ç«¯æµ‹è¯•çš„å…¨æµç¨‹è¯„ä¼°ã€‚",
    "dimension_quote": "Real-world backend development mandates a continuous workflow spanning five distinct stages: (1) Repository Exploration (Expl.), (2) Code Editing (Code), (3) Environment Setup (Env.), (4) Deployment (Deploy), and (5) End-to-End Testing (E2E).",
    "evaluation_method": "é€šè¿‡å¤–éƒ¨APIçº§åˆ«çš„é›†æˆæµ‹è¯•æ¥ä¸¥æ ¼è¯„ä¼°æ­£ç¡®æ€§ï¼Œåªæœ‰å½“éƒ¨ç½²çš„æœåŠ¡æ­£ç¡®å¯åŠ¨å¹¶è¡¨ç°å‡ºé¢„æœŸè¡Œä¸ºæ—¶æ‰ç»™äºˆé€šè¿‡ã€‚",
    "evaluation_method_quote": "Once the service is launched, we evaluate correctness strictly via external API-level integration tests, awarding credit only when the deployed service starts correctly and exhibits the expected behavior.",
    "context_dependency": "å¤šæ–‡ä»¶é¡¹ç›®ã€ä»“åº“çº§åˆ«ï¼Œéœ€è¦æ¢ç´¢å’Œç†è§£æ•´ä¸ªä»£ç åº“ç»“æ„ã€‚",
    "context_dependency_quote": "The evolution of Large Language Models (LLMs) into autonomous agents has expanded the scope of AI coding from localized code generation to complex, repository-level, and execution-driven problem solving.",
    "problem_domain": "åç«¯å¼€å‘ï¼Œæ¶µç›–æ•°æ®åˆ†æã€æœç´¢ç³»ç»Ÿã€å•†åŠ¡å¹³å°ã€æ”¯ä»˜ç½‘å…³å’Œå¼€å‘è€…å·¥å…·ç­‰å¤šä¸ªçœŸå®ä¸–ç•Œé¢†åŸŸã€‚",
    "problem_domain_quote": "Beyond technical heterogeneity, the tasks are drawn from a broad spectrum of real-world domains, ensuring that the benchmark reflects practical engineering needs. These domains range from data analytics and search systems to commerce platforms, payment gateways, and developer tooling.",
    "problem_difficulty": "å·¥ç¨‹çº§ï¼Œæ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„ç”Ÿäº§ç¯å¢ƒå¤æ‚æ€§å’Œå…¨ç”Ÿå‘½å‘¨æœŸéœ€æ±‚ã€‚",
    "problem_difficulty_quote": "However, current benchmarks predominantly evaluate code logic in static contexts, neglecting the dynamic, full-process requirements of real-world engineering, particularly in back-end development which demands rigorous environment configuration and service deployment.",
    "language": "C#ã€Rustã€JavaScriptã€Pythonã€Javaã€Goã€PHPã€Rubyï¼Œå…±8ç§ç¼–ç¨‹è¯­è¨€ã€‚",
    "language_quote": "Applied to 2,000 open-source repositories, it yields 224 curated tasks spanning 8 backend programming languages and 19 frameworks, preserving the heterogeneity of real-world backend stacks.",
    "data_size": "åŒ…å«224ä¸ªä»»åŠ¡ã€‚",
    "data_size_quote": "ABC-Bench comprises 224 tasks, offering a diverse and balanced representation of modern backend ecosystems.",
    "source_type": "ä»2000ä¸ªå¼€æºã€MITè®¸å¯çš„ä»“åº“ä¸­ç­›é€‰å’Œæ„å»ºã€‚",
    "source_type_quote": "We initiate the process by filtering a pool of 2,000 open-source, MIT-licensed repositories to isolate high-quality backend candidates.",
    "last_updated": "2026",
    "last_updated_quote": "arXiv:2601.11077v1  [cs.SE]  16 Jan 2026",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œé€šè¿‡åä¸ºABC-Pipelineçš„è‡ªåŠ¨åŒ–å·¥ä½œæµç¨‹ä»å¼€æºä»“åº“ç”Ÿæˆã€‚",
    "build_type_quote": "To construct realistic backend development tasks at scale, we build the ABC-Pipeline, an automated workflow that converts open-source backend repositories into full-lifecycle development Tasks",
    "contamination_status": "æ–‡ä¸­æœªæ˜ç¡®æåŠã€‚",
    "contamination_status_quote": NaN,
    "dataset_license": "MITè®¸å¯è¯",
    "dataset_license_quote": "We initiate the process by filtering a pool of 2,000 open-source, MIT-licensed repositories to isolate high-quality backend candidates.",
    "task_granularity": "å…¨ç”Ÿå‘½å‘¨æœŸåç«¯å¼€å‘ä»»åŠ¡ï¼ŒåŒ…æ‹¬ä»£ç ä¿®å¤ã€ç¯å¢ƒé…ç½®ã€å®¹å™¨åŒ–éƒ¨ç½²ç­‰ã€‚",
    "task_granularity_quote": "Each task goes beyond localized code edits and requires the agent to configure the environment and instantiate a containerized service.",
    "evaluation_metrics": "pass@1ç‡",
    "evaluation_metrics_quote": "Even the top-performing model achieves only a 63.2% pass@1 rate",
    "input_modality": "è‡ªç„¶è¯­è¨€ä»»åŠ¡æŒ‡ä»¤ï¼Œä»¥åŠéœ€è¦æ¢ç´¢çš„ä»£ç ä»“åº“ã€‚",
    "input_modality_quote": "Within this workspace, the agent is granted full autonomy to explore the repository, modify code, install dependencies, and update Docker configurations",
    "output_modality": "ä»£ç ä¿®æ”¹ã€ç¯å¢ƒé…ç½®ï¼ˆå¦‚Dockerfileï¼‰ã€ä»¥åŠæœ€ç»ˆèƒ½é€šè¿‡APIæµ‹è¯•çš„å¯è¿è¡ŒæœåŠ¡ã€‚",
    "output_modality_quote": "Upon submission of the solution or when the interaction budget limits are reached, the evaluation system attempts to build and launch the backend service in a separate inner container using the agentâ€™s generated code and configurations.",
    "task_io_type": "æ–‡æœ¬ï¼ˆä»»åŠ¡æŒ‡ä»¤ï¼‰åˆ°å¯è¿è¡Œçš„æœåŠ¡ï¼ˆä»£ç +é…ç½®+éƒ¨ç½²ï¼‰ã€‚",
    "task_io_type_quote": "The evaluation setup launches an outer container that hosts the agent, delivers the task prompt.",
    "execution_environment": "æ ‡å‡†åŒ–çš„ã€éš”ç¦»çš„æ²™ç®±ç¯å¢ƒï¼Œä½¿ç”¨Dockerå®¹å™¨è¿›è¡Œæ„å»ºå’Œéƒ¨ç½²ã€‚",
    "execution_environment_quote": "We evaluate models and agents using a standardized, isolated sandbox environment, which strictly separates the agentâ€™s workspace from the backend service under test.",
    "unique_features": "1. ä¸“æ³¨äºè¯„ä¼°æ™ºèƒ½ä½“åç«¯ç¼–ç çš„å…¨ç”Ÿå‘½å‘¨æœŸèƒ½åŠ›ã€‚2. åŒ…å«ç¯å¢ƒé…ç½®å’Œå®¹å™¨åŒ–éƒ¨ç½²è¦æ±‚ã€‚3. ä½¿ç”¨ç«¯åˆ°ç«¯APIæµ‹è¯•è¿›è¡ŒéªŒè¯ï¼Œè€Œéå•å…ƒæµ‹è¯•ã€‚4. é€šè¿‡è‡ªåŠ¨åŒ–æµæ°´çº¿(ABC-Pipeline)ä»çœŸå®å¼€æºä»“åº“å¤§è§„æ¨¡æ„å»ºä»»åŠ¡ã€‚5. æ¶µç›–8ç§ç¼–ç¨‹è¯­è¨€å’Œ19ç§æ¡†æ¶ï¼ŒæŠ€æœ¯æ ˆå¤šæ ·ã€‚",
    "unique_features_quote": "Distinct from previous evaluations, ABC-Bench require the agents to manage the entire development lifecycle from repository exploration to instantiating containerized services and pass the external end-to-end API tests.",
    "data_size_quantity": 224,
    "data_size_unit": "ä¸ªä»»åŠ¡",
    "last_updated_year": 2026,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['C#', 'Rust', 'JavaScript', 'Python', 'Java', 'Go', 'PHP', 'Ruby']",
    "dimension_normalized": "['æ™ºèƒ½ä½“åç«¯ç¼–ç èƒ½åŠ›', 'ä»“åº“æ¢ç´¢', 'ä»£ç ç¼–è¾‘', 'ç¯å¢ƒé…ç½®', 'éƒ¨ç½²', 'ç«¯åˆ°ç«¯æµ‹è¯•', 'å…¨æµç¨‹è¯„ä¼°']",
    "evaluation_method_normalized": "['pass@1ç‡']",
    "problem_domain_normalized": "['åç«¯å¼€å‘', 'æ•°æ®åˆ†æ', 'æœç´¢ç³»ç»Ÿ', 'å•†åŠ¡å¹³å°', 'æ”¯ä»˜ç½‘å…³', 'å¼€å‘è€…å·¥å…·', 'çœŸå®ä¸–ç•Œé¢†åŸŸ']",
    "source_type_normalized": "['å¼€æº', 'MITè®¸å¯', 'ä»“åº“']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "MIT",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.10011_output/content.md",
    "benchmark_name": "BIRD",
    "benchmark_name_quote": "On BIRD, Memo-SQL achieves 68.5% execution accuracy, setting a new state of the art among open, zero-fine-tuning methods...",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Evaluated on the BIRD benchmark (Li et al., 2023a), Memo-SQL achieves 68.5% execution accuracy on the dev-new set...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "å°†è‡ªç„¶è¯­è¨€é—®é¢˜è½¬æ¢ä¸ºå¯æ‰§è¡Œçš„SQLæŸ¥è¯¢ï¼ˆNL2SQLï¼‰",
    "task_description_quote": "Given a natural language question q and a database schema S, the goal of NL2SQL is to generate an executable SQL query y.",
    "dimension": "SQLæŸ¥è¯¢ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆæ‰§è¡Œå‡†ç¡®æ€§ï¼‰",
    "dimension_quote": "On BIRD, Memo-SQL achieves 68.5% execution accuracy...",
    "evaluation_method": "æ‰§è¡Œå‡†ç¡®æ€§ï¼ˆexecution accuracyï¼‰",
    "evaluation_method_quote": "On BIRD, Memo-SQL achieves 68.5% execution accuracy...",
    "context_dependency": "ä¾èµ–æ•°æ®åº“æ¨¡å¼ï¼ˆschemaï¼‰",
    "context_dependency_quote": "Given a natural language question q and a database schema S, the goal of NL2SQL is to generate an executable SQL query y.",
    "problem_domain": "æ•°æ®åº“æŸ¥è¯¢ï¼ˆDatabase Queryingï¼‰ï¼Œå•†ä¸šæ™ºèƒ½ï¼ˆBIï¼‰",
    "problem_domain_quote": "As outlined in Section A.1 and Figure 1, real-world BI systems log both user feedback and system revisions...",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "SQL",
    "language_quote": "Given a natural language question q and a database schema S, the goal of NL2SQL is to generate an executable SQL query y.",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆä»è‡ªç„¶è¯­è¨€ç”ŸæˆSQLæŸ¥è¯¢ï¼‰",
    "task_granularity_quote": "Given a natural language question q and a database schema S, the goal of NL2SQL is to generate an executable SQL query y.",
    "evaluation_metrics": "æ‰§è¡Œå‡†ç¡®æ€§ï¼ˆexecution accuracyï¼‰",
    "evaluation_metrics_quote": "On BIRD, Memo-SQL achieves 68.5% execution accuracy...",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆé—®é¢˜ï¼‰å’Œæ•°æ®åº“æ¨¡å¼",
    "input_modality_quote": "Given a natural language question q and a database schema S, the goal of NL2SQL is to generate an executable SQL query y.",
    "output_modality": "ä»£ç ï¼ˆSQLæŸ¥è¯¢ï¼‰",
    "output_modality_quote": "Given a natural language question q and a database schema S, the goal of NL2SQL is to generate an executable SQL query y.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ï¼ˆè‡ªç„¶è¯­è¨€åˆ°SQLï¼‰",
    "task_io_type_quote": "Given a natural language question q and a database schema S, the goal of NL2SQL is to generate an executable SQL query y.",
    "execution_environment": "æ•°æ®åº“æ‰§è¡Œç¯å¢ƒ",
    "execution_environment_quote": "observes the execution result from the database",
    "unique_features": "ä¸“æ³¨äºç°å®ä¸–ç•Œã€å¤§è§„æ¨¡æ•°æ®åº“çš„NL2SQLä»»åŠ¡ï¼ŒåŒ…å«æœ‰å™ªå£°å’Œé¢†åŸŸç‰¹å®šçš„è¡¨è¿°ã€‚",
    "unique_features_quote": "As outlined in Section A.1 and Figure 1, real-world BI systems log both user feedback and system revisions...",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['SQL']",
    "dimension_normalized": "['SQLæŸ¥è¯¢ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆæ‰§è¡Œå‡†ç¡®æ€§ï¼‰']",
    "evaluation_method_normalized": "['æ‰§è¡Œå‡†ç¡®æ€§ï¼ˆexecution accuracyï¼‰']",
    "problem_domain_normalized": "['æ•°æ®åº“æŸ¥è¯¢ï¼ˆDatabase Queryingï¼‰', 'å•†ä¸šæ™ºèƒ½ï¼ˆBIï¼‰']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.10955_output/content.md",
    "benchmark_name": "ToolBench",
    "benchmark_name_quote": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens...",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„æµ‹åŸºå‡†æ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†ä½¿ç”¨å¤–éƒ¨å·¥å…·æ‰§è¡Œå¤šæ­¥ä»»åŠ¡çš„èƒ½åŠ›ã€‚",
    "task_description_quote": "LLM agents can interact with external tools and execute multi-step tasks across various domains.",
    "dimension": "å·¥å…·è°ƒç”¨èƒ½åŠ›ã€å¤šæ­¥ä»»åŠ¡æ‰§è¡Œã€èµ„æºæ¶ˆè€—ï¼ˆç»æµä¸è®¡ç®—æˆæœ¬ï¼‰",
    "dimension_quote": "These results elevate the agent-tool interface to a first-class security frontier, demanding a paradigm shift from validating final answers to monitoring the economic and computational cost of the entire agentic process.",
    "evaluation_method": "é€šè¿‡æ”»å‡»æ–¹æ³•è¯±å¯¼ä»£ç†è¿›è¡Œå¤šè½®ã€å†—é•¿çš„å·¥å…·è°ƒç”¨åºåˆ—ï¼Œå¹¶æµ‹é‡ç”±æ­¤äº§ç”Ÿçš„èµ„æºæ¶ˆè€—ï¼ˆå¦‚æ€»è¾“å‡ºä»¤ç‰Œæ•°ã€æˆæœ¬è†¨èƒ€å€æ•°ã€èƒ½è€—å¢åŠ ã€GPU KVç¼“å­˜å ç”¨ç‡ã€ç³»ç»Ÿååé‡ä¸‹é™ï¼‰ã€‚",
    "evaluation_method_quote": "Our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658Ã—, and raises energy by 100â€“560Ã—. It drives GPU KV cache occupancy from <1% to 35â€“74% and cuts co-running throughput by approximately 50%.",
    "context_dependency": "å¤šè½®ä»£ç†-å·¥å…·äº¤äº’ï¼Œå…·æœ‰çŠ¶æ€æ€§ã€‚",
    "context_dependency_quote": "To overcome the limitations of single-turn attacks, our methodology is designed to exploit the multi-turn, stateful nature of agent-tool interactions.",
    "problem_domain": "é€šç”¨å·¥å…·è°ƒç”¨ä»»åŠ¡ï¼ˆå¦‚æ—¥æœŸ/æ—¶é—´ã€è´§å¸è½¬æ¢ã€æœç´¢ã€æ•°æ®è½¬æ¢ç­‰ï¼‰ã€‚",
    "problem_domain_quote": "In modern automated agents, such tool calls are frequent for routine capabilities (date/time, currency conversion, search, data transforms, etc.)",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": NaN,
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "å¤šæ­¥ä»»åŠ¡æ‰§è¡Œä¸å·¥å…·è°ƒç”¨",
    "task_granularity_quote": "LLM agents can interact with external tools and execute multi-step tasks across various domains.",
    "evaluation_metrics": "æ€»è¾“å‡ºä»¤ç‰Œæ•°ã€æˆæœ¬è†¨èƒ€å€æ•°ã€èƒ½è€—å€æ•°ã€GPU KVç¼“å­˜å ç”¨ç‡ã€ç³»ç»Ÿååé‡ä¸‹é™ç™¾åˆ†æ¯”",
    "evaluation_metrics_quote": "Our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658Ã—, and raises energy by 100â€“560Ã—. It drives GPU KV cache occupancy from <1% to 35â€“74% and cuts co-running throughput by approximately 50%.",
    "input_modality": "è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼ˆç”¨æˆ·ä»»åŠ¡ï¼‰",
    "input_modality_quote": "Let ğ‘âˆˆX denote a user query issued to the agent stack.",
    "output_modality": "ä»£ç ï¼ˆå·¥å…·è°ƒç”¨ï¼‰ä¸è‡ªç„¶è¯­è¨€ï¼ˆæœ€ç»ˆç­”æ¡ˆï¼‰",
    "output_modality_quote": "The agent policy ğ´parses the LLM outputs from ğ‘€, routes tool invocations, handles retries/repairs, and decides when to stop.",
    "task_io_type": "è‡ªç„¶è¯­è¨€åˆ°å·¥å…·è°ƒç”¨åºåˆ—åŠæœ€ç»ˆç­”æ¡ˆ",
    "task_io_type_quote": "An agentâ€™s workflow is a trajectory ğœ= {(ğ‘ğ‘¡,ğ‘Ÿğ‘¡)}ğ‘› ğ‘¡=1, a sequence of tool calls (ğ‘ğ‘¡) and tool responses (ğ‘Ÿğ‘¡).",
    "execution_environment": "ä¸éµå¾ªæ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰çš„å¤–éƒ¨å·¥å…·æœåŠ¡å™¨äº¤äº’çš„ç¯å¢ƒã€‚",
    "execution_environment_quote": "The agent interacts with an external MCP server ğ‘‡ğœƒ, whose behavior is controlled by a configuration template ğœƒ. The MCP server exposes a set of tool functions F and communicates strictly via the MCP protocol...",
    "unique_features": "æœ¬æ–‡å¹¶éæå‡ºæ–°çš„è¯„æµ‹åŸºå‡†ï¼Œè€Œæ˜¯åˆ©ç”¨ç°æœ‰çš„ToolBenchå’ŒBFCLåŸºå‡†æ¥æ¼”ç¤ºä¸€ç§æ–°å‹çš„ç»æµæ€§æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ã€‚è¯¥æ”»å‡»é€šè¿‡æ“çºµå·¥å…·æœåŠ¡å™¨çš„å“åº”ï¼Œè¯±å¯¼LLMä»£ç†è¿›è¡Œå†—é•¿çš„å¤šè½®å·¥å…·è°ƒç”¨ï¼Œä»è€Œåœ¨ä¿æŒä»»åŠ¡ç»“æœæ­£ç¡®çš„å‰æä¸‹ï¼Œæå¤§åœ°æ”¾å¤§èµ„æºæ¶ˆè€—ã€‚è¿™æ­ç¤ºäº†ä»£ç†-å·¥å…·äº¤äº’å±‚ä½œä¸ºä¸€ä¸ªå…³é”®å®‰å…¨æ”»å‡»é¢çš„é‡è¦æ€§ã€‚",
    "unique_features_quote": "We introduce a stealthy, multi-turn economic DoS attack that operates at the tool layer under the guise of a correctly completed task. ... These results elevate the agent-tool interface to a first-class security frontier...",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['å·¥å…·è°ƒç”¨èƒ½åŠ›', 'å¤šæ­¥ä»»åŠ¡æ‰§è¡Œ', 'èµ„æºæ¶ˆè€—ï¼ˆç»æµä¸è®¡ç®—æˆæœ¬ï¼‰']",
    "evaluation_method_normalized": "['æ€»è¾“å‡ºä»¤ç‰Œæ•°', 'æˆæœ¬è†¨èƒ€å€æ•°', 'èƒ½è€—å€æ•°', 'GPU KVç¼“å­˜å ç”¨ç‡', 'ç³»ç»Ÿååé‡ä¸‹é™ç™¾åˆ†æ¯”']",
    "problem_domain_normalized": "['é€šç”¨å·¥å…·è°ƒç”¨ä»»åŠ¡ï¼ˆå¦‚æ—¥æœŸ/æ—¶é—´ã€è´§å¸è½¬æ¢ã€æœç´¢ã€æ•°æ®è½¬æ¢ç­‰ï¼‰']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.10820_output/content.md",
    "benchmark_name": "æœªå‘½åï¼ˆå†…éƒ¨æ•°æ®é›†ï¼‰",
    "benchmark_name_quote": "On a novel in-house dataset, our approach achieves 38% and 150% improvement in the evaluation metric over manually crafted and unplanned workflows respectively.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We devise a first-of-its-kind, PySpark-based, multi-turn, repository-level benchmarking dataset that faithfully mirrors real-world production ML feature engineering pipelines.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„æµ‹åŸºå‡†æ—¨åœ¨è¯„ä¼°å¤šæ™ºèƒ½ä½“æ¡†æ¶åœ¨çœŸå®å·¥ä¸šç¯å¢ƒä¸­è¿›è¡Œæœºå™¨å­¦ä¹ ç‰¹å¾å·¥ç¨‹çš„èƒ½åŠ›ï¼Œå…·ä½“ä»»åŠ¡åŒ…æ‹¬ç”ŸæˆPySparkè„šæœ¬ã€å•å…ƒæµ‹è¯•å’Œé…ç½®æ–‡ä»¶ï¼Œç”¨äºæ„å»ºç”µå­å•†åŠ¡ç”¨æˆ·-æ¨èç®¡é“ä¸­çš„ç‰¹å¾ã€‚",
    "task_description_quote": "We introduce a newly developed benchmark dataset that closely mirrors real-world industrial environments for machine learning featurization, specifically leveraging PySpark for large-scale data processing. It comprises 10 tasks that involve generating PySpark scripts, unit tests, and configuration files, each focused on constructing features for user-offer recommendation pipelines in e-commerce environments.",
    "dimension": "å¤šæ­¥ã€ä»“åº“çº§ä»£ç ç”Ÿæˆçš„å¯é æ€§ä¸æ­£ç¡®æ€§ï¼Œä»¥åŠæ™ºèƒ½ä½“åœ¨å¤æ‚å·¥ä½œæµä¸­çš„åè°ƒèƒ½åŠ›ã€‚",
    "dimension_quote": "We devise a first-of-its-kind, PySpark-based, multi-turn, repository-level benchmarking dataset that faithfully mirrors real-world production ML feature engineering pipelines.",
    "evaluation_method": "ä½¿ç”¨ pass@3 æŒ‡æ ‡ï¼Œå³ä¸‰æ¬¡è¿è¡Œä¸­æˆåŠŸæ¬¡æ•°çš„æ¯”ä¾‹ã€‚",
    "evaluation_method_quote": "Our primary metric is pass@3, the fraction of successful runs out of three.",
    "context_dependency": "ä»“åº“çº§ä»£ç ç”Ÿæˆï¼Œä¾èµ–ä»£ç åº“READMEã€å¯é‡ç”¨å·¥å…·ã€é…ç½®æ–‡ä»¶ï¼ˆFSC, DFRï¼‰å’Œå›¢é˜Ÿç‰¹å®šçš„å·¥ä½œæµã€‚",
    "context_dependency_quote": "Each task is described using three inputs: (i) FSC (Feature Specification Config), a YAML file specifying target features, base columns and their datasets, and computation logic; (ii) DFR (DataFrame Registry), a YAML file describing base datasets and dependencies; and (iii) a run file specifying the codebase README, reusable utilities, and working repository.",
    "problem_domain": "æœºå™¨å­¦ä¹ ç‰¹å¾å·¥ç¨‹ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å¤§è§„æ¨¡æ•°æ®å¤„ç†çš„ç”µå­å•†åŠ¡æ¨èç³»ç»Ÿã€‚",
    "problem_domain_quote": "It comprises 10 tasks that involve generating PySpark scripts, unit tests, and configuration files, each focused on constructing features for user-offer recommendation pipelines in e-commerce environments.",
    "problem_difficulty": "ç”Ÿäº§çº§å¤æ‚åº¦ï¼Œæ¨¡æ‹ŸçœŸå®å·¥ä¸šç¯å¢ƒä¸­çš„å¤æ‚å·¥ä½œæµã€‚",
    "problem_difficulty_quote": "Unlike community datasets such as SWE-bench [9] or MLE-bench [2], which primarily utilize Pandas, our dataset is tailored to the complexities of production-scale workflows.",
    "language": "ä¸»è¦æ¶‰åŠ PySparkï¼ˆç”¨äºå¤§è§„æ¨¡æ•°æ®å¤„ç†ï¼‰ã€‚",
    "language_quote": "We devise a first-of-its-kind, PySpark-based, multi-turn, repository-level benchmarking dataset...",
    "data_size": "åŒ…å«10ä¸ªä»»åŠ¡ï¼Œå¤–åŠ ä¸€ä¸ªç‹¬ç«‹çš„ç¬¬0ä¸ªä»»åŠ¡ç”¨äºæ–¹æ³•å¼€å‘ï¼ˆä¸å‚ä¸è¯„æµ‹ï¼‰ã€‚",
    "data_size_quote": "It comprises 10 tasks that involve generating PySpark scripts, unit tests, and configuration files... Additionally, a separate held-out (0th) task is used exclusively for developing our proposed approach and is not included in benchmarking.",
    "source_type": "å†…éƒ¨æ„å»ºï¼Œæ—¨åœ¨ç´§å¯†æ¨¡æ‹ŸçœŸå®å·¥ä¸šç¯å¢ƒã€‚",
    "source_type_quote": "We introduce a newly developed benchmark dataset that closely mirrors real-world industrial environments for machine learning featurization...",
    "last_updated": "2026ï¼ˆæ ¹æ®é¢„å°æœ¬æ—¥æœŸæ¨æ–­ï¼‰",
    "last_updated_quote": "arXiv:2601.10820v1  [cs.LG]  15 Jan 2026",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆå†…éƒ¨æ•°æ®é›†ï¼‰",
    "build_type_quote": "We devise a first-of-its-kind... benchmarking dataset...",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»“åº“çº§ä»£ç ç”Ÿæˆï¼Œæ¶‰åŠç”Ÿæˆå¤šä¸ªæ–‡ä»¶ï¼ˆè„šæœ¬ã€æµ‹è¯•ã€é…ç½®ï¼‰ã€‚",
    "task_granularity_quote": "It comprises 10 tasks that involve generating PySpark scripts, unit tests, and configuration files...",
    "evaluation_metrics": "pass@3",
    "evaluation_metrics_quote": "Our primary metric is pass@3, the fraction of successful runs out of three.",
    "input_modality": "é…ç½®æ–‡ä»¶ï¼ˆYAMLï¼‰ã€æ•°æ®æ³¨å†Œè¡¨ã€ä»£ç åº“æ–‡æ¡£å’Œå·¥å…·ã€‚",
    "input_modality_quote": "Each task is described using three inputs: (i) FSC (Feature Specification Config), a YAML file... (ii) DFR (DataFrame Registry), a YAML file... (iii) a run file specifying the codebase README, reusable utilities...",
    "output_modality": "ä»£ç ï¼ˆPySparkè„šæœ¬ï¼‰ã€é…ç½®æ–‡ä»¶ã€å•å…ƒæµ‹è¯•æ–‡ä»¶ã€‚",
    "output_modality_quote": "It comprises 10 tasks that involve generating PySpark scripts, unit tests, and configuration files...",
    "task_io_type": "é…ç½®ä¸æ–‡æ¡£åˆ°ä»£ç ",
    "task_io_type_quote": "Each task is described using three inputs... involve generating PySpark scripts, unit tests, and configuration files...",
    "execution_environment": "PySparkåˆ†å¸ƒå¼è¿è¡Œæ—¶ç¯å¢ƒã€‚",
    "execution_environment_quote": "specifically leveraging PySpark for large-scale data processing.",
    "unique_features": "é¦–ä¸ªä¸“æ³¨äºPySparkã€å¤šè½®æ¬¡ã€ä»“åº“çº§çš„åŸºå‡†æ•°æ®é›†ï¼Œä¸“é—¨é’ˆå¯¹ç”Ÿäº§çº§æœºå™¨å­¦ä¹ ç‰¹å¾å·¥ç¨‹ç®¡é“ï¼Œæ¨¡æ‹ŸçœŸå®å·¥ä¸šç¯å¢ƒçš„å¤æ‚æ€§ï¼ˆå¦‚å¤„ç†å»¶è¿Ÿçš„Sparkè¾“å‡ºã€æ•°æ®åˆ†å¸ƒçŸ¥è¯†ã€ç‰¹å¾å®Œæ•´æ€§æ£€æŸ¥ï¼‰ã€‚ä¸ä¸»è¦ä½¿ç”¨Pandasçš„ç¤¾åŒºæ•°æ®é›†ï¼ˆå¦‚SWE-bench, MLE-benchï¼‰å½¢æˆå¯¹æ¯”ã€‚",
    "unique_features_quote": "We devise a first-of-its-kind, PySpark-based, multi-turn, repository-level benchmarking dataset that faithfully mirrors real-world production ML feature engineering pipelines. Unlike community datasets such as SWE-bench [9] or MLE-bench [2], which primarily utilize Pandas, our dataset is tailored to the complexities of production-scale workflows.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2026,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['PySpark']",
    "dimension_normalized": "['å¤šæ­¥', 'ä»“åº“çº§ä»£ç ç”Ÿæˆçš„å¯é æ€§ä¸æ­£ç¡®æ€§', 'æ™ºèƒ½ä½“åœ¨å¤æ‚å·¥ä½œæµä¸­çš„åè°ƒèƒ½åŠ›']",
    "evaluation_method_normalized": "['pass@3']",
    "problem_domain_normalized": "['æœºå™¨å­¦ä¹ ç‰¹å¾å·¥ç¨‹', 'å¤§è§„æ¨¡æ•°æ®å¤„ç†çš„ç”µå­å•†åŠ¡æ¨èç³»ç»Ÿ']",
    "source_type_normalized": "['å†…éƒ¨æ„å»º']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.11960_output/content.md",
    "benchmark_name": "MATH-500",
    "benchmark_name_quote": "achieving average accuracy gains of 3.1% on MATH-500 and 2.4% on APPS",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Experiments across multiple benchmarks show that R2PO consistently outperforms baselines, achieving average accuracy gains of 3.1% on MATH-500 and 2.4% on APPS",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "æ•°å­¦é—®é¢˜æ±‚è§£",
    "task_description_quote": "mathematical problem solving",
    "dimension": "æ•°å­¦æ¨ç†èƒ½åŠ›",
    "dimension_quote": "mathematical reasoning",
    "evaluation_method": "å‡†ç¡®ç‡ (accuracy)",
    "evaluation_method_quote": "achieving average accuracy gains of 3.1% on MATH-500",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "æ•°å­¦",
    "problem_domain_quote": "mathematical reasoning",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": NaN,
    "language_quote": NaN,
    "data_size": "500ä¸ªé—®é¢˜",
    "data_size_quote": "MATH-500",
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "æ•°å­¦é—®é¢˜æ±‚è§£",
    "task_granularity_quote": "mathematical problem solving",
    "evaluation_metrics": "å‡†ç¡®ç‡",
    "evaluation_metrics_quote": "achieving average accuracy gains of 3.1% on MATH-500",
    "input_modality": NaN,
    "input_modality_quote": NaN,
    "output_modality": NaN,
    "output_modality_quote": NaN,
    "task_io_type": NaN,
    "task_io_type_quote": NaN,
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": NaN,
    "unique_features_quote": NaN,
    "data_size_quantity": 500,
    "data_size_unit": "ä¸ªé—®é¢˜",
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['æ•°å­¦æ¨ç†èƒ½åŠ›']",
    "evaluation_method_normalized": "['å‡†ç¡®ç‡']",
    "problem_domain_normalized": "['æ•°å­¦']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.13864_output/content.md",
    "benchmark_name": "HardSecBench",
    "benchmark_name_quote": "In this work, we introduce HardSecBench, a benchmark with 924 tasks spanning Verilog Register Transfer Level (RTL) and firmware-level C, covering 76 hardware-relevant Common Weakness Enumeration (CWE) entries.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "In this work, we introduce HardSecBench... This critical research gap motivates us to design a benchmark for assessing security awareness under realistic specifications.",
    "dataset_url": NaN,
    "dataset_url_quote": "Our data and code will be released soon.",
    "task_description": "è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹åœ¨ç¡¬ä»¶ä»£ç ç”Ÿæˆä¸­çš„å®‰å…¨æ„è¯†ã€‚å…·ä½“ä»»åŠ¡æ˜¯æ ¹æ®åŠŸèƒ½éœ€æ±‚ç”Ÿæˆç¡¬ä»¶è®¾è®¡ä»£ç ï¼ˆVerilog RTLæˆ–Cè¯­è¨€å›ºä»¶ï¼‰ï¼ŒåŒæ—¶è¦æ±‚ç”Ÿæˆçš„ä»£ç æ»¡è¶³éšå«çš„å®‰å…¨è¦æ±‚ï¼Œé¿å…ç‰¹å®šçš„ç¡¬ä»¶å®‰å…¨æ¼æ´ã€‚",
    "task_description_quote": "a benchmark for assessing security awareness under realistic specifications... each task includes a structured specification that separates functional and security requirements... evaluates whether models implement protections checked by the security requirements.",
    "dimension": "ç¡¬ä»¶ä»£ç ç”Ÿæˆçš„å®‰å…¨æ€§æ„è¯†",
    "dimension_quote": "assessing security awareness... evaluate the systematic security of LLM-based hardware designs",
    "evaluation_method": "é€šè¿‡æ¨¡æ‹Ÿæˆ–æ‰§è¡Œé’ˆå¯¹æ€§çš„æµ‹è¯•å·¥å…·æ¥éªŒè¯å®‰å…¨è¦æ±‚ã€‚æ¯ä¸ªä»»åŠ¡éƒ½æœ‰ç‹¬ç«‹çš„æµ‹è¯•å·¥å…·ï¼Œè¿™äº›å·¥å…·ä¼šä¸»åŠ¨è§¦å‘ä¸å®‰å…¨ç›¸å…³çš„è¡Œä¸ºï¼Œå¹¶æ ¹æ®æ¨¡æ‹Ÿè¯æ®ç»™å‡ºPASS/FAILçš„ç¡®å®šæ€§ç»“æœã€‚",
    "evaluation_method_quote": "HardSecBench avoids subjective judging and scores security using simulation evidence from targeted harnesses that actively trigger security relevant behaviors... Each harness is self-contained and takes the form of a testbench for RTL tasks or a standalone C driver for firmware tasks. The test harness emits a standardized PASS/FAIL trace so that verification can parse results deterministically.",
    "context_dependency": "æ¨¡å—çº§è®¾è®¡ã€‚æ¯ä¸ªä»»åŠ¡åŒ…å«ä¸€ä¸ªç»“æ„åŒ–çš„é—®é¢˜æè¿°ã€I/Oæ¥å£è§„èŒƒä»¥åŠåŠŸèƒ½å’Œå®‰å…¨éœ€æ±‚é›†ã€‚",
    "context_dependency_quote": "each task includes a structured specification... including a problem statement, an I/O interface specification, and two requirement sets: functional requirements Rf_i and security requirements Rs_i.",
    "problem_domain": "ç¡¬ä»¶è®¾è®¡ä¸å›ºä»¶å¼€å‘ï¼Œå…·ä½“æ¶‰åŠVerilog RTLè®¾è®¡å’ŒCè¯­è¨€å›ºä»¶å¼€å‘ã€‚",
    "problem_domain_quote": "hardware and firmware development... spanning Verilog Register Transfer Level (RTL) and firmware-level C",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "Verilogï¼ˆç”¨äºRTLè®¾è®¡ï¼‰å’ŒCï¼ˆç”¨äºå›ºä»¶å¼€å‘ï¼‰",
    "language_quote": "spanning Verilog Register Transfer Level (RTL) and firmware-level C",
    "data_size": "åŒ…å«924ä¸ªä»»åŠ¡",
    "data_size_quote": "a benchmark with 924 tasks",
    "source_type": "é€šè¿‡ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ„å»ºæµç¨‹è‡ªåŠ¨åˆæˆã€‚æµç¨‹ä»CWEï¼ˆé€šç”¨ç¼ºé™·æšä¸¾ï¼‰å®šä¹‰å‡ºå‘ï¼Œç”Ÿæˆä»»åŠ¡ç§å­ï¼Œç„¶åç”±æ™ºèƒ½ä½“æ‰©å±•ä¸ºå®Œæ•´çš„è§„èŒƒã€å‚è€ƒå®ç°å’Œå¯æ‰§è¡Œçš„æµ‹è¯•å·¥å…·ã€‚",
    "source_type_quote": "We develop a multi-agent construction pipeline that synthesizes benchmark samples... Starting from CWE-derived seeds, the Architect produces Pi... The Expert synthesizes the golden implementation... the Tester derives atomic harnesses...",
    "last_updated": "2026-01-20 (æ ¹æ®arXivç‰ˆæœ¬v1æ—¥æœŸ)",
    "last_updated_quote": "arXiv:2601.13864v1  [cs.CR]  20 Jan 2026",
    "build_type": "å®˜æ–¹è‡ªå»ºã€‚è®ºæ–‡ä½œè€…å›¢é˜Ÿé€šè¿‡æå‡ºçš„è‡ªåŠ¨åŒ–æµç¨‹æ„å»ºã€‚",
    "build_type_quote": "we design an automated pipeline that scales benchmark construction... Using this pipeline, we build HardSecBench",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆã€‚æ ¹æ®åŠŸèƒ½éœ€æ±‚ç”Ÿæˆå®Œæ•´çš„ç¡¬ä»¶æ¨¡å—æˆ–å›ºä»¶ä»£ç ã€‚",
    "task_granularity_quote": "hardware code generation... The Expert synthesizes the golden implementation to satisfy both Rf_i and Rs_i",
    "evaluation_metrics": "åŸºäºæ¨¡æ‹Ÿè¯æ®çš„å®‰å…¨æµ‹è¯•é€šè¿‡/å¤±è´¥ï¼ˆPASS/FAILï¼‰ã€‚",
    "evaluation_metrics_quote": "scores security using simulation evidence... The test harness emits a standardized PASS/FAIL trace",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆç»“æ„åŒ–è§„èŒƒï¼ŒåŒ…å«é—®é¢˜æè¿°ã€I/Oæ¥å£å’ŒåŠŸèƒ½éœ€æ±‚ï¼‰ã€‚å®‰å…¨éœ€æ±‚åœ¨è¯„ä¼°æ—¶å¯¹æ¨¡å‹éšè—ã€‚",
    "input_modality_quote": "a structured specification Pi for task i, including a problem statement, an I/O interface specification, and two requirement sets: functional requirements Rf_i and security requirements Rs_i... during evaluation, we give the target model without Rs_i",
    "output_modality": "ä»£ç ï¼ˆVerilog RTLæˆ–Cè¯­è¨€ï¼‰ã€‚",
    "output_modality_quote": "Verilog Register Transfer Level (RTL) and firmware-level C",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ã€‚è¾“å…¥æ˜¯è‡ªç„¶è¯­è¨€è§„èŒƒï¼Œè¾“å‡ºæ˜¯ç¡¬ä»¶è®¾è®¡ä»£ç ã€‚",
    "task_io_type_quote": "synthesizing the golden implementation from the specification Pi",
    "execution_environment": "é’ˆå¯¹ç¡¬ä»¶è®¾è®¡çš„æ¨¡æ‹Ÿç¯å¢ƒï¼ˆå¦‚RTLä»¿çœŸå™¨ï¼‰å’Œé’ˆå¯¹Cå›ºä»¶çš„æ‰§è¡Œç¯å¢ƒã€‚",
    "execution_environment_quote": "verify by simulation... testbench for RTL tasks or a standalone C driver for firmware tasks",
    "unique_features": "1. ä¸“æ³¨äºç¡¬ä»¶ä»£ç ç”Ÿæˆçš„å®‰å…¨æ€§è¯„ä¼°ï¼Œå¡«è¡¥äº†ç°æœ‰åŸºå‡†çš„ç©ºç™½ã€‚2. è¦†ç›–76ç§ç¡¬ä»¶ç›¸å…³çš„CWEæ¼æ´ç±»å‹ã€‚3. é‡‡ç”¨å¤šæ™ºèƒ½ä½“è‡ªåŠ¨åŒ–æ„å»ºæµç¨‹ï¼Œå°†è§„èŒƒã€å®ç°å’ŒéªŒè¯è§£è€¦ï¼Œç¡®ä¿è¯„ä¼°çš„å®¢è§‚æ€§å’Œå¯é æ€§ã€‚4. è§„èŒƒæ˜ç¡®åŒºåˆ†åŠŸèƒ½éœ€æ±‚å’Œå®‰å…¨éœ€æ±‚ï¼Œåœ¨è¯„ä¼°æ—¶ä»…å‘æ¨¡å‹æä¾›åŠŸèƒ½éœ€æ±‚ï¼Œä»¥æµ‹è¯•å…¶éšå«çš„å®‰å…¨æ„è¯†ã€‚",
    "unique_features_quote": "Existing studies have primarily focused on evaluating the functional correctness of LLM-generated code, yet paid limited attention to its security issues... covers 76 hardware-relevant Common Weakness Enumeration (CWE) entries... a multi-agent pipeline that decouples synthesis from verification... separates functional requirements Rf_i from security requirements Rs_i, so that the functional specification can be used to elicit implementations without revealing security intent",
    "data_size_quantity": 924,
    "data_size_unit": "ä¸ªä»»åŠ¡",
    "last_updated_year": 2026,
    "last_updated_month": 1,
    "last_updated_day": 20,
    "language_normalized": "['Verilog', 'C']",
    "dimension_normalized": "['ç¡¬ä»¶ä»£ç ç”Ÿæˆçš„å®‰å…¨æ€§æ„è¯†']",
    "evaluation_method_normalized": "['åŸºäºæ¨¡æ‹Ÿè¯æ®çš„å®‰å…¨æµ‹è¯•é€šè¿‡/å¤±è´¥']",
    "problem_domain_normalized": "['ç¡¬ä»¶è®¾è®¡ä¸å›ºä»¶å¼€å‘', 'Verilog RTLè®¾è®¡', 'Cè¯­è¨€å›ºä»¶å¼€å‘']",
    "source_type_normalized": "['é€šè¿‡ä¸€ä¸ªå¤šæ™ºèƒ½ä½“æ„å»ºæµç¨‹è‡ªåŠ¨åˆæˆ']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å•æ–‡ä»¶",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.13943_output/content.md",
    "benchmark_name": "RepoGenesis",
    "benchmark_name_quote": "To bridge this gap, we introduce RepoGenesis, the first multilingual benchmark for repository-level end-to-end web microservice generation, comprising 106 repositories (60 Python, 46 Java) across 18 domains and 11 frameworks...",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "To bridge this gap, we introduce RepoGenesis, the first multilingual benchmark for repository-level web microservice generation.",
    "dataset_url": "https://github.com/pzy2000/RepoGenesis/",
    "dataset_url_quote": "We release our benchmark at https://github.com/pzy2000/RepoGenesis/.",
    "task_description": "ä»è‡ªç„¶è¯­è¨€éœ€æ±‚æ–‡æ¡£ï¼ˆREADME.mdï¼‰ç”Ÿæˆå®Œæ•´çš„ã€å¯éƒ¨ç½²çš„å¾®æœåŠ¡ä»“åº“ï¼ŒåŒ…æ‹¬æºä»£ç ã€é…ç½®æ–‡ä»¶å’Œä¾èµ–è§„èŒƒã€‚",
    "task_description_quote": "Given a requirement document (README.md) specifying service functionality, API endpoints with input/output schemas, authentication mechanisms, and operational constraints, models must generate a fully deployable repository including all source code, configuration files, and dependency specifications.",
    "dimension": "æ¶æ„è®¾è®¡ã€ä¾èµ–ç®¡ç†ã€ç³»ç»Ÿçº§ä¸€è‡´æ€§ã€åŠŸèƒ½æ­£ç¡®æ€§ã€APIå®ç°å®Œæ•´æ€§ã€å¯éƒ¨ç½²æ€§",
    "dimension_quote": "RepoGenesis targets critical yet under-evaluated capabilities essential for real-world engineering: architectural design, dependency management, and system-level consistency.",
    "evaluation_method": "é»‘ç›’æµ‹è¯•ï¼ŒåŒ…æ‹¬éƒ¨ç½²å¾®æœåŠ¡ã€æ‰§è¡Œæµ‹è¯•ç”¨ä¾‹å¹¶è®¡ç®—æŒ‡æ ‡ã€‚",
    "evaluation_method_quote": "We evaluate generated repositories using black-box testing: deploying the microservice, executing test cases, and computing metrics.",
    "context_dependency": "ä»“åº“çº§åˆ«ï¼Œéœ€è¦ç”ŸæˆåŒ…å«å¤šä¸ªæ–‡ä»¶ã€é…ç½®å’Œä¾èµ–çš„å®Œæ•´é¡¹ç›®ã€‚",
    "context_dependency_quote": "the first benchmark specifically designed to evaluate repository-level web microservice generation.",
    "problem_domain": "Webå¾®æœåŠ¡å¼€å‘ï¼Œæ¶µç›–18ä¸ªé¢†åŸŸï¼ŒåŒ…æ‹¬è®¤è¯ã€å†…å®¹ç®¡ç†ã€èŠå¤©ã€æ¸¸æˆåç«¯ã€æ–‡ä»¶ç®¡ç†ã€æ•°æ®æœç´¢ã€ç”¨æˆ·ç³»ç»Ÿã€è°ƒåº¦ç­‰ã€‚",
    "problem_domain_quote": "comprising 106 repositories (60 Python, 46 Java) across 18 domains and 11 frameworks",
    "problem_difficulty": "åˆ†ä¸ºç®€å•ã€ä¸­ç­‰ã€å›°éš¾ä¸‰ä¸ªçº§åˆ«ï¼ŒåŸºäºä»£ç å¤æ‚åº¦æŒ‡æ ‡ã€‚",
    "problem_difficulty_quote": "Repositories are classified into Easy, Medium, and Hard difficulty levels based on code complexity metrics",
    "language": "Python å’Œ Java",
    "language_quote": "comprising 106 repositories (60 Python, 46 Java)",
    "data_size": "åŒ…å«106ä¸ªä»“åº“ï¼ˆ76ä¸ªç”¨äºè®­ç»ƒï¼Œ30ä¸ªç”¨äºè¯„ä¼°ï¼‰ï¼Œå…±1258ä¸ªAPIç«¯ç‚¹å’Œ2335ä¸ªæµ‹è¯•ç”¨ä¾‹ã€‚",
    "data_size_quote": "comprising 106 repositories (60 Python, 46 Java)... with 1,258 API endpoints and 2,335 test cases",
    "source_type": "æ··åˆæ¥æºï¼šæ¥è‡ªGitHubçš„çœŸå®ä¸–ç•Œä»“åº“ï¼ˆ6ä¸ªï¼‰å’Œä¸“å®¶ç›‘ç£ä¸‹ç”Ÿæˆçš„ä»“åº“ï¼ˆ100ä¸ªï¼‰ã€‚",
    "source_type_quote": "We collected repositories from two sources: Real-world GitHub Repositories... Expert-supervised Repository Generation.",
    "last_updated": "2026-01-20 (æ ¹æ®arXivç‰ˆæœ¬v1æ—¥æœŸ)",
    "last_updated_quote": "arXiv:2601.13943v1  [cs.SE]  20 Jan 2026",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œç»“åˆäº†çœŸå®é¡¹ç›®æ”¶é›†å’Œä¸“å®¶ç›‘ç£ç”Ÿæˆã€‚",
    "build_type_quote": "We introduce RepoGenesis... The benchmark includes both real-world GitHub projects and expert-curated implementations",
    "contamination_status": "æ–‡ä¸­æœªæ˜ç¡®è®¨è®ºæ•°æ®æ±¡æŸ“çŠ¶æ€ã€‚",
    "contamination_status_quote": NaN,
    "dataset_license": "æ–‡ä¸­æœªæåŠæ•°æ®é›†çš„è®¸å¯è¯ã€‚",
    "dataset_license_quote": NaN,
    "task_granularity": "å®Œæ•´çš„ä»“åº“ç”Ÿæˆ",
    "task_granularity_quote": "repository-level end-to-end web microservice generation",
    "evaluation_metrics": "Pass@1ï¼ˆåŠŸèƒ½æ­£ç¡®æ€§ï¼‰ã€APIè¦†ç›–ç‡ï¼ˆACï¼Œå®ç°å®Œæ•´æ€§ï¼‰ã€éƒ¨ç½²æˆåŠŸç‡ï¼ˆDSRï¼Œå¯éƒ¨ç½²æ€§ï¼‰",
    "evaluation_metrics_quote": "We evaluate open-source agents (e.g., DeepCode) and commercial IDEs (e.g., Cursor) using Pass@1, API Coverage (AC), and Deployment Success Rate (DSR).",
    "input_modality": "è‡ªç„¶è¯­è¨€éœ€æ±‚æ–‡æ¡£ï¼ˆREADME.mdæ–‡ä»¶ï¼‰",
    "input_modality_quote": "Given a requirement document (README.md) specifying service functionality, API endpoints with input/output schemas...",
    "output_modality": "ä»£ç ä»“åº“ï¼ŒåŒ…å«æºä»£ç æ–‡ä»¶ã€é…ç½®æ–‡ä»¶å’Œä¾èµ–è§„èŒƒã€‚",
    "output_modality_quote": "The agent generates a complete repository from scratch, including source code files, configuration files, and dependency specifications.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ä»“åº“",
    "task_io_type_quote": "Given a requirement document... models must generate a fully deployable repository...",
    "execution_environment": "æ²™ç›’æ‰§è¡Œç¯å¢ƒ",
    "execution_environment_quote": "Repositories are executed in sandboxed environments",
    "unique_features": "é¦–ä¸ªä¸“æ³¨äºWebå¾®æœåŠ¡å®Œæ•´ä»“åº“ç”Ÿæˆçš„å¤šè¯­è¨€åŸºå‡†ï¼›åŒ…å«ä¸¥æ ¼çš„â€œè¯„å®¡-åé©³â€è´¨é‡ä¿è¯æµç¨‹ï¼Œæ¶‰åŠä¸‰ä¸ªLLMè¯„å®¡å‘˜å’Œä¸€ä¸ªäººå·¥é¢†åŸŸä¸»å¸­ï¼›è¯„ä¼°ç»´åº¦åŒ…æ‹¬åŠŸèƒ½æ­£ç¡®æ€§ã€APIè¦†ç›–ç‡å’Œéƒ¨ç½²æˆåŠŸç‡ã€‚",
    "unique_features_quote": "the first multilingual benchmark for repository-level web microservice generation... we implement a rigorous quality assurance mechanism featuring a â€œreview-rebuttalâ€ loop with three LLM reviewers and a human Area Chair... We propose a multi-dimensional evaluation methodology (i.e., Pass@1, AC, and DSR)",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2026,
    "last_updated_month": 1,
    "last_updated_day": 20,
    "language_normalized": "['Python', 'Java']",
    "dimension_normalized": "['æ¶æ„è®¾è®¡', 'ä¾èµ–ç®¡ç†', 'ç³»ç»Ÿçº§ä¸€è‡´æ€§', 'åŠŸèƒ½æ­£ç¡®æ€§', 'APIå®ç°å®Œæ•´æ€§', 'å¯éƒ¨ç½²æ€§']",
    "evaluation_method_normalized": "['Pass@1', 'APIè¦†ç›–ç‡', 'éƒ¨ç½²æˆåŠŸç‡']",
    "problem_domain_normalized": "['Webå¾®æœåŠ¡å¼€å‘', 'è®¤è¯', 'å†…å®¹ç®¡ç†', 'èŠå¤©', 'æ¸¸æˆåç«¯', 'æ–‡ä»¶ç®¡ç†', 'æ•°æ®æœç´¢', 'ç”¨æˆ·ç³»ç»Ÿ', 'è°ƒåº¦']",
    "source_type_normalized": "['GitHubçš„çœŸå®ä¸–ç•Œä»“åº“', 'ä¸“å®¶ç›‘ç£ä¸‹ç”Ÿæˆçš„ä»“åº“']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.14027_output/content.md",
    "benchmark_name": "Putnam 2025",
    "benchmark_name_quote": "We evaluated Numina-Lean-Agent on the Putnam 2025 benchmark and compared its performance with other existing provers.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We evaluated Numina-Lean-Agent on the Putnam 2025 benchmark and compared its performance with other existing provers.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "å½¢å¼å®šç†è¯æ˜ï¼Œå³åœ¨ä¸¥æ ¼å®šä¹‰çš„é€»è¾‘ç³»ç»Ÿï¼ˆå¦‚Leanï¼‰ä¸­ä¸ºæ•°å­¦å®šç†æ„å»ºæœºå™¨å¯éªŒè¯çš„è¯æ˜ã€‚",
    "task_description_quote": "Formal theorem proving aims to construct machine-verifiable proofs for mathematical theorems within rigorously defined logical systems, such as Lean (2015) and Isabelle (Paulson, 1994).",
    "dimension": "å½¢å¼å®šç†è¯æ˜èƒ½åŠ›ï¼Œè§£å†³æ•°å­¦ç«èµ›é—®é¢˜çš„èƒ½åŠ›ã€‚",
    "dimension_quote": "Using Claude Opus 4.5 (Anthropic, 2025) as the base model, Numina-Lean-Agent successfully solved all 12 problems in the Putnam 2025, achieving state-of-the-art performance.",
    "evaluation_method": "åœ¨å½¢å¼å®šç†è¯æ˜å™¨ï¼ˆLeanï¼‰ä¸­éªŒè¯ç”Ÿæˆçš„è¯æ˜æ˜¯å¦æ­£ç¡®ï¼ŒæˆåŠŸè§£å†³é—®é¢˜çš„æ•°é‡ã€‚",
    "evaluation_method_quote": "Notably, we used the formal statements provided by Seed-Prover 1.5. ... Under these settings, Numina-Lean-Agent achieved state-of-the-art performance, successfully solving 12 out of 12 problems on Putnam 2025.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "æ•°å­¦ï¼Œç‰¹åˆ«æ˜¯æ•°å­¦ç«èµ›ï¼ˆPutnamï¼‰çº§åˆ«çš„å®šç†è¯æ˜ã€‚",
    "problem_domain_quote": "We evaluated Numina-Lean-Agent on the Putnam 2025 benchmark... Beyond standard automated proving, Numina-Lean-Agent serves as a general mathematical reasoning system...",
    "problem_difficulty": "ç«èµ›çº§ï¼ˆPutnamæ•°å­¦ç«èµ›éš¾åº¦ï¼‰ã€‚",
    "problem_difficulty_quote": "We evaluated Numina-Lean-Agent on the Putnam 2025 benchmark... Due to their substantially higher difficulty and longer proof search trajectories, problem A5 is assigned a larger budget...",
    "language": "å½¢å¼åŒ–æ•°å­¦è¯­è¨€ï¼ˆLeanï¼‰ã€‚",
    "language_quote": "Formal theorem proving aims to construct machine-verifiable proofs for mathematical theorems within rigorously defined logical systems, such as Lean (2015)...",
    "data_size": "åŒ…å«12ä¸ªé—®é¢˜ã€‚",
    "data_size_quote": "Numina-Lean-Agent successfully solved all 12 problems in the Putnam 2025...",
    "source_type": "æ•°å­¦ç«èµ›ï¼ˆPutnamï¼‰é—®é¢˜ã€‚",
    "source_type_quote": "We evaluated Numina-Lean-Agent on the Putnam 2025 benchmark...",
    "last_updated": "2025",
    "last_updated_quote": "Putnam 2025",
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "å®šç†è¯æ˜ï¼ˆç”Ÿæˆå®Œæ•´çš„ã€å¯éªŒè¯çš„å½¢å¼åŒ–è¯æ˜ï¼‰ã€‚",
    "task_granularity_quote": "Formal theorem proving aims to construct machine-verifiable proofs for mathematical theorems...",
    "evaluation_metrics": "è§£å†³é—®é¢˜çš„æ•°é‡ï¼ˆå¦‚12/12ï¼‰ã€‚",
    "evaluation_metrics_quote": "Numina-Lean-Agent successfully solved all 12 problems in the Putnam 2025, achieving state-of-the-art performance.",
    "input_modality": "å½¢å¼åŒ–æ•°å­¦é™ˆè¿°ï¼ˆå®šç†ï¼‰ã€‚",
    "input_modality_quote": "Notably, we used the formal statements provided by Seed-Prover 1.5.",
    "output_modality": "å½¢å¼åŒ–è¯æ˜ï¼ˆLeanä»£ç ï¼‰ã€‚",
    "output_modality_quote": "Formal theorem proving aims to construct machine-verifiable proofs...",
    "task_io_type": "å®šç†åˆ°è¯æ˜",
    "task_io_type_quote": "Formal theorem proving aims to construct machine-verifiable proofs for mathematical theorems...",
    "execution_environment": "å½¢å¼å®šç†è¯æ˜å™¨ç¯å¢ƒï¼ˆå¦‚Leanç¼–è¯‘å™¨ï¼‰ã€‚",
    "execution_environment_quote": "Lean-LSP-MCP (Dressler, 2025) is a Model Context Protocol (MCP) server explicitly designed for the Lean theorem prover. Acting as a bridge between LLMs and the Lean kernel via the Language Server Protocol (LSP)...",
    "unique_features": "Putnamæ•°å­¦ç«èµ›æ˜¯ä¸€ä¸ªè‘—åçš„ã€é«˜éš¾åº¦çš„å¤§å­¦æ•°å­¦ç«èµ›ï¼Œå…¶é—®é¢˜è¢«ç”¨ä½œè¯„ä¼°å½¢å¼å®šç†è¯æ˜ç³»ç»Ÿèƒ½åŠ›çš„åŸºå‡†ã€‚",
    "unique_features_quote": "We evaluated Numina-Lean-Agent on the Putnam 2025 benchmark and compared its performance with other existing provers.",
    "data_size_quantity": 12,
    "data_size_unit": "ä¸ªé—®é¢˜",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['å½¢å¼åŒ–æ•°å­¦è¯­è¨€ï¼ˆLeanï¼‰']",
    "dimension_normalized": "['å½¢å¼å®šç†è¯æ˜èƒ½åŠ›', 'è§£å†³æ•°å­¦ç«èµ›é—®é¢˜çš„èƒ½åŠ›']",
    "evaluation_method_normalized": "['è§£å†³é—®é¢˜çš„æ•°é‡ï¼ˆå¦‚12/12ï¼‰']",
    "problem_domain_normalized": "['æ•°å­¦', 'ç‰¹åˆ«æ˜¯æ•°å­¦ç«èµ›ï¼ˆPutnamï¼‰çº§åˆ«çš„å®šç†è¯æ˜']",
    "source_type_normalized": "['æ•°å­¦ç«èµ›ï¼ˆPutnamï¼‰é—®é¢˜']",
    "problem_difficulty_normalized": "ç«èµ›çº§",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.15195_output/content.md",
    "benchmark_name": "AIDev-pop dataset",
    "benchmark_name_quote": "In this paper, we conduct a large-scale empirical study on agent-authored pull requests using the AIDev-pop dataset [26], which comprises over 33k PRs submitted by five major coding agents across GitHub projects with more than 100 stars.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "In this paper, we conduct a large-scale empirical study on agent-authored pull requests using the AIDev-pop dataset [26]...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„æµ‹AIç¼–ç ä»£ç†åœ¨çœŸå®è½¯ä»¶å¼€å‘å·¥ä½œæµï¼ˆå¦‚æäº¤Pull Requestï¼‰ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯å…¶å¤±è´¥åŸå› ã€‚",
    "task_description_quote": "In this paper, we conduct a large-scale study of 33k agent-authored PRs made by five coding agents across GitHub. (RQ1) We first quantitatively characterize merged and not-merged PRs... (RQ2) To further investigate why some agentic PRs are not merged, we qualitatively analyze 600 PRs to derive a hierarchical taxonomy of rejection patterns.",
    "dimension": "AIç¼–ç ä»£ç†åœ¨çœŸå®åä½œç¯å¢ƒï¼ˆPull Requestï¼‰ä¸­çš„æˆåŠŸç‡ã€å¤±è´¥æ¨¡å¼ã€ä»»åŠ¡ç±»å‹è¡¨ç°ã€ä»£ç å˜æ›´è§„æ¨¡ã€CIç»“æœã€è¯„å®¡åŠ¨æ€ã€‚",
    "dimension_quote": "We first quantitatively characterize merged and not-merged PRs along four broad dimensions: 1) merge outcomes across task types, 2) code changes, 3) CI build results, and 4) review dynamics.",
    "evaluation_method": "å®šé‡åˆ†æï¼ˆç»Ÿè®¡åˆå¹¶ç‡ã€ä»£ç å˜æ›´è¡Œæ•°ã€æ–‡ä»¶æ•°ã€CIæ£€æŸ¥å¤±è´¥æ•°ã€è¯„å®¡è¯„è®ºæ•°ã€è¯„å®¡ä¿®è®¢æ•°ï¼Œä½¿ç”¨æ•ˆåº”é‡Cliff's deltaã€æ ¸å¯†åº¦ä¼°è®¡ã€é€»è¾‘å›å½’å»ºæ¨¡ï¼‰å’Œå®šæ€§åˆ†æï¼ˆå¯¹600ä¸ªè¢«æ‹’PRè¿›è¡Œæ‰‹åŠ¨ç¼–ç ï¼Œå»ºç«‹æ‹’ç»æ¨¡å¼åˆ†ç±»æ³•ï¼‰ã€‚",
    "evaluation_method_quote": "We perform a quantitative characterization of agent-authored pull requests along four dimensions... we rely on effect size measures, using Cliffâ€™s delta (Î´) to quantify the magnitude of difference... we use kernel density estimates... we use logistic regression modeling... We randomly select a subset of 600 rejected PRs for qualitative analysis... Following open coding [1], two authors independently label each PR with its primary reason for rejection.",
    "context_dependency": "å¤šæ–‡ä»¶é¡¹ç›®çº§åˆ«çš„çœŸå®åä½œç¯å¢ƒï¼Œæ¶‰åŠä»£ç å®¡æŸ¥ã€CI/CDæµæ°´çº¿éªŒè¯å’Œè¿­ä»£ä¿®è®¢ã€‚",
    "context_dependency_quote": "...how agents perform when integrated into real development workflows involving CI validation, code review, and iterative revision.",
    "problem_domain": "é€šç”¨è½¯ä»¶å¼€å‘ï¼Œæ¶µç›–åŠŸèƒ½ã€ä¿®å¤ã€æ€§èƒ½ã€é‡æ„ã€æ ·å¼ã€æ–‡æ¡£ã€æµ‹è¯•ã€æ‚åŠ¡ã€æ„å»ºã€CIç­‰å¤šç§ä»»åŠ¡ç±»å‹ã€‚",
    "problem_domain_quote": "These tasks consist of 11 categories: feature, fix, performance, refactoring, style, documentation, test, chore, build, CI, and other [7, 26].",
    "problem_difficulty": "çœŸå®ä¸–ç•Œå·¥ç¨‹çº§éš¾åº¦ï¼Œæ¶‰åŠä¸é¡¹ç›®å·¥ä½œæµã€å¼€å‘è€…æœŸæœ›å’Œé¡¹ç›®åè°ƒçš„å¯¹é½ã€‚",
    "problem_difficulty_quote": "Overall, our results suggest that agentic PR failures stem from misalignment with repository workflows (e.g., CI/CD failures), developer expectations (e.g., unwanted or incorrect features), and a lack of project coordination (e.g., reviewer abandonment).",
    "language": "æ–‡ä¸­æœªæ˜ç¡®æåŠæ•°æ®é›†æ¶µç›–çš„å…·ä½“ç¼–ç¨‹è¯­è¨€ï¼Œæ•°æ®é›†ç”±æ¥è‡ªGitHubçš„çœŸå®PRæ„æˆï¼Œå¯èƒ½åŒ…å«å¤šç§è¯­è¨€ã€‚",
    "language_quote": NaN,
    "data_size": "è¶…è¿‡33kä¸ªç”±äº”ä¸ªä¸»è¦ç¼–ç ä»£ç†æäº¤çš„Pull Requestã€‚",
    "data_size_quote": "In this paper, we conduct a large-scale study of 33k agent-authored PRs made by five coding agents across GitHub.",
    "source_type": "æ¥è‡ªGitHubä¸Šæ˜Ÿæ ‡æ•°è¶…è¿‡100çš„é¡¹ç›®çš„çœŸå®Pull Requestã€‚",
    "source_type_quote": "...over 33k PRs submitted by five major coding agents across GitHub projects with more than 100 stars.",
    "last_updated": "2026ï¼ˆæ ¹æ®è®ºæ–‡ä¼šè®®æ—¥æœŸæ¨æ–­ï¼‰",
    "last_updated_quote": "MSR 2026, Rio de Janeiro, Brazil",
    "build_type": "åŸºäºçœŸå®GitHubé¡¹ç›®æ•°æ®æ„å»ºï¼ˆAIDev-popæ•°æ®é›†ï¼‰ã€‚",
    "build_type_quote": "...using the AIDev-pop dataset [26], which comprises over 33k PRs submitted by five major coding agents across GitHub projects...",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "Pull Requestçº§åˆ«çš„ä»£ç è´¡çŒ®ï¼ŒåŒ…æ‹¬ç”Ÿæˆã€ä¿®æ”¹ã€ä¿®å¤ç­‰ã€‚",
    "task_granularity_quote": "AI coding agents are now submitting pull requests (PRs) to software projects, acting not just as assistants but as autonomous contributors.",
    "evaluation_metrics": "åˆå¹¶ç‡ã€ä»£ç å˜æ›´è¡Œæ•°ï¼ˆ#LOC Changesï¼‰ã€å˜æ›´æ–‡ä»¶æ•°ï¼ˆ#File Changesï¼‰ã€CIæ£€æŸ¥å¤±è´¥æ•°ï¼ˆ#Failed CI Checksï¼‰ã€è¯„å®¡è¯„è®ºæ•°ï¼ˆ#Review Commentsï¼‰ã€è¯„å®¡ä¿®è®¢æ•°ï¼ˆ#Review Revisionsï¼‰ã€æ•ˆåº”é‡ï¼ˆCliff's deltaï¼‰ã€‚",
    "evaluation_metrics_quote": "We analyze the magnitude of proposed code changes by measuring a) the total number of added and removed lines of code (#LOC Changes), and b) the number of files modified by each PR (#File Changes)... we extract the number of failed check-runs (#Failed CI Checks)... we compute a) the number of review comments in a PR (#Review Comments), and b) the number of review revisions each PR receives (#Review Revisions)... we rely on effect size measures, using Cliffâ€™s delta (Î´)...",
    "input_modality": "æ–‡ä¸­æœªæ˜ç¡®æè¿°æ•°æ®é›†ä¸­æ¯ä¸ªä»»åŠ¡çš„å…·ä½“è¾“å…¥å½¢å¼ï¼Œä½†æ ¹æ®ä¸Šä¸‹æ–‡ï¼Œè¾“å…¥å¯èƒ½åŒ…æ‹¬é—®é¢˜æè¿°ã€ä»£ç ä¸Šä¸‹æ–‡æˆ–è¯„å®¡åé¦ˆã€‚",
    "input_modality_quote": NaN,
    "output_modality": "ä»£ç å˜æ›´ï¼ˆä»¥Pull Requestå½¢å¼æäº¤ï¼‰ã€‚",
    "output_modality_quote": "AI coding agents... now generate code changes, respond to reviewer feedback, and participate in the software lifecycle as autonomous agents.",
    "task_io_type": "å¤šç§ç±»å‹ï¼Œå¯èƒ½åŒ…æ‹¬æ–‡æœ¬åˆ°ä»£ç ï¼ˆå¦‚æ ¹æ®æè¿°å®ç°åŠŸèƒ½ï¼‰ã€ä»£ç åˆ°ä»£ç ï¼ˆå¦‚ä¿®å¤ã€é‡æ„ï¼‰ã€ä»¥åŠäº¤äº’å¼ä»»åŠ¡ï¼ˆå“åº”è¯„å®¡ï¼‰ã€‚",
    "task_io_type_quote": "Coding agents have been extensively benchmarked across a range of tasks, from code generation [5, 33], testing [23, 31, 39], to automated program repair [8, 22, 30].",
    "execution_environment": "çœŸå®é¡¹ç›®çš„CI/CDæµæ°´çº¿ç¯å¢ƒã€‚",
    "execution_environment_quote": "Not-merged PRs tend to involve larger code changes, touch more files, and often do not pass the projectâ€™s CI/CD pipeline validation.",
    "unique_features": "ä¸“æ³¨äºAIç¼–ç ä»£ç†åœ¨çœŸå®ã€åä½œçš„è½¯ä»¶å¼€å‘å·¥ä½œæµï¼ˆPull Requestï¼‰ä¸­çš„è¡¨ç°è¯„ä¼°ï¼Œè€Œéå­¤ç«‹çš„ä»£ç ç”Ÿæˆä»»åŠ¡ã€‚å®ƒåˆ†æäº†ç¤¾ä¼šæŠ€æœ¯å› ç´ å’Œäººç±»-AIåä½œå¯¹PRæˆåŠŸçš„å½±å“ã€‚",
    "unique_features_quote": "While prior work evaluates agents in isolated tasks, we lack a systematic assessment of how agents perform when integrated into real development workflows involving CI validation, code review, and iterative revision... Together, our findings highlight key socio-technical and human-AI collaboration factors that are critical to improving the success of future agentic workflows.",
    "data_size_quantity": 33,
    "data_size_unit": "kä¸ª",
    "last_updated_year": 2026,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['AIç¼–ç ä»£ç†åœ¨çœŸå®åä½œç¯å¢ƒï¼ˆPull Requestï¼‰ä¸­çš„æˆåŠŸç‡', 'å¤±è´¥æ¨¡å¼', 'ä»»åŠ¡ç±»å‹è¡¨ç°', 'ä»£ç å˜æ›´è§„æ¨¡', 'CIç»“æœ', 'è¯„å®¡åŠ¨æ€']",
    "evaluation_method_normalized": "['åˆå¹¶ç‡', 'ä»£ç å˜æ›´è¡Œæ•°ï¼ˆ#LOC Changesï¼‰', 'å˜æ›´æ–‡ä»¶æ•°ï¼ˆ#File Changesï¼‰', 'CIæ£€æŸ¥å¤±è´¥æ•°ï¼ˆ#Failed CI Checksï¼‰', 'è¯„å®¡è¯„è®ºæ•°ï¼ˆ#Review Commentsï¼‰', 'è¯„å®¡ä¿®è®¢æ•°ï¼ˆ#Review Revisionsï¼‰', \"æ•ˆåº”é‡ï¼ˆCliff's deltaï¼‰\"]",
    "problem_domain_normalized": "['é€šç”¨è½¯ä»¶å¼€å‘', 'åŠŸèƒ½', 'ä¿®å¤', 'æ€§èƒ½', 'é‡æ„', 'æ ·å¼', 'æ–‡æ¡£', 'æµ‹è¯•', 'æ‚åŠ¡', 'æ„å»º', 'CI']",
    "source_type_normalized": "['æ¥è‡ªGitHubä¸Šæ˜Ÿæ ‡æ•°è¶…è¿‡100çš„é¡¹ç›®çš„çœŸå®Pull Request']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.15165_output/content.md",
    "benchmark_name": "GSM8K",
    "benchmark_name_quote": "Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K)...",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "On complex reasoning benchmarks, it achieves competitive results (e.g., 89.1% accuracy on GSM8K, 45.1% on MATH)...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "æ•°å­¦æ¨ç†ï¼ˆå°å­¦æ•°å­¦åº”ç”¨é¢˜ï¼‰",
    "task_description_quote": "...in general reasoning tasks like mathematics and coding... On complex reasoning benchmarks, it achieves competitive results (e.g., 89.1% accuracy on GSM8K)...",
    "dimension": "æ•°å­¦æ¨ç†èƒ½åŠ›",
    "dimension_quote": "...in general reasoning tasks like mathematics and coding...",
    "evaluation_method": "å‡†ç¡®ç‡ (accuracy)",
    "evaluation_method_quote": "...89.1% accuracy on GSM8K...",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "æ•°å­¦",
    "problem_domain_quote": "...in general reasoning tasks like mathematics...",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": NaN,
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "æ•°å­¦é—®é¢˜æ±‚è§£",
    "task_granularity_quote": "...in general reasoning tasks like mathematics...",
    "evaluation_metrics": "å‡†ç¡®ç‡ (accuracy)",
    "evaluation_metrics_quote": "...89.1% accuracy on GSM8K...",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆæ•°å­¦é—®é¢˜æè¿°ï¼‰",
    "input_modality_quote": "...in general reasoning tasks like mathematics...",
    "output_modality": "è‡ªç„¶è¯­è¨€ï¼ˆæ•°å­¦æ¨ç†æ­¥éª¤ä¸ç­”æ¡ˆï¼‰",
    "output_modality_quote": NaN,
    "task_io_type": "æ–‡æœ¬åˆ°æ–‡æœ¬ï¼ˆæ•°å­¦é—®é¢˜åˆ°è§£ç­”ï¼‰",
    "task_io_type_quote": "...in general reasoning tasks like mathematics...",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": NaN,
    "unique_features_quote": NaN,
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['æ•°å­¦æ¨ç†èƒ½åŠ›']",
    "evaluation_method_normalized": "['å‡†ç¡®ç‡ (accuracy)']",
    "problem_domain_normalized": "['æ•°å­¦']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.15188_output/content.md",
    "benchmark_name": "æœªæ˜ç¡®å‘½åï¼Œä½†åŸºäºHumanEvalæ„å»ºçš„ABAPä»£ç ç”ŸæˆåŸºå‡†",
    "benchmark_name_quote": "The experimental procedure for the empirical investigation of ABAP code generation by LLMs is based on several central aspects: A benchmark with 180 tasks serves as the data basis, covering both general algorithmic problems (based on HumanEval) and specific SAP scenarios.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "This work aims to apply a benchmark approach to ABAP code generation as a prerequisite to empirically investigate and systematically evaluate the performance of LLMs in generating ABAP code. By such an environment it can be examined to what extent LLMs can produce ABAP programs that are syntactically correct, semantically accurate, and practically relevant code.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ç”ŸæˆABAPä»£ç çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬è¯­æ³•æ­£ç¡®æ€§ã€è¯­ä¹‰å‡†ç¡®æ€§å’Œå®é™…ç›¸å…³æ€§ã€‚",
    "task_description_quote": "The aim of the study is to empirically analyze to what extent various LLMs can generate syntactically correct and functional ABAP code, how effectively they use compiler feedback for iterative improvement, and which task types pose special challenges.",
    "dimension": "ä»£ç ç”Ÿæˆæ­£ç¡®æ€§ã€è¿­ä»£æ”¹è¿›èƒ½åŠ›ã€å¯¹ä¸åŒä»»åŠ¡ç±»å‹çš„é€‚åº”æ€§",
    "dimension_quote": "The aim of the study is to empirically analyze to what extent various LLMs can generate syntactically correct and functional ABAP code, how effectively they use compiler feedback for iterative improvement, and which task types pose special challenges.",
    "evaluation_method": "é€šè¿‡è‡ªåŠ¨åŒ–å•å…ƒæµ‹è¯•éªŒè¯åŠŸèƒ½æ­£ç¡®æ€§ï¼Œå¹¶è¿›è¡Œè¯¦ç»†çš„é”™è¯¯åˆ†æå’ŒKaplan-Meierç”Ÿå­˜åˆ†æä»¥è¯„ä¼°å­¦ä¹ æ›²çº¿ã€‚",
    "evaluation_method_quote": "Success is measured based on functional correctness, which is verified by these automated unit tests. In addition, a detailed error analysis and a Kaplan-Meier survival analysis are carried out to evaluate the learning curves.",
    "context_dependency": "å•å‡½æ•°/æ–¹æ³•çº§åˆ«ï¼ˆè¦æ±‚ç”Ÿæˆå…¨å±€ç±»ä¸­çš„é™æ€æ–¹æ³•ï¼‰",
    "context_dependency_quote": "Global classes with static methods are required, as the test setup requires this structure and this ensures the comparability of the results.",
    "problem_domain": "é€šç”¨ç®—æ³•é—®é¢˜ï¼ˆåŸºäºHumanEvalï¼‰å’Œç‰¹å®šçš„SAPä¸šåŠ¡åœºæ™¯ï¼ˆå¦‚æ•°æ®åº“æ“ä½œï¼‰",
    "problem_domain_quote": "A benchmark with 180 tasks serves as the data basis, covering both general algorithmic problems (based on HumanEval) and specific SAP scenarios.",
    "problem_difficulty": "è¦†ç›–ä¸åŒéš¾åº¦çº§åˆ«ï¼ŒåŒ…æ‹¬åŸºç¡€ç®—æ³•å’Œå®é™…SAPç”¨ä¾‹",
    "problem_difficulty_quote": "The test cases cover different levels of difficulty and types of tasks, enabling a broad evaluation of the models.",
    "language": "ABAP",
    "language_quote": "This work investigates the performance of Large Language Models (LLMs) in generating ABAP code.",
    "data_size": "180ä¸ªä»»åŠ¡ï¼Œå…¶ä¸­164ä¸ªåŸºäºHumanEvalçš„æ ‡å‡†åŒ–ä»»åŠ¡ï¼Œ16ä¸ªABAPç‰¹å®šä»»åŠ¡",
    "data_size_quote": "The developed benchmark comprises a total of 180 test tasks, which are divided into two groups: 164 standardized tasks based on the HumanEval dataset [3], which was originally developed for evaluating code generation in Python, and 16 ABAP-specific tasks.",
    "source_type": "æ”¹ç¼–è‡ªHumanEvalæ•°æ®é›†ï¼ˆPythonä»»åŠ¡ï¼‰å’Œæºè‡ªå®é™…SAPåœºæ™¯çš„ç‰¹å®šä»»åŠ¡",
    "source_type_quote": "164 standardized tasks based on the HumanEval dataset [3], which was originally developed for evaluating code generation in Python, and 16 ABAP-specific tasks. These 16 specific tasks are derived from practical SAP-specific scenarios focusing on the processing of database tables.",
    "last_updated": "2026",
    "last_updated_quote": "arXiv:2601.15188v1  [cs.SE]  21 Jan 2026",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆæœ¬ç ”ç©¶æ„å»ºï¼‰",
    "build_type_quote": "The developed benchmark comprises a total of 180 test tasks... The dataset for the benchmark is available in the appendix A.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆä»è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆå®Œæ•´çš„ABAPç±»å’Œæ–¹æ³•ï¼‰",
    "task_granularity_quote": "The aim of the study is to empirically analyze to what extent various LLMs can generate syntactically correct and functional ABAP code... Both class definition and implementation code are to be provided, as both are required for carrying out the evaluation.",
    "evaluation_metrics": "åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆé€šè¿‡å•å…ƒæµ‹è¯•éªŒè¯ï¼‰",
    "evaluation_metrics_quote": "Success is measured based on functional correctness, which is verified by these automated unit tests.",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆä»»åŠ¡æè¿°ï¼‰",
    "input_modality_quote": "It consists of programming tasks formulated in natural language and validated by predefined unit tests, thus enabling an objective measurement of functional correctness [3].",
    "output_modality": "ä»£ç ï¼ˆABAPç±»å’Œæ–¹æ³•ï¼‰",
    "output_modality_quote": "Both class definition and implementation code are to be provided, as both are required for carrying out the evaluation.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "It consists of programming tasks formulated in natural language and validated by predefined unit tests, thus enabling an objective measurement of functional correctness [3].",
    "execution_environment": "æ ‡å‡†åŒ–çš„SAP ABAPè¿è¡Œæ—¶ç¯å¢ƒï¼ˆDockeré•œåƒï¼‰",
    "execution_environment_quote": "The entire process is fully automated and uses a standardized SAP environment (Docker image), which is addressed via a Python control and the ADT interface.",
    "unique_features": "1. ä¸“æ³¨äºä¼ä¸šçº§ã€é¢†åŸŸç‰¹å®šçš„ç¼–ç¨‹è¯­è¨€ABAPã€‚2. åŒ…å«è¿­ä»£åé¦ˆå¾ªç¯ï¼ˆæœ€å¤š5æ¬¡ï¼‰ï¼Œåˆ©ç”¨ABAPç¼–è¯‘å™¨é”™è¯¯ä¿¡æ¯è¿›è¡Œæ”¹è¿›ã€‚3. ä»»åŠ¡åˆ†ä¸ºäº”ç±»ï¼šå­—ç¬¦ä¸²å¤„ç†ã€åˆ—è¡¨/æ•°ç»„æ“ä½œã€æ•°å­¦è®¡ç®—ã€é€»è¾‘æ¡ä»¶ã€ABAPæ•°æ®åº“æ“ä½œã€‚4. ç»“åˆäº†é€šç”¨ç®—æ³•ä»»åŠ¡ï¼ˆæ”¹ç¼–è‡ªHumanEvalï¼‰å’Œå®é™…SAPä¸šåŠ¡åœºæ™¯ä»»åŠ¡ã€‚",
    "unique_features_quote": "The tasks are divided into five thematic categories, including for example String Handling and ABAP Database Operation. A special focus is placed on increasing the probability of successful code generation through an iterative feedback process: each task is processed in up to five feedback loops, during which the system provides the LLM with feedback from the ABAP compiler or automated unit tests.",
    "data_size_quantity": 180,
    "data_size_unit": "ä¸ªä»»åŠ¡",
    "last_updated_year": 2026,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['ABAP']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆæ­£ç¡®æ€§', 'è¿­ä»£æ”¹è¿›èƒ½åŠ›', 'å¯¹ä¸åŒä»»åŠ¡ç±»å‹çš„é€‚åº”æ€§']",
    "evaluation_method_normalized": "['åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆé€šè¿‡å•å…ƒæµ‹è¯•éªŒè¯ï¼‰']",
    "problem_domain_normalized": "['é€šç”¨ç®—æ³•é—®é¢˜ï¼ˆåŸºäºHumanEvalï¼‰', 'ç‰¹å®šçš„SAPä¸šåŠ¡åœºæ™¯ï¼ˆå¦‚æ•°æ®åº“æ“ä½œï¼‰']",
    "source_type_normalized": "['æ”¹ç¼–è‡ªHumanEvalæ•°æ®é›†ï¼ˆPythonä»»åŠ¡ï¼‰', 'æºè‡ªå®é™…SAPåœºæ™¯çš„ç‰¹å®šä»»åŠ¡']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.15728_output/content.md",
    "benchmark_name": "BIRD-Python",
    "benchmark_name_quote": "To address this gap, we introduce BIRD-Python, a benchmark designed for cross-paradigm evaluation.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "we introduce BIRD-Python, a benchmark designed for cross-paradigm evaluation.",
    "dataset_url": "https://github.com/1050727345hu-web/Bird-Python",
    "dataset_url_quote": "Resources are available at https://github.com/1050727345hu-web/Bird-Python.",
    "task_description": "è¯„ä¼°æ¨¡å‹å°†è‡ªç„¶è¯­è¨€è¯·æ±‚è½¬æ¢ä¸ºç”¨äºæ ¸å¿ƒæ•°æ®æ£€ç´¢å’Œåˆ†æçš„Pythonä»£ç çš„èƒ½åŠ›ï¼Œç‰¹åˆ«é’ˆå¯¹åŸºäºæ–‡ä»¶ï¼ˆå¦‚CSVï¼‰çš„æ•°æ®ç¯å¢ƒã€‚",
    "task_description_quote": "there is a notable absence of a dedicated benchmark to systematically evaluate a modelâ€™s ability to translate natural language requests into Python code specifically tailored for core data retrieval and analysis.",
    "dimension": "è·¨èŒƒå¼è¯„ä¼°ï¼ˆText-to-Python vs. Text-to-SQLï¼‰ã€æ˜¾å¼é€»è¾‘å¤„ç†èƒ½åŠ›ã€å¯¹æ¨¡ç³Šç”¨æˆ·æ„å›¾çš„æ•æ„Ÿæ€§ã€çŸ¥è¯†åŸºç¡€ï¼ˆKnowledge Groundingï¼‰",
    "dimension_quote": "a benchmark designed for cross-paradigm evaluation... making it highly sensitive to underspecified user intent... the need for explicit logic formulation makes knowledge groundingâ€”rather than the choice of paradigm aloneâ€”the primary bottleneck",
    "evaluation_method": "æ‰§è¡Œç”Ÿæˆçš„ä»£ç å¹¶ä¸æ ‡å‡†ç­”æ¡ˆæ¯”è¾ƒï¼Œç¡®ä¿Pythonä»£ç èƒ½ç²¾ç¡®å¤ç°SQLçš„æ‰§è¡Œç»“æœã€‚",
    "evaluation_method_quote": "ensuring the code explicitly replicated the DBMS-level operations... ensuring the Python ground truth faithfully replicates the SQL execution outcomes within a procedural environment.",
    "context_dependency": "å•æŸ¥è¯¢ä»»åŠ¡ï¼Œè¾“å…¥åŒ…å«è‡ªç„¶è¯­è¨€é—®é¢˜ã€å¤–éƒ¨çŸ¥è¯†å’Œæ•°æ®ç¯å¢ƒï¼ˆæ–‡ä»¶ï¼‰ã€‚",
    "context_dependency_quote": "Each instance is a triplet (Q, K, Ysql) consisting of the natural language question, external knowledge, and the ground truth SQL query.",
    "problem_domain": "æ•°æ®æŸ¥è¯¢ä¸åˆ†æï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹åŸºäºæ–‡ä»¶ï¼ˆå¦‚CSVï¼‰çš„æ•°æ®ç¯å¢ƒã€‚",
    "problem_domain_quote": "evaluating Text-to-Python in data querying tasks on file-based environments",
    "problem_difficulty": "ä¼ä¸šçº§åœºæ™¯ï¼ŒåŒ…å«å¤§è§„æ¨¡æ•°æ®åº“å’Œå¤–éƒ¨çŸ¥è¯†ï¼Œä»£è¡¨ç°å®ä¸–ç•ŒæŒ‘æˆ˜ã€‚",
    "problem_difficulty_quote": "BIRD (Li et al., 2024b) incorporates large-scale databases and external knowledge to better represent enterprise-level scenarios.",
    "language": "Pythonï¼ˆç”¨äºç”Ÿæˆä»£ç ï¼‰ï¼Œæ•°æ®æºä¸ºCSVç­‰æ–‡ä»¶æ ¼å¼ã€‚",
    "language_quote": "By generating executable Python code... we transformed the underlying relational databases into standalone file formats (i.e., CSVs)",
    "data_size": "åŸºäºBIRDåŸºå‡†å¼€å‘é›†çš„1,534ä¸ªæŸ¥è¯¢-SQLå¯¹è¿›è¡Œæ„å»ºå’Œä¿®æ­£ã€‚",
    "data_size_quote": "Our study employs the development set of the BIRD benchmark (Li et al., 2024b), comprising 1,534 query-SQL pairs.",
    "source_type": "åŸºäºBIRDåŸºå‡†ï¼ˆText-to-SQLæ•°æ®é›†ï¼‰è¿›è¡Œè½¬æ¢å’Œä¿®æ­£ã€‚é€šè¿‡æ¨¡å‹è¾…åŠ©éªŒè¯å’Œä¸“å®¶åŒé‡ç›²å®¡æµç¨‹è¿›è¡Œå‡€åŒ–ã€‚",
    "source_type_quote": "We conduct a thorough empirical study by adapting the BIRD benchmark (Li et al., 2024b) to evaluate Text-to-Python performance... We implemented a two-stage purification pipeline that integrates Model-in-the-Loop verification with expert review.",
    "last_updated": "2026ï¼ˆæ ¹æ®é¢„å°æœ¬æ—¥æœŸæ¨æ–­ï¼‰",
    "last_updated_quote": "arXiv:2601.15728v1  [cs.AI]  22 Jan 2026",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œç”±ç ”ç©¶å›¢é˜ŸåŸºäºç°æœ‰æ•°æ®é›†è½¬æ¢ã€ä¿®æ­£å’ŒéªŒè¯ã€‚",
    "build_type_quote": "we introduce BIRD-Python... We systematically refined the original dataset... we reconstructed SQL logic into Python",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆä»è‡ªç„¶è¯­è¨€ç”Ÿæˆå®Œæ•´çš„ã€å¯æ‰§è¡Œçš„Pythonè„šæœ¬ï¼‰ã€‚",
    "task_granularity_quote": "the model synthesizes a program P such that Exec(P, D) â†’ A... the output Pcode is an imperative script",
    "evaluation_metrics": "æ‰§è¡Œå‡†ç¡®æ€§ï¼ˆExecution Accuracyï¼‰ï¼Œç¡®ä¿ç”Ÿæˆçš„Pythonä»£ç ä¸ä¿®æ­£åçš„SQLæŸ¥è¯¢ç»“æœä¸€è‡´ã€‚",
    "evaluation_metrics_quote": "ensuring the Python ground truth faithfully replicates the SQL execution outcomes",
    "input_modality": "è‡ªç„¶è¯­è¨€æŸ¥è¯¢ã€å¤–éƒ¨çŸ¥è¯†ã€æ•°æ®ç¯å¢ƒï¼ˆæ–‡ä»¶ï¼‰ã€‚",
    "input_modality_quote": "Given a query Q and data environment D... Each instance is a triplet (Q, K, Ysql) consisting of the natural language question, external knowledge, and the ground truth SQL query.",
    "output_modality": "å¯æ‰§è¡Œçš„Pythonä»£ç ï¼ˆè„šæœ¬ï¼‰ã€‚",
    "output_modality_quote": "the output Pcode is an imperative script requiring explicit procedural logic.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ï¼ˆè‡ªç„¶è¯­è¨€åˆ°Pythonè„šæœ¬ï¼‰ã€‚",
    "task_io_type_quote": "translate natural language requests into Python code",
    "execution_environment": "åŸºäºæ–‡ä»¶çš„æ•°æ®ç¯å¢ƒï¼ˆå¦‚CSVï¼‰ï¼Œä½¿ç”¨Pandasç­‰åº“è¿›è¡Œæ•°æ®æ“ä½œã€‚",
    "execution_environment_quote": "D represents in-memory dataframes Ddf... leveraging powerful libraries like Pandas for robust data manipulation",
    "unique_features": "é¦–ä¸ªä¸“é—¨ç”¨äºåœ¨åŸºäºæ–‡ä»¶çš„ç¯å¢ƒä¸­è¯„ä¼°Text-to-Pythonæ•°æ®æŸ¥è¯¢ä»»åŠ¡çš„åŸºå‡†ï¼›é€šè¿‡ç³»ç»ŸåŒ–ä¿®æ­£åŸå§‹æ•°æ®é›†ä»¥å‡å°‘æ ‡æ³¨å™ªå£°å¹¶å¯¹é½æ‰§è¡Œè¯­ä¹‰ï¼Œå»ºç«‹äº†è·¨èŒƒå¼ï¼ˆSQL vs. Pythonï¼‰æ¯”è¾ƒçš„ä¸€è‡´åŸºçº¿ï¼›æ­ç¤ºäº†ç”±äºPythonéœ€è¦æ˜¾å¼è¿‡ç¨‹é€»è¾‘è€Œå¯¼è‡´çš„èŒƒå¼æ ¹æœ¬å·®å¼‚ã€‚",
    "unique_features_quote": "we propose the first dedicated benchmark BIRD-Python for evaluating Text-to-Python in data querying tasks on file-based environments... We systematically refined the original dataset to reduce annotation noise and align execution semantics, thereby establishing a consistent and standardized baseline for comparison... Our analysis reveals a fundamental paradigmatic divergence: whereas SQL leverages implicit DBMS behaviors through its declarative structure, Python requires explicit procedural logic",
    "data_size_quantity": 1534,
    "data_size_unit": "ä¸ªæŸ¥è¯¢-SQLå¯¹",
    "last_updated_year": 2026,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['è·¨èŒƒå¼è¯„ä¼°', 'æ˜¾å¼é€»è¾‘å¤„ç†èƒ½åŠ›', 'å¯¹æ¨¡ç³Šç”¨æˆ·æ„å›¾çš„æ•æ„Ÿæ€§', 'çŸ¥è¯†åŸºç¡€']",
    "evaluation_method_normalized": "['æ‰§è¡Œå‡†ç¡®æ€§']",
    "problem_domain_normalized": "['æ•°æ®æŸ¥è¯¢ä¸åˆ†æ']",
    "source_type_normalized": "['BIRDåŸºå‡†']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.16172_output/content.md",
    "benchmark_name": "miniF2F",
    "benchmark_name_quote": "We evaluate on the miniF2F benchmark.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We evaluate on the miniF2F benchmark. ... We evaluate on miniF2F-test (Lean 4 split; 244 theorems)...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "å®šç†è¯æ˜ã€‚è¯¥åŸºå‡†åŒ…å«å½¢å¼åŒ–å®šç†ï¼Œè¦æ±‚æ¨¡å‹ç”Ÿæˆèƒ½åœ¨Leanè¯æ˜åŠ©æ‰‹ä¸­æˆåŠŸç¼–è¯‘çš„è¯æ˜ç­–ç•¥åºåˆ—ã€‚",
    "task_description_quote": "We evaluate on miniF2F-test (Lean 4 split; 244 theorems)... A theorem is counted as solved if and only if Lean returns exit code 0; any output containing sorry is treated as failure.",
    "dimension": "å½¢å¼åŒ–å®šç†è¯æ˜çš„æ­£ç¡®æ€§",
    "dimension_quote": "achieving impressive results through sophisticated training... Our results on the miniF2F benchmark show that this simple structural guidance yields a 43% relative improvement over the baseline...",
    "evaluation_method": "pass@kï¼ˆæœ¬æ–‡ä¸­k=16ï¼‰ï¼Œé€šè¿‡Leanç¼–è¯‘å™¨éªŒè¯ç”Ÿæˆçš„è¯æ˜ç­–ç•¥åºåˆ—æ˜¯å¦æˆåŠŸç¼–è¯‘ä¸”ä¸åŒ…å«`sorry`ã€‚",
    "evaluation_method_quote": "This simple approach yields 21.7% pass@16 compared to 15.2% for standard sampling... A theorem is counted as solved if and only if Lean returns exit code 0; any output containing sorry is treated as failure.",
    "context_dependency": "å•ä¸€å®šç†è¯æ˜ã€‚è¾“å…¥æ˜¯å½¢å¼åŒ–çš„å®šç†é™ˆè¿°ï¼Œè¾“å‡ºæ˜¯ç›¸åº”çš„è¯æ˜ç­–ç•¥åºåˆ—ã€‚",
    "context_dependency_quote": "For each theorem, we run up to k = 16 attempts... Each attempt is assembled into a Lean 4 file containing import Mathlib followed by the full benchmark theorem statement and := by with the generated tactics.",
    "problem_domain": "å½¢å¼åŒ–æ•°å­¦å®šç†è¯æ˜ï¼Œæ¶‰åŠæ•°å­¦é¢†åŸŸï¼ˆå¦‚ä»£æ•°ã€æ‹“æ‰‘ç­‰ï¼‰ã€‚",
    "problem_domain_quote": "We evaluate on miniF2F-test (Lean 4 split; 244 theorems)... The integration of Language Models with formal proof assistants has evolved rapidly.",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "Lean 4ï¼ˆä¸€ç§å®šç†è¯æ˜è¯­è¨€/ç¼–ç¨‹è¯­è¨€ï¼‰",
    "language_quote": "We evaluate on miniF2F-test (Lean 4 split; 244 theorems) using DeepSeek-Prover-V1.5-RL2 in completion-style prompting. ... Lean version: Lean 4 v4.27.0-rc1",
    "data_size": "244ä¸ªå®šç†ï¼ˆminiF2F-testçš„Lean 4å­é›†ï¼‰",
    "data_size_quote": "We evaluate on miniF2F-test (Lean 4 split; 244 theorems)...",
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆç”Ÿæˆè¯æ˜ç­–ç•¥åºåˆ—ï¼‰",
    "task_granularity_quote": "The model then generates the proof completion after the skeleton prefix.",
    "evaluation_metrics": "pass@16",
    "evaluation_metrics_quote": "This simple approach yields 21.7% pass@16 compared to 15.2% for standard sampling...",
    "input_modality": "å½¢å¼åŒ–å®šç†é™ˆè¿°ï¼ˆä»£ç ï¼‰",
    "input_modality_quote": "Formally, let T be the space of theorem statements in Lean... where: â€¢ x âˆˆT is the formal theorem statement (the goal).",
    "output_modality": "è¯æ˜ç­–ç•¥åºåˆ—ï¼ˆä»£ç ï¼‰",
    "output_modality_quote": "and P be the space of valid tactic sequences. ... The model then generates the proof completion after the skeleton prefix.",
    "task_io_type": "ä»£ç åˆ°ä»£ç ï¼ˆä»å®šç†é™ˆè¿°ç”Ÿæˆè¯æ˜ç­–ç•¥ï¼‰",
    "task_io_type_quote": "The generation process is modeled as PÎ¸(y | x, s), where the model Î¸ is constrained to complete the proof y given the enforced structural start s.",
    "execution_environment": "ç‰¹å®šçš„Lean 4å’ŒMathlibç¯å¢ƒï¼Œéœ€è¦ç¼–è¯‘éªŒè¯ã€‚",
    "execution_environment_quote": "We compile using lake env lean --json inside a pinned Mathlib environment (Appendix B) with a per-attempt timeout of 60 seconds. ... â€¢ Lean version: Lean 4 v4.27.0-rc1 â€¢ Mathlib commit: 3c327186024184e988ebbcae1b7d7795eaacdee3",
    "unique_features": "è¯¥åŸºå‡†ä¸“æ³¨äºå½¢å¼åŒ–å®šç†è¯æ˜ï¼Œä½¿ç”¨Leanè¯æ˜åŠ©æ‰‹è¿›è¡Œä¸¥æ ¼çš„ç¼–è¯‘éªŒè¯ï¼Œæ˜¯è¯„ä¼°ç¥ç»å®šç†è¯æ˜å™¨æ€§èƒ½çš„å…³é”®æµ‹è¯•é›†ã€‚",
    "unique_features_quote": "Systems leveraging reinforcement learning (RL) and massive tree searches have achieved strong results on formal proof benchmarks... We evaluate on miniF2F-test (Lean 4 split; 244 theorems)...",
    "data_size_quantity": 244,
    "data_size_unit": "ä¸ªå®šç†",
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Lean 4']",
    "dimension_normalized": "['å½¢å¼åŒ–å®šç†è¯æ˜çš„æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['pass@16']",
    "problem_domain_normalized": "['å½¢å¼åŒ–æ•°å­¦å®šç†è¯æ˜', 'æ•°å­¦é¢†åŸŸ']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.15879_output/content.md",
    "benchmark_name": "C3-Bench (Controllable Code Completion Benchmark)",
    "benchmark_name_quote": "we present the first instruction-guided code completion benchmark, Controllable Code Completion Benchmark (C3-Bench), comprising 2,195 carefully designed completion tasks.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "To address this limitation, we present the first instruction-guided code completion benchmark, Controllable Code Completion Benchmark (C3-Bench)...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°ä»£ç å¤§æ¨¡å‹åœ¨ä»£ç è¡¥å…¨ä»»åŠ¡ä¸­éµå¾ªç”¨æˆ·ç»†ç²’åº¦æŒ‡ä»¤çš„èƒ½åŠ›ï¼Œå³â€œå¯æ§ä»£ç è¡¥å…¨â€ã€‚å®ƒæ‰©å±•äº†ä¼ ç»Ÿçš„ä»£ç è¡¥å…¨ï¼Œè¦æ±‚æ¨¡å‹ä¸ä»…åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆä¸­é—´ä»£ç ï¼Œè¿˜è¦éµå¾ªç‰¹å®šçš„å®ç°è¦æ±‚æˆ–è§„æ¨¡è¦æ±‚ã€‚",
    "task_description_quote": "CCC extends traditional code completion by incorporating diverse middle code variants and fine-grained control instructions. This enhancement enables comprehensive assessment of both functional correctness and instruction adherence...",
    "dimension": "åŠŸèƒ½æ­£ç¡®æ€§ä¸æŒ‡ä»¤éµå¾ªèƒ½åŠ›",
    "dimension_quote": "This enhancement enables comprehensive assessment of both functional correctness and instruction adherence...",
    "evaluation_method": "è‡ªåŠ¨è¯„åˆ†æœºåˆ¶ï¼Œæ— éœ€äººå·¥å¹²é¢„ã€‚åŒ…å«ä¸¤ç§ä¸»è¦è¯„ä¼°æœºåˆ¶ï¼šå®ç°æ§åˆ¶è¡¥å…¨ï¼ˆICCï¼‰å’Œè§„æ¨¡æ§åˆ¶è¡¥å…¨ï¼ˆSCCï¼‰ã€‚ICCé€šè¿‡å•å…ƒæµ‹è¯•éªŒè¯åŠŸèƒ½æ­£ç¡®æ€§ï¼Œå¹¶é€šè¿‡æµ‹é‡ç”Ÿæˆä»£ç ä¸æŒ‡ä»¤è¦æ±‚çš„å¯¹é½åº¦æ¥è¯„ä¼°æŒ‡ä»¤éµå¾ªã€‚SCCä¾§é‡äºç»“æ„ç¬¦åˆæ€§è€ŒéåŠŸèƒ½å®Œæ•´æ€§ï¼Œå› æ­¤ä¸ä½¿ç”¨å•å…ƒæµ‹è¯•éªŒè¯ã€‚",
    "evaluation_method_quote": "The benchmark implements two primary evaluation mechanisms: Implementation-Control Completion (ICC)... Scale-Control Completion (SCC)... Notably, C3-Bench employs automated scoring mechanisms, ensuring objective evaluation without human intervention.",
    "context_dependency": "è€ƒè™‘å·¦å³ä»£ç ä¸Šä¸‹æ–‡çš„ä¸­é—´ä»£ç æ®µè¡¥å…¨",
    "context_dependency_quote": "Code completion represents a specialized code generation task that requires models to generate intermediate code segments while considering both left and right context.",
    "problem_domain": "é€šç”¨ç¼–ç¨‹",
    "problem_domain_quote": NaN,
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "Python",
    "language_quote": "C3-Bench comprises 2,195 high-quality Python CCC instances...",
    "data_size": "åŒ…å«2,195ä¸ªé«˜è´¨é‡çš„å¯æ§ä»£ç è¡¥å…¨å®ä¾‹ï¼Œå…¶ä¸­1,286ä¸ªä¸ºICCä»»åŠ¡ï¼Œ909ä¸ªä¸ºSCCä»»åŠ¡ã€‚",
    "data_size_quote": "C3-Bench comprises 2,195 high-quality Python CCC instances, encompassing 1,286 ICC and 909 SCC task instances, respectively.",
    "source_type": "åŸºäºä¸¤ä¸ªå¹¿æ³›ä½¿ç”¨çš„é«˜è´¨é‡ä»£ç è¯„ä¼°æ•°æ®é›†HumanEvalå’ŒSAFIMæ„å»ºï¼Œé€šè¿‡æŠ½è±¡è¯­æ³•æ ‘æå–ä¸­é—´ä»£ç æ®µå¹¶ç”Ÿæˆç­‰æ•ˆå®ç°å˜ä½“ã€‚",
    "source_type_quote": "The dataset and its associated unit tests are derived from two widely-used, high-quality code evaluation datasets: HumanEval [8] and SAFIM [10].",
    "last_updated": "2026-01-22 (è®ºæ–‡é¢„å°æœ¬æ—¥æœŸ)",
    "last_updated_quote": "arXiv:2601.15879v1  [cs.SE]  22 Jan 2026",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "we present the first instruction-guided code completion benchmark, Controllable Code Completion Benchmark (C3-Bench)...",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "å¯æ§ä»£ç è¡¥å…¨",
    "task_granularity_quote": "We propose the concept of Controllable Code Completion (CCC)... CCC extends traditional code completion by incorporating diverse middle code variants and fine-grained control instructions.",
    "evaluation_metrics": "åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆé€šè¿‡å•å…ƒæµ‹è¯•ï¼‰å’ŒæŒ‡ä»¤éµå¾ªå¯¹é½åº¦",
    "evaluation_metrics_quote": "The evaluation encompasses two aspects: functional correctness, verified through Ti, and instruction adherence, assessed by measuring the alignment between the implementation approach in Gi and the requirements specified in Ii.",
    "input_modality": "ä»£ç ä¸Šä¸‹æ–‡ï¼ˆå‰ç¼€å’Œåç¼€ä»£ç ï¼‰ä¸è‡ªç„¶è¯­è¨€æŒ‡ä»¤",
    "input_modality_quote": "A CCC instance augments this framework by incorporating I (Fine-Grained Instruction), which specifies implementation requirements, thus forming a tuple (P, S, G, T, I).",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "Given a dataset {(Pi, Si, Gi, Ti, Ii)}, we train an LLM M to generate completions such that M(Pi, Si, Ii) â†’Gi.",
    "task_io_type": "ä»£ç ä¸è‡ªç„¶è¯­è¨€æŒ‡ä»¤åˆ°ä»£ç ",
    "task_io_type_quote": "CCC extends traditional code completion by incorporating diverse middle code variants and fine-grained control instructions.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "é¦–ä¸ªæŒ‡ä»¤å¼•å¯¼çš„ä»£ç è¡¥å…¨è¯„æµ‹åŸºå‡†ï¼Œä¸“æ³¨äºè¯„ä¼°æ¨¡å‹åœ¨è¡¥å…¨è¿‡ç¨‹ä¸­éµå¾ªç”¨æˆ·æŒ‡ä»¤çš„èƒ½åŠ›ã€‚åŒ…å«ä¸¤ç§ä»»åŠ¡ç±»å‹ï¼šå®ç°æ§åˆ¶è¡¥å…¨ï¼ˆICCï¼Œå«ç»“æ„è§„èŒƒã€ç®—æ³•å®ç°ã€æ§åˆ¶æµã€å…³é”®å‚æ•°å››ç±»è¦æ±‚ï¼‰å’Œè§„æ¨¡æ§åˆ¶è¡¥å…¨ï¼ˆSCCï¼Œå«è¡Œè·¨åº¦ã€å¤šè¡Œã€è¯­å¥å—ä¸‰ç±»è¦æ±‚ï¼‰ã€‚è¶…è¿‡50%çš„ICCæ¡ˆä¾‹åŒ…å«åŒä¸€ä»£ç ä¸Šä¸‹æ–‡çš„ä¸‰ç§ä¸åŒå®ç°ã€‚",
    "unique_features_quote": "we present the first instruction-guided benchmark, Controllable Code Completion Benchmark... The benchmark implements two primary evaluation mechanisms: Implementation-Control Completion (ICC)... Scale-Control Completion (SCC)... The resulting ICC dataset incorporates these validated implementations, with over 50% of cases containing three distinct implementations for the same code context.",
    "data_size_quantity": 2195,
    "data_size_unit": "ä¸ªå®ä¾‹",
    "last_updated_year": 2026,
    "last_updated_month": 1,
    "last_updated_day": 22,
    "language_normalized": "['Python']",
    "dimension_normalized": "['åŠŸèƒ½æ­£ç¡®æ€§ä¸æŒ‡ä»¤éµå¾ªèƒ½åŠ›']",
    "evaluation_method_normalized": "['åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆé€šè¿‡å•å…ƒæµ‹è¯•ï¼‰å’ŒæŒ‡ä»¤éµå¾ªå¯¹é½åº¦']",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹']",
    "source_type_normalized": "['åŸºäºä¸¤ä¸ªå¹¿æ³›ä½¿ç”¨çš„é«˜è´¨é‡ä»£ç è¯„ä¼°æ•°æ®é›†HumanEvalå’ŒSAFIMæ„å»ºï¼Œé€šè¿‡æŠ½è±¡è¯­æ³•æ ‘æå–ä¸­é—´ä»£ç æ®µå¹¶ç”Ÿæˆç­‰æ•ˆå®ç°å˜ä½“ã€‚']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç è¡¥å…¨",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.16863_output/content.md",
    "benchmark_name": "AIME 2025, LiveCodeBench, DarkBench",
    "benchmark_name_quote": "Empirical validation on challenging benchmarks (AIME 2025, LiveCodeBench) demonstrates that this topology allows ensembles of small (<20B) consumer-grade models to match or exceed the performance of state-of-the-art 100B+ parameter models. Furthermore, testing on the DarkBench safety suite [15] reveals that the topological governance mechanism inherently reduces sycophancy...",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Empirical validation on challenging benchmarks (AIME 2025, LiveCodeBench) demonstrates that this topology allows ensembles of small (<20B) consumer-grade models to match or exceed the performance of state-of-the-art 100B+ parameter models.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "æ–‡ä¸­æœªè¯¦ç»†æè¿°è¿™äº›åŸºå‡†çš„å…·ä½“ä»»åŠ¡ã€‚æœ¬æ–‡ä¸»è¦ä½¿ç”¨å®ƒä»¬æ¥éªŒè¯æ‰€æå‡ºçš„NSEDåè®®çš„æ€§èƒ½ã€‚",
    "task_description_quote": "Empirical validation on challenging benchmarks (AIME 2025, LiveCodeBench) demonstrates that this topology allows ensembles of small (<20B) consumer-grade models to match or exceed the performance of state-of-the-art 100B+ parameter models.",
    "dimension": "æ–‡ä¸­æœªè¯¦ç»†æè¿°è¿™äº›åŸºå‡†çš„è¯„æµ‹ç»´åº¦ã€‚æœ¬æ–‡ä½¿ç”¨å®ƒä»¬æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼ˆå¯èƒ½åŒ…æ‹¬ä»£ç ç”Ÿæˆèƒ½åŠ›ã€å®‰å…¨æ€§ç­‰ï¼‰ã€‚",
    "dimension_quote": "Empirical validation on challenging benchmarks (AIME 2025, LiveCodeBench) demonstrates that this topology allows ensembles of small (<20B) consumer-grade models to match or exceed the performance of state-of-the-art 100B+ parameter models. Furthermore, testing on the DarkBench safety suite [15] reveals that the topological governance mechanism inherently reduces sycophancy...",
    "evaluation_method": NaN,
    "evaluation_method_quote": NaN,
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": NaN,
    "problem_domain_quote": NaN,
    "problem_difficulty": "å…·æœ‰æŒ‘æˆ˜æ€§çš„ï¼ˆchallengingï¼‰",
    "problem_difficulty_quote": "Empirical validation on challenging benchmarks (AIME 2025, LiveCodeBench)...",
    "language": NaN,
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": "2025 (AIME 2025)",
    "last_updated_quote": "Empirical validation on challenging benchmarks (AIME 2025, LiveCodeBench)...",
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": NaN,
    "task_granularity_quote": NaN,
    "evaluation_metrics": NaN,
    "evaluation_metrics_quote": NaN,
    "input_modality": NaN,
    "input_modality_quote": NaN,
    "output_modality": NaN,
    "output_modality_quote": NaN,
    "task_io_type": NaN,
    "task_io_type_quote": NaN,
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "DarkBenchè¢«æè¿°ä¸ºä¸€ä¸ªå®‰å…¨æµ‹è¯•å¥—ä»¶ï¼ˆsafety suiteï¼‰ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹åœ¨å‡å°‘å¥‰æ‰¿ï¼ˆsycophancyï¼‰æ–¹é¢çš„èƒ½åŠ›ã€‚",
    "unique_features_quote": "Furthermore, testing on the DarkBench safety suite [15] reveals that the topological governance mechanism inherently reduces sycophancy...",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆèƒ½åŠ›', 'å®‰å…¨æ€§']",
    "evaluation_method_normalized": "[]",
    "problem_domain_normalized": "[]",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "ç«èµ›çº§",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.16809_output/content.md",
    "benchmark_name": "AIDev",
    "benchmark_name_quote": "We utilize the AIDev dataset [30], a large-scale collection of agent-authored pull requests (PRs) from real-world GitHub repositories.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We utilize the AIDev dataset [30], a large-scale collection of agent-authored pull requests (PRs) from real-world GitHub repositories.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ã€æ–‡ä¸­ä»…æåŠå®éªŒå­é›†ï¼Œæœªæè¿°å®Œæ•´æ•°æ®é›†ã€‘æœ¬æ–‡ä½¿ç”¨è¯¥æ•°æ®é›†æ¥è¿½è¸ªAIä»£ç†ç”Ÿæˆä»£ç åœ¨å¼€æºé¡¹ç›®ä¸­çš„é•¿æœŸç”Ÿå­˜æƒ…å†µï¼Œä½†æœªæè¿°AIDevæ•°æ®é›†æœ¬èº«è®¾è®¡çš„åŸå§‹ä»»åŠ¡ã€‚",
    "task_description_quote": "We investigate this hypothesis through survival analysis of 201 open-source projects, tracking over 200,000 code units authored by AI agents versus humans.",
    "dimension": "ä»£ç çš„é•¿æœŸç”Ÿå­˜èƒ½åŠ›å’Œæ¼”åŒ–ç‰¹æ€§",
    "dimension_quote": "We investigate this hypothesis through survival analysis... tracking over 200,000 code units authored by AI agents versus humans.",
    "evaluation_method": "ç”Ÿå­˜åˆ†æï¼ˆSurvival Analysisï¼‰ï¼ŒåŒ…æ‹¬é£é™©æ¯”ï¼ˆHazard Ratioï¼‰ã€ä¿®æ”¹ç‡ã€ä¸­ä½ç”Ÿå­˜æ—¶é—´ç­‰",
    "evaluation_method_quote": "We address this gap using survival analysis, which is a statistical framework from medicine and reliability engineering that models time-to-event data while handling right-censored observations [28].",
    "context_dependency": "å¤šæ–‡ä»¶é¡¹ç›®ï¼ˆæ¥è‡ª201ä¸ªå¼€æºé¡¹ç›®ï¼‰",
    "context_dependency_quote": "By tracking over 200,000 code units across 201 open-source projects from the AIDev dataset [30]...",
    "problem_domain": "é€šç”¨è½¯ä»¶å¼€å‘ï¼ˆå¤šè¯­è¨€ç”Ÿæ€ç³»ç»Ÿï¼‰",
    "problem_domain_quote": "The language distribution spans multiple ecosystems: Python (24%), TypeScript (22%), Go (9%), C# (7%), Rust (5%), with the remaining 33% distributed across C, C++, Java, PHP, and other languages.",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "Python, TypeScript, Go, C#, Rust, C, C++, Java, PHP ç­‰å¤šç§è¯­è¨€",
    "language_quote": "The language distribution spans multiple ecosystems: Python (24%), TypeScript (22%), Go (9%), C# (7%), Rust (5%), with the remaining 33% distributed across C, C++, Java, PHP, and other languages.",
    "data_size": "æœ€ç»ˆé˜Ÿåˆ—åŒ…å«201ä¸ªä»£ç ä»“åº“å’Œ5,171ä¸ªæ‹‰å–è¯·æ±‚ï¼ˆ3,003ä¸ªAIç”Ÿæˆï¼Œ2,168ä¸ªäººç±»ç”Ÿæˆï¼‰ï¼Œè¿½è¸ªäº†è¶…è¿‡200,000ä¸ªä»£ç å•å…ƒ",
    "data_size_quote": "After applying all filters, our final cohort comprises 201 repositories and 5,171 PRs (3,003 agent-authored, 2,168 human-authored)... tracking over 200,000 code units",
    "source_type": "æ¥è‡ªçœŸå®ä¸–ç•ŒGitHubä»“åº“çš„æ‹‰å–è¯·æ±‚",
    "source_type_quote": "We utilize the AIDev dataset [30], a large-scale collection of agent-authored pull requests (PRs) from real-world GitHub repositories.",
    "last_updated": "2025å¹´ï¼ˆæ•°æ®æ”¶é›†æˆªæ­¢æ—¥æœŸï¼‰",
    "last_updated_quote": "As reported in the AIDev dataset [30], the PR inclusion cutoff is August 1, 2025... observation end date (December 31, 2025)",
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": "æ ‡å‡†å¼€æºè½¯ä»¶è®¸å¯è¯ï¼ˆè¿‡æ»¤äº†æ— å£°æ˜æˆ–éè½¯ä»¶è®¸å¯è¯çš„ä»“åº“ï¼‰",
    "dataset_license_quote": "We exclude repositories without declared licenses or with non-software licenses (e.g., CC0, Unlicense, or â€œNoneâ€), restricting the analysis to standard open-source software.",
    "task_granularity": "ä»£ç ç”Ÿå­˜åˆ†æï¼ˆè¿½è¸ªä»£ç ä»åˆå¹¶åˆ°è¢«ä¿®æ”¹çš„æ—¶é—´ï¼‰",
    "task_granularity_quote": "We frame code modification as a survival analysis problem, where code â€œsurvivesâ€ until it is modified and â€œdiesâ€ when altered.",
    "evaluation_metrics": "é£é™©æ¯”ï¼ˆHazard Ratioï¼‰ã€ä¿®æ”¹ç‡ã€ä¸­ä½ç”Ÿå­˜æ—¶é—´ã€AUC-ROCã€Macro F1",
    "evaluation_metrics_quote": "agent-authored code is modified significantly less frequently than human-authored code (Hazard Ratio = 0.842 at the line level)... Bag-of-words textual features achieve AUC-ROC 0.671... predicting modification timing remains challenging (Macro F1 = 0.285)",
    "input_modality": "ä»£ç ï¼ˆæ¥è‡ªæ‹‰å–è¯·æ±‚çš„æºä»£ç æ–‡ä»¶ï¼‰",
    "input_modality_quote": "We track only source code files (e.g., .py, .js, .java, .cpp, .rs) and exclude configuration and documentation files.",
    "output_modality": "ç”Ÿå­˜åˆ†æç»“æœï¼ˆä¿®æ”¹äº‹ä»¶ã€æ—¶é—´ã€æ„å›¾ï¼‰",
    "output_modality_quote": "We structure our investigation around three research questions: RQ1 (Survival)... RQ2 (Intent)... RQ3 (Forecasting)...",
    "task_io_type": "ä»£ç åˆ°åˆ†æï¼ˆè¾“å…¥æ˜¯ä»£ç ï¼Œè¾“å‡ºæ˜¯å¯¹å…¶ç”Ÿå­˜ç‰¹æ€§çš„åˆ†æï¼‰",
    "task_io_type_quote": "We investigate this hypothesis through survival analysis of 201 open-source projects, tracking over 200,000 code units authored by AI agents versus humans.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºAIç”Ÿæˆä»£ç åœ¨çœŸå®å¼€æºé¡¹ç›®ä¸­çš„é•¿æœŸç”Ÿå­˜å’Œæ¼”åŒ–åˆ†æï¼Œè€Œéå³æ—¶ç”Ÿæˆæ­£ç¡®æ€§ï¼›ä½¿ç”¨ç”Ÿå­˜åˆ†ææ¡†æ¶ï¼›åŒºåˆ†æ–‡ä»¶çº§å’Œè¡Œçº§ç²’åº¦ï¼›åˆ†æä¿®æ”¹æ„å›¾ï¼ˆçº æ­£æ€§ã€é€‚åº”æ€§ç­‰ï¼‰",
    "unique_features_quote": "This work makes the following contributions: â€¢ First Survival Analysis of AI Code: To the best of our knowledge, this is the first application of time-to-event methods to track individual agent-generated code units from birth through modification in production repositories.",
    "data_size_quantity": 200000,
    "data_size_unit": "ä¸ªä»£ç å•å…ƒ",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python', 'TypeScript', 'Go', 'C#', 'Rust', 'C', 'C++', 'Java', 'PHP', 'å¤šç§è¯­è¨€']",
    "dimension_normalized": "['ä»£ç çš„é•¿æœŸç”Ÿå­˜èƒ½åŠ›å’Œæ¼”åŒ–ç‰¹æ€§']",
    "evaluation_method_normalized": "['é£é™©æ¯”ï¼ˆHazard Ratioï¼‰', 'ä¿®æ”¹ç‡', 'ä¸­ä½ç”Ÿå­˜æ—¶é—´', 'AUC-ROC', 'Macro F1']",
    "problem_domain_normalized": "['é€šç”¨è½¯ä»¶å¼€å‘ï¼ˆå¤šè¯­è¨€ç”Ÿæ€ç³»ç»Ÿï¼‰']",
    "source_type_normalized": "['æ¥è‡ªçœŸå®ä¸–ç•ŒGitHubä»“åº“çš„æ‹‰å–è¯·æ±‚']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "JSON",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.16839_output/content.md",
    "benchmark_name": "AIDev",
    "benchmark_name_quote": "This data mining challenge focuses on AIDev, the first large-scale, openly available dataset capturing agent-authored pull requests (Agentic-PRs) from real-world GitHub repositories.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We base our analysis on the AIDev dataset [25], a large-scale dataset of around 933k agentic PRs spanning real-world GitHub repositories and generated by five AI agents: OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„æµ‹åŸºå‡†æ—¨åœ¨è¯„ä¼°AIç¼–ç åŠ©æ‰‹åœ¨æ„å»ºç³»ç»Ÿï¼ˆå¦‚Maven, Gradle, CMake, Makeï¼‰ä¸­ç”Ÿæˆçš„æ„å»ºä»£ç çš„è´¨é‡ï¼Œå…·ä½“å…³æ³¨å…¶å¼•å…¥æˆ–æ¶ˆé™¤ä»£ç å¼‚å‘³ï¼ˆcode smellsï¼‰çš„èƒ½åŠ›ï¼Œä»¥åŠå¼€å‘è€…å¯¹AIç”Ÿæˆçš„æ„å»ºä»£ç çš„æ¥å—ç¨‹åº¦ã€‚",
    "task_description_quote": "This data mining challenge focuses on AIDev, the first large-scale, openly available dataset capturing agent-authored pull requests (Agentic-PRs) from real-world GitHub repositories. Our paper leverages this dataset to investigate (RQ1) whether AI coding agents generate build code with quality issues (e.g., code smells), (RQ2) to what extent AI agents can eliminate code smells from build code, and (RQ3) to what extent Agentic-PRs are accepted by developers.",
    "dimension": "æ„å»ºä»£ç è´¨é‡ï¼ˆå¯ç»´æŠ¤æ€§ã€å®‰å…¨æ€§ï¼‰ã€å¼€å‘è€…æ¥å—åº¦",
    "dimension_quote": "We identified 364 maintainability- and security-related build smells across varying severity levels... Notably, more than 61% of Agentic-PRs are approved and merged with minimal human intervention.",
    "evaluation_method": "ä½¿ç”¨é™æ€åˆ†æå·¥å…·Snifferæ£€æµ‹æ„å»ºä»£ç ä¸­çš„ä»£ç å¼‚å‘³ï¼Œå¹¶è¿›è¡Œäººå·¥æ ‡æ³¨å’Œå®šæ€§åˆ†æã€‚",
    "evaluation_method_quote": "To identify code smells, Sniffer [9] analyzes each build file independently... To answer RQ2, we focused on build-related changes in which code smells were eliminated. Two authors individually examined the build code changes, their associated commit history, andâ€”when availableâ€”developersâ€™ comments and discussions.",
    "context_dependency": "çœŸå®GitHubä»“åº“ä¸­çš„æ‹‰å–è¯·æ±‚ï¼ˆPRsï¼‰ï¼Œæ¶‰åŠæ„å»ºæ–‡ä»¶çš„ä¿®æ”¹ã€‚",
    "context_dependency_quote": "We base our analysis on the AIDev dataset [25], a large-scale dataset of around 933k agentic PRs spanning real-world GitHub repositories... We kept only PRs that changed at least one build file across the studied build systems...",
    "problem_domain": "è½¯ä»¶æ„å»ºç³»ç»Ÿï¼ˆBuild Systemsï¼‰ï¼ŒåŒ…æ‹¬ä¾èµ–ç®¡ç†ã€ç¼–è¯‘ã€æ‰“åŒ…å’Œéƒ¨ç½²ã€‚",
    "problem_domain_quote": "Modern software development relies heavily on build tools such as Maven [33], Gradle [10], and Cmake [39] to manage dependencies, compilation, packaging, and deployment...",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "æ„å»ºè„šæœ¬è¯­è¨€ï¼ˆMavençš„XML, Gradleçš„Groovy/Kotlin DSL, CMake, Makefileï¼‰",
    "language_quote": "We kept only PRs that changed at least one build file across the studied build systems, such as Maven (XML-based pom.xml), Gradle (.gradle and .gradle.kts), CMake (CMakeLists.txt and .cmake files), and Make (Makefile and .mk).",
    "data_size": "æœ€ç»ˆæ•°æ®é›†åŒ…å«387ä¸ªç”±AIä»£ç†ç”Ÿæˆçš„æ„å»ºç›¸å…³PRsï¼Œæ¶‰åŠ945ä¸ªæ„å»ºæ–‡ä»¶ã€‚",
    "data_size_quote": "We obtained a final dataset of 387 PRs containing authentic build-system changes authored by AI agents... Next, we retrieved each fileâ€™s content both before and after the AI-generated change, yielding a total of 945 files.",
    "source_type": "æ¥è‡ªçœŸå®GitHubä»“åº“çš„AIä»£ç†ç”Ÿæˆçš„æ‹‰å–è¯·æ±‚ï¼ˆAgentic-PRsï¼‰ã€‚",
    "source_type_quote": "We base our analysis on the AIDev dataset [25], a large-scale dataset of around 933k agentic PRs spanning real-world GitHub repositories and generated by five AI agents...",
    "last_updated": "2026",
    "last_updated_quote": "Anwar Ghammam and Mohamed Almukhtar. 2026. AI builds, We Analyze: An Empirical Study of AI-Generated Build Code Quality.",
    "build_type": "åŸºäºç°æœ‰AIDevæ•°æ®é›†è¿›è¡Œç­›é€‰å’Œæ„å»ºã€‚",
    "build_type_quote": "We base our analysis on the AIDev dataset [25]... To ensure that our dataset captures code changes related to build code, we filtered the AIDev dataset based on the modified files in each agentic PR.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "æ„å»ºä»£ç çš„ç”Ÿæˆä¸ä¿®æ”¹ï¼ˆåœ¨æ‹‰å–è¯·æ±‚çº§åˆ«ï¼‰ã€‚",
    "task_granularity_quote": "We kept only PRs that changed at least one build file across the studied build systems...",
    "evaluation_metrics": "ä»£ç å¼‚å‘³ï¼ˆCode Smellsï¼‰çš„å¼•å…¥/æ¶ˆé™¤æ•°é‡ã€ä¸¥é‡æ€§åˆ†å¸ƒã€æ‹‰å–è¯·æ±‚åˆå¹¶ç‡ã€‚",
    "evaluation_metrics_quote": "We identified 364 maintainability- and security-related build smells across varying severity levels... Notably, more than 61% of Agentic-PRs are approved and merged with minimal human intervention.",
    "input_modality": "æ„å»ºæ–‡ä»¶çš„åŸå§‹ç‰ˆæœ¬ï¼ˆä»£ç ï¼‰ã€‚",
    "input_modality_quote": "Next, we retrieved each fileâ€™s content both before and after the AI-generated change, obtaining paired snapshots that capture the exact AI-changes applied to the build code.",
    "output_modality": "æ„å»ºæ–‡ä»¶çš„ä¿®æ”¹åç‰ˆæœ¬ï¼ˆä»£ç ï¼‰ã€‚",
    "output_modality_quote": "Next, we retrieved each fileâ€™s content both before and after the AI-generated change, obtaining paired snapshots that capture the exact AI-changes applied to the build code.",
    "task_io_type": "ä»£ç åˆ°ä»£ç ï¼ˆæ„å»ºä»£ç çš„ä¿®æ”¹ï¼‰ã€‚",
    "task_io_type_quote": "We kept only PRs that changed at least one build file across the studied build systems...",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "é¦–ä¸ªä¸“æ³¨äºè¯„ä¼°AIç”Ÿæˆæ„å»ºä»£ç è´¨é‡çš„å¤§è§„æ¨¡å®è¯ç ”ç©¶æ•°æ®é›†ï¼Œè¦†ç›–å¤šç§ä¸»æµæ„å»ºç³»ç»Ÿï¼ˆMaven, Gradle, CMake, Makeï¼‰ï¼Œå¹¶å…³è”äº†ä»£ç å¼‚å‘³æ£€æµ‹å’Œå¼€å‘è€…æ¥å—åº¦åˆ†æã€‚",
    "unique_features_quote": "This data mining challenge focuses on AIDev, the first large-scale, openly available dataset capturing agent-authored pull requests (Agentic-PRs) from real-world GitHub repositories... The first empirical study to assess the quality of AI-generated build code.",
    "data_size_quantity": 387,
    "data_size_unit": "ä¸ª",
    "last_updated_year": 2026,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['æ„å»ºè„šæœ¬è¯­è¨€', 'Mavençš„XML', 'Gradleçš„Groovy/Kotlin DSL', 'CMake', 'Makefile']",
    "dimension_normalized": "['æ„å»ºä»£ç è´¨é‡', 'å¯ç»´æŠ¤æ€§', 'å®‰å…¨æ€§', 'å¼€å‘è€…æ¥å—åº¦']",
    "evaluation_method_normalized": "['ä»£ç å¼‚å‘³', 'Code Smells', 'å¼•å…¥/æ¶ˆé™¤æ•°é‡', 'ä¸¥é‡æ€§åˆ†å¸ƒ', 'æ‹‰å–è¯·æ±‚åˆå¹¶ç‡']",
    "problem_domain_normalized": "['è½¯ä»¶æ„å»ºç³»ç»Ÿ', 'Build Systems', 'ä¾èµ–ç®¡ç†', 'ç¼–è¯‘', 'æ‰“åŒ…', 'éƒ¨ç½²']",
    "source_type_normalized": "['çœŸå®GitHubä»“åº“', 'AIä»£ç†ç”Ÿæˆçš„æ‹‰å–è¯·æ±‚', 'Agentic-PRs']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.18282_output/content.md",
    "benchmark_name": "ToolBench",
    "benchmark_name_quote": "Evaluation on ToolBench across proprietary and open-source models demonstrates significant improvements in parameter generation accuracy and reasoning coherence for multi-parameter functions, while providing enhanced interpretability for debugging AI agent behaviors.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Evaluation on ToolBench across proprietary and open-source models demonstrates significant improvements... We evaluate TAFC on TOOLBENCH[15], which contains over 16,000 real-world REST APIs spanning 49 categories.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„æµ‹åŸºå‡†æ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨çœŸå®ä¸–ç•ŒREST APIä¸Šè¿›è¡Œå·¥å…·è°ƒç”¨å’Œå‡½æ•°è°ƒç”¨çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬å•å·¥å…·ã€ç±»åˆ«å†…å¤šå·¥å…·å’Œè·¨é›†åˆå¤šå·¥å…·ç­‰å¤æ‚åœºæ™¯ä¸‹çš„æ¨ç†ä¸åè°ƒèƒ½åŠ›ã€‚",
    "task_description_quote": "The benchmark provides three instruction types: I1-Inst (single-tool), I2-Inst (intra-category multi-tool), and I3-Inst (intra-collection multi-tool), testing increasingly complex reasoning and coordination capabilities.",
    "dimension": "å‡½æ•°è°ƒç”¨å‚æ•°ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œæ¨ç†é€æ˜åº¦",
    "dimension_quote": "Evaluation on ToolBench across proprietary and open-source models demonstrates significant improvements in parameter generation accuracy and reasoning coherence for multi-parameter functions...",
    "evaluation_method": "éµå¾ªToolEvalåè®®ï¼ŒæŠ¥å‘Šé€šè¿‡ç‡ï¼ˆPass Rateï¼‰å’Œèƒœç‡ï¼ˆWin Rateï¼‰ã€‚é€šè¿‡ç‡æŒ‡åœ¨å›ºå®šè®¡ç®—é¢„ç®—ä¸‹æˆåŠŸå®Œæˆä»»åŠ¡çš„æ¯”ä¾‹ï¼›èƒœç‡æŒ‡ç”±LLMæ³•å®˜å¯¹è§£å†³æ–¹æ¡ˆè·¯å¾„è¿›è¡Œä¸¤ä¸¤åå¥½æ¯”è¾ƒï¼Œè€ƒè™‘æ¨ç†è´¨é‡å’Œå‚æ•°æ­£ç¡®æ€§ã€‚",
    "evaluation_method_quote": "We follow the ToolEval protocol[15] and report: (1) Pass Rateâ€”successful task completion under a fixed computational budget...; (2) Win Rateâ€”pairwise preference comparison between solution paths by an LLM judge..., considering reasoning quality and parameter correctness.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "çœŸå®ä¸–ç•Œçš„REST APIè°ƒç”¨ï¼Œæ¶µç›–49ä¸ªç±»åˆ«",
    "problem_domain_quote": "TOOLBENCH[15], which contains over 16,000 real-world REST APIs spanning 49 categories.",
    "problem_difficulty": "åŒ…å«ä»å•å·¥å…·åˆ°è·¨é›†åˆå¤šå·¥å…·ç­‰ä¸åŒå¤æ‚åº¦çš„ä»»åŠ¡ï¼Œéš¾åº¦é€’å¢",
    "problem_difficulty_quote": "The benchmark provides three instruction types: I1-Inst (single-tool), I2-Inst (intra-category multi-tool), and I3-Inst (intra-collection multi-tool), testing increasingly complex reasoning and coordination capabilities.",
    "language": NaN,
    "language_quote": NaN,
    "data_size": "åŒ…å«è¶…è¿‡16,000ä¸ªçœŸå®ä¸–ç•Œçš„REST API",
    "data_size_quote": "TOOLBENCH[15], which contains over 16,000 real-world REST APIs spanning 49 categories.",
    "source_type": "çœŸå®ä¸–ç•Œçš„REST API",
    "source_type_quote": "TOOLBENCH[15], which contains over 16,000 real-world REST APIs spanning 49 categories.",
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "å‡½æ•°è°ƒç”¨ï¼ˆå·¥å…·è°ƒç”¨ï¼‰",
    "task_granularity_quote": "Function calling, which enables LLMs to invoke external functions with structured parameters, has emerged as a fundamental capability for building practical AI systems that can perform complex, multi-step tasks.",
    "evaluation_metrics": "é€šè¿‡ç‡ï¼ˆPass Rateï¼‰ã€èƒœç‡ï¼ˆWin Rateï¼‰",
    "evaluation_metrics_quote": "We follow the ToolEval protocol[15] and report: (1) Pass Rate...; (2) Win Rate...",
    "input_modality": "è‡ªç„¶è¯­è¨€æŒ‡ä»¤ï¼ˆå¯èƒ½åŒ…å«ä¸Šä¸‹æ–‡å’Œå‡½æ•°æè¿°ï¼‰",
    "input_modality_quote": "where x denotes user input and C represents the context (function descriptions, conversation history).",
    "output_modality": "ç»“æ„åŒ–çš„å‡½æ•°è°ƒç”¨å‚æ•°ï¼ˆå¯èƒ½é™„å¸¦æ¨ç†è¿‡ç¨‹ï¼‰",
    "output_modality_quote": "Function calling, which enables LLMs to invoke external functions with structured parameters...",
    "task_io_type": "è‡ªç„¶è¯­è¨€åˆ°ç»“æ„åŒ–å‡½æ•°è°ƒç”¨",
    "task_io_type_quote": "Function calling, which enables LLMs to invoke external functions with structured parameters...",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºè¯„ä¼°LLMåœ¨çœŸå®ä¸–ç•ŒREST APIä¸Šçš„å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼ŒåŒ…å«å•å·¥å…·ã€ç±»åˆ«å†…å¤šå·¥å…·å’Œè·¨é›†åˆå¤šå·¥å…·ä¸‰ç§å¤æ‚åº¦é€’å¢çš„æŒ‡ä»¤ç±»å‹ï¼Œå¹¶é‡‡ç”¨LLMä½œä¸ºæ³•å®˜è¿›è¡Œåå¥½è¯„ä¼°ã€‚",
    "unique_features_quote": "The benchmark provides three instruction types: I1-Inst (single-tool), I2-Inst (intra-category multi-tool), and I3-Inst (intra-collection multi-tool), testing increasingly complex reasoning and coordination capabilities. ... Win Rateâ€”pairwise preference comparison between solution paths by an LLM judge...",
    "data_size_quantity": 16000,
    "data_size_unit": "ä¸ª",
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['å‡½æ•°è°ƒç”¨å‚æ•°ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œæ¨ç†é€æ˜åº¦']",
    "evaluation_method_normalized": "['é€šè¿‡ç‡ï¼ˆPass Rateï¼‰', 'èƒœç‡ï¼ˆWin Rateï¼‰']",
    "problem_domain_normalized": "['çœŸå®ä¸–ç•Œçš„REST APIè°ƒç”¨ï¼Œæ¶µç›–49ä¸ªç±»åˆ«']",
    "source_type_normalized": "['çœŸå®ä¸–ç•Œçš„REST API']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "JSON",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.18749_output/content.md",
    "benchmark_name": "AIDev dataset",
    "benchmark_name_quote": "We begin with the AIDev dataset (Nov. 2025) released by Li et al. [15]",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We conduct a large-scale empirical analysis of 40,214 PRs collected from the AIDev dataset.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„æµ‹åŸºå‡†æ—¨åœ¨è¯„ä¼°AIä»£ç†è‡ªåŠ¨ç”Ÿæˆçš„Pull Requestï¼ˆPRï¼‰çš„è´¨é‡å’Œå¯æ¥å—æ€§ï¼Œå¹¶ä¸äººç±»å¼€å‘è€…åˆ›å»ºçš„PRè¿›è¡Œæ¯”è¾ƒã€‚",
    "task_description_quote": "The primary goal of this study is to enhance PR quality by leveraging the complementary strengths of humans and GenAI. To compare the factors that characterize the accepted PRs that were created by humans and agents...",
    "dimension": "Pull Requestçš„åˆå¹¶ç»“æœï¼ˆæ˜¯å¦è¢«æ¥å—ï¼‰ã€è´¨é‡ç‰¹å¾ã€ä»¥åŠå½±å“åˆå¹¶çš„å› ç´ ã€‚",
    "dimension_quote": "We extract 64 features across six families and fit statistical regression models to compare PR merge outcomes for human and agentic PRs, as well as across three AI agents.",
    "evaluation_method": "ä½¿ç”¨é€»è¾‘å›å½’æ¨¡å‹ï¼Œé€šè¿‡AUCã€ç²¾ç¡®åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°å’ŒBrieråˆ†æ•°ç­‰æŒ‡æ ‡è¯„ä¼°æ¨¡å‹é¢„æµ‹PRåˆå¹¶ç»“æœçš„èƒ½åŠ›ã€‚",
    "evaluation_method_quote": "We evaluate the discriminatory power of our models using AUC (0.5 = random, 1 = perfect), precision, recall, and F1 score (0 = worst, 1 = best), and calibration using the Brier score (0 = best, 1 = worst), via five-fold cross-validation.",
    "context_dependency": "åŸºäºçœŸå®GitHubä»“åº“çš„Pull Requestï¼ŒåŒ…å«ä»£ç å˜æ›´ã€æäº¤ã€è¯„è®ºã€å®¡æŸ¥ã€æ—¶é—´çº¿äº‹ä»¶å’Œç”¨æˆ·ä¿¡æ¯ç­‰å®Œæ•´ä¸Šä¸‹æ–‡ã€‚",
    "context_dependency_quote": "Of these, 33,596 include rich metadata such as comments, issues, reviews, commits, repositories, timelines, and user information.",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ï¼Œç‰¹åˆ«æ˜¯åä½œè½¯ä»¶å¼€å‘ä¸­çš„Pull Requestæµç¨‹ã€‚",
    "problem_domain_quote": "Pull Request, AI Agent, Empirical Study, Software Engineering, Human-AI Collaboration",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "æ–‡ä¸­æœªæ˜ç¡®æåŠæ•°æ®é›†æ¶µç›–çš„ç¼–ç¨‹è¯­è¨€ï¼Œä»…æåŠæ•°æ®æ¥æºä¸ºGitHub PRã€‚",
    "language_quote": NaN,
    "data_size": "åŒ…å«æ€»è®¡40,214ä¸ªPRï¼Œå…¶ä¸­33,596ä¸ªä¸ºAIä»£ç†ç”Ÿæˆçš„PRï¼ˆAgentic PRsï¼‰ï¼Œ6,618ä¸ªä¸ºäººç±»å¼€å‘è€…åˆ›å»ºçš„PRï¼ˆHuman PRsï¼‰ã€‚",
    "data_size_quote": "we first extract 64 features across 6 families from a dataset of 6,618 human-authored PRs and 33,596 agentic PRs from the AIDev dataset [15].",
    "source_type": "æ•°æ®æ¥æºäºGitHubï¼Œç”±Liç­‰äººæ„å»ºçš„AIDevæ•°æ®é›†ï¼Œå…¶ä¸­AIä»£ç†PRç”±äº”ä¸ªä¸»è¦çš„è‡ªä¸»AIä»£ç†åˆ›å»ºã€‚",
    "source_type_quote": "We begin with the AIDev dataset (Nov. 2025) released by Li et al. [15], which consists of 932,791 Agentic PRs created by five major autonomous AI agents.",
    "last_updated": "2025å¹´11æœˆ",
    "last_updated_quote": "We begin with the AIDev dataset (Nov. 2025) released by Li et al. [15]",
    "build_type": "ç”±ç ”ç©¶äººå‘˜ï¼ˆLiç­‰äººï¼‰æ„å»ºçš„å­¦æœ¯æ•°æ®é›†ã€‚",
    "build_type_quote": "released by Li et al. [15]",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "Pull Requestçº§åˆ«çš„åˆ†æï¼Œæ¶‰åŠä»£ç å˜æ›´ã€æè¿°ã€æäº¤è€…å±æ€§ã€ä»“åº“å±æ€§ã€é—®é¢˜é“¾æ¥å’Œå®¡æŸ¥è®¨è®ºç­‰å¤šä¸ªç»´åº¦ã€‚",
    "task_granularity_quote": "We extract 64 features across six families... These features include change set size, number of files added/deleted/changed, task types, external links, and AI agent type...",
    "evaluation_metrics": "AUCã€ç²¾ç¡®åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°ã€Brieråˆ†æ•°ã€‚",
    "evaluation_metrics_quote": "We evaluate the discriminatory power of our models using AUC (0.5 = random, 1 = perfect), precision, recall, and F1 score (0 = worst, 1 = best), and calibration using the Brier score (0 = best, 1 = worst)...",
    "input_modality": "Pull Requestçš„å…ƒæ•°æ®ï¼ŒåŒ…æ‹¬ä»£ç å˜æ›´ã€æ–‡æœ¬æè¿°ã€æäº¤è€…ä¿¡æ¯ã€ä»“åº“ä¿¡æ¯ã€é—®é¢˜é“¾æ¥å’Œå®¡æŸ¥è®¨è®ºæ–‡æœ¬ã€‚",
    "input_modality_quote": "rich metadata such as comments, issues, reviews, commits, repositories, timelines, and user information.",
    "output_modality": "äºŒå…ƒåˆ†ç±»ç»“æœï¼ˆPRè¢«åˆå¹¶æˆ–æœªè¢«åˆå¹¶ï¼‰ã€‚",
    "output_modality_quote": "we determine the merge status of each PR by checking if the merged_at field of a PR is NULL [15]. If so, the PR is considered not merged; otherwise, it is regarded as merged.",
    "task_io_type": "å¤šæ¨¡æ€å…ƒæ•°æ®åˆ°åˆ†ç±»ç»“æœ",
    "task_io_type_quote": "We extract 64 features across six families... to represent different dimensions of a PR, ranging from code changes to social interactions.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºAIä»£ç†ç”Ÿæˆçš„Pull Requestï¼ˆAgentic PRï¼‰ä¸äººç±»PRçš„å¯¹æ¯”åˆ†æï¼Œæå–äº†64ä¸ªç‰¹å¾ï¼Œæ¶µç›–å…­å¤§ç±»ï¼šPRå˜æ›´å¤§å°ä¸æäº¤ç‰¹å¾ã€PRæè¿°ã€æäº¤è€…å±æ€§ã€ä»“åº“å±æ€§ã€é—®é¢˜é“¾æ¥ä¸ä¸Šä¸‹æ–‡ã€å®¡æŸ¥ä¸è®¨è®ºã€‚",
    "unique_features_quote": "We extract 64 features across six families... These features include change set size, number of files added/deleted/changed, task types, external links, and AI agent type (e.g., OpenAI Codex, Copilot).",
    "data_size_quantity": 40214,
    "data_size_unit": "ä¸ªPR",
    "last_updated_year": 2025,
    "last_updated_month": 11,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['Pull Requestçš„åˆå¹¶ç»“æœ', 'è´¨é‡ç‰¹å¾', 'å½±å“åˆå¹¶çš„å› ç´ ']",
    "evaluation_method_normalized": "['AUC', 'ç²¾ç¡®åº¦', 'å¬å›ç‡', 'F1åˆ†æ•°', 'Brieråˆ†æ•°']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'åä½œè½¯ä»¶å¼€å‘ä¸­çš„Pull Requestæµç¨‹']",
    "source_type_normalized": "['GitHub', 'AIDevæ•°æ®é›†', 'AIä»£ç†PR']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.18418_output/content.md",
    "benchmark_name": "SWE-Bench Verified",
    "benchmark_name_quote": "We verify the modelâ€™s agentic capabilities on SWE-Bench Verified.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We verify the modelâ€™s agentic capabilities on SWE-Bench Verified. ... Evaluating our models on SWE-Bench Verified, we surpass the previous state-of-the-art open MT recipe...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è§£å†³ä»“åº“çº§åˆ«çš„è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ï¼Œä¾‹å¦‚ä¿®å¤bugæˆ–å®ç°åŠŸèƒ½ï¼Œéœ€è¦æ™ºèƒ½ä½“è‡ªä¸»ã€è¿­ä»£åœ°å¯¼èˆªå¤æ‚ä»£ç åº“ã€ç†è§£è·¨æ–‡ä»¶ä¾èµ–ã€åº”ç”¨ç¼–è¾‘å¹¶é€šè¿‡æµ‹è¯•æ‰§è¡ŒéªŒè¯æ›´æ”¹ã€‚",
    "task_description_quote": "This shift toward agentic software engineering (Jimenez et al., 2023; Badertdinov et al., 2025a; Wu et al., 2025) reflects the demands of real-world development, where resolving issues requires code agents to autonomously and iteratively navigate complex codebases, understand cross-file dependencies, apply edits, and validate changes through test execution.",
    "dimension": "æ™ºèƒ½ä½“è½¯ä»¶å·¥ç¨‹èƒ½åŠ›ï¼ŒåŒ…æ‹¬å¯¼èˆªã€ç¼–è¾‘ã€æµ‹è¯•å’Œè¿­ä»£é—®é¢˜è§£å†³ã€‚",
    "dimension_quote": "We verify the modelâ€™s agentic capabilities on SWE-Bench Verified.",
    "evaluation_method": "é€šè¿‡æµ‹è¯•å¥—ä»¶ï¼ˆè¯„ä¼°é¢„è¨€æœºï¼‰éªŒè¯æ›´æ”¹æ˜¯å¦æˆåŠŸï¼Œä½¿ç”¨è§£å†³ç‡ï¼ˆresolution rateï¼‰ä½œä¸ºæŒ‡æ ‡ã€‚",
    "evaluation_method_quote": "We formalize an agentic software engineering task as a tuple (R, q, E), where R is a repository state, q is a natural language problem description (e.g., bug report, issue), and E is an evaluation oracle (typically a test suite). ... Our best performing 32B and 72B models reach resolution rates of 56.1% and 58.5%, respectively.",
    "context_dependency": "å¤šæ–‡ä»¶é¡¹ç›®çº§åˆ«çš„ä¸Šä¸‹æ–‡ï¼Œæ¶‰åŠæ•´ä¸ªä»£ç ä»“åº“çš„çŠ¶æ€ã€‚",
    "context_dependency_quote": "The capabilities of code-generating large language models have rapidly expanded from synthesizing isolated functions (Jain et al., 2024; Wang et al., 2024c) to tackling repository-level software engineering tasks (Jimenez et al., 2023). ... where R is a repository state",
    "problem_domain": "é€šç”¨è½¯ä»¶å·¥ç¨‹ï¼ŒåŒ…æ‹¬bugä¿®å¤ã€åŠŸèƒ½å®ç°ç­‰ã€‚",
    "problem_domain_quote": "agentic software engineering ... resolving issues",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "æ–‡ä¸­æœªæ˜ç¡®æåŠSWE-Bench Verifiedæ‰€æ¶µç›–çš„ç¼–ç¨‹è¯­è¨€ã€‚",
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": "åŸºäºGitHub Pull Requestsï¼ˆPRsï¼‰ä¸­çš„çœŸå®è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ã€‚",
    "source_type_quote": "We materialize these principles through a large-scale data synthesis effort that leverages different elements from GitHub Pull Requests to construct two complementary data types...",
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "æ™ºèƒ½ä½“è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ï¼Œæ¶‰åŠå¤šæ­¥éª¤äº¤äº’ï¼ˆå®šä½ã€è¯»å–ã€ç¼–è¾‘ã€æµ‹è¯•ã€ä¿®è®¢ï¼‰ã€‚",
    "task_granularity_quote": "a typical development workflow follows the pattern: localize (identifying relevant files) â†’read (understanding code context) â†’edit (applying modifications) â†’test (validating changes) â†’revise (refining based on feedback).",
    "evaluation_metrics": "è§£å†³ç‡ï¼ˆresolution rateï¼‰ã€‚",
    "evaluation_metrics_quote": "Our best performing 32B and 72B models reach resolution rates of 56.1% and 58.5%, respectively.",
    "input_modality": "è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°ï¼ˆå¦‚bugæŠ¥å‘Šã€issueï¼‰å’Œä»£ç ä»“åº“çŠ¶æ€ã€‚",
    "input_modality_quote": "where q is a natural language problem description (e.g., bug report, issue)",
    "output_modality": "ä¸€ç³»åˆ—æ™ºèƒ½ä½“åŠ¨ä½œï¼ˆå¦‚æœç´¢æ–‡ä»¶ã€è¯»å–ä»£ç ã€åº”ç”¨ç¼–è¾‘ã€è¿è¡Œæµ‹è¯•ï¼‰ï¼Œæœ€ç»ˆç›®æ ‡æ˜¯ç”ŸæˆæˆåŠŸçš„ä»£ç æ›´æ”¹ã€‚",
    "output_modality_quote": "Actions correspond to tool calls such as searching for files, reading code, applying edits, or running tests",
    "task_io_type": "è‡ªç„¶è¯­è¨€å’Œä»£ç ä»“åº“çŠ¶æ€åˆ°ä¸€ç³»åˆ—æ™ºèƒ½ä½“åŠ¨ä½œå’Œä»£ç æ›´æ”¹ã€‚",
    "task_io_type_quote": "We formalize an agentic software engineering task as a tuple (R, q, E), where R is a repository state, q is a natural language problem description (e.g., bug report, issue), and E is an evaluation oracle (typically a test suite).",
    "execution_environment": "çœŸå®çš„å¼€å‘ç¯å¢ƒï¼ŒåŒ…æ‹¬Dockerç¯å¢ƒã€æ„å»ºç³»ç»Ÿã€æµ‹è¯•å¥—ä»¶å’Œlintersã€‚",
    "execution_environment_quote": "we construct 3.1B-token environmentally-native trajectories (Denv) from PR-derived software engineering tasks using their Docker environments and unit tests, generating agentic rollouts where our agent interacts with real build systems, test suites, and linters",
    "unique_features": "ä¸“æ³¨äºè¯„ä¼°æ™ºèƒ½ä½“åœ¨çœŸå®ã€åŠ¨æ€çš„è½¯ä»¶å·¥ç¨‹å·¥ä½œæµç¨‹ä¸­çš„èƒ½åŠ›ï¼Œå¼ºè°ƒå¤šæ­¥éª¤äº¤äº’å’Œä»æ‰§è¡Œåé¦ˆä¸­å­¦ä¹ ã€‚",
    "unique_features_quote": "This shift toward agentic software engineering ... reflects the demands of real-world development, where resolving issues requires code agents to autonomously and iteratively navigate complex codebases, understand crossæ–‡ä»¶ dependencies, apply edits, and validate changes through test execution.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['æ™ºèƒ½ä½“è½¯ä»¶å·¥ç¨‹èƒ½åŠ›', 'å¯¼èˆª', 'ç¼–è¾‘', 'æµ‹è¯•', 'è¿­ä»£é—®é¢˜è§£å†³']",
    "evaluation_method_normalized": "['è§£å†³ç‡']",
    "problem_domain_normalized": "['é€šç”¨è½¯ä»¶å·¥ç¨‹', 'bugä¿®å¤', 'åŠŸèƒ½å®ç°']",
    "source_type_normalized": "['GitHub Pull Requests', 'çœŸå®è½¯ä»¶å·¥ç¨‹ä»»åŠ¡']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.19825_output/content.md",
    "benchmark_name": "Spider-Route, Bird-Route",
    "benchmark_name_quote": "We name the extended datasets as Spider-Route and Bird-Route, respectively.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We construct realistic benchmarks by extending existing NL-to-SQL datasets. ... We construct a more realistic benchmark (Section III).",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "åœ¨åŒ…å«å¤šä¸ªæ•°æ®åº“çš„ä¼ä¸šç¯å¢ƒä¸­ï¼Œå°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è·¯ç”±åˆ°æœ€ç›¸å…³çš„æ•°æ®åº“ã€‚",
    "task_description_quote": "We address the task of routing natural language queries in multi-database enterprise environments. ... The task is to rank the DBs in D for the question q based on their relevance in terms of answerability (whether the DB can provide the correct answer).",
    "dimension": "æŸ¥è¯¢è·¯ç”±çš„å‡†ç¡®æ€§å’Œé²æ£’æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°æ®åº“é¢†åŸŸé‡å ã€æŸ¥è¯¢æ¨¡ç³Šçš„ç°å®åœºæ™¯ä¸‹çš„æ€§èƒ½ã€‚",
    "dimension_quote": "Our study shows that routing becomes increasingly challenging with larger, domain-overlapping DB repositories and ambiguous queries, motivating the need for more structured and robust reasoning-based solutions.",
    "evaluation_method": "ä½¿ç”¨å¬å›ç‡ï¼ˆrecallï¼‰å’Œå¹³å‡ç²¾åº¦å‡å€¼ï¼ˆmean average precision, mAPï¼‰ç­‰æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ã€‚",
    "evaluation_method_quote": "Consequently, the cross-domain setting exhibits disproportionately higher recall and mean average precision (mAP) compared to the in-domain setting...",
    "context_dependency": "ä¾èµ–äºå®Œæ•´çš„æ•°æ®åº“æ¨¡å¼ï¼ˆSchemaï¼‰ï¼ŒåŒ…æ‹¬è¡¨ã€åˆ—ã€æ•°æ®ç±»å‹ã€ä¸»å¤–é”®å…³ç³»ï¼Œä»¥åŠç‰¹å®šäºæ•°æ®åº“çš„å…ƒæ•°æ®ï¼ˆå¦‚åˆ—å’Œå€¼çš„æè¿°ï¼‰ã€‚",
    "context_dependency_quote": "Each DB is associated with comprehensive schema information, including table and column names, data types, and primary-foreign relationships, which is defined through SQL Data Definition Language (DDL) scripts. ... Along with the schema definition for the BIRD-Route dataset we use the meta-data in the form of column and value descriptions for richer schema representation.",
    "problem_domain": "è·¨é¢†åŸŸï¼ŒåŒ…æ‹¬åŒ»ç–—ã€é‡‘èã€æ•™è‚²ã€ä½“è‚²ç­‰å¤šä¸ªä¸“ä¸šé¢†åŸŸã€‚",
    "problem_domain_quote": "The BirdSQL [3] dataset is a collection of DBs spanning over a diverse set of domains such as medical, finance, education and sports.",
    "problem_difficulty": "å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿç°å®ä¼ä¸šç¯å¢ƒä¸­æ•°æ®åº“é¢†åŸŸé‡å ã€æŸ¥è¯¢æ¨¡ç³Šçš„å¤æ‚æƒ…å†µã€‚",
    "problem_difficulty_quote": "Our study shows that routing becomes increasingly challenging with larger, domain-overlapping DB repositories and ambiguous queries... The inferior performance observed relative to their original results demonstrates the increased difficulty and realism of our setting.",
    "language": "è‡ªç„¶è¯­è¨€ï¼ˆæŸ¥è¯¢ï¼‰å’ŒSQLï¼ˆæ•°æ®åº“æ¨¡å¼ï¼‰ã€‚",
    "language_quote": "We address the task of routing natural language queries... A sample in the original Spider dataset [2] consists of a DB, a NL question posed on the DB and the corresponding SQL query.",
    "data_size": "Spider-Route: 206ä¸ªæ•°æ®åº“ï¼Œ11,831ä¸ªé—®é¢˜ï¼ˆè®­ç»ƒ5,892ï¼Œæµ‹è¯•5,939ï¼‰ã€‚Bird-Route: 80ä¸ªæ•°æ®åº“ï¼Œ10,962ä¸ªé—®é¢˜ï¼ˆè®­ç»ƒ5,461ï¼Œæµ‹è¯•5,501ï¼‰ã€‚",
    "data_size_quote": "Spider route: Total Number of DBs 206, Total Questions 11,831, Train Questions 5,892, Test Questions 5,939. BIRD route: Total Number of DBs 80, Total Questions 10,962, Train Questions 5,461, Test Questions 5,501.",
    "source_type": "é€šè¿‡æ‰©å±•ç°æœ‰çš„è·¨é¢†åŸŸæ–‡æœ¬åˆ°SQLæ•°æ®é›†ï¼ˆSpiderå’ŒBIRD-SQLï¼‰æ„å»ºè€Œæˆã€‚",
    "source_type_quote": "We extend two existing datasets constructed for cross-domain NL-to-SQL, viz., Spider [2] and BIRD-SQL [3].",
    "last_updated": "2026-01-27 (æ ¹æ®arXivç‰ˆæœ¬æ—¥æœŸ)",
    "last_updated_quote": "arXiv:2601.19825v1  [cs.AI]  27 Jan 2026",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆæœ¬æ–‡ä½œè€…æ„å»ºï¼‰",
    "build_type_quote": "We construct realistic benchmarks by extending existing NL-to-SQL datasets. ... We construct a more realistic benchmark (Section III).",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "æŸ¥è¯¢è·¯ç”±ï¼ˆä¸ºè‡ªç„¶è¯­è¨€é—®é¢˜æ‰¾åˆ°æœ€ç›¸å…³çš„æ•°æ®åº“ï¼‰",
    "task_granularity_quote": "The task is to rank the DBs in D for the question q based on their relevance in terms of answerability (whether the DB can provide the correct answer).",
    "evaluation_metrics": "å¬å›ç‡ï¼ˆrecallï¼‰ï¼Œå¹³å‡ç²¾åº¦å‡å€¼ï¼ˆmAPï¼‰",
    "evaluation_metrics_quote": "Consequently, the cross-domain setting exhibits disproportionately higher recall and mean average precision (mAP) compared to the in-domain setting...",
    "input_modality": "è‡ªç„¶è¯­è¨€æŸ¥è¯¢",
    "input_modality_quote": "We address the task of routing natural language queries...",
    "output_modality": "æ•°æ®åº“æ’ååˆ—è¡¨",
    "output_modality_quote": "The task is to rank the DBs in D for the question q...",
    "task_io_type": "è‡ªç„¶è¯­è¨€åˆ°æ•°æ®åº“æ’å",
    "task_io_type_quote": "routing natural language queries... to appropriate data sources",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "1. é’ˆå¯¹ç°æœ‰åŸºå‡†çš„å±€é™æ€§ï¼ˆæ•°æ®åº“åº“è§„æ¨¡å°ã€åˆ†å¸ƒä¸å‡ï¼‰è¿›è¡Œäº†æ”¹è¿›ï¼Œæ„å»ºäº†æ›´ç°å®ã€æ›´å¹³è¡¡çš„åŸºå‡†ã€‚2. å®šä¹‰äº†ä¸¤ç§å®éªŒè®¾ç½®ï¼ˆåŸŸå†…å’Œè·¨åŸŸï¼‰ï¼Œå¹¶åœ¨ä¸¤ç§è®¾ç½®ä¸‹ä¿æŒå®Œå…¨ç›¸åŒçš„æ•°æ®åº“åº“ï¼Œä»¥å®ç°å…¬å¹³æ¯”è¾ƒã€‚3. åŸºäºä¸¤ä¸ªæˆç†Ÿçš„NL-to-SQLæ•°æ®é›†ï¼ˆSpiderå’ŒBIRDï¼‰æ„å»ºï¼Œç¡®ä¿äº†æ•°æ®è´¨é‡ã€‚",
    "unique_features_quote": "However, by preserving the original DB splits... the resulting repository size is extremely small and uneven across settings... This leads to a skewed distribution... Consequently, the cross-domain setting exhibits disproportionately higher recall and mean average precision (mAP) compared to the in-domain setting, which is counter-intuitive and highlights limitations of the benchmark... We construct a more realistic benchmark... We merge all DBs from both the train and test splits of the original datasets [2], [3] to form a unified DB repository and perform a 50â€“50% random split of the queries within each DB into train and test queries... This formulation preserves an identical repository across both settings and enables a fair and meaningful comparison.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2026,
    "last_updated_month": 1,
    "last_updated_day": 27,
    "language_normalized": "['è‡ªç„¶è¯­è¨€', 'SQL']",
    "dimension_normalized": "['æŸ¥è¯¢è·¯ç”±çš„å‡†ç¡®æ€§', 'é²æ£’æ€§']",
    "evaluation_method_normalized": "['å¬å›ç‡', 'å¹³å‡ç²¾åº¦å‡å€¼']",
    "problem_domain_normalized": "['åŒ»ç–—', 'é‡‘è', 'æ•™è‚²', 'ä½“è‚²']",
    "source_type_normalized": "['æ‰©å±•ç°æœ‰çš„è·¨é¢†åŸŸæ–‡æœ¬åˆ°SQLæ•°æ®é›†']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "JSON",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.20615_output/content.md",
    "benchmark_name": "DRAINCODE",
    "benchmark_name_quote": "This paper introduces DRAIN-CODE, the first adversarial attack targeting the computational efficiency of RAG-based code generation systems.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "This paper introduces DRAIN-CODE, the first adversarial attack targeting the computational efficiency of RAG-based code generation systems.",
    "dataset_url": "https://github.com/DeepSoftwareAnalytics/DrainCode",
    "dataset_url_quote": "We provide code and data at https://github.com/DeepSoftwareAnalytics/DrainCode.",
    "task_description": "è¯¥è¯„æµ‹åŸºå‡†æ—¨åœ¨è¯„ä¼°æ£€ç´¢å¢å¼ºä»£ç ç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿåœ¨é¢ä¸´ä¸Šä¸‹æ–‡æŠ•æ¯’æ”»å‡»æ—¶çš„è®¡ç®—æ•ˆç‡ï¼ˆå»¶è¿Ÿå’Œèƒ½è€—ï¼‰å®‰å…¨æ€§ã€‚å®ƒä¸æ˜¯ä¸€ä¸ªä¼ ç»Ÿçš„ä»£ç ç”Ÿæˆèƒ½åŠ›è¯„æµ‹é›†ï¼Œè€Œæ˜¯ä¸€ä¸ªæ”»å‡»æ¡†æ¶å’Œè¯„ä¼°é›†ï¼Œç”¨äºè¡¡é‡æ¨¡å‹åœ¨æ¶æ„æ„é€ çš„æ£€ç´¢ä¸Šä¸‹æ–‡ä¸‹ï¼Œç”Ÿæˆä»£ç çš„é•¿åº¦ã€å»¶è¿Ÿå’Œèƒ½è€—çš„å¢åŠ æƒ…å†µã€‚",
    "task_description_quote": "This paper introduces DRAIN-CODE, the first adversarial attack targeting the computational efficiency of RAG-based code generation systems. By strategically poisoning retrieval contexts through mutation-based approach, DRAINCODE forces LLMs to produce significantly longer outputs, thereby increasing GPU latency and energy consumption.",
    "dimension": "ä»£ç ç”Ÿæˆç³»ç»Ÿçš„è®¡ç®—æ•ˆç‡å®‰å…¨æ€§ï¼ˆéåŠŸèƒ½æ€§å®‰å…¨ï¼‰ï¼Œå…·ä½“åŒ…æ‹¬è¾“å‡ºé•¿åº¦ã€æ¨ç†å»¶è¿Ÿå’Œèƒ½è€—ã€‚",
    "dimension_quote": "targeting the computational efficiency of RAG-based code generation systems... increasing GPU latency and energy consumption.",
    "evaluation_method": "é€šè¿‡æŠ•æ¯’æ£€ç´¢ä¸Šä¸‹æ–‡ï¼Œè¿«ä½¿æ¨¡å‹ç”Ÿæˆæ›´é•¿çš„è¾“å‡ºï¼Œç„¶åæµ‹é‡è¾“å‡ºé•¿åº¦ã€æ¨ç†å»¶è¿Ÿå’Œèƒ½è€—çš„å¢åŠ ç™¾åˆ†æ¯”ï¼Œå¹¶ä¸åŸºçº¿ï¼ˆå¹²å‡€ä¸Šä¸‹æ–‡ï¼‰è¿›è¡Œæ¯”è¾ƒã€‚åŒæ—¶è¯„ä¼°æ”»å‡»åç”Ÿæˆä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚",
    "evaluation_method_quote": "Our experiments show that DRAINCODE achieves up to a 85% increase in latency, a 49% increase in energy consumption, and more than a 3Ã— increase in output length compared to the baseline... while maintaining 95â€“99% functional accuracy.",
    "context_dependency": "ä¾èµ–äºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„ä¸Šä¸‹æ–‡ï¼Œæ”»å‡»é€šè¿‡æ±¡æŸ“æ£€ç´¢è¯­æ–™åº“ä¸­çš„ä»£ç ç‰‡æ®µä¸Šä¸‹æ–‡æ¥å®ç°ã€‚",
    "context_dependency_quote": "By strategically poisoning retrieval contexts... We propose DRAINCODE, a RAG-based code generation attack framework that leverages the retrieval corpus to execute energy consumption attacks.",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹å®‰å…¨ï¼Œç‰¹åˆ«æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆåº”ç”¨ä¸­çš„å®‰å…¨æ€§ä¸èµ„æºæ¶ˆè€—ã€‚",
    "problem_domain_quote": "Stealthy Energy Consumption Attacks on Retrieval-Augmented Code Generation via Context Poisoning... making it useful for evaluating LLM security in resource-constrained environments.",
    "problem_difficulty": "å·¥ç¨‹çº§/å®‰å…¨ç ”ç©¶çº§ï¼Œæ¶‰åŠå¯¹RAGç³»ç»Ÿè¿›è¡Œå¯¹æŠ—æ€§æ”»å‡»ä»¥è¯„ä¼°å…¶é²æ£’æ€§ã€‚",
    "problem_difficulty_quote": "This is critical given LLMsâ€™ integration into IDEs and developer tooling, where frequent invocations can incur substantial computational costs.",
    "language": "æ–‡ä¸­æœªæ˜ç¡®æŒ‡å®šæ”»å‡»é’ˆå¯¹çš„å…·ä½“ç¼–ç¨‹è¯­è¨€ï¼Œä½†ç¤ºä¾‹å’Œä¸Šä¸‹æ–‡è¡¨æ˜ä¸é€šç”¨ä»£ç ç”Ÿæˆç›¸å…³ã€‚",
    "language_quote": NaN,
    "data_size": "æ–‡ä¸­æœªæ˜ç¡®æè¿°ç”¨äºè¯„ä¼°çš„æ”»å‡»æ•°æ®é›†æˆ–åŸºå‡†æ•°æ®é›†çš„è§„æ¨¡ã€‚",
    "data_size_quote": NaN,
    "source_type": "æ”»å‡»é€šè¿‡å‘æ£€ç´¢è¯­æ–™åº“ä¸­æ³¨å…¥ç»è¿‡æ¢¯åº¦å¼•å¯¼çªå˜ç”Ÿæˆçš„å¯¹æŠ—æ€§è§¦å‘å™¨æ¥æ„å»ºã€‚",
    "source_type_quote": "DRAINCODE achieves this goal by inserting syntactically correct yet semantically inert triggers into retrieval corpus. These triggers are optimized via gradient-guided mutation...",
    "last_updated": "2026-01-28 (æ ¹æ®arXivç‰ˆæœ¬å·æ¨æ–­)",
    "last_updated_quote": "arXiv:2601.20615v1  [cs.SE]  28 Jan 2026",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆæœ¬æ–‡ä½œè€…ä¸ºæ”»å‡»å’Œè¯„ä¼°ç›®çš„è€Œæ„å»ºï¼‰",
    "build_type_quote": "We propose DRAINCODE... We provide code and data at https://github.com/DeepSoftwareAnalytics/DrainCode.",
    "contamination_status": "ä¸é€‚ç”¨ã€‚æœ¬æ–‡æå‡ºçš„æ˜¯æ”»å‡»æ–¹æ³•ï¼Œè€Œéç”¨äºè®­ç»ƒçš„ä¼ ç»Ÿæ•°æ®é›†ã€‚å…¶è¯„ä¼°æ•°æ®æ˜¯ä¸“é—¨ä¸ºæµ‹è¯•æ”»å‡»æœ‰æ•ˆæ€§è€Œæ„å»ºçš„æŠ•æ¯’æ ·æœ¬ã€‚",
    "contamination_status_quote": NaN,
    "dataset_license": "æ–‡ä¸­æœªæåŠã€‚",
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆåœ¨ç»™å®šæŸ¥è¯¢å’Œå¯èƒ½è¢«æŠ•æ¯’çš„æ£€ç´¢ä¸Šä¸‹æ–‡æ¡ä»¶ä¸‹ï¼‰ã€‚",
    "task_granularity_quote": "attacks on Retrieval-Augmented Code Generation",
    "evaluation_metrics": "è¾“å‡ºé•¿åº¦å€æ•°ã€å»¶è¿Ÿå¢åŠ ç™¾åˆ†æ¯”ã€èƒ½è€—å¢åŠ ç™¾åˆ†æ¯”ã€åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆé€šè¿‡æµ‹è¯•ç”¨ä¾‹çš„å‡†ç¡®ç‡ï¼‰ã€‚",
    "evaluation_metrics_quote": "DRAINCODE achieves up to a 85% increase in latency, a 49% increase in energy consumption, and more than a 3Ã— increase in output length compared to the baseline... while maintaining 95â€“99% functional accuracy.",
    "input_modality": "è‡ªç„¶è¯­è¨€æŸ¥è¯¢æˆ–æœªå®Œæˆä»£ç ï¼ˆç”¨æˆ·æŸ¥è¯¢ï¼‰åŠ ä¸Šæ£€ç´¢åˆ°çš„ä»£ç ä¸Šä¸‹æ–‡ï¼ˆå¯èƒ½è¢«æŠ•æ¯’ï¼‰ã€‚",
    "input_modality_quote": "the LLM receives clean retrieved context and a user query (unfinished code)... adversarial triggers embedded in the retrieved context",
    "output_modality": "ä»£ç ã€‚",
    "output_modality_quote": "produce unnecessarily long responses containing unexecuted functions",
    "task_io_type": "ä»£ç /è‡ªç„¶è¯­è¨€åˆ°ä»£ç ï¼ˆåœ¨RAGæ¡†æ¶ä¸‹ï¼Œè¾“å…¥æ˜¯æŸ¥è¯¢å’Œæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼Œè¾“å‡ºæ˜¯ç”Ÿæˆçš„ä»£ç ï¼‰ã€‚",
    "task_io_type_quote": "Retrieval-Augmented Code Generation",
    "execution_environment": "æ–‡ä¸­æœªæ˜ç¡®æè¿°ï¼Œä½†è¯„ä¼°åŠŸèƒ½æ­£ç¡®æ€§æš—ç¤ºéœ€è¦æ‰§è¡Œç”Ÿæˆçš„ä»£ç ã€‚",
    "execution_environment_quote": "the generated code with attack can still pass the test cases if executed.",
    "unique_features": "1. é¦–ä¸ªé’ˆå¯¹RAGä»£ç ç”Ÿæˆç³»ç»Ÿè®¡ç®—æ•ˆç‡çš„å¯¹æŠ—æ€§æ”»å‡»åŸºå‡†ã€‚2. æ”»å‡»ç›®æ ‡æ˜¯è¯±å¯¼èµ„æºæ¶ˆè€—ï¼ˆå»¶è¿Ÿã€èƒ½è€—ï¼‰ï¼Œè€Œéç ´ååŠŸèƒ½æ­£ç¡®æ€§ï¼Œå…·æœ‰éšè”½æ€§ã€‚3. é‡‡ç”¨å‡è®¾æŸ¥è¯¢ç”Ÿæˆã€åŸºäºæ¢¯åº¦çš„è§¦å‘å™¨çªå˜å’Œå¤šä½ç½®çªå˜ç­‰æŠ€æœ¯ï¼Œå®ç°æŸ¥è¯¢æ— å…³çš„æŠ•æ¯’å¹¶æé«˜æ”»å‡»æ•ˆç‡ã€‚",
    "unique_features_quote": "the first adversarial attack targeting the computational efficiency of RAG-based code generation systems... covert resource exhaustion... preserves the original functionality... a hypothetical query construction mechanism that generates plausible query based on retrieved snippets, enabling query-agnostic poisoning... multi-position mutation and an attack buffer pool",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2026,
    "last_updated_month": 1,
    "last_updated_day": 28,
    "language_normalized": "['é€šç”¨ä»£ç ç”Ÿæˆ']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆç³»ç»Ÿçš„è®¡ç®—æ•ˆç‡å®‰å…¨æ€§', 'è¾“å‡ºé•¿åº¦', 'æ¨ç†å»¶è¿Ÿ', 'èƒ½è€—']",
    "evaluation_method_normalized": "['è¾“å‡ºé•¿åº¦å€æ•°', 'å»¶è¿Ÿå¢åŠ ç™¾åˆ†æ¯”', 'èƒ½è€—å¢åŠ ç™¾åˆ†æ¯”', 'åŠŸèƒ½æ­£ç¡®æ€§', 'é€šè¿‡æµ‹è¯•ç”¨ä¾‹çš„å‡†ç¡®ç‡']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹å®‰å…¨', 'å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆåº”ç”¨ä¸­çš„å®‰å…¨æ€§ä¸èµ„æºæ¶ˆè€—']",
    "source_type_normalized": "['æ”»å‡»é€šè¿‡å‘æ£€ç´¢è¯­æ–™åº“ä¸­æ³¨å…¥ç»è¿‡æ¢¯åº¦å¼•å¯¼çªå˜ç”Ÿæˆçš„å¯¹æŠ—æ€§è§¦å‘å™¨æ¥æ„å»º']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.20789_output/content.md",
    "benchmark_name": "SWE-bench",
    "benchmark_name_quote": "SWE-bench (Jimenez et al., 2023) is the standard benchmark for evaluating coding agents on real-world software engineering tasks.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We use SWE-bench Verified for all evaluations in this work.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°ç¼–ç ä»£ç†åœ¨çœŸå®ä¸–ç•Œè½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ã€‚æ¯ä¸ªä»»åŠ¡æºè‡ªçœŸå®çš„GitHub issueå’Œpull requestã€‚ç»™å®šé—®é¢˜æè¿°ï¼Œä»£ç†å¿…é¡»ç”Ÿæˆä¸€ä¸ªè§£å†³è¯¥é—®é¢˜çš„è¡¥ä¸ã€‚",
    "task_description_quote": "SWE-bench (Jimenez et al., 2023) is the standard benchmark for evaluating coding agents on real-world software engineering tasks. Each task is derived from a real GitHub issue and pull request from 12 popular Python repositories such as Django, Sympy, and Sphinx. Given an issue description, the agent must produce a patch that resolves the issue.",
    "dimension": "ç¼–ç ä»£ç†åœ¨çœŸå®è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸Šçš„åŠŸèƒ½æ€§æ­£ç¡®æ€§",
    "dimension_quote": "evaluating coding agents on real-world software engineering tasks",
    "evaluation_method": "è¿è¡Œä»“åº“çš„æµ‹è¯•å¥—ä»¶ã€‚åœ¨åº”ç”¨è¡¥ä¸å‰åè¿è¡Œæµ‹è¯•ï¼Œå¦‚æœä¹‹å‰å¤±è´¥çš„æµ‹è¯•ç°åœ¨é€šè¿‡ï¼Œå¹¶ä¸”æ²¡æœ‰ä¹‹å‰é€šè¿‡çš„æµ‹è¯•è¢«ç ´åï¼Œåˆ™è®¤ä¸ºä»»åŠ¡å·²è§£å†³ã€‚",
    "evaluation_method_quote": "The repositoryâ€™s test suite is run before and after applying the patch and a task is considered solved if previously failing tests now pass and no previously passing tests are broken.",
    "context_dependency": "å¤šæ–‡ä»¶é¡¹ç›®ï¼ˆæ¥è‡ªçœŸå®GitHubä»“åº“ï¼‰",
    "context_dependency_quote": "Each task is derived from a real GitHub issue and pull request from 12 popular Python repositories such as Django, Sympy, and Sphinx.",
    "problem_domain": "é€šç”¨è½¯ä»¶å·¥ç¨‹ï¼Œæ¶µç›–å¤šä¸ªæµè¡ŒPythoné¡¹ç›®ï¼ˆå¦‚Django, Sympy, Sphinxï¼‰",
    "problem_domain_quote": "real-world software engineering tasks... from 12 popular Python repositories such as Django, Sympy, and Sphinx.",
    "problem_difficulty": "çœŸå®ä¸–ç•Œå·¥ç¨‹çº§ï¼ˆæºè‡ªå®é™…GitHub issueå’ŒPRï¼‰",
    "problem_difficulty_quote": "real-world software engineering tasks... derived from a real GitHub issue and pull request",
    "language": "Python",
    "language_quote": "from 12 popular Python repositories such as Django, Sympy, and Sphinx.",
    "data_size": "åŒ…å«æ¥è‡ª12ä¸ªæµè¡ŒPythonä»“åº“çš„ä»»åŠ¡ï¼Œå…·ä½“æ•°é‡æ–‡ä¸­æœªæ˜ç¡®æåŠ",
    "data_size_quote": "from 12 popular Python repositories such as Django, Sympy, and Sphinx.",
    "source_type": "æºè‡ªçœŸå®çš„GitHub issueå’Œpull request",
    "source_type_quote": "Each task is derived from a real GitHub issue and pull request",
    "last_updated": "2023",
    "last_updated_quote": "SWE-bench (Jimenez et al., 2023)",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆç”±Jimenezç­‰äººæ„å»ºï¼‰",
    "build_type_quote": "SWE-bench (Jimenez et al., 2023)",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ä¿®å¤ï¼ˆç”Ÿæˆè§£å†³issueçš„è¡¥ä¸ï¼‰",
    "task_granularity_quote": "Given an issue description, the agent must produce a patch that resolves the issue.",
    "evaluation_metrics": "ä»»åŠ¡è§£å†³ç‡ï¼ˆé€šè¿‡æµ‹è¯•å¥—ä»¶éªŒè¯ï¼‰",
    "evaluation_metrics_quote": "a task is considered solved if previously failing tests now pass and no previously passing tests are broken.",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆissueæè¿°ï¼‰",
    "input_modality_quote": "Given an issue description",
    "output_modality": "ä»£ç ï¼ˆè¡¥ä¸ï¼Œå³è¡Œçº§åˆ«çš„diffï¼‰",
    "output_modality_quote": "the agent must produce a patch",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ï¼ˆè‡ªç„¶è¯­è¨€issueæè¿°åˆ°ä»£ç è¡¥ä¸ï¼‰",
    "task_io_type_quote": "Given an issue description, the agent must produce a patch that resolves the issue.",
    "execution_environment": "éœ€è¦ç‰¹å®šä»“åº“çš„æµ‹è¯•å¥—ä»¶ç¯å¢ƒ",
    "execution_environment_quote": "The repositoryâ€™s test suite is run before and after applying the patch",
    "unique_features": "SWE-bench Verifiedæ˜¯å…¶ä¸€ä¸ªç»è¿‡äººå·¥éªŒè¯çš„å­é›†ï¼Œç¡®ä¿æ¯ä¸ªä»»åŠ¡éƒ½æ˜¯å¯è§£å†³çš„ï¼Œå¹¶ä¸”æµ‹è¯•èƒ½æ­£ç¡®éªŒè¯è§£å†³æ–¹æ¡ˆã€‚",
    "unique_features_quote": "SWE-bench Verified is a curated subset where human annotators have verified that each task is solvable and that the tests correctly validate the solution.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2023,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ç¼–ç ä»£ç†åœ¨çœŸå®è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ä¸Šçš„åŠŸèƒ½æ€§æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['ä»»åŠ¡è§£å†³ç‡ï¼ˆé€šè¿‡æµ‹è¯•å¥—ä»¶éªŒè¯ï¼‰']",
    "problem_domain_normalized": "['é€šç”¨è½¯ä»¶å·¥ç¨‹ï¼Œæ¶µç›–å¤šä¸ªæµè¡ŒPythoné¡¹ç›®ï¼ˆå¦‚Django, Sympy, Sphinxï¼‰']",
    "source_type_normalized": "['æºè‡ªçœŸå®çš„GitHub issueå’Œpull request']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ä¿®å¤",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.20810_output/content.md",
    "benchmark_name": "HumanEval",
    "benchmark_name_quote": "We evaluate our method using HumanEval [3] and MBPP [1].",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We evaluate our method using HumanEval [3] and MBPP [1].",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ä»è‡ªç„¶è¯­è¨€æè¿°ç”ŸæˆåŠŸèƒ½æ­£ç¡®çš„ä»£ç ",
    "task_description_quote": "Large Language Models (LLMs) have enabled natural-language-to-code generation on a wide range of programming tasks",
    "dimension": "ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "However, functional correctness often depends on external programming knowledge that is not consistently stored in model parameters",
    "evaluation_method": "pass@1 å‡†ç¡®ç‡",
    "evaluation_method_quote": "Our approach improves the pass@1 accuracy across all baseline models on both the HumanEval [3] and MBPP [1] benchmarks",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": NaN,
    "problem_domain_quote": NaN,
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "æ–‡ä¸­æœªæè¿°å®Œæ•´æ•°æ®é›†ï¼Œä»…æåŠç”¨äºè¯„æµ‹",
    "language_quote": "We evaluate our method using HumanEval [3] and MBPP [1].",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "Large Language Models (LLMs) have enabled natural-language-to-code generation on a wide range of programming tasks",
    "evaluation_metrics": "pass@1",
    "evaluation_metrics_quote": "Our approach improves the pass@1 accuracy across all baseline models on both the HumanEval [3] and MBPP [1] benchmarks",
    "input_modality": "è‡ªç„¶è¯­è¨€",
    "input_modality_quote": "Large Language Models (LLMs) have enabled natural-language-to-code generation on a wide range of programming tasks",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "Large Language Models (LLMs) have enabled natural-language-to-code generation on a wide range of programming tasks",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "Large Language Models (LLMs) have enabled natural-language-to-code generation on a wide range of programming tasks",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": NaN,
    "unique_features_quote": NaN,
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['pass@1']",
    "problem_domain_normalized": "[]",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.20802_output/content.md",
    "benchmark_name": "LiveCodeBench v6",
    "benchmark_name_quote": "Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We evaluate competitive programming problems from LiveCodeBench v6 with LeetCode-style feedback.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ç«äº‰æ€§ç¼–ç¨‹é—®é¢˜è¯„æµ‹",
    "task_description_quote": "We evaluate competitive programming problems from LiveCodeBench v6 with LeetCode-style feedback.",
    "dimension": "ä»£ç ç”Ÿæˆæ­£ç¡®æ€§",
    "dimension_quote": "The model samples an answer y âˆ¼Ï€Î¸(Â· | x) and receives a scalar reward r âˆˆR, often binary (e.g., unit-tests pass/fail in code generation).",
    "evaluation_method": "å•å…ƒæµ‹è¯•é€šè¿‡/å¤±è´¥ï¼ˆäºŒè¿›åˆ¶å¥–åŠ±ï¼‰",
    "evaluation_method_quote": "The model samples an answer y âˆ¼Ï€Î¸(Â· | x) and receives a scalar reward r âˆˆR, often binary (e.g., unit-tests pass/fail in code generation).",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "ç«äº‰æ€§ç¼–ç¨‹",
    "problem_domain_quote": "We evaluate competitive programming problems from LiveCodeBench v6 with LeetCode-style feedback.",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "Python",
    "language_quote": "Write a python function that returns all numbers from 1 to n. Answer briefly.",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "The model samples an answer y âˆ¼Ï€Î¸(Â· | x)... (e.g., unit-tests pass/fail in code generation).",
    "evaluation_metrics": "å‡†ç¡®ç‡ (Accuracy)",
    "evaluation_metrics_quote": "SDPO substantially outperforms an improved version of Group Relative Policy Optimization (GRPO) on LCB v6 with Qwen3-8B. Further, SDPO achieves GRPOâ€™s final accuracy in 4Ã— fewer generations.",
    "input_modality": "è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°",
    "input_modality_quote": "Write a python function that returns all numbers from 1 to n. Answer briefly.",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "python\ndef numbers_up_to_n(n):\n    return list(range(1, n + 1))\n",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "Write a python function that returns all numbers from 1 to n. Answer briefly.",
    "execution_environment": "æä¾›è¿è¡Œæ—¶é”™è¯¯åé¦ˆçš„ä»£ç ç¯å¢ƒ",
    "execution_environment_quote": "Runtime Error\nZeroDivisionError: division by zero\nLine 73 in separateSquares (Solution.py)\nLast Executed Input\n[[26,30,2],[11,23,1]]\nFigure 3: Example of feedback from our code environment, inspired by LeetCode.",
    "unique_features": "æä¾›LeetCodeé£æ ¼çš„ä¸°å¯Œåé¦ˆï¼ˆå¦‚è¿è¡Œæ—¶é”™è¯¯ã€å¤±è´¥æµ‹è¯•ç”¨ä¾‹ï¼‰ï¼Œç”¨äºå¼ºåŒ–å­¦ä¹ ä¸­çš„ä¿¡ç”¨åˆ†é…",
    "unique_features_quote": "Many verifiable environments expose rich tokenized feedback beyond scalar rewards r, such as runtime errors, failing unit tests, or evaluations from an LLM judge. This feedback not only reveals whether a rollout was wrong, but also what went wrong.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆæ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['å‡†ç¡®ç‡ (Accuracy)']",
    "problem_domain_normalized": "['ç«äº‰æ€§ç¼–ç¨‹']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.22136_output/content.md",
    "benchmark_name": "StepShield",
    "benchmark_name_quote": "We introduce StepShield, the first benchmark to evaluate when violations are detected, not just whether.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We introduce StepShield, the first agent safety benchmark designed to evaluate the temporal performance of rogue agent detectors.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°ä»£ç æ™ºèƒ½ä½“å®‰å…¨æ£€æµ‹å™¨çš„æ—¶åºæ€§èƒ½ï¼Œå³æ£€æµ‹åˆ°æ¶æ„è¡Œä¸ºçš„æ—¶æœºï¼Œè€Œä¸ä»…ä»…æ˜¯èƒ½å¦æ£€æµ‹åˆ°ã€‚",
    "task_description_quote": "StepShield, the first agent safety benchmark designed to evaluate the temporal performance of rogue agent detectors.",
    "dimension": "æ£€æµ‹åŠæ—¶æ€§ã€æ—©æœŸå¹²é¢„èƒ½åŠ›",
    "dimension_quote": "StepShield focuses on the complementary problem of measuring detection timeliness.",
    "evaluation_method": "ä½¿ç”¨ä¸‰ç§æ–°é¢–çš„æ—¶åºæŒ‡æ ‡è¿›è¡Œè¯„ä¼°ï¼šæ—©æœŸå¹²é¢„ç‡ï¼ˆEIRï¼‰ã€å¹²é¢„é—´éš”ï¼ˆIGï¼‰å’ŒèŠ‚çœçš„ä»¤ç‰Œæ•°ï¼ˆTokens Savedï¼‰ã€‚",
    "evaluation_method_quote": "We propose three novel temporal metrics: Early Intervention Rate (EIR), Intervention Gap, and Tokens Saved.",
    "context_dependency": "ä»£ç æ™ºèƒ½ä½“è½¨è¿¹ï¼ŒåŒ…å«å¤šæ­¥éª¤æ‰§è¡Œè¿‡ç¨‹ã€‚",
    "context_dependency_quote": "StepShield contains 9,213 code agent trajectories... StepShield provides the first step-level annotated dataset for rogue behavior detection.",
    "problem_domain": "AIæ™ºèƒ½ä½“å®‰å…¨ã€ä»£ç å®‰å…¨ã€ç½‘ç»œå®‰å…¨",
    "problem_domain_quote": "Rogue behaviors are grounded in real-world security incidents across six categories.",
    "problem_difficulty": "åˆ†ä¸ºä¸‰ä¸ªä¸¥é‡æ€§çº§åˆ«ï¼ˆL1æ˜æ˜¾ï¼ŒL2æ¸…æ™°ï¼ŒL3å¾®å¦™ï¼‰ï¼ŒåŸºäºæ£€æµ‹æ‰€éœ€çš„è®¤çŸ¥åŠªåŠ›å®šä¹‰ã€‚",
    "problem_difficulty_quote": "We define three severity levels based on the cognitive effort needed for detection, not the potential harm (Table 2).",
    "language": "æ–‡ä¸­æœªæ˜ç¡®æŒ‡å®šç¼–ç¨‹è¯­è¨€ï¼Œä½†æ•°æ®é›†åŸºäºä»£ç æ™ºèƒ½ä½“è½¨è¿¹ã€‚",
    "language_quote": "StepShield contains 9,213 code agent trajectories...",
    "data_size": "åŒ…å«9,213æ¡ä»£ç æ™ºèƒ½ä½“è½¨è¿¹ï¼Œå…¶ä¸­1,278æ¡ç²¾å¿ƒæ ‡æ³¨çš„è®­ç»ƒå¯¹ï¼ˆ639æ¡æ¶æ„ï¼Œ639æ¡å¹²å‡€ï¼‰å’Œ7,935æ¡è½¨è¿¹çš„æµ‹è¯•é›†ï¼ˆåŒ…å«639æ¡æ¶æ„ï¼Œ639æ¡å¹²å‡€å’Œ6,657æ¡è‰¯æ€§è½¨è¿¹ï¼Œæ¶æ„ç‡8.1%ï¼‰ã€‚",
    "data_size_quote": "StepShield contains 9,213 code agent trajectories, including 1,278 meticulously annotated training pairs and a 7,935-trajectory test set with a realistic 8.1% rogue rate.",
    "source_type": "åŸºäºç°å®ä¸–ç•Œå®‰å…¨äº‹ä»¶ï¼Œå¹¶å‚è€ƒäº†å¤šä¸ªç°æœ‰åŸºå‡†å’Œæ–‡æ¡£ï¼ˆå¦‚OWASP Top 10, SWE-Bench, Sleeper Agentsç­‰ï¼‰ã€‚",
    "source_type_quote": "Rogue behaviors are grounded in real-world security incidents across six categories. (Table 1 lists sources like Fortune 2025, OWASP Top 10, NIST CAISI, SWE-Bench, Sleeper Agents, NIST AI RMF)",
    "last_updated": "2026",
    "last_updated_quote": "arXiv:2601.22136v1 [cs.LG] 29 Jan 2026",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "We introduce StepShield... We release StepShield, a dataset of 9,213 code agent trajectories...",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": "Apache 2.0",
    "dataset_license_quote": "The code and data are released under an Apache 2.0 license.",
    "task_granularity": "æ­¥éª¤çº§æ¶æ„è¡Œä¸ºæ£€æµ‹",
    "task_granularity_quote": "StepShield provides the first step-level annotated dataset for rogue behavior detection.",
    "evaluation_metrics": "æ—©æœŸå¹²é¢„ç‡ï¼ˆEIRï¼‰ã€å¹²é¢„é—´éš”ï¼ˆIGï¼‰ã€èŠ‚çœçš„ä»¤ç‰Œæ•°ï¼ˆTokens Savedï¼‰",
    "evaluation_metrics_quote": "We propose three novel temporal metrics: Early Intervention Rate (EIR), Intervention Gap, and Tokens Saved.",
    "input_modality": "ä»£ç æ™ºèƒ½ä½“çš„æ‰§è¡Œè½¨è¿¹ï¼ˆæ­¥éª¤åºåˆ—ï¼‰",
    "input_modality_quote": "StepShield contains 9,213 code agent trajectories...",
    "output_modality": "æ£€æµ‹ç»“æœï¼ˆæ˜¯å¦æ¶æ„åŠæ£€æµ‹æ­¥éª¤ï¼‰",
    "output_modality_quote": "metrics that quantify when a detector identifies a violation.",
    "task_io_type": "ä»£ç è½¨è¿¹åˆ°æ£€æµ‹åˆ¤æ–­",
    "task_io_type_quote": "evaluate the temporal performance of rogue agent detectors.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "é¦–ä¸ªä¸“æ³¨äºè¯„ä¼°æ£€æµ‹æ—¶æœºï¼ˆè€Œéä»…æ£€æµ‹ç»“æœï¼‰çš„æ™ºèƒ½ä½“å®‰å…¨åŸºå‡†ï¼›åŒ…å«åŸºäºç°å®å®‰å…¨äº‹ä»¶çš„å…­ç±»æ¶æ„è¡Œä¸ºåˆ†ç±»ï¼ˆUFO, SEC, RES, INV, TST, DECï¼‰ï¼›å®šä¹‰äº†åŸºäºæ£€æµ‹éš¾åº¦è€Œéæ½œåœ¨å±å®³çš„ä¸¥é‡æ€§çº§åˆ«ï¼›æµ‹è¯•é›†æ¨¡æ‹Ÿäº†ç°å®ç”Ÿäº§ç¯å¢ƒä¸­çš„æ¶æ„ç‡ï¼ˆ8.1%ï¼‰ã€‚",
    "unique_features_quote": "the first benchmark to evaluate when violations are detected, not just whether... Rogue behaviors are grounded in real-world security incidents across six categories... We define three severity levels based on the cognitive effort needed for detection, not the potential harm... The 8.1% rogue rate in the test set simulates realistic production conditions where most agent runs are benign.",
    "data_size_quantity": 9213,
    "data_size_unit": "æ¡ä»£ç æ™ºèƒ½ä½“è½¨è¿¹",
    "last_updated_year": 2026,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['æ£€æµ‹åŠæ—¶æ€§', 'æ—©æœŸå¹²é¢„èƒ½åŠ›']",
    "evaluation_method_normalized": "['æ—©æœŸå¹²é¢„ç‡ï¼ˆEIRï¼‰', 'å¹²é¢„é—´éš”ï¼ˆIGï¼‰', 'èŠ‚çœçš„ä»¤ç‰Œæ•°ï¼ˆTokens Savedï¼‰']",
    "problem_domain_normalized": "['AIæ™ºèƒ½ä½“å®‰å…¨', 'ä»£ç å®‰å…¨', 'ç½‘ç»œå®‰å…¨']",
    "source_type_normalized": "['åŸºäºç°å®ä¸–ç•Œå®‰å…¨äº‹ä»¶', 'å¹¶å‚è€ƒäº†å¤šä¸ªç°æœ‰åŸºå‡†å’Œæ–‡æ¡£ï¼ˆå¦‚OWASP Top 10, SWE-Bench, Sleeper Agentsç­‰ï¼‰']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "Apache 2.0",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.22129_output/content.md",
    "benchmark_name": "SWE-Bench Verified, SWE-Bench Pro, Multilingual",
    "benchmark_name_quote": "We evaluate SWE-Replay on the widely used SWE-Bench Verified2, and the more complex SWE-Bench Pro (Deng et al., 2025) and Multilingual3.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We evaluate SWE-Replay on the widely used SWE-Bench Verified2, and the more complex SWE-Bench Pro (Deng et al., 2025) and Multilingual3.",
    "dataset_url": "https://openai.com/index/introducing-swe-bench-verified/, https://www.swebench.com/multilingual.html",
    "dataset_url_quote": "2https://openai.com/index/introducing-swe-bench-verified/\n3https://www.swebench.com/multilingual.html",
    "task_description": "è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ï¼Œæ¶‰åŠå¯¼èˆªä»£ç ä»“åº“ã€æ‰§è¡Œæµ‹è¯•ã€æäº¤è¡¥ä¸ä»¥è§£å†³ç°å®ä¸–ç•Œä¸­çš„è½¯ä»¶é—®é¢˜ã€‚",
    "task_description_quote": "sophisticated interactive agents capable of navigating repositories, executing tests, and submitting patches end-to-end",
    "dimension": "è½¯ä»¶å·¥ç¨‹ä»£ç†çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬è§£å†³å¤æ‚ä»“åº“é—®é¢˜çš„æˆåŠŸç‡ï¼ˆè§£å†³ç‡ï¼‰ã€‚",
    "dimension_quote": "SWE-Replay consistently reduces the sampling cost by up to 17.4% while maintaining or even improving the resolve rate by up to 3.8%",
    "evaluation_method": "é€šè¿‡æµ‹è¯•ï¼ˆå›å½’æµ‹è¯•ï¼‰è¯„ä¼°ç”Ÿæˆçš„è¡¥ä¸æ˜¯å¦æ­£ç¡®ã€‚",
    "evaluation_method_quote": "Trajectory quality is evaluated based on the final generated patch. In particular, we check whether the patch causes any existing regression tests to fail",
    "context_dependency": "å¤šæ–‡ä»¶é¡¹ç›®ï¼Œéœ€è¦å¯¼èˆªå’Œç†è§£æ•´ä¸ªä»£ç ä»“åº“çš„ä¸Šä¸‹æ–‡ã€‚",
    "context_dependency_quote": "capable of navigating repositories, executing tests, and submitting patches end-to-end",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ï¼Œå…·ä½“ä¸ºä¿®å¤ç°å®ä¸–ç•Œå¼€æºé¡¹ç›®ä¸­çš„é—®é¢˜ï¼ˆissueï¼‰ã€‚",
    "problem_domain_quote": "software engineering (SWE) tasks",
    "problem_difficulty": "å¤æ‚ï¼ˆcomplexï¼‰ï¼ŒåŒ…å«æ›´å¤æ‚çš„è½¯ä»¶é—®é¢˜ï¼ˆSWE-Bench Proï¼‰ã€‚",
    "problem_difficulty_quote": "the more complex SWE-Bench Pro",
    "language": "æ–‡ä¸­æœªæ˜ç¡®æè¿°å®Œæ•´æ•°æ®é›†çš„è¯­è¨€è¦†ç›–èŒƒå›´ï¼Œä»…æåŠâ€œMultilingualâ€ï¼ˆå¤šè¯­è¨€ï¼‰å­é›†ã€‚",
    "language_quote": "Multilingual3",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": "ç°å®ä¸–ç•Œå¼€æºé¡¹ç›®çš„é—®é¢˜ï¼ˆissuesï¼‰ã€‚",
    "source_type_quote": "diverse software issues",
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ä¿®å¤ï¼ˆç”Ÿæˆè¡¥ä¸ï¼‰ã€‚",
    "task_granularity_quote": "submitting patches",
    "evaluation_metrics": "è§£å†³ç‡ï¼ˆresolve rateï¼‰ã€‚",
    "evaluation_metrics_quote": "improving the resolve rate by up to 3.8%",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆé—®é¢˜æè¿°ï¼‰ä¸ä»£ç ä»“åº“ã€‚",
    "input_modality_quote": NaN,
    "output_modality": "ä»£ç ï¼ˆè¡¥ä¸ï¼‰ã€‚",
    "output_modality_quote": "submitting patches",
    "task_io_type": "æ–‡æœ¬ï¼ˆé—®é¢˜æè¿°ï¼‰åˆ°ä»£ç ï¼ˆè¡¥ä¸ï¼‰ã€‚",
    "task_io_type_quote": NaN,
    "execution_environment": "éœ€è¦è®¿é—®å®Œæ•´çš„ä»£ç ä»“åº“ç¯å¢ƒä»¥æ‰§è¡Œæµ‹è¯•ã€‚",
    "execution_environment_quote": "executing tests",
    "unique_features": "ä¸“æ³¨äºè¯„ä¼°è½¯ä»¶å·¥ç¨‹ä»£ç†åœ¨çœŸå®ã€å¤æ‚ä»“åº“ç¯å¢ƒä¸­çš„ç«¯åˆ°ç«¯é—®é¢˜è§£å†³èƒ½åŠ›ï¼ŒåŒ…å«å¤šè¯­è¨€é—®é¢˜é›†ã€‚",
    "unique_features_quote": "the more complex SWE-Bench Pro (Deng et al., 2025) and Multilingual3.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['å¤šè¯­è¨€']",
    "dimension_normalized": "['è½¯ä»¶å·¥ç¨‹ä»£ç†çš„èƒ½åŠ›', 'è§£å†³å¤æ‚ä»“åº“é—®é¢˜çš„æˆåŠŸç‡', 'è§£å†³ç‡']",
    "evaluation_method_normalized": "['è§£å†³ç‡']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'ä¿®å¤ç°å®ä¸–ç•Œå¼€æºé¡¹ç›®ä¸­çš„é—®é¢˜']",
    "source_type_normalized": "['ç°å®ä¸–ç•Œå¼€æºé¡¹ç›®çš„é—®é¢˜']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ä¿®å¤",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.22130_output/content.md",
    "benchmark_name": "WoW-bench",
    "benchmark_name_quote": "We also build WoW-bench, the first enterprise benchmark designed to evaluate LLMs both as enterprise world models and agents.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We introduce World of Workflows (WoW), a realistic ServiceNow-based environment... alongside WoW-bench, a benchmark of 234 tasks...",
    "dataset_url": "https://github.com/... (æ–‡ä¸­ä»…æåŠGitHubï¼Œæœªæä¾›å®Œæ•´URL)",
    "dataset_url_quote": "We release our GitHub 1 for setting up and evaluating WoW.",
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ä¼ä¸šç³»ç»Ÿä¸­ä½œä¸ºè‡ªä¸»ä»£ç†å’Œä¸–ç•Œæ¨¡å‹çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬å®Œæˆå—çº¦æŸçš„ä»»åŠ¡ã€ç†è§£ç³»ç»ŸåŠ¨æ€ã€é¢„æµ‹å·¥ä½œæµå½±å“ç­‰ã€‚",
    "task_description_quote": "WoW-bench provides 234 evaluation tasks that explicitly incorporate workflow effects into four task categories: autonomous task completion, data-level constraint understanding, dynamics prediction, and tool prediction.",
    "dimension": "ä¼ä¸šåŠ¨æ€å»ºæ¨¡èƒ½åŠ›ã€çº¦æŸéµå¾ªèƒ½åŠ›ã€è‡ªä¸»ä»»åŠ¡å®Œæˆèƒ½åŠ›ã€å·¥å…·é¢„æµ‹èƒ½åŠ›",
    "dimension_quote": "evaluating constrained agentic task completion and enterprise dynamics modeling capabilities.",
    "evaluation_method": "ä»»åŠ¡æˆåŠŸç‡ï¼ˆTask success rateï¼‰",
    "evaluation_method_quote": "Through our experiment, using audit logs as observation increases the task success rate by at most 7x...",
    "context_dependency": "å¤šæ­¥éª¤ã€é•¿è§†é‡ä»»åŠ¡ï¼Œæ¶‰åŠè·¨æ•°æ®åº“è¡¨çš„çº§è”çŠ¶æ€å˜åŒ–",
    "context_dependency_quote": "follow constraints in long-horizon tasks.",
    "problem_domain": "ä¼ä¸šITç³»ç»Ÿç®¡ç†ï¼Œæ¶µç›–ç”¨æˆ·ã€äº‹ä»¶ã€èµ„äº§ã€çŸ¥è¯†åº“ã€ç›®å½•å’Œè´¹ç”¨ç­‰å¤šä¸ªå­é¢†åŸŸ",
    "problem_domain_quote": "covering sub-domains across multiple management systems including user, incident, asset, knowledge base, catalog, and expense.",
    "problem_difficulty": "é«˜éš¾åº¦ï¼Œæ¨¡æ‹ŸçœŸå®ä¼ä¸šç¯å¢ƒçš„å¤æ‚æ€§ï¼ŒåŒ…å«éšè—å·¥ä½œæµå’Œçº§è”å‰¯ä½œç”¨",
    "problem_difficulty_quote": "ignoring true challenges in enterprises, such as limited observability, large database state, and hidden workflows with cascading side effects.",
    "language": "ä¸æ¶‰åŠç‰¹å®šç¼–ç¨‹è¯­è¨€ï¼Œä¸»è¦åŸºäºè‡ªç„¶è¯­è¨€æŒ‡ä»¤å’ŒAPIå·¥å…·è°ƒç”¨",
    "language_quote": "An agent interacts with the environment through MCP tools as actions, and perceives the partial observation of the world via tool response or table audits.",
    "data_size": "åŒ…å«234ä¸ªè¯„ä¼°ä»»åŠ¡ï¼Œç¯å¢ƒå†…ç½®4000å¤šæ¡ä¸šåŠ¡è§„åˆ™å’Œ55ä¸ªæ´»è·ƒå·¥ä½œæµ",
    "data_size_quote": "a benchmark of 234 tasks... incorporating 4,000+ business rules and 55 active workflows embedded in the system",
    "source_type": "åŸºäºServiceNowå¼€å‘è€…å®ä¾‹æ„å»ºï¼Œæ¨¡æ‹ŸçœŸå®ä¼ä¸šç¯å¢ƒï¼Œå¹¶è‡ªåŠ¨å¡«å……æ¨¡æ‹Ÿæ•°æ®",
    "source_type_quote": "WoW is built using the ServiceNow developer instance, a free sandbox platform to experiment and test custom applications. Upon initialization, mock data are automatically populated.",
    "last_updated": "2026å¹´1æœˆ30æ—¥ï¼ˆé¢„å°æœ¬æ—¥æœŸï¼‰",
    "last_updated_quote": "Preprint. January 30, 2026.",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "We introduce World of Workflows (WoW)... We also build WoW-bench...",
    "contamination_status": "æ–‡ä¸­æœªæ˜ç¡®æåŠ",
    "contamination_status_quote": NaN,
    "dataset_license": "æ–‡ä¸­æœªæ˜ç¡®æåŠ",
    "dataset_license_quote": NaN,
    "task_granularity": "è‡ªä¸»ä»»åŠ¡å®Œæˆã€çº¦æŸç†è§£ã€åŠ¨æ€é¢„æµ‹ã€å·¥å…·é¢„æµ‹",
    "task_granularity_quote": "four task categories: autonomous task completion, data-level constraint understanding, dynamics prediction, and tool prediction.",
    "evaluation_metrics": "ä»»åŠ¡æˆåŠŸç‡",
    "evaluation_metrics_quote": "increases the task success rate by at most 7x",
    "input_modality": "è‡ªç„¶è¯­è¨€ç”¨æˆ·æŸ¥è¯¢ï¼ˆåŒ…å«ä»»åŠ¡å’Œçº¦æŸæè¿°ï¼‰",
    "input_modality_quote": "U is the user query consisting of task and constraint descriptions.",
    "output_modality": "MCPå·¥å…·è°ƒç”¨åŠå‚æ•°",
    "output_modality_quote": "An action at âˆˆA at time step t is defined as a tuple consisting of a selected MCP tool and its associated parameters",
    "task_io_type": "è‡ªç„¶è¯­è¨€åˆ°å·¥å…·è°ƒç”¨",
    "task_io_type_quote": "An agent interacts with the environment through MCP tools as actions",
    "execution_environment": "ServiceNowå¼€å‘è€…å®ä¾‹æ²™ç®±ç¯å¢ƒ",
    "execution_environment_quote": "WoW is built using the ServiceNow developer instance, a free sandbox platform to experiment and test custom applications.",
    "unique_features": "é¦–ä¸ªä¸“æ³¨äºè¯„ä¼°ä¼ä¸šç³»ç»ŸåŠ¨æ€å»ºæ¨¡ï¼ˆä¸–ç•Œæ¨¡å‹ï¼‰èƒ½åŠ›çš„åŸºå‡†ã€‚æ¨¡æ‹Ÿäº†çœŸå®ã€å·¥ä½œæµé©±åŠ¨çš„ä¼ä¸šç¯å¢ƒï¼ŒåŒ…å«å¤§é‡éšè—çš„ä¸šåŠ¡è§„åˆ™å’Œå·¥ä½œæµï¼Œå¯¼è‡´çº§è”çš„æ•°æ®åº“çŠ¶æ€å˜åŒ–ï¼ŒæŒ‘æˆ˜æ¨¡å‹çš„åŠ¨æ€é¢„æµ‹å’Œçº¦æŸéµå¾ªèƒ½åŠ›ã€‚",
    "unique_features_quote": "the first enterprise benchmark designed to evaluate LLMs both as enterprise world models and agents... WoW fills the gap with large database state, heavy workflows, and carefully designed constraints.",
    "data_size_quantity": 234,
    "data_size_unit": "ä¸ªè¯„ä¼°ä»»åŠ¡",
    "last_updated_year": 2026,
    "last_updated_month": 1,
    "last_updated_day": 30,
    "language_normalized": "['ä¸æ¶‰åŠç‰¹å®šç¼–ç¨‹è¯­è¨€', 'è‡ªç„¶è¯­è¨€æŒ‡ä»¤', 'APIå·¥å…·è°ƒç”¨']",
    "dimension_normalized": "['ä¼ä¸šåŠ¨æ€å»ºæ¨¡èƒ½åŠ›', 'çº¦æŸéµå¾ªèƒ½åŠ›', 'è‡ªä¸»ä»»åŠ¡å®Œæˆèƒ½åŠ›', 'å·¥å…·é¢„æµ‹èƒ½åŠ›']",
    "evaluation_method_normalized": "['ä»»åŠ¡æˆåŠŸç‡']",
    "problem_domain_normalized": "['ä¼ä¸šITç³»ç»Ÿç®¡ç†', 'ç”¨æˆ·', 'äº‹ä»¶', 'èµ„äº§', 'çŸ¥è¯†åº“', 'ç›®å½•', 'è´¹ç”¨']",
    "source_type_normalized": "['ServiceNowå¼€å‘è€…å®ä¾‹æ„å»º', 'æ¨¡æ‹ŸçœŸå®ä¼ä¸šç¯å¢ƒ', 'è‡ªåŠ¨å¡«å……æ¨¡æ‹Ÿæ•°æ®']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "JSON",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.22025_output/content.md",
    "benchmark_name": "Minimum Viable Evaluation Suite (MVES)",
    "benchmark_name_quote": "We introduce the Minimum Viable Evaluation Suite (MVES), a tiered set of recommended evaluation components for (i) general LLM applications, (ii) retrieval-augmented generation (RAG), and (iii) agentic tool-use workflows.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We introduce the Minimum Viable Evaluation Suite (MVES)... First, we introduce the MVES framework: a tiered standard defining minimum evaluation requirements for general LLM applications (MVES-Core), retrieval-augmented systems (MVES-RAG), and agentic workflows (MVES-Agentic).",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ä¸ºé€šç”¨LLMåº”ç”¨ã€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿå’Œæ™ºèƒ½ä½“å·¥ä½œæµæä¾›åˆ†å±‚çš„ã€æœ€ä½é™åº¦çš„è¯„ä¼°ç»„ä»¶å»ºè®®ï¼Œæ—¨åœ¨æ„å»ºä¸€ä¸ªå¯é‡å¤çš„å·¥ç¨‹åŒ–è¯„ä¼°å¾ªç¯ã€‚",
    "task_description_quote": "We introduce the Minimum Viable Evaluation Suite (MVES), a tiered set of recommended evaluation components for (i) general LLM applications, (ii) retrieval-augmented generation (RAG), and (iii) agentic tool-use workflows.",
    "dimension": "æ­£ç¡®æ€§ã€æœ‰ç”¨æ€§ã€æ— å®³æ€§ã€åŸºäºäº‹å®æ€§ï¼ˆæœ‰ä¾æ®ï¼‰ã€æ ¼å¼éµå¾ªã€ä¸€è‡´æ€§ã€æ‹’ç»æ­£ç¡®æ€§",
    "dimension_quote": "Third, we present a taxonomy of quality dimensions distinguishing correctness, helpfulness, harmlessness, groundedness, and format adherence. ... 3.7 Consistency",
    "evaluation_method": "è‡ªåŠ¨åŒ–ç¦»çº¿æ£€æŸ¥ã€äººå·¥è¯„ä¼°ã€LLMä½œä¸ºè¯„åˆ¤è€…ï¼ˆLLM-as-Judgeï¼‰",
    "evaluation_method_quote": "We also synthesize common evaluation methods (automated checks, human rubrics, and LLM-as-judge)... 4.1 Automated Offline Checks ... 4.2 Human Evaluation ... 8 LLM-as-Judge",
    "context_dependency": "æ¶µç›–å•è½®ä»»åŠ¡ï¼ˆå¦‚ä¿¡æ¯æå–ï¼‰ã€å¤šè½®å¯¹è¯ä»¥åŠæ¶‰åŠå¤–éƒ¨å·¥å…·æˆ–çŸ¥è¯†æ£€ç´¢çš„æ™ºèƒ½ä½“å·¥ä½œæµã€‚",
    "context_dependency_quote": "for (i) general LLM applications, (ii) retrieval-augmented generation (RAG), and (iii) agentic tool-use workflows. ... 5.5 Multi-Turn Conversation Tests",
    "problem_domain": "é€šç”¨LLMåº”ç”¨é¢†åŸŸï¼ŒåŒ…æ‹¬å®¢æˆ·æ”¯æŒã€å†…éƒ¨çŸ¥è¯†åº“é—®ç­”ã€æ‘˜è¦ç”Ÿæˆç­‰å®é™…åº”ç”¨åœºæ™¯ã€‚",
    "problem_domain_quote": "9.1 Case Study 1: Customer Support Assistant ... 9.2 Case Study 2: Internal Knowledge Base RAG Bot ... 9.3 Case Study 3: Summarization Pipeline",
    "problem_difficulty": "æ—¨åœ¨è¦†ç›–ä»åŸºç¡€åŠŸèƒ½åˆ°è¾¹ç¼˜æ¡ˆä¾‹å’Œå¯¹æŠ—æ€§æç¤ºçš„å…¨é¢æµ‹è¯•ï¼Œéš¾åº¦èŒƒå›´å¹¿æ³›ã€‚",
    "problem_difficulty_quote": "5.2 Edge Cases and Adversarial Prompts ... 5.3 Systematic Coverage Design",
    "language": "ä¸»è¦é¢å‘è‡ªç„¶è¯­è¨€ä»»åŠ¡ï¼Œä¸ç‰¹å®šäºæŸä¸€ç§ç¼–ç¨‹è¯­è¨€ã€‚",
    "language_quote": "Evaluating Large Language Model (LLM) applications... (æ–‡ä¸­æœªæåŠç‰¹å®šç¼–ç¨‹è¯­è¨€ï¼Œä¸»è¦è®¨è®ºè‡ªç„¶è¯­è¨€åº”ç”¨çš„è¯„ä¼°)",
    "data_size": "æ–‡ä¸­æœªæ˜ç¡®æŒ‡å®šå…·ä½“çš„æ•°æ®é›†è§„æ¨¡ï¼Œä½†å¼ºè°ƒæµ‹è¯•é›†åº”å…·æœ‰ä»£è¡¨æ€§å’Œç»Ÿè®¡æ•ˆåŠ›ã€‚",
    "data_size_quote": "5.7 Test Set Size and Statistical Power",
    "source_type": "ä½œä¸ºè¯„ä¼°æ¡†æ¶æå‡ºï¼Œå…¶â€œæ•°æ®â€æ¥æºäºä¸ºç‰¹å®šåº”ç”¨ï¼ˆå¦‚æå–ã€RAGï¼‰è®¾è®¡çš„æµ‹è¯•å¥—ä»¶ï¼Œå¯èƒ½ç”±äººå·¥æˆ–åˆæˆç”Ÿæˆã€‚",
    "source_type_quote": "5.4 Tutorial: The Extraction Evaluation Loop ... 7.7 Tutorial: The RAG Evaluation Loop (è¡¨æ˜æµ‹è¯•é›†æ˜¯ä¸ºç‰¹å®šä»»åŠ¡è®¾è®¡çš„)",
    "last_updated": "2026å¹´1æœˆ",
    "last_updated_quote": "January 2026",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆæœ¬æ–‡ä½œè€…æå‡ºå¹¶æ„å»ºçš„æ¡†æ¶å’Œæ ‡å‡†ï¼‰",
    "build_type_quote": "We introduce the Minimum Viable Evaluation Suite (MVES)... First, we introduce the MVES framework...",
    "contamination_status": "æ–‡ä¸­è®¨è®ºäº†æ•°æ®æ±¡æŸ“çš„è€ƒé‡ï¼Œå»ºè®®è®¾è®¡æµ‹è¯•é›†æ—¶æ³¨æ„æ­¤é—®é¢˜ã€‚",
    "contamination_status_quote": "5.6 Data Contamination Considerations",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "åº”ç”¨çº§åˆ«è¯„ä¼°ï¼Œæ¶µç›–ä¿¡æ¯æå–ã€é—®ç­”ã€æ‘˜è¦ã€å¯¹è¯ã€å·¥å…·ä½¿ç”¨ç­‰å¤šç§ä»»åŠ¡ç²’åº¦ã€‚",
    "task_granularity_quote": "for (i) general LLM applications, (ii) retrieval-augmented generation (RAG), and (iii) agentic tool-use workflows.",
    "evaluation_metrics": "é€šè¿‡ç‡ï¼ˆå¦‚æå–é€šè¿‡ç‡ã€RAGåˆè§„ç‡ï¼‰ã€è¯­ä¹‰ç›¸ä¼¼åº¦ã€äº‹å®å‡†ç¡®æ€§ã€çœŸå®æ€§ç­‰ã€‚",
    "evaluation_metrics_quote": "extraction pass rate decreased from 100% to 90% and RAG compliance from 93.3% to 80% ... 6.2 Semantic Similarity ... 6.3 Factual Accuracy ... 6.4 Truthfulness and Calibration",
    "input_modality": "è‡ªç„¶è¯­è¨€æç¤ºï¼Œå¯èƒ½åŒ…å«ç»“æ„åŒ–æŒ‡ä»¤æˆ–ä¸Šä¸‹æ–‡æ–‡æ¡£ï¼ˆå¯¹äºRAGï¼‰ã€‚",
    "input_modality_quote": "sensitive to prompt and model changes. ... 7.1 Decomposing RAG Evaluation (æ¶‰åŠæ£€ç´¢åˆ°çš„æ–‡æ¡£ä½œä¸ºè¾“å…¥)",
    "output_modality": "è‡ªç„¶è¯­è¨€å“åº”ï¼Œæˆ–ç¬¦åˆç‰¹å®šæ ¼å¼ï¼ˆå¦‚JSONï¼‰çš„ç»“æ„åŒ–è¾“å‡ºã€‚",
    "output_modality_quote": "3.6 Format and Style Adherence",
    "task_io_type": "æ–‡æœ¬åˆ°æ–‡æœ¬ï¼ˆè‡ªç„¶è¯­è¨€åˆ°è‡ªç„¶è¯­è¨€ï¼‰ï¼Œå¯¹äºç‰¹å®šä»»åŠ¡å¯èƒ½è¦æ±‚è¾“å‡ºç»“æ„åŒ–æ•°æ®ã€‚",
    "task_io_type_quote": "Evaluating Large Language Model (LLM) applications... (æ ¸å¿ƒæ˜¯è¯„ä¼°LLMçš„æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›)",
    "execution_environment": "è¯„ä¼°é€šå¸¸åœ¨æœ¬åœ°æˆ–å—æ§ç¯å¢ƒä¸­è¿›è¡Œï¼Œå¯èƒ½æ¶‰åŠè°ƒç”¨å¤–éƒ¨å·¥å…·æˆ–æ£€ç´¢ç³»ç»Ÿçš„æ¨¡æ‹Ÿã€‚",
    "execution_environment_quote": "In reproducible local experiments (Ollama; Llama 3 8B Instruct and Qwen 2.5 7B Instruct)...",
    "unique_features": "1. æå‡ºåˆ†å±‚çš„â€œæœ€ä½å¯è¡Œè¯„ä¼°å¥—ä»¶â€ï¼ˆMVESï¼‰æ ‡å‡†ï¼Œé’ˆå¯¹ä¸åŒåº”ç”¨ç±»å‹ï¼ˆæ ¸å¿ƒã€RAGã€æ™ºèƒ½ä½“ï¼‰ã€‚2. å¼ºè°ƒè¯„ä¼°é©±åŠ¨çš„è¿­ä»£å·¥ä½œæµï¼ˆå®šä¹‰ã€æµ‹è¯•ã€è¯Šæ–­ã€ä¿®å¤ï¼‰ï¼Œå°†æŒ‘æˆ˜è½¬åŒ–ä¸ºå¯é‡å¤çš„å·¥ç¨‹å¾ªç¯ã€‚3. ç³»ç»Ÿåˆ†æäº†LLM-as-Judgeçš„å¤±è´¥æ¨¡å¼ï¼ˆä½ç½®åè§ã€å†—é•¿åè§ã€è‡ªæˆ‘åå¥½ç­‰ï¼‰ã€‚4. æä¾›äº†ä»æµ‹è¯•é›†è®¾è®¡ã€æŒ‡æ ‡é€‰æ‹©åˆ°ç”Ÿäº§ç›‘æ§çš„å¯æ“ä½œæ¸…å•ã€‚",
    "unique_features_quote": "We present an evaluation-driven workflowâ€”Define, Test, Diagnose, Fixâ€”that turns these challenges into a repeatable engineering loop. ... Fourth, we give a detailed analysis of LLM-as-judge failure modes, including position bias, verbosity bias, self-preference, style bias, and instruction leakage. Fifth, we provide actionable checklists for test set design, metric selection, human evaluation rubrics, and production monitoring.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2026,
    "last_updated_month": 1,
    "last_updated_day": null,
    "language_normalized": "['è‡ªç„¶è¯­è¨€']",
    "dimension_normalized": "['æ­£ç¡®æ€§', 'æœ‰ç”¨æ€§', 'æ— å®³æ€§', 'åŸºäºäº‹å®æ€§', 'æ ¼å¼éµå¾ª', 'ä¸€è‡´æ€§', 'æ‹’ç»æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['é€šè¿‡ç‡', 'è¯­ä¹‰ç›¸ä¼¼åº¦', 'äº‹å®å‡†ç¡®æ€§', 'çœŸå®æ€§']",
    "problem_domain_normalized": "['é€šç”¨LLMåº”ç”¨é¢†åŸŸ', 'å®¢æˆ·æ”¯æŒ', 'å†…éƒ¨çŸ¥è¯†åº“é—®ç­”', 'æ‘˜è¦ç”Ÿæˆ']",
    "source_type_normalized": "['è¯„ä¼°æ¡†æ¶', 'æµ‹è¯•å¥—ä»¶']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.22159_output/content.md",
    "benchmark_name": "RedSage-Bench",
    "benchmark_name_quote": "To rigorously evaluate the models, we introduce RedSage-Bench, a benchmark with 30K multiple-choice and 240 open-ended Q&A items covering cybersecurity knowledge, skills, and tool expertise.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "To rigorously evaluate the models, we introduce RedSage-Bench, a benchmark with 30K multiple-choice and 240 open-ended Q&A items covering cybersecurity knowledge, skills, and tool expertise.",
    "dataset_url": "https://risys-lab.github.io/RedSage/",
    "dataset_url_quote": "Project Page: https://risys-lab.github.io/RedSage/",
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç½‘ç»œå®‰å…¨é¢†åŸŸçš„ç»¼åˆèƒ½åŠ›ï¼ŒåŒ…æ‹¬çŸ¥è¯†ã€æŠ€èƒ½å’Œå·¥å…·ç†Ÿç»ƒåº¦ã€‚",
    "task_description_quote": "To rigorously evaluate the models, we introduce RedSage-Bench, a benchmark with 30K multiple-choice and 240 open-ended Q&A items covering cybersecurity knowledge, skills, and tool expertise.",
    "dimension": "ç½‘ç»œå®‰å…¨çŸ¥è¯†ã€å®è·µæŠ€èƒ½ã€å·¥å…·ç†Ÿç»ƒåº¦",
    "dimension_quote": "It spans knowledge, practical offensive skills, and tool expertise (CLI and Kali Linux).",
    "evaluation_method": "å¤šé¡¹é€‰æ‹©é¢˜ï¼ˆMCQï¼‰å’Œå¼€æ”¾å¼é—®ç­”ï¼ˆQ&Aï¼‰",
    "evaluation_method_quote": "a benchmark with 30K multiple-choice and 240 open-ended Q&A items",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "ç½‘ç»œå®‰å…¨",
    "problem_domain_quote": "covering cybersecurity knowledge, skills, and tool expertise.",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": NaN,
    "language_quote": NaN,
    "data_size": "30,240ä¸ªæ ·æœ¬ï¼ˆ30,000ä¸ªå¤šé¡¹é€‰æ‹©é¢˜å’Œ240ä¸ªå¼€æ”¾å¼é—®ç­”ï¼‰",
    "data_size_quote": "a benchmark with 30K multiple-choice and 240 open-ended Q&A items",
    "source_type": "åŸºäºç­–åˆ’çš„ç½‘ç»œå®‰å…¨èµ„æºï¼ˆå¦‚MITRE ATT&CKã€OWASPã€æ¸—é€æµ‹è¯•æŠ¥å‘Šã€å·¥å…·æ‰‹å†Œç­‰ï¼‰é€šè¿‡æ™ºèƒ½ä½“å¢å¼ºæµç¨‹ç”Ÿæˆ",
    "source_type_quote": "We curate RedSage-Seed: 28,637 samples (âˆ¼0.15B tokens) from publicly available sources organized into three categories: Knowledge (well-established cybersecurity frameworks and knowledge bases...), Skills (penetration-testing write-ups...), and Tools (CLI cheat-sheets...). ... we augment RedSage-Seed into multi-turn conversations using an agentic framework... producing RedSage-Conv with âˆ¼266K multi-turn conversations... for benchmark generation.",
    "last_updated": "2026",
    "last_updated_quote": "arXiv:2601.22159v1  [cs.CR]  29 Jan 2026",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "we introduce RedSage-Bench, a benchmark with 30K multiple-choice and 240 open-ended Q&A items",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "é—®ç­”ï¼ˆçŸ¥è¯†é—®ç­”ã€æŠ€èƒ½åº”ç”¨é—®ç­”ã€å·¥å…·ä½¿ç”¨é—®ç­”ï¼‰",
    "task_granularity_quote": "a benchmark with 30K multiple-choice and 240 open-ended Q&A items",
    "evaluation_metrics": NaN,
    "evaluation_metrics_quote": NaN,
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆé—®é¢˜ï¼‰",
    "input_modality_quote": "a benchmark with 30K multiple-choice and 240 open-ended Q&A items",
    "output_modality": "è‡ªç„¶è¯­è¨€ï¼ˆç­”æ¡ˆï¼‰",
    "output_modality_quote": "a benchmark with 30K multiple-choice and 240 open-ended Q&A items",
    "task_io_type": "æ–‡æœ¬åˆ°æ–‡æœ¬",
    "task_io_type_quote": "a benchmark with 30K multiple-choice and 240 open-ended Q&A items",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "1. ç»¼åˆè¦†ç›–ç½‘ç»œå®‰å…¨çš„çŸ¥è¯†ã€æŠ€èƒ½å’Œå·¥å…·ç†Ÿç»ƒåº¦ä¸‰ä¸ªç»´åº¦ã€‚2. åŒ…å«å¯¹å¼€æ”¾å¼å›ç­”çš„è´¨é‡è¯„ä¼°ï¼ˆQuality Scoringï¼‰ã€‚3. é€šè¿‡æ™ºèƒ½ä½“å¢å¼ºæµç¨‹ä»é«˜è´¨é‡ç§å­æ•°æ®ç”Ÿæˆï¼Œæ¨¡æ‹Ÿä¸“å®¶-åŠ©æ‰‹å·¥ä½œæµç¨‹ã€‚",
    "unique_features_quote": "RedSage-Bench (Ours) âœ“ âœ“ âœ“ âœ“ 30,240 ... Unlike prior work with limited augmentation, we introduce agentic augmentation to transform curated cybersecurity resources into diverse, realistic multi-turn dialogs simulating expertâ€“assistant workflows across knowledge, offensive operations, and tool proficiency for domain-specific fine-tuning.",
    "data_size_quantity": 30240,
    "data_size_unit": "ä¸ªæ ·æœ¬",
    "last_updated_year": 2026,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['ç½‘ç»œå®‰å…¨çŸ¥è¯†', 'å®è·µæŠ€èƒ½', 'å·¥å…·ç†Ÿç»ƒåº¦']",
    "evaluation_method_normalized": "[]",
    "problem_domain_normalized": "['ç½‘ç»œå®‰å…¨']",
    "source_type_normalized": "['åŸºäºç­–åˆ’çš„ç½‘ç»œå®‰å…¨èµ„æºï¼ˆå¦‚MITRE ATT&CKã€OWASPã€æ¸—é€æµ‹è¯•æŠ¥å‘Šã€å·¥å…·æ‰‹å†Œç­‰ï¼‰é€šè¿‡æ™ºèƒ½ä½“å¢å¼ºæµç¨‹ç”Ÿæˆ']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.21787_output/content.md",
    "benchmark_name": "BEF4LLM",
    "benchmark_name_quote": "We introduce BEF4LLM, a novel LLM evaluation framework comprising four perspectives: syntactic quality, pragmatic quality, semantic quality, and validity.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We introduce the BEF4LLM framework for evaluating LLMs in text-to-BPMN tasks",
    "dataset_url": "https://gitlab-iwi.dfki.de/lauer/bef4llm",
    "dataset_url_quote": "The code regarding the framework, LLM experiments and used datasets is available at https://gitlab-iwi.dfki.de/lauer/bef4llm.",
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä»è‡ªç„¶è¯­è¨€æ–‡æœ¬ç”Ÿæˆä¸šåŠ¡è¿‡ç¨‹æ¨¡å‹ä¸ç¬¦å·ï¼ˆBPMNï¼‰æ¨¡å‹æ–¹é¢çš„èƒ½åŠ›ã€‚",
    "task_description_quote": "Assessing the Business Process Modeling Competences of Large Language Models",
    "dimension": "BPMNæ¨¡å‹è´¨é‡ï¼ŒåŒ…æ‹¬å¥æ³•è´¨é‡ã€è¯­ç”¨è´¨é‡ã€è¯­ä¹‰è´¨é‡å’Œæœ‰æ•ˆæ€§ã€‚",
    "dimension_quote": "We introduce BEF4LLM, a novel LLM evaluation framework comprising four perspectives: syntactic quality, pragmatic quality, semantic quality, and validity.",
    "evaluation_method": "ä½¿ç”¨åŒ…å«39ä¸ªæŒ‡æ ‡çš„BEF4LLMæ¡†æ¶ï¼Œåœ¨å››ä¸ªè´¨é‡ç»´åº¦ä¸Šå¯¹ç”Ÿæˆçš„BPMNæ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚",
    "evaluation_method_quote": "We propose the BEF4LLM framework, building upon the SIQ framework [13], consisting of 39 metrics that allow for assessing BPMN models in four process model quality dimensions - syntactic, pragmatic, semantic, and validity - specifically developed to assess LLMs in BPMN modeling.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "ä¸šåŠ¡è¿‡ç¨‹ç®¡ç†ï¼ˆBPMï¼‰ï¼Œä¸šåŠ¡è¿‡ç¨‹å»ºæ¨¡ã€‚",
    "problem_domain_quote": "The modeling of business processes using the Business Process Model and Notation (BPMN) is fundamental to organizational analysis, communication, and automation [1, 2].",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "BPMNï¼ˆä¸šåŠ¡è¿‡ç¨‹å»ºæ¨¡ç¬¦å·ï¼‰ï¼Œä¸€ç§å›¾å½¢åŒ–å»ºæ¨¡è¯­è¨€ã€‚",
    "language_quote": "Business process modeling refers to the systematic abstraction and representation of organizational activities as structured process models. Among available modeling approaches, the BPMN standard is extensively adopted in the information systems (IS) discipline due to its expressive power and rigorous semantics [2].",
    "data_size": "åŒ…å«105ä¸ªç²¾å¿ƒç­–åˆ’çš„æ–‡æœ¬-BPMNå¯¹ã€‚",
    "data_size_quote": "Using the BEF4LLM framework, a large-scale benchmark of open-source LLMs is conducted, including 17 different-sized LLMs of various families on 105 curated text-BPMN pairs.",
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": "2026",
    "last_updated_quote": "arXiv:2601.21787v1  [cs.SE]  29 Jan 2026",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "We introduce BEF4LLM, a novel LLM evaluation framework... Using the BEF4LLM framework, a large-scale benchmark... is conducted, including 17 different-sized LLMs... on 105 curated text-BPMN pairs.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "æ–‡æœ¬åˆ°BPMNæ¨¡å‹ç”Ÿæˆ",
    "task_granularity_quote": "We introduce the BEF4LLM framework for evaluating LLMs in text-to-BPMN tasks",
    "evaluation_metrics": "å¥æ³•è´¨é‡ã€è¯­ç”¨è´¨é‡ã€è¯­ä¹‰è´¨é‡å’Œæœ‰æ•ˆæ€§å››ä¸ªç»´åº¦ä¸‹çš„39ä¸ªå…·ä½“æŒ‡æ ‡ã€‚",
    "evaluation_metrics_quote": "We propose the BEF4LLM framework, building upon the SIQ framework [13], consisting of 39 metrics that allow for assessing BPMN models in four process model quality dimensions - syntactic, pragmatic, semantic, and validity...",
    "input_modality": "è‡ªç„¶è¯­è¨€æ–‡æœ¬",
    "input_modality_quote": "Recent advances in large language models (LLMs) have significantly expanded the possibilities for generating BPMN models directly from natural language...",
    "output_modality": "BPMNæ¨¡å‹ï¼ˆç»“æ„åŒ–å›¾å½¢åŒ–è¡¨ç¤ºï¼‰",
    "output_modality_quote": "The creation of Business Process Model and Notation (BPMN) models is a complex and time-consuming task...",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ï¼ˆBPMN-XMLï¼‰",
    "task_io_type_quote": "Valid BPMN-XML generation remains a major challenge for current LLMs",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "é¦–ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°LLMåœ¨BPMNå»ºæ¨¡ä»»åŠ¡ä¸Šçš„å¤§è§„æ¨¡åŸºå‡†ï¼ŒåŒ…å«ä¸äººç±»å»ºæ¨¡ä¸“å®¶çš„æ€§èƒ½å¯¹æ¯”ã€‚",
    "unique_features_quote": "This marks the first extensive benchmark of open-source LLMs in BPMN modeling... Further, we compare the performance of LLMs with human experts on a smaller subset of the data.",
    "data_size_quantity": 105,
    "data_size_unit": "ä¸ª",
    "last_updated_year": 2026,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['BPMN']",
    "dimension_normalized": "['BPMNæ¨¡å‹è´¨é‡', 'å¥æ³•è´¨é‡', 'è¯­ç”¨è´¨é‡', 'è¯­ä¹‰è´¨é‡', 'æœ‰æ•ˆæ€§']",
    "evaluation_method_normalized": "['å¥æ³•è´¨é‡', 'è¯­ç”¨è´¨é‡', 'è¯­ä¹‰è´¨é‡', 'æœ‰æ•ˆæ€§']",
    "problem_domain_normalized": "['ä¸šåŠ¡è¿‡ç¨‹ç®¡ç†', 'ä¸šåŠ¡è¿‡ç¨‹å»ºæ¨¡']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.23009_output/content.md",
    "benchmark_name": "SolEval+ Benchmark",
    "benchmark_name_quote": "Experiments on the SolEval+ Benchmark, a rigorous suite derived from high-quality real-world projects, demonstrate that SolAgent achieves a Pass@1 rate of up to 64.39%...",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Experiments on the SolEval+ Benchmark, a rigorous suite derived from high-quality real-world projects, demonstrate that SolAgent achieves a Pass@1 rate of up to 64.39%...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„æµ‹åŸºå‡†æ—¨åœ¨è¯„ä¼°æ™ºèƒ½åˆçº¦ï¼ˆSolidityä»£ç ï¼‰çš„ç”Ÿæˆè´¨é‡ï¼ŒåŒ…æ‹¬åŠŸèƒ½æ­£ç¡®æ€§å’Œå®‰å…¨æ€§ã€‚",
    "task_description_quote": "Smart contracts are the backbone of the decentralized web, yet ensuring their functional correctness and security remains a critical challenge. ... Experiments on the SolEval+ Benchmark, a rigorous suite derived from high-quality real-world projects...",
    "dimension": "æ™ºèƒ½åˆçº¦çš„åŠŸèƒ½æ­£ç¡®æ€§ã€å®‰å…¨æ€§ã€å¯æ‰§è¡Œæ€§",
    "dimension_quote": "Smart contract development demands a rigorous balance between functional correctness and security assurance. ... The primary barrier to automated smart contract generation is the correctness bottleneck. ... They tend to generate code that remains vulnerable to edge cases (e.g., reentrancy, integer overflow)...",
    "evaluation_method": "Pass@1ç‡ã€ç¼–è¯‘å¤±è´¥ç‡ã€æµ‹è¯•å¤±è´¥ç‡ã€åŒ…å«æ¼æ´çš„åˆçº¦ç™¾åˆ†æ¯”",
    "evaluation_method_quote": "Experiments on the SolEval+ Benchmark... demonstrate that SolAgent achieves a Pass@1 rate of up to 64.39%... We measured the Compilation Failure Rate and then analyzed the contracts using unit tests (for functional correctness) and Slither (for security).",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "åŒºå—é“¾ã€æ™ºèƒ½åˆçº¦ã€Solidityå¼€å‘",
    "problem_domain_quote": "Smart contracts, self-executing programs running on blockchains like Ethereum, have become the cornerstone of the decentralized web (Web3)... SolAgent: A Specialized Multi-Agent Framework for Solidity Code Generation",
    "problem_difficulty": "æºè‡ªé«˜è´¨é‡çœŸå®ä¸–ç•Œé¡¹ç›®çš„ä¸¥æ ¼æµ‹è¯•é›†",
    "problem_difficulty_quote": "Experiments on the SolEval+ Benchmark, a rigorous suite derived from high-quality real-world projects...",
    "language": "Solidity",
    "language_quote": "SolAgent: A Specialized Multi-Agent Framework for Solidity Code Generation",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": "æºè‡ªé«˜è´¨é‡çš„çœŸå®ä¸–ç•Œæ™ºèƒ½åˆçº¦é¡¹ç›®",
    "source_type_quote": "Experiments on the SolEval+ Benchmark, a rigorous suite derived from high-quality real-world projects...",
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "æ™ºèƒ½åˆçº¦ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "SolAgent: A Specialized Multi-Agent Framework for Solidity Code Generation",
    "evaluation_metrics": "Pass@1, Compilation Failure Rate, Test Failure Rate, Vulnerable Contracts %",
    "evaluation_metrics_quote": "Experiments on the SolEval+ Benchmark... demonstrate that SolAgent achieves a Pass@1 rate of up to 64.39%... We measured the Compilation Failure Rate and then analyzed the contracts using unit tests (for functional correctness) and Slither (for security).",
    "input_modality": "è‡ªç„¶è¯­è¨€éœ€æ±‚æè¿°",
    "input_modality_quote": "We prompted GPT-5.1 and Claude-Sonnet-4.5 to generate 50 smart contracts based on requirements from a standard dataset.",
    "output_modality": "Solidityæ™ºèƒ½åˆçº¦ä»£ç ",
    "output_modality_quote": "SolAgent: A Specialized Multi-Agent Framework for Solidity Code Generation",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "We prompted GPT-5.1 and Claude-Sonnet-4.5 to generate 50 smart contracts based on requirements from a standard dataset.",
    "execution_environment": "Forgeæµ‹è¯•æ¡†æ¶ã€Slitheré™æ€åˆ†æå·¥å…·",
    "execution_environment_quote": "In the inner loop, the agent utilizes the Forge [8] compiler and testing framework to iteratively verify and fix functional errors... In the outer loop, it leverages the Slither [11, 38] static analyzer to detect and remediate security vulnerabilities.",
    "unique_features": "ä¸“æ³¨äºæ™ºèƒ½åˆçº¦ï¼ˆSolidityï¼‰ç”Ÿæˆä¸å®‰å…¨æ€§çš„è¯„æµ‹åŸºå‡†ï¼Œæºè‡ªçœŸå®ä¸–ç•Œé«˜è´¨é‡é¡¹ç›®ï¼Œå¼ºè°ƒåŠŸèƒ½æ­£ç¡®æ€§ä¸å®‰å…¨æ¼æ´æ£€æµ‹çš„åŒé‡è¯„ä¼°ã€‚",
    "unique_features_quote": "Experiments on the SolEval+ Benchmark, a rigorous suite derived from high-quality real-world projects... Smart contract development demands a rigorous balance between functional correctness and security assurance.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Solidity']",
    "dimension_normalized": "['æ™ºèƒ½åˆçº¦çš„åŠŸèƒ½æ­£ç¡®æ€§', 'å®‰å…¨æ€§', 'å¯æ‰§è¡Œæ€§']",
    "evaluation_method_normalized": "['Pass@1', 'Compilation Failure Rate', 'Test Failure Rate', 'Vulnerable Contracts %']",
    "problem_domain_normalized": "['åŒºå—é“¾', 'æ™ºèƒ½åˆçº¦', 'Solidityå¼€å‘']",
    "source_type_normalized": "['æºè‡ªé«˜è´¨é‡çš„çœŸå®ä¸–ç•Œæ™ºèƒ½åˆçº¦é¡¹ç›®']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.23059_output/content.md",
    "benchmark_name": "Tufano et al. (2019) æ•°æ®é›†",
    "benchmark_name_quote": "A key contribution of the work by Tufano et al. [56] was the release of a benchmark dataset, which remains a standard for training and evaluating ABF models [3, 8, 14, 19, 23, 24, 39, 63, 65].",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹å¹¶æ„å»ºå…¶æ‰©å±•ç‰ˆæœ¬",
    "is_original_proposal_quote": "First, we build a new dataset for ABF based on the one introduced by Tufano et al. [56].",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è‡ªåŠ¨ç¨‹åºä¿®å¤ï¼ˆAutomated Bug Fixing, ABFï¼‰ï¼Œæ—¨åœ¨å°†åŒ…å«é”™è¯¯çš„ä»£ç æ–¹æ³•ï¼ˆbuggy methodï¼‰è½¬æ¢ä¸ºä¿®å¤åçš„ç‰ˆæœ¬ï¼ˆfixed versionï¼‰ã€‚",
    "task_description_quote": "ABF involves transforming a buggy method into its fixed equivalent.",
    "dimension": "ä»£ç ä¿®å¤çš„å‡†ç¡®æ€§ï¼Œä»¥åŠä»£ç æ³¨é‡Šå¯¹ä¿®å¤èƒ½åŠ›çš„å½±å“ã€‚",
    "dimension_quote": "In this study, we investigate how the presence or absence of comments, both during training and at inference time, impacts the bug-fixing capabilities of LLMs.",
    "evaluation_method": "é€šè¿‡å¾®è°ƒæ¨¡å‹ï¼ˆå¦‚CodeT5+ã€DeepSeek-Coderï¼‰å¹¶æ¯”è¾ƒå…¶åœ¨æœ‰/æ— æ³¨é‡Šçš„ä¸åŒè®­ç»ƒå’Œæ¨ç†ç»„åˆä¸‹çš„å‡†ç¡®æ€§ï¼ˆaccuracyï¼‰ã€‚",
    "evaluation_method_quote": "We fine-tuned two types of modelsâ€”CodeT5+ [64] and DeepSeek-Coder [20]â€”under all combinations of these conditions (with vs. without comments)... The greatest improvements occur when comments are present in both phases, increasing accuracy by up to threefold.",
    "context_dependency": "å•æ–¹æ³•çº§åˆ«ï¼ˆsingle methodï¼‰ã€‚",
    "context_dependency_quote": "Such a dataset, mined from GitHub, contains couples of buggy Java methods and their respective fixed versions.",
    "problem_domain": "é€šç”¨è½¯ä»¶å·¥ç¨‹ï¼Œå…·ä½“ä¸ºç¨‹åºç¼ºé™·ä¿®å¤ã€‚",
    "problem_domain_quote": "Automated Bug Fixing (ABF) has emerged as one of the most researched and impactful coding tasks... ABF involves identifying and resolving bugs within a method.",
    "problem_difficulty": "æ–‡ä¸­æœªæ˜ç¡®æè¿°åŸå§‹æ•°æ®é›†çš„éš¾åº¦ï¼Œä½†æåŠäº†æŒ‰æ–¹æ³•é•¿åº¦ï¼ˆtokenæ•°ï¼‰åˆ’åˆ†äº†å°å‹ã€ä¸­å‹å’Œå¤§å‹æ–¹æ³•ã€‚",
    "problem_difficulty_quote": "Differently from the previous work, which only considered small- (â‰¤50 tokens) and medium-sized (â‰¤100 tokens) methods in two distinct datasets, we merge them in a single dataset, in which we also include big methods (â‰¤512 tokens).",
    "language": "Java",
    "language_quote": "Such a dataset, mined from GitHub, contains couples of buggy Java methods and their respective fixed versions.",
    "data_size": "æ–‡ä¸­æœªæ˜ç¡®æåŠåŸå§‹æ•°æ®é›†çš„å…·ä½“å®ä¾‹æ•°é‡ã€‚",
    "data_size_quote": NaN,
    "source_type": "ä»GitHubä»“åº“å’Œæäº¤è®°å½•ä¸­æŒ–æ˜ï¼ˆmined from GitHubï¼‰ã€‚",
    "source_type_quote": "Such a dataset, mined from GitHub, contains couples of buggy Java methods and their respective fixed versions.",
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆç”±Tufanoç­‰äººæ„å»ºï¼‰ã€‚",
    "build_type_quote": "A key contribution of the work by Tufano et al. [56] was the release of a benchmark dataset...",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ä¿®å¤ï¼ˆå°†buggyæ–¹æ³•è½¬æ¢ä¸ºfixedæ–¹æ³•ï¼‰ã€‚",
    "task_granularity_quote": "ABF involves transforming a buggy method into its fixed equivalent.",
    "evaluation_metrics": "å‡†ç¡®æ€§ï¼ˆaccuracyï¼‰ã€‚",
    "evaluation_metrics_quote": "The greatest improvements occur when comments are present in both phases, increasing accuracy by up to threefold.",
    "input_modality": "ä»£ç ï¼ˆå¯èƒ½é™„å¸¦æ³¨é‡Šï¼‰ã€‚",
    "input_modality_quote": "The input sequence represent a buggy method...",
    "output_modality": "ä»£ç ï¼ˆä¿®å¤åçš„æ–¹æ³•ï¼‰ã€‚",
    "output_modality_quote": "...while the output sequence is the fixed version of the same method.",
    "task_io_type": "ä»£ç åˆ°ä»£ç ï¼ˆbuggy code to fixed codeï¼‰ã€‚",
    "task_io_type_quote": "ABF involves transforming a buggy method into its fixed equivalent.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "1. åŸå§‹æ•°æ®é›†å¯¹ä»£ç è¿›è¡Œäº†é¢„å¤„ç†ï¼ˆæŠ½è±¡æ ‡è¯†ç¬¦ã€ç§»é™¤æ³¨é‡Šï¼‰ä»¥é€‚åº”æ—©æœŸRNNæ¨¡å‹ã€‚2. æœ¬æ–‡åŸºäºæ­¤æ„å»ºäº†ä¸€ä¸ªæ–°æ•°æ®é›†ï¼Œä¿ç•™äº†åŸå§‹ä»£ç ï¼ˆåŒ…æ‹¬æ ‡è¯†ç¬¦å’Œæ³¨é‡Šï¼‰ï¼Œå¹¶æ‰©å±•äº†æ–¹æ³•é•¿åº¦èŒƒå›´ï¼ˆâ‰¤512 tokensï¼‰ã€‚3. æœ¬æ–‡ä½¿ç”¨LLMä¸ºç¼ºå°‘æ³¨é‡Šçš„æ–¹æ³•è‡ªåŠ¨ç”Ÿæˆäº†äº”ç§ç±»å‹çš„é«˜è´¨é‡æ³¨é‡Šï¼ˆæè¿°æ–¹æ³•åŠŸèƒ½ã€å®ç°æ–¹å¼ã€ä½¿ç”¨æ„å›¾ã€å­˜åœ¨åŸå› ã€å±æ€§/æ¡ä»¶ï¼‰ã€‚",
    "unique_features_quote": "Given the limitations of the specific ML model they adopted (Recurrent Neural Networks â€” RNNs), they needed to pre-process the source code of the methods so that identifiers are abstracted (i.e., they appear like IDi, where i is an incremental number) and comments are removed... First, we build a new dataset for ABF based on the one introduced by Tufano et al. [56]. To do this, we consider the same repositories and commits, but we re-extract the instances by considering the raw source code (i.e., we do not abstract identifiers and keep the original comments)... we also include big methods (â‰¤512 tokens)... we use an LLM to generate five comments, documenting what the method does, how it does it, the intended usage of the method, the reason why the method is there, and properties of the method (e.g., pre-conditions and post-conditions).",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Java']",
    "dimension_normalized": "['ä»£ç ä¿®å¤çš„å‡†ç¡®æ€§', 'ä»£ç æ³¨é‡Šå¯¹ä¿®å¤èƒ½åŠ›çš„å½±å“']",
    "evaluation_method_normalized": "['å‡†ç¡®æ€§']",
    "problem_domain_normalized": "['é€šç”¨è½¯ä»¶å·¥ç¨‹', 'ç¨‹åºç¼ºé™·ä¿®å¤']",
    "source_type_normalized": "['ä»GitHubä»“åº“å’Œæäº¤è®°å½•ä¸­æŒ–æ˜']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ä¿®å¤",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2601.21947_output/content.md",
    "benchmark_name": "ToolBench",
    "benchmark_name_quote": "For a model like Llama-3-8B (Dubey et al., 2024) with a vocabulary of 128,256, integrating a large benchmark like ToolBench would require adding nearly 47,000 new tokens.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Evaluation results with nearly 47,000 tools show that ToolWeaver significantly outperforms state-of-the-art methods... (è¿™è¡¨æ˜æœ¬æ–‡æ˜¯ä½¿ç”¨ä¸€ä¸ªåŒ…å«çº¦47,000ä¸ªå·¥å…·çš„æ•°æ®é›†è¿›è¡Œè¯„æµ‹ï¼Œè€Œéæå‡ºè¯¥æ•°æ®é›†)",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„æµ‹åŸºå‡†æ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½¿ç”¨å¤–éƒ¨å·¥å…·ï¼ˆå¦‚APIï¼‰çš„èƒ½åŠ›ï¼Œä»¥å®Œæˆå¤æ‚çš„ã€äº¤äº’å¼çš„ç°å®ä¸–ç•Œä»»åŠ¡ã€‚",
    "task_description_quote": "LLMs have rapidly evolved into powerful interactive agents by integrating with external tools, enabling them to access dynamic information and perform comprehensive real-world tasks... The growing scale of toolsets, highlighted by benchmarks like ToolBench (Qin et al., 2023) and API-Bank (Li et al., 2023), creates a major scalability problem.",
    "dimension": "å·¥å…·ä½¿ç”¨çš„å¯æ‰©å±•æ€§ã€æ³›åŒ–èƒ½åŠ›å’Œå¤æ‚æ¨ç†èƒ½åŠ›ï¼ˆç‰¹åˆ«æ˜¯å·¥å…·é—´çš„åä½œå…³ç³»ï¼‰ã€‚",
    "dimension_quote": "This approach also creates a semantic bottleneck that hinders the learning of collaborative tool relationships... To address these challenges, we propose ToolWeaver, a framework that fundamentally rethinks tool representation... enabling the model to learn from a dense collaborative signal.",
    "evaluation_method": "æ–‡ä¸­æœªæ˜ç¡®æè¿°ToolBenchçš„å…·ä½“è¯„ä¼°æ–¹æ³•ã€‚æœ¬æ–‡é€šè¿‡åœ¨å…¶ä¸Šæµ‹è¯•ToolWeaveræ¨¡å‹æ¥è¯„ä¼°å¤æ‚ä»»åŠ¡å®Œæˆåº¦ã€‚",
    "evaluation_method_quote": "Experimental validation on a large benchmark of nearly 47,000 tools demonstrates that ToolWeaver significantly outperforms state-of-the-art methods in complex task completion...",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "é€šç”¨å·¥å…·ä½¿ç”¨ï¼Œæ¶µç›–ä»é€šç”¨æœåŠ¡åˆ°é¢†åŸŸç‰¹å®šAPIçš„å„ç§å·¥å…·ã€‚",
    "problem_domain_quote": "Concurrently, the number and diversity of available tools have grown substantially, ranging from general services to domain-specific APIs...",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": NaN,
    "language_quote": NaN,
    "data_size": "åŒ…å«è¿‘47,000ä¸ªå·¥å…·çš„å¤§è§„æ¨¡åŸºå‡†ã€‚",
    "data_size_quote": "Evaluation results with nearly 47,000 tools show that ToolWeaver significantly outperforms state-of-the-art methods... integrating a large benchmark like ToolBench would require adding nearly 47,000 new tokens.",
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "å·¥å…·é€‰æ‹©ä¸è°ƒç”¨ï¼ˆTool Selection and Invocationï¼‰ã€‚",
    "task_granularity_quote": "To streamline this process, a promising generative paradigm (Hao et al., 2023; Wang et al., 2024b) reframes tool selection as a next-token prediction task.",
    "evaluation_metrics": NaN,
    "evaluation_metrics_quote": NaN,
    "input_modality": "è‡ªç„¶è¯­è¨€ç”¨æˆ·æŸ¥è¯¢ã€‚",
    "input_modality_quote": "Given a user query q and a large tool corpus D = {d1, . . . , dN} where |D| = N, an agent typically follows a multi-stage process...",
    "output_modality": "å·¥å…·æ ‡è¯†ç¬¦åºåˆ—ï¼ˆç”¨äºé€‰æ‹©å·¥å…·ï¼‰ä»¥åŠå¯èƒ½çš„å‚æ•°å’ŒåŠ¨ä½œã€‚",
    "output_modality_quote": "This is achieved by mapping each tool d âˆˆD to a unique, specially added token in the language modelâ€™s vocabulary... Instead of a single token, it represents each tool as a compositional sequence of discrete codes.",
    "task_io_type": "æ–‡æœ¬åˆ°å·¥å…·è°ƒç”¨åºåˆ—ã€‚",
    "task_io_type_quote": "Given a user query q... reframes tool selection as a next-token prediction task.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "è¯¥åŸºå‡†ï¼ˆToolBenchï¼‰ä»¥å…¶å¤§è§„æ¨¡ï¼ˆè¿‘47,000ä¸ªå·¥å…·ï¼‰è€Œè‘—ç§°ï¼Œç”¨äºæµ‹è¯•LLMå·¥å…·ä½¿ç”¨çš„å¯æ‰©å±•æ€§ã€‚",
    "unique_features_quote": "The growing scale of toolsets, highlighted by benchmarks like ToolBench (Qin et al., 2023)... integrating a large benchmark like ToolBench would require adding nearly 47,000 new tokens.",
    "data_size_quantity": 47000,
    "data_size_unit": "ä¸ªå·¥å…·",
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['å·¥å…·ä½¿ç”¨çš„å¯æ‰©å±•æ€§', 'æ³›åŒ–èƒ½åŠ›', 'å¤æ‚æ¨ç†èƒ½åŠ›ï¼ˆç‰¹åˆ«æ˜¯å·¥å…·é—´çš„åä½œå…³ç³»ï¼‰']",
    "evaluation_method_normalized": "[]",
    "problem_domain_normalized": "['é€šç”¨å·¥å…·ä½¿ç”¨', 'æ¶µç›–ä»é€šç”¨æœåŠ¡åˆ°é¢†åŸŸç‰¹å®šAPIçš„å„ç§å·¥å…·']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2602.00933_output/content.md",
    "benchmark_name": "MCP-Atlas",
    "benchmark_name_quote": "We introduce MCP-Atlas, a large-scale benchmark for evaluating tool-use competency, comprising 36 real MCP servers and 220 tools.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We introduce MCP-Atlas, a large-scale benchmark for evaluating tool-use competency...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä»£ç†åœ¨ç°å®ã€å¤šæ­¥éª¤å·¥ä½œæµä¸­ä½¿ç”¨å¤–éƒ¨å·¥å…·çš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆMCPï¼‰å‘ç°å’Œè°ƒç”¨å·¥å…·çš„èƒ½åŠ›ã€‚",
    "task_description_quote": "MCP-Atlas is designed to evaluate LLM agents on realistic, single-turn tasks requiring multi-tool orchestration over live MCP servers.",
    "dimension": "å·¥å…·ä½¿ç”¨èƒ½åŠ›ï¼ŒåŒ…æ‹¬å·¥å…·å‘ç°ã€å‚æ•°åŒ–ã€è·¨æœåŠ¡å™¨ç¼–æ’ã€é”™è¯¯æ¢å¤å’Œæ•ˆç‡ã€‚",
    "dimension_quote": "complemented by internal diagnostics on tool discovery, parameterization, syntax, error recovery, and efficiency.",
    "evaluation_method": "åŸºäºå£°æ˜çš„è¯„åˆ†è§„åˆ™ï¼Œæ ¹æ®æ¨¡å‹æœ€ç»ˆç­”æ¡ˆä¸­æ»¡è¶³çš„äº‹å®å£°æ˜æˆäºˆéƒ¨åˆ†å­¦åˆ†ï¼Œè¾…ä»¥å†…éƒ¨è¯Šæ–­ã€‚",
    "evaluation_method_quote": "We score tasks using a claims-based rubric that awards partial credit based on the factual claims satisfied in the modelâ€™s final answer, complemented by internal diagnostics...",
    "context_dependency": "å•è½®ç”¨æˆ·æç¤ºï¼Œä½†éœ€è¦è·¨å¤šä¸ªæœåŠ¡å™¨è¿›è¡Œå¤šå·¥å…·ç¼–æ’ã€‚",
    "context_dependency_quote": "realistic single-turn prompts that necessitate complex, multi-tool workflows.",
    "problem_domain": "è·¨å¤šä¸ªé¢†åŸŸï¼ŒåŒ…æ‹¬æœç´¢ã€æ•°æ®åˆ†æã€ç”Ÿäº§åŠ›ã€é‡‘èå’Œç¼–ç ã€‚",
    "problem_domain_quote": "spanning diverse domains, including search, data analytics, productivity, finance, and coding (see Table 1).",
    "problem_difficulty": "å¤æ‚ã€å¤šæ­¥éª¤çš„ç°å®ä¸–ç•Œä»»åŠ¡ï¼Œéœ€è¦ç¼–æ’3-6æ¬¡å·¥å…·è°ƒç”¨ï¼Œå¤§å¤šæ•°ä»»åŠ¡éœ€è¦è·¨æœåŠ¡å™¨ç»„åˆï¼Œçº¦ä¸‰åˆ†ä¹‹ä¸€æ¶‰åŠæ¡ä»¶åˆ†æ”¯ã€‚",
    "problem_difficulty_quote": "Tasks use natural language prompts that avoid naming tools and reliably elicit complex behaviors, including 3-6 tool calls per task, with the vast majority of tasks requiring cross-server composition, and approximately one-third involving conditional branching.",
    "language": "è‡ªç„¶è¯­è¨€ï¼ˆæç¤ºï¼‰å’Œé€šè¿‡MCPåè®®è°ƒç”¨çš„å·¥å…·ï¼ˆä¸é™äºç‰¹å®šç¼–ç¨‹è¯­è¨€ï¼‰ã€‚",
    "language_quote": "Tasks use natural language prompts...",
    "data_size": "åŒ…å«1000ä¸ªä»»åŠ¡ï¼Œæ¶µç›–36ä¸ªæœåŠ¡å™¨å’Œ220ä¸ªå·¥å…·ã€‚å…¬å¼€å‘å¸ƒäº†500ä¸ªä»»åŠ¡çš„å­é›†ã€‚",
    "data_size_quote": "The full data set includes 1,000 tasks spanning 36 servers and 220 tools. ... We open source a 500-task subset from the MCP-Atlas benchmark data.",
    "source_type": "é€šè¿‡ç³»ç»ŸåŒ–äººå·¥éªŒè¯è€Œéè‡ªåŠ¨ç”Ÿæˆåˆ›å»ºçš„ä»»åŠ¡ï¼Œä½¿ç”¨çœŸå®çš„MCPæœåŠ¡å™¨å®ç°ã€‚",
    "source_type_quote": "we achieve scale (1,000 tasks across 36 servers and 220 tools) while maintaining quality through systematic manual verification rather than automated generation. ... we ensure evaluation fidelity by using exclusively real MCP servers...",
    "last_updated": "2026-01-31 (æ ¹æ®arXivç‰ˆæœ¬æ—¥æœŸ)",
    "last_updated_quote": "arXiv:2602.00933v1  [cs.SE]  31 Jan 2026",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆç”±Scale AIå›¢é˜Ÿæ„å»ºï¼‰",
    "build_type_quote": "1Scale AI ... We introduce MCP-Atlas...",
    "contamination_status": "é€šè¿‡ä¿ç•™500ä¸ªä»»åŠ¡çš„éšè—éªŒè¯é›†æ¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œç¡®ä¿æœªæ¥æ’è¡Œæ¦œçš„å®Œæ•´æ€§ã€‚",
    "contamination_status_quote": "The remaining 500 tasks are retained as a held-out validation set to prevent overfitting and enable future leaderboard integrity.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "å·¥å…·ä½¿ç”¨ä¸ç¼–æ’ï¼ˆæ ¹æ®è‡ªç„¶è¯­è¨€æŒ‡ä»¤å‘ç°ã€è°ƒç”¨å¹¶ç»„åˆå¤šä¸ªå¤–éƒ¨å·¥å…·ä»¥å®Œæˆä»»åŠ¡ï¼‰ã€‚",
    "task_granularity_quote": "evaluating tool-use competency ... requiring agents to identify and orchestrate 3-6 tool calls across multiple servers.",
    "evaluation_metrics": "åŸºäºå£°æ˜çš„é€šè¿‡ç‡ï¼ˆéƒ¨åˆ†å­¦åˆ†ï¼‰ï¼Œä»¥åŠå†…éƒ¨è¯Šæ–­æŒ‡æ ‡ï¼ˆå·¥å…·å‘ç°ã€å‚æ•°/ç±»å‹æ­£ç¡®æ€§ã€é”™è¯¯æ¢å¤ã€æ•ˆç‡ï¼‰ã€‚",
    "evaluation_metrics_quote": "pass rates ... claims-based rubric ... internal diagnostics capturing discovery, parameter/type correctness, error recovery, and efficiency",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆå•è½®æç¤ºï¼‰ã€‚",
    "input_modality_quote": "Tasks use natural language prompts...",
    "output_modality": "æ–‡æœ¬ï¼ˆæœ€ç»ˆç­”æ¡ˆï¼‰ï¼Œå…¶ä¸­åŒ…å«ä»å·¥å…·è¾“å‡ºä¸­æå–çš„äº‹å®å£°æ˜ã€‚",
    "output_modality_quote": "We score tasks using a claims-based rubric that awards partial credit based on the factual claims satisfied in the modelâ€™s final answer...",
    "task_io_type": "æ–‡æœ¬åˆ°æ–‡æœ¬ï¼ˆä½†ä¸­é—´è¿‡ç¨‹æ¶‰åŠå·¥å…·è°ƒç”¨ä¸ç¼–æ’ï¼‰ã€‚",
    "task_io_type_quote": "single-turn, natural-language request ... final answer",
    "execution_environment": "å®¹å™¨åŒ–ç¯å¢ƒï¼ŒåŒ…å«çœŸå®çš„MCPæœåŠ¡å™¨ã€æ²™ç›’åŒ–æ–‡ä»¶ç³»ç»Ÿå’Œå…è®¸åˆ—è¡¨ç½‘ç»œå‡ºå£ï¼Œæ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„å®‰å…¨çº¦æŸã€‚",
    "execution_environment_quote": "Servers are hosted in an isolated container, mirroring real-world security constraints (e.g., sandboxed file systems and allow-listed network egress).",
    "unique_features": "1. ä½¿ç”¨çœŸå®çš„MCPæœåŠ¡å™¨è€Œéæ¨¡æ‹Ÿç¯å¢ƒã€‚2. æ¯ä¸ªä»»åŠ¡åŒ…å«ç³»ç»Ÿæ€§çš„å¹²æ‰°å·¥å…·ï¼ˆ5-10ä¸ªï¼‰ä»¥æµ‹è¯•å·¥å…·å‘ç°èƒ½åŠ›ã€‚3. é‡‡ç”¨åŸºäºå£°æ˜çš„è¯„åˆ†è§„åˆ™ï¼Œå‡å°‘ä¸»è§‚æ€§ã€‚4. å¼ºè°ƒè·¨æœåŠ¡å™¨ç¼–æ’å’Œæ¡ä»¶é€»è¾‘ã€‚",
    "unique_features_quote": "using exclusively real MCP servers and systematically including 5â€“10 plausible distractors per task ... claims-based evaluation ... the vast majority of tasks requiring cross-server composition, and approximately one-third involving conditional branching.",
    "data_size_quantity": 1000,
    "data_size_unit": "ä¸ªä»»åŠ¡",
    "last_updated_year": 2026,
    "last_updated_month": 1,
    "last_updated_day": 31,
    "language_normalized": "['è‡ªç„¶è¯­è¨€', 'MCPåè®®']",
    "dimension_normalized": "['å·¥å…·ä½¿ç”¨èƒ½åŠ›', 'å·¥å…·å‘ç°', 'å‚æ•°åŒ–', 'è·¨æœåŠ¡å™¨ç¼–æ’', 'é”™è¯¯æ¢å¤', 'æ•ˆç‡']",
    "evaluation_method_normalized": "['é€šè¿‡ç‡', 'éƒ¨åˆ†å­¦åˆ†', 'å·¥å…·å‘ç°', 'å‚æ•°/ç±»å‹æ­£ç¡®æ€§', 'é”™è¯¯æ¢å¤', 'æ•ˆç‡']",
    "problem_domain_normalized": "['æœç´¢', 'æ•°æ®åˆ†æ', 'ç”Ÿäº§åŠ›', 'é‡‘è', 'ç¼–ç ']",
    "source_type_normalized": "['äººå·¥éªŒè¯', 'MCPæœåŠ¡å™¨']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "æ–‡æœ¬",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°æ–‡æœ¬",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  }
]