{"id": "2411.04905", "title": "OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models", "abstract": "Large language models (LLMs) for code have become indispensable in various domains, including code generation, reasoning tasks and agent systems. While open-access code LLMs are increasingly approaching the performance levels of proprietary models, high-quality code LLMs suitable for rigorous scientific investigation, particularly those with reproducible data processing pipelines and transparent training protocols, remain limited. The scarcity is due to various challenges, including resource constraints, ethical considerations, and the competitive advantages of keeping models advanced. To address the gap, we introduce OpenCoder, a top-tier code LLM that not only achieves performance comparable to leading models but also serves as an \"open cookbook\" for the research community. Unlike most prior efforts, we release not only model weights and inference code, but also the reproducible training data, complete data processing pipeline, rigorous experimental ablation results, and detailed training protocols for open scientific research. Through this comprehensive release, we identify the key ingredients for building a top-tier code LLM: (1) code optimized heuristic rules for data cleaning and methods for data deduplication, (2) recall of text corpus related to code and (3) high-quality synthetic data in both annealing and supervised fine-tuning stages. By offering this level of openness, we aim to broaden access to all aspects of a top-tier code LLM, with OpenCoder serving as both a powerful model and an open foundation to accelerate research, and enable reproducible advancements in code AI.", "arxiv_url": "https://arxiv.org/abs/2411.04905", "authors": ["Siming Huang", "Tianhao Cheng", "J. K. Liu", "Jiaran Hao", "Liuyihan Song", "Yang Xu", "J. Yang", "Jiaheng Liu", "Chenchen Zhang", "Linzheng Chai", "Ruifeng Yuan", "Zhaoxiang Zhang", "Jie Fu", "Qian Liu", "Ge Zhang", "Zili Wang", "Yuan Qi", "Yinghui Xu", "Wei Chu"], "first_author": "Siming Huang", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Pre-Training", "tags": ["Reproducible data-processing pipeline", "Code-level deduplication", "Heuristic code cleaning rules", "Text corpus retrieval for code", "High-quality synthetic annealing data", "Two-stage instruction tuning", "Decontamination procedures", "Ablation studies on data ingredients"], "summary": "本文提出并开源了OpenCoder——一款顶级代码大模型及其可复现的数据处理流水线、去重/清洗规则、合成数据策略和完整训练协议，并通过详尽消融实验揭示构建高性能代码LLM的关键数据要素。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2411.04905v3", "published": "2024-11-07", "update_time": "2025-03-20", "download_time": "2025-12-16 10:35:10"}
{"id": "2409.12186", "title": "Qwen2.5-Coder Technical Report", "abstract": "In this report, we introduce the Qwen2.5-Coder series, a significant upgrade from its predecessor, CodeQwen1.5. This series includes six models: Qwen2.5-Coder-(0.5B/1.5B/3B/7B/14B/32B). As a code-specific model, Qwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrained on a vast corpus of over 5.5 trillion tokens. Through meticulous data cleaning, scalable synthetic data generation, and balanced data mixing, Qwen2.5-Coder demonstrates impressive code generation capabilities while retaining general and math skills. These models have been evaluated on a wide range of code-related tasks, achieving state-of-the-art (SOTA) performance across more than 10 benchmarks, including code generation, completion, reasoning, and repair, consistently outperforming larger models of the same model size. We believe that the release of the Qwen2.5-Coder series will advance research in code intelligence and, with its permissive licensing, support wider adoption by developers in real-world applications.", "arxiv_url": "https://arxiv.org/abs/2409.12186", "authors": ["Binyuan Hui", "Jian Yang", "Zeyu Cui", "Jiaxi Yang", "Dayiheng Liu", "Lei Zhang", "Tianyu Liu", "Jiajun Zhang", "Bowen Yu", "Keming Lu", "Kai Dang", "Yang Fan", "Yichang Zhang", "An Yang", "Rui Men", "Fei Huang", "Bo Zheng", "Yibo Miao", "Shanghaoran Quan", "Yunlong Feng", "Xingzhang Ren", "Xuancheng Ren", "Jingren Zhou", "Junyang Lin"], "first_author": "Binyuan Hui", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Pre-Training", "tags": ["file-level pretraining", "repo-level pretraining", "instruction-tuning recipe", "scalable synthetic code data generation", "data decontamination and filtering", "balanced code/math/natural-language mixture", "long-context code modeling", "evaluation across code generation/completion/reasoning/repair"], "summary": "本文介绍并发布了Qwen2.5-Coder系列代码专用大模型，通过在超大规模混合语料上继续预训练并结合精细的数据清洗、合成数据生成与指令微调，从而在多项代码基准上取得了领先性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2409.12186v3", "published": "2024-09-18", "update_time": "2024-11-12", "download_time": "2025-12-16 10:36:02"}
{"id": "2308.12950", "title": "Code Llama: Open Foundation Models for Code", "abstract": "We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67% and 65% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.", "arxiv_url": "https://arxiv.org/abs/2308.12950", "authors": ["Baptiste Rozière", "Jonas Gehring", "Fabian Gloeckle", "Sten Sootla", "Itai Gat", "Xiaoqing Ellen Tan", "Yossi Adi", "Jingyu Liu", "Romain Sauvestre", "Tal Remez", "Jérémy Rapin", "Artyom Kozhevnikov", "Ivan Evtimov", "Joanna Bitton", "Manish Bhatt", "Cristian Canton Ferrer", "Aaron Grattafiori", "Wenhan Xiong", "Alexandre Défossez", "Jade Copet", "Faisal Azhar", "Hugo Touvron", "Louis Martin", "Nicolas Usunier", "Thomas Scialom", "Gabriel Synnaeve"], "first_author": "Baptiste Rozière", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Pre-Training", "tags": ["Infilling-capable autoregressive training", "RoPE positional embedding scaling for 100k-token contexts", "Instruction fine-tuning with synthetic self-instruct unit tests", "Python-specialized fine-tuning", "Foundation-model specialization from general LLM", "Mixed code and natural-language training batches", "Permissive commercial-friendly model release"], "summary": "本文提出并开源了 Code Llama，一系列基于 Llama 2 的代码专用大模型，集成了中间填充能力、指令式微调、Python 专化及扩展到 100k 令牌的长上下文支持，并在多个代码生成基准上达成开源模型的领先性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2308.12950v3", "published": "2023-08-24", "update_time": "2024-01-31", "download_time": "2025-12-16 10:36:41"}
{"id": "2305.06161", "title": "StarCoder: may the source be with you!", "abstract": "The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.", "arxiv_url": "https://arxiv.org/abs/2305.06161", "authors": ["Raymond Li", "Loubna Ben Allal", "Yangtian Zi", "Niklas Muennighoff", "Denis Kocetkov", "Chenghao Mou", "Marc Marone", "Christopher Akiki", "Jia Li", "Jenny Chim", "Qian Liu", "Evgenii Zheltonozhskii", "Terry Yue Zhuo", "Thomas Wang", "Olivier Dehaene", "Mishig Davaadorj", "Joel Lamy-Poirier", "João Monteiro", "Oleh Shliazhko", "Nicolas Gontier", "Nicholas Meade", "Armel Zebaze", "Ming-Ho Yee", "Logesh Kumar Umapathi", "Jian Zhu", "Benjamin Lipkin", "Muhtasham Oblokulov", "Zhiruo Wang", "Rudra Murthy", "Jason Stillerman", "Siva Sankalp Patel", "Dmitry Abulkhanov", "Marco Zocca", "Manan Dey", "Zhihan Zhang", "Nour Fahmy", "Urvashi Bhattacharyya", "Wenhao Yu", "Swayam Singh", "Sasha Luccioni", "Paulo Villegas", "Maxim Kunakov", "Fedor Zhdanov", "Manuel Romero", "Tony Lee", "Nadav Timor", "Jennifer Ding", "Claire Schlesinger", "Hailey Schoelkopf", "Jan Ebert", "Tri Dao", "Mayank Mishra", "Alex Gu", "Jennifer Robinson", "Carolyn Jane Anderson", "Brendan Dolan-Gavitt", "Danish Contractor", "Siva Reddy", "Daniel Fried", "Dzmitry Bahdanau", "Yacine Jernite", "Carlos Muñoz Ferrandis", "Sean Hughes", "Thomas Wolf", "Arjun Guha", "Leandro von Werra", "Harm de Vries"], "first_author": "Raymond Li", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Pre-Training", "tags": ["8K-context code modeling", "Fill-in-the-Middle infilling", "Multi-Query Attention for fast large-batch inference", "PII detection and redaction pipeline with annotated dataset", "Attribution tracing via lightweight membership check + BM25 search", "Open-source responsible model release under OpenRAIL-M", "Python fine-tuning for specialized performance", "Comprehensive multi-benchmark evaluation", "Data governance and opt-out tooling for training corpora"], "summary": "本文介绍了BigCode社区开源的15.5B参数代码模型StarCoder及其基座StarCoderBase，具备8K上下文、代码填充能力和基于多查询注意力的快速大批量推理，并辅以PII去识别数据集、归属追踪工具及以更可商用的OpenRAIL-M许可进行的安全开放发布。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2305.06161v2", "published": "2023-05-09", "update_time": "2023-12-13", "download_time": "2025-12-16 10:37:26"}
{"id": "2307.14936", "title": "PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback", "abstract": "Large Language Models for Code (Code LLM) are flourishing. New and powerful models are released on a weekly basis, demonstrating remarkable performance on the code generation task. Various approaches have been proposed to boost the code generation performance of pre-trained Code LLMs, such as supervised fine-tuning, instruction tuning, reinforcement learning, etc. In this paper, we propose a novel RRTF (Rank Responses to align Test&Teacher Feedback) framework, which can effectively and efficiently boost pre-trained large language models for code generation. Under this framework, we present PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval benchmark. Furthermore, through an extensive evaluation on CoderEval and LeetCode benchmarks, we show that PanGu-Coder2 consistently outperforms all previous Code LLMs.", "arxiv_url": "https://arxiv.org/abs/2307.14936", "authors": ["Bo Shen", "Jiaxin Zhang", "Taihong Chen", "Daoguang Zan", "Bing Geng", "An Fu", "Muhan Zeng", "Ailun Yu", "Jichuan Ji", "Jingyang Zhao", "Yuenan Guo", "Qianxiang Wang"], "first_author": "Bo Shen", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Instruction-Tuning", "tags": ["Ranking-feedback fine-tuning", "Test-case-guided ranking", "Preference-ranking objective", "RLHF-alternative (ranking-based)", "Evol-Instruct sampling", "Instruction tuning for code", "Data-quality focused curation", "Efficient SFT-style optimization", "Pass-rate optimization"], "summary": "本文提出RRTF（Rank Responses to align Test&Teacher Feedback）框架，使用基于测试与教师偏好的响应排序作为反馈对预训练代码模型进行高效指令微调，从而得到PanGu‑Coder2并在多项基准上显著提升代码生成通过率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2307.14936v1", "published": "2023-07-27", "update_time": "2023-07-27", "download_time": "2025-12-16 10:37:54"}
{"id": "2306.08568", "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct", "abstract": "Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on HumanEval and HumanEval+. Our code, model weights, and data are public at https://github.com/nlpxucan/WizardLM", "arxiv_url": "https://arxiv.org/abs/2306.08568", "authors": ["Ziyang Luo", "Can Xu", "Pu Zhao", "Qingfeng Sun", "Xiubo Geng", "Wenxiang Hu", "Chongyang Tao", "Jing Ma", "Qingwei Lin", "Daxin Jiang"], "first_author": "Ziyang Luo", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Instruction-Tuning", "tags": ["Code Evol-Instruct", "Heuristic-driven instruction evolution", "Adversarial instruction generation", "Instruction complexity augmentation", "Time/space complexity constraints", "Instruction-following fine-tuning", "Robustness via evolved prompts", "Performance-oriented instruction design"], "summary": "本文提出面向代码的Code Evol-Instruct指令进化方法，对开源代码大模型进行指令微调以得到显著提升代码生成性能的模型，并实验证明在多项代码生成基准上优于现有开源（且部分闭源）模型，同时强调指令复杂性对性能的关键作用。", "quality": "High", "conference": "International Conference on Learning Representations (ICLR 2024)", "pdf_url": "https://arxiv.org/pdf/2306.08568v2", "published": "2023-06-14", "update_time": "2025-05-27", "download_time": "2025-12-16 10:38:29"}
{"id": "2107.03374", "title": "Evaluating Large Language Models Trained on Code", "abstract": "We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.", "arxiv_url": "https://arxiv.org/abs/2107.03374", "authors": ["Mark Chen", "Jerry Tworek", "Heewoo Jun", "Qiming Yuan", "Henrique Ponde de Oliveira Pinto", "Jared Kaplan", "Harri Edwards", "Yuri Burda", "Nicholas Joseph", "Greg Brockman", "Alex Ray", "Raul Puri", "Gretchen Krueger", "Michael Petrov", "Heidy Khlaaf", "Girish Sastry", "Pamela Mishkin", "Brooke Chan", "Scott Gray", "Nick Ryder", "Mikhail Pavlov", "Alethea Power", "Lukasz Kaiser", "Mohammad Bavarian", "Clemens Winter", "Philippe Tillet", "Felipe Petroski Such", "Dave Cummings", "Matthias Plappert", "Fotios Chantzis", "Elizabeth Barnes", "Ariel Herbert-Voss", "William Hebgen Guss", "Alex Nichol", "Alex Paino", "Nikolas Tezak", "Jie Tang", "Igor Babuschkin", "Suchir Balaji", "Shantanu Jain", "William Saunders", "Christopher Hesse", "Andrew N. Carr", "Jan Leike", "Josh Achiam", "Vedant Misra", "Evan Morikawa", "Alec Radford", "Matthew Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "Peter Welinder", "Bob McGrew", "Dario Amodei", "Sam McCandlish", "Ilya Sutskever", "Wojciech Zaremba"], "first_author": "Mark Chen", "category": ["Technical", "Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["unit-test-based evaluation", "pass@k metric", "docstring-conditioned program synthesis", "multi-sample generation and selection", "log-probability ranking of samples", "fine-tuning on standalone functions", "sandboxed execution for safe evaluation", "analysis of failure modes (long operation chains, variable binding)"], "summary": "本文提出并评估了基于公开代码微调的GPT类模型用于从Python docstring生成函数的方法，发布了基于单元测试的基准与pass@k评估指标，展示了模型性能、采样策略效果、局限性及潜在影响。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2107.03374v2", "published": "2021-07-07", "update_time": "2021-07-14", "download_time": "2025-12-16 10:39:10"}
{"id": "2406.11931", "title": "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence", "abstract": "We present DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. Specifically, DeepSeek-Coder-V2 is further pre-trained from an intermediate checkpoint of DeepSeek-V2 with additional 6 trillion tokens. Through this continued pre-training, DeepSeek-Coder-V2 substantially enhances the coding and mathematical reasoning capabilities of DeepSeek-V2, while maintaining comparable performance in general language tasks. Compared to DeepSeek-Coder-33B, DeepSeek-Coder-V2 demonstrates significant advancements in various aspects of code-related tasks, as well as reasoning and general capabilities. Additionally, DeepSeek-Coder-V2 expands its support for programming languages from 86 to 338, while extending the context length from 16K to 128K. In standard benchmark evaluations, DeepSeek-Coder-V2 achieves superior performance compared to closed-source models such as GPT4-Turbo, Claude 3 Opus, and Gemini 1.5 Pro in coding and math benchmarks.", "arxiv_url": "https://arxiv.org/abs/2406.11931", "authors": ["DeepSeek-AI", "Qihao Zhu", "Daya Guo", "Zhihong Shao", "Dejian Yang", "Peiyi Wang", "Runxin Xu", "Y. Wu", "Yukun Li", "Huazuo Gao", "Shirong Ma", "Wangding Zeng", "Xiao Bi", "Zihui Gu", "Hanwei Xu", "Damai Dai", "Kai Dong", "Liyue Zhang", "Yishi Piao", "Zhibin Gou", "Zhenda Xie", "Zhewen Hao", "Bingxuan Wang", "Junxiao Song", "Deli Chen", "Xin Xie", "Kang Guan", "Yuxiang You", "Aixin Liu", "Qiushi Du", "Wenjun Gao", "Xuan Lu", "Qinyu Chen", "Yaohui Wang", "Chengqi Deng", "Jiashi Li", "Chenggang Zhao", "Chong Ruan", "Fuli Luo", "Wenfeng Liang"], "first_author": "DeepSeek-AI", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Pre-Training", "tags": ["Mixture-of-Experts (MoE) architecture with activation-parameter efficiency", "Continued large-scale pre-training (additional 6T tokens, total 10.2T)", "Large multilingual code corpus curation (338 programming languages)", "128K long-context code modeling", "Reinforcement learning from compiler-and-test-based preferences", "Group Relative Policy Optimization (GRPO) for alignment", "Fill-in-middle fine-tuning for code completion", "Open-source release of hundred-billion-parameter code models"], "summary": "DeepSeek-Coder-V2是一个开源的MoE大规模代码语言模型，通过在额外6兆训练令牌上继续预训练、扩展到338种语言和128K上下文并结合基于编译器/测试的RL对齐方法，实现了在代码与数学推理任务上匹敌或超越多款闭源模型的性能并公开发布模型。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2406.11931v1", "published": "2024-06-17", "update_time": "2024-06-17", "download_time": "2025-12-16 10:39:48"}
{"id": "2402.19173", "title": "StarCoder 2 and The Stack v2: The Next Generation", "abstract": "The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data.", "arxiv_url": "https://arxiv.org/abs/2402.19173", "authors": ["Anton Lozhkov", "Raymond Li", "Loubna Ben Allal", "Federico Cassano", "Joel Lamy-Poirier", "Nouamane Tazi", "Ao Tang", "Dmytro Pykhtar", "Jiawei Liu", "Yuxiang Wei", "Tianyang Liu", "Max Tian", "Denis Kocetkov", "Arthur Zucker", "Younes Belkada", "Zijian Wang", "Qian Liu", "Dmitry Abulkhanov", "Indraneil Paul", "Zhuang Li", "Wen-Ding Li", "Megan Risdal", "Jia Li", "Jian Zhu", "Terry Yue Zhuo", "Evgenii Zheltonozhskii", "Nii Osae Osae Dade", "Wenhao Yu", "Lucas Krauß", "Naman Jain", "Yixuan Su", "Xuanli He", "Manan Dey", "Edoardo Abati", "Yekun Chai", "Niklas Muennighoff", "Xiangru Tang", "Muhtasham Oblokulov", "Christopher Akiki", "Marc Marone", "Chenghao Mou", "Mayank Mishra", "Alex Gu", "Binyuan Hui", "Tri Dao", "Armel Zebaze", "Olivier Dehaene", "Nicolas Patry", "Canwen Xu", "Julian McAuley", "Han Hu", "Torsten Scholak", "Sebastien Paquet", "Jennifer Robinson", "Carolyn Jane Anderson", "Nicolas Chapados", "Mostofa Patwary", "Nima Tajbakhsh", "Yacine Jernite", "Carlos Muñoz Ferrandis", "Lingming Zhang", "Sean Hughes", "Thomas Wolf", "Arjun Guha", "Leandro von Werra", "Harm de Vries"], "first_author": "Anton Lozhkov", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Pre-Training", "tags": ["software-archive-based corpus", "multilingual source-code corpus", "license-aware filtering", "deduplication and provenance (SWHIDs)", "PII redaction and malicious-code removal", "notebook and pull-request curation", "two-stage context window training (4k→16k)", "training with trillions of tokens (overcompute vs Chinchilla)", "open-weight release with transparent data provenance", "opt-out/governance tooling", "comprehensive cross-benchmark evaluation"], "summary": "BigCode项目基于Software Heritage构建了大规模多语言代码语料并发布了新版数据集与可公开权重的Code LLM，通过许可证感知去重、PII/恶意代码处理和两阶段（4k→16k）训练在多项代码基准上进行了全面评估并公开了数据可溯源标识。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2402.19173v1", "published": "2024-02-29", "update_time": "2024-02-29", "download_time": "2025-12-16 10:40:22"}
{"id": "2401.14196", "title": "DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence", "abstract": "The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.", "arxiv_url": "https://arxiv.org/abs/2401.14196", "authors": ["Daya Guo", "Qihao Zhu", "Dejian Yang", "Zhenda Xie", "Kai Dong", "Wentao Zhang", "Guanting Chen", "Xiao Bi", "Y. Wu", "Y. K. Li", "Fuli Luo", "Yingfei Xiong", "Wenfeng Liang"], "first_author": "Daya Guo", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Pre-Training", "tags": ["Repository-level data construction", "Repository-level deduplication", "Fill-in-the-middle pretraining", "16K long-context modeling", "Project-level/cross-file code understanding", "Training-from-scratch on large-scale code corpus", "Instruction-tuned code models", "Open-source permissive licensing"], "summary": "本文提出了DeepSeek-Coder系列从零训练的开源代码大模型（1.3B–33B），使用基于仓库的高质量项目级语料、填空中间任务和16K上下文窗口，在多项基准上取得领先的开源性能并提供宽松商业许可。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2401.14196v2", "published": "2024-01-25", "update_time": "2024-01-26", "download_time": "2025-12-16 10:40:55"}
{"id": "2306.11644", "title": "Textbooks Are All You Need", "abstract": "We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality\" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.", "arxiv_url": "https://arxiv.org/abs/2306.11644", "authors": ["Suriya Gunasekar", "Yi Zhang", "Jyoti Aneja", "Caio César Teodoro Mendes", "Allie Del Giorno", "Sivakanth Gopi", "Mojan Javaheripi", "Piero Kauffmann", "Gustavo de Rosa", "Olli Saarikivi", "Adil Salim", "Shital Shah", "Harkirat Singh Behl", "Xin Wang", "Sébastien Bubeck", "Ronen Eldan", "Adam Tauman Kalai", "Yin Tat Lee", "Yuanzhi Li"], "first_author": "Suriya Gunasekar", "category": ["Technical", "Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Pre-Training", "tags": ["Textbook-quality data curation", "Synthetic textbook/exercise generation from a teacher LLM", "Data-driven scaling (quality over quantity)", "Finetuning on exercise-style prompts", "Compute- and parameter-efficient code model", "Emergence analysis in small code models", "Training data contamination analysis", "Pass@1-based code generation evaluation"], "summary": "本文提出phi-1，一种仅1.3B参数但通过精选“教科书质量”语料与对抗性合成练习进行预训练与微调，从而在代码生成基准上以远小于常规模型的计算与数据规模取得极高性能，并观察到小模型的涌现行为与数据质量驱动的性能提升。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2306.11644v2", "published": "2023-06-20", "update_time": "2023-10-02", "download_time": "2025-12-16 10:41:36"}
{"id": "2305.07922", "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation", "abstract": "Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations in terms of architecture and pretraining tasks. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks. The former paradigm is limited by inflexibility in applications while in the latter, the model is treated as a single system for all tasks, leading to suboptimal performance on a subset of tasks. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some downstream tasks and hence result in substantial performance degrade. To address these limitations, we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks. Such flexibility is enabled by our proposed mixture of pretraining objectives to mitigate the pretrain-finetune discrepancy. These objectives cover span denoising, contrastive learning, text-code matching, and causal LM pretraining tasks, on both unimodal and bimodal multilingual code corpora. Furthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs without training from scratch to efficiently scale up our models, and explore instruction-tuning to align with natural language instructions. We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art (SoTA) model performance on various code-related tasks, such as code generation and completion, math programming, and text-to-code retrieval tasks. Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA results on HumanEval code generation task against other open code LLMs.", "arxiv_url": "https://arxiv.org/abs/2305.07922", "authors": ["Yue Wang", "Hung Le", "Akhilesh Deepak Gotmare", "Nghi D. Q. Bui", "Junnan Li", "Steven C. H. Hoi"], "first_author": "Yue Wang", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Pre-Training", "tags": ["Modular encoder-decoder design", "Mixture of pretraining objectives", "Span denoising and causal LM combination", "Text-code contrastive learning", "Text-code matching for cross-modal alignment", "Shallow-encoder / deep-frozen-decoder initialization", "Instruction tuning for code", "Retrieval-augmented generation"], "summary": "本文提出CodeT5+—一种可灵活组合模块的编码器-解码器代码大模型，采用跨度去噪、对比学习、文本-代码匹配与因果语言建模等混合预训练目标，并通过冻结大型解码器加浅层编码器的初始化与指令调优，在多种代码理解与生成基准上取得领先性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2305.07922v2", "published": "2023-05-13", "update_time": "2023-05-20", "download_time": "2025-12-16 10:42:08"}
{"id": "2305.02309", "title": "CodeGen2: Lessons for Training LLMs on Programming and Natural Languages", "abstract": "Large language models (LLMs) have demonstrated remarkable abilities in representation learning for program synthesis and understanding tasks. The quality of the learned representations appears to be dictated by the neural scaling laws as a function of the number of model parameters and observations, while imposing upper bounds on the model performance by the amount of available data and compute, which is costly.   In this study, we attempt to render the training of LLMs for program synthesis more efficient by unifying four key components: (1) model architectures, (2) learning methods, (3) infill sampling, and, (4) data distributions. Specifically, for the model architecture, we attempt to unify encoder and decoder-based models into a single prefix-LM. For learning methods, (i) causal language modeling, (ii) span corruption, (iii) infilling are unified into a simple learning algorithm. For infill sampling, we explore the claim of a \"free lunch\" hypothesis. For data distributions, the effect of a mixture distribution and multi-epoch training of programming and natural languages on model performance is explored.   We conduct a comprehensive series of empirical experiments on 1B LLMs, for which failures and successes of this exploration are distilled into five lessons. We will provide a final recipe for training and release CodeGen2 models in size 1B, 3.7B, 7B, and, 16B parameters, along with the training framework as open-source: https://github.com/salesforce/CodeGen.", "arxiv_url": "https://arxiv.org/abs/2305.02309", "authors": ["Erik Nijkamp", "Hiroaki Hayashi", "Caiming Xiong", "Silvio Savarese", "Yingbo Zhou"], "first_author": "Erik Nijkamp", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Pre-Training", "tags": ["Prefix-LM unification", "mixture objective (causal + span corruption)", "infill / fill-in-the-middle sampling", "code and natural language data mixture", "multi-epoch pretraining", "extensive ablation study of training choices", "training recipe and engineering", "open-source model & training release"], "summary": "本文通过对Prefix-LM架构、因果语言建模与span损坏混合目标、infill采样策略以及代码与自然语言混合多轮训练的系统消融实验，总结出训练CodeGen2系列模型的经验并开源训练配方与模型。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2305.02309v2", "published": "2023-05-03", "update_time": "2023-07-11", "download_time": "2025-12-16 10:42:46"}
{"id": "2303.17568", "title": "CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X", "abstract": "Large pre-trained code generation models, such as OpenAI Codex, can generate syntax- and function-correct code, making the coding of programmers more productive and our pursuit of artificial general intelligence closer. In this paper, we introduce CodeGeeX, a multilingual model with 13 billion parameters for code generation. CodeGeeX is pre-trained on 850 billion tokens of 23 programming languages as of June 2022. Our extensive experiments suggest that CodeGeeX outperforms multilingual code models of similar scale for both the tasks of code generation and translation on HumanEval-X. Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go. In addition, we build CodeGeeX-based extensions on Visual Studio Code, JetBrains, and Cloud Studio, generating 4.7 billion tokens for tens of thousands of active users per week. Our user study demonstrates that CodeGeeX can help to increase coding efficiency for 83.4% of its users. Finally, CodeGeeX is publicly accessible and in Sep. 2022, we open-sourced its code, model weights (the version of 850B tokens), API, extensions, and HumanEval-X at https://github.com/THUDM/CodeGeeX.", "arxiv_url": "https://arxiv.org/abs/2303.17568", "authors": ["Qinkai Zheng", "Xiao Xia", "Xu Zou", "Yuxiao Dong", "Shan Wang", "Yufei Xue", "Zihan Wang", "Lei Shen", "Andi Wang", "Yang Li", "Teng Su", "Zhilin Yang", "Jie Tang"], "first_author": "Qinkai Zheng", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Pre-Training", "tags": ["Multilingual code pretraining", "Autoregressive transformer pretraining", "Hand-crafted cross-language test suites", "Functional correctness evaluation via unit tests", "IDE integrations and real-world user study", "Large-scale code corpus (850B tokens, 23 languages)", "Cross-language code translation", "Open-source model and toolchain"], "summary": "论文提出并开源了 CodeGeeX——一个 13B 参数的多语言代码预训练模型，构建了用于多语言功能性评测的手工跨语种测试集并展示了在代码生成、翻译任务上的性能与实际 IDE 集成及用户增效效果。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2303.17568v2", "published": "2023-03-30", "update_time": "2024-07-10", "download_time": "2025-12-16 10:43:41"}
{"id": "2301.03988", "title": "SantaCoder: don't reach for the stars!", "abstract": "The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identifiable Information (PII) redaction pipeline, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data. We train 1.1B parameter models on the Java, JavaScript, and Python subsets of The Stack and evaluate them on the MultiPL-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ GitHub stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java, JavaScript, and Python portions of MultiPL-E, despite being a substantially smaller model. All models are released under an OpenRAIL license at https://hf.co/bigcode.", "arxiv_url": "https://arxiv.org/abs/2301.03988", "authors": ["Loubna Ben Allal", "Raymond Li", "Denis Kocetkov", "Chenghao Mou", "Christopher Akiki", "Carlos Munoz Ferrandis", "Niklas Muennighoff", "Mayank Mishra", "Alex Gu", "Manan Dey", "Logesh Kumar Umapathi", "Carolyn Jane Anderson", "Yangtian Zi", "Joel Lamy Poirier", "Hailey Schoelkopf", "Sergey Troshin", "Dmitry Abulkhanov", "Manuel Romero", "Michael Lappert", "Francesco De Toni", "Bernardo García del Río", "Qian Liu", "Shamik Bose", "Urvashi Bhattacharyya", "Terry Yue Zhuo", "Ian Yu", "Paulo Villegas", "Marco Zocca", "Sourab Mangrulkar", "David Lansky", "Huu Nguyen", "Danish Contractor", "Luis Villa", "Jia Li", "Dzmitry Bahdanau", "Yacine Jernite", "Sean Hughes", "Daniel Fried", "Arjun Guha", "Harm de Vries", "Leandro von Werra"], "first_author": "Loubna Ben Allal", "category": ["Technical", "Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Pre-Training", "tags": ["PII redaction pipeline", "PII annotation benchmark (400 files)", "Fill-in-the-middle pretraining objective", "Multi-Query Attention (MQA) ablations", "Near-duplicate filtering", "Repository-star filtering as data-quality proxy", "Comments-to-code and char-to-token ratio filters", "1.1B-parameter code model training", "Text-to-code benchmark evaluation", "Open-source release and governance"], "summary": "本文总结了 BigCode 社区在训练 1.1B 参数代码模型过程中的工作，包含 PII 删除管道与 400 文件标注基准、对 MQA 与 FIM 的消融实验、对多种数据预处理（如近重复、仓库 star 筛选、注释比率等）对性能影响的研究，并在多语言 text-to-code 基准上展示小型模型优于更大开源模型的结果且公开发布模型与工具。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2301.03988v2", "published": "2023-01-09", "update_time": "2023-02-24", "download_time": "2025-12-16 10:44:24"}
{"id": "2203.13474", "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis", "abstract": "Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multi-turn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: https://github.com/salesforce/CodeGen.", "arxiv_url": "https://arxiv.org/abs/2203.13474", "authors": ["Erik Nijkamp", "Bo Pang", "Hiroaki Hayashi", "Lifu Tu", "Huan Wang", "Yingbo Zhou", "Silvio Savarese", "Caiming Xiong"], "first_author": "Erik Nijkamp", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Prompting", "tags": ["Multi-turn program synthesis", "Progressive natural-language specification", "Subproblem factorization/decomposition", "Emergent zero-shot code generation", "Scaling study across model sizes", "Sequential curriculum training on mixed code corpora", "Multi-step synthesis evaluation benchmark", "Open-source training library and checkpoints"], "summary": "本文提出并开源了一系列大规模自回归代码模型与训练库，并构建首个多轮程序合成基准（MTPB），实验证明将问题分解为多轮自然语言子任务能显著提升代码生成性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2203.13474v5", "published": "2022-03-25", "update_time": "2023-02-27", "download_time": "2025-12-16 10:45:05"}
{"id": "2312.02120", "title": "Magicoder: Empowering Code Generation with OSS-Instruct", "abstract": "We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters. Magicoder models are trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets to generate diverse instruction data for code. Our main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs through the wealth of open-source references for the production of more realistic and controllable data. The orthogonality of OSS-Instruct and other data generation methods like Evol-Instruct further enables us to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks. Notably, MagicoderS-CL-7B based on CodeLlama even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1 ). Overall, OSS-Instruct opens a new direction for crafting diverse synthetic instruction data for code using abundant open-source references.", "arxiv_url": "https://arxiv.org/abs/2312.02120", "authors": ["Yuxiang Wei", "Zhe Wang", "Jiawei Liu", "Yifeng Ding", "Lingming Zhang"], "first_author": "Yuxiang Wei", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Instruction-Tuning", "tags": ["Open-source-seed-driven instruction synthesis", "Synthetic instruction-tuning dataset", "Prompting teacher LLMs with code snippets", "Teacher-student distillation via synthetic data", "Data decontamination for leakage control", "Combining multiple synthetic-data generation methods"], "summary": "本文提出 OSS-INSTRUCT，一种以开源代码片段为种子由强模型生成多样化合成指令数据的方法，并基于此训练并开源了在多项代码生成基准上超越同规模模型的 Magicoder 系列。", "quality": "High", "conference": "ICML 2024", "pdf_url": "https://arxiv.org/pdf/2312.02120v2", "published": "2023-12-04", "update_time": "2024-06-07", "download_time": "2025-12-16 10:45:42"}
{"id": "2308.07124", "title": "OctoPack: Instruction Tuning Code Large Language Models", "abstract": "Finetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack.", "arxiv_url": "https://arxiv.org/abs/2308.07124", "authors": ["Niklas Muennighoff", "Qian Liu", "Armel Zebaze", "Qinkai Zheng", "Binyuan Hui", "Terry Yue Zhuo", "Swayam Singh", "Xiangru Tang", "Leandro von Werra", "Shayne Longpre"], "first_author": "Niklas Muennighoff", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Instruction-Tuning", "tags": ["Commit-based code edit pairs", "Commit message as natural instructions", "Instruction tuning for code LLMs", "Multilingual multi-task code evaluation (synthesis, repair, explanation)", "Permissive-license data curation", "Single-file commit filtering and quality heuristics"], "summary": "本文构建了基于 Git 提交的巨量 COMMITPACK 数据集并以其为指令数据对代码大模型进行指令调优，同时推出覆盖多语言与多任务的评测套件以验证所训模型的跨语言与任务泛化能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2308.07124v2", "published": "2023-08-14", "update_time": "2024-02-18", "download_time": "2025-12-16 10:46:22"}
{"id": "2411.12882", "title": "ProSec: Fortifying Code LLMs with Proactive Security Alignment", "abstract": "While recent code-specific large language models (LLMs) have greatly enhanced their code generation capabilities, the safety of these models remains under-explored, posing potential risks as insecure code generated by these models may introduce vulnerabilities into real-world systems. Existing methods collect security-focused datasets from real-world vulnerabilities for instruction tuning in order to mitigate such issues. However, they are largely constrained by the data sparsity of vulnerable code, and have limited applicability in the multi-stage post-training workflows of modern LLMs. In this paper, we propose ProSec, a novel proactive security alignment approach designed to align code LLMs with secure coding practices. ProSec systematically exposes the vulnerabilities in a code LLM by synthesizing vulnerability-inducing coding scenarios from Common Weakness Enumerations (CWEs) and generates fixes to vulnerable code snippets, allowing the model to learn secure practices through preference learning objectives. The scenarios synthesized by ProSec trigger 25x more vulnerable code than a normal instruction-tuning dataset, resulting in a security-focused alignment dataset 7x larger than the previous work. Experiments show that models trained with ProSec are 25.2% to 35.4% more secure compared to previous work without degrading models' utility.", "arxiv_url": "https://arxiv.org/abs/2411.12882", "authors": ["Xiangzhe Xu", "Zian Su", "Jinyao Guo", "Kaiyuan Zhang", "Zhenting Wang", "Xiangyu Zhang"], "first_author": "Xiangzhe Xu", "category": [], "field": "Unknown", "task": "Unknown", "tags": [], "summary": "未提供论文内容，无法进行分类和总结。", "quality": "Low", "conference": null, "pdf_url": "https://arxiv.org/pdf/2411.12882v3", "published": "2024-11-19", "update_time": "2025-06-06", "download_time": "2025-12-16 10:47:25"}
{"id": "2505.08503", "title": "ICVul: A Well-labeled C/C++ Vulnerability Dataset with Comprehensive Metadata and VCCs", "abstract": "Machine learning-based software vulnerability detection requires high-quality datasets, which is essential for training effective models. To address challenges related to data label quality, diversity, and comprehensiveness, we constructed ICVul, a dataset emphasizing data quality and enriched with comprehensive metadata, including Vulnerability-Contributing Commits (VCCs). We began by filtering Common Vulnerabilities and Exposures from the NVD, retaining only those linked to GitHub fix commits. Then we extracted functions and files along with relevant metadata from these commits and used the SZZ algorithm to trace VCCs. To further enhance label reliability, we developed the ESC (Eliminate Suspicious Commit) technique, ensuring credible data labels. The dataset is stored in a relational-like database for improved usability and data integrity. Both ICVul and its construction framework are publicly accessible on GitHub, supporting research in related field.", "arxiv_url": "https://arxiv.org/abs/2505.08503", "authors": ["Chaomeng Lu", "Tianyu Li", "Toon Dehaene", "Bert Lagaisse"], "first_author": "Chaomeng Lu", "category": ["Benchmark"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["VCC tracing", "SZZ blame analysis", "Eliminate Suspicious Commit (ESC) filtering", "C/C++ function-level vulnerability labels", "CWE-to-commit mapping", "GitHub CVE-to-fix-commit linking", "Relational schema for repo/commit/file/function", "Noise reduction in fix commits"], "summary": "本文构建并公开了一个面向C/C++的高质量漏洞数据集，包含详细元数据、VCC（引入漏洞的提交）追溯以及用于剔除可疑修复提交的ESC过滤流程，以提升漏洞检测训练数据的可靠性与可用性。", "quality": "High", "conference": "IEEE/ACM International Conference on Mining Software Repositories (MSR) 2025", "pdf_url": "https://arxiv.org/pdf/2505.08503v1", "published": "2025-05-13", "update_time": "2025-05-13", "download_time": "2025-12-16 13:23:44"}
{"id": "2406.06887", "title": "$\\textbf{PLUM}$: Improving Code LMs with Execution-Guided On-Policy Preference Learning Driven By Synthetic Test Cases", "abstract": "Preference learning provides a promising solution to address the limitations of supervised fine-tuning (SFT) for code language models, where the model is not explicitly trained to differentiate between correct and incorrect code. Recent findings demonstrate that on-policy data is the key to successful preference learning, where the preference data is collected using the same policy LM being trained. Inspired by this, we propose PLUM, an on-policy $\\textbf{P}$reference $\\textbf{L}$earning framework A$\\textbf{u}$gmented with test cases for code L$\\textbf{M}$ s. The framework operates in three key stages: (1) automatic generation of test cases from natural language instructions, (2) creation of a preference data by evaluating candidate code solutions sampled from the policy, which can then be used to (3) train the policy LM. PLUM levitates the need to train reward models, allowing for large scale on-policy and online preference data collation. PLUM is evaluated on both standard benchmarks (HumanEval, MBPP) and more challenging ones (LiveCodeBench), delivering substantial improvements over original SFT'ed models and other execution-feedback-driven approaches. We show PLUM's benefits are consistent across various widely-used code LMs even they have been well-trained with SFT. For example, PLUM increases pass rates by up to 4.8% on average on standard benchmarks and 11.8% on LiveCodeBench, demonstrating its effectiveness and generalizability. We also demonstrate the benefits of on-policy and online preference learning by comprehensive experimentation.", "arxiv_url": "https://arxiv.org/abs/2406.06887", "authors": ["Dylan Zhang", "Shizhe Diao", "Xueyan Zou", "Hao Peng"], "first_author": "Dylan Zhang", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Instruction-Tuning", "tags": ["On-policy preference learning", "Synthetic test-case synthesis", "Execution-guided preference labeling", "Reward-model-free training", "Online preference data collection", "Self-consistency as oracle", "Runtime execution feedback"], "summary": "提出 PLUM：一种通过自动从自然语言说明生成测试用例并基于模型自采样解的执行结果构建偏好标签，进行在线在策略偏好学习以提升代码大模型生成正确性且无需训练奖励模型的方法。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2406.06887v4", "published": "2024-06-11", "update_time": "2024-10-12", "download_time": "2025-12-16 13:24:27"}
{"id": "2307.04349", "title": "RLTF: Reinforcement Learning from Unit Test Feedback", "abstract": "The goal of program synthesis, or code generation, is to generate executable code based on given descriptions. Recently, there has been an increasing number of studies employing reinforcement learning (RL) to improve the performance of large language models (LLMs) for code. However, current representative works either rely solely on offline frameworks, limiting the exploration of new sample spaces, or fall short in the utilization of unit test signals, not accounting for specific error locations within the code. To address these issues, we propose RLTF, i.e., Reinforcement Learning from Unit Test Feedback, a novel online RL framework with unit test feedback of multi-granularity for refining code LLMs. Our approach generates data in real-time during training and simultaneously utilizes fine-grained feedback signals to guide the model towards producing higher-quality code. Extensive experiments show that RLTF achieves state-of-the-art performance on the APPS and the MBPP benchmarks. Our code is available at: https://github.com/Zyq-scut/RLTF.", "arxiv_url": "https://arxiv.org/abs/2307.04349", "authors": ["Jiate Liu", "Yiqin Zhu", "Kaiwen Xiao", "Qiang Fu", "Xiao Han", "Wei Yang", "Deheng Ye"], "first_author": "Jiate Liu", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Online Reinforcement Learning", "Unit-test-guided rewards", "Fine-grained error localization", "Adaptive reward scaling", "Online sample buffer", "Critic sampling", "Policy optimization for program synthesis"], "summary": "该文提出RLTF，一种针对程序合成的在线强化学习框架，通过实时生成样本并从单元测试中提取多粒度（细粒度与自适应）反馈来对代码大模型进行优化，在APPS和MBPP上达到领先性能。", "quality": "High", "conference": "Transactions on Machine Learning Research (TMLR) 2024", "pdf_url": "https://arxiv.org/pdf/2307.04349v2", "published": "2023-07-10", "update_time": "2023-11-13", "download_time": "2025-12-16 13:25:42"}
{"id": "2301.13816", "title": "Execution-based Code Generation using Deep Reinforcement Learning", "abstract": "The utilization of programming language (PL) models, pre-trained on large-scale code corpora, as a means of automating software engineering processes has demonstrated considerable potential in streamlining various code generation tasks such as code completion, code translation, and program synthesis. However, current approaches mainly rely on supervised fine-tuning objectives borrowed from text generation, neglecting unique sequence-level characteristics of code, including but not limited to compilability as well as syntactic and functional correctness. To address this limitation, we propose PPOCoder, a new framework for code generation that synergistically combines pre-trained PL models with Proximal Policy Optimization (PPO) which is a widely used deep reinforcement learning technique. By utilizing non-differentiable feedback from code execution and structure alignment, PPOCoder seamlessly integrates external code-specific knowledge into the model optimization process. It's important to note that PPOCoder is a task-agnostic and model-agnostic framework that can be used across different code generation tasks and PLs. Extensive experiments on three code generation tasks demonstrate the effectiveness of our proposed approach compared to SOTA methods, achieving significant improvements in compilation success rates and functional correctness across different PLs.", "arxiv_url": "https://arxiv.org/abs/2301.13816", "authors": ["Parshin Shojaee", "Aneesh Jain", "Sindhu Tipirneni", "Chandan K. Reddy"], "first_author": "Parshin Shojaee", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Execution-based reward design", "PPO fine-tuning of pre-trained code models", "Compiler and unit-test feedback", "AST and DFG structural matching", "KL-divergence penalty to prevent forgetting", "Model- and task-agnostic RL framework"], "summary": "本文提出PPOCoder，将PPO强化学习与基于执行（编译/单元测试）和AST/DFG结构对齐的非可微奖励相结合，对预训练代码模型进行微调，从而在代码补全、代码翻译和程序合成等任务上显著提升可编译性与功能正确性。", "quality": "High", "conference": "Transactions on Machine Learning Research (TMLR) 2023", "pdf_url": "https://arxiv.org/pdf/2301.13816v4", "published": "2023-01-31", "update_time": "2023-07-19", "download_time": "2025-12-16 13:26:46"}
{"id": "2207.01780", "title": "CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning", "abstract": "Program synthesis or code generation aims to generate a program that satisfies a problem specification. Recent approaches using large-scale pretrained language models (LMs) have shown promising results, yet they have some critical limitations. In particular, they often follow a standard supervised fine-tuning procedure to train a code generation model only from the pairs of natural-language problem descriptions and ground-truth programs. Such paradigm largely ignores some important but potentially useful signals in the problem specification such as unit tests, which thus often results in poor performance when solving complex unseen coding tasks. To address the limitations, we propose \"CodeRL\", a new framework for program synthesis tasks through pretrained LMs and deep reinforcement learning (RL). Specifically, during training, we treat the code-generating LM as an actor network, and introduce a critic network that is trained to predict the functional correctness of generated programs and provide dense feedback signals to the actor. During inference, we introduce a new generation procedure with a critical sampling strategy that allows a model to automatically regenerate programs based on feedback from example unit tests and critic scores. For the model backbones, we extended the encoder-decoder architecture of CodeT5 with enhanced learning objectives, larger model sizes, and better pretraining data. Our method not only achieves new SOTA results on the challenging APPS benchmark, but also shows strong zero-shot transfer capability with new SOTA results on the simpler MBPP benchmark.", "arxiv_url": "https://arxiv.org/abs/2207.01780", "authors": ["Hung Le", "Yue Wang", "Akhilesh Deepak Gotmare", "Silvio Savarese", "Steven C. H. Hoi"], "first_author": "Hung Le", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Actor-Critic RL for code generation", "Unit-test-driven reward signals", "Critic as token-level error predictor", "Critic-guided resampling (critical sampling)", "Compiler-error-conditioned program repair", "Fine-tuning encoder-decoder code LM", "Optimizing functional correctness"], "summary": "本文提出CodeRL框架，将预训练代码语言模型作为actor并引入critic进行错误预测，利用基于单元测试的回报和critic导向的重采样与修复策略，在训练与生成阶段优化程序的功能性正确性以提升代码生成性能。", "quality": "High", "conference": "NeurIPS 2022", "pdf_url": "https://arxiv.org/pdf/2207.01780v3", "published": "2022-07-05", "update_time": "2022-11-03", "download_time": "2025-12-16 13:27:58"}
{"id": "2410.01215", "title": "From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging", "abstract": "While large language models have made significant strides in code generation, the pass rate of the generated code is bottlenecked on subtle errors, often requiring human intervention to pass tests, especially for complex problems. Existing LLM-based debugging systems treat generated programs as monolithic units, failing to address bugs at multiple levels of granularity, from low-level syntax errors to high-level algorithmic flaws. In this paper, we introduce Multi-Granularity Debugger (MGDebugger), a hierarchical code debugger by isolating, identifying, and resolving bugs at various levels of granularity. MGDebugger decomposes problematic code into a hierarchical tree structure of subfunctions, with each level representing a particular granularity of error. During debugging, it analyzes each subfunction and iteratively resolves bugs in a bottom-up manner. To effectively test each subfunction, we propose an LLM-simulated Python executor, which traces code execution and tracks important variable states to pinpoint errors accurately. Extensive experiments demonstrate that MGDebugger outperforms existing debugging systems, achieving an 18.9% improvement in accuracy over seed generations in HumanEval and a 97.6% repair success rate in HumanEvalFix. Furthermore, MGDebugger effectively fixes bugs across different categories and difficulty levels, demonstrating its robustness and effectiveness.", "arxiv_url": "https://arxiv.org/abs/2410.01215", "authors": ["Yuling Shi", "Songsong Wang", "Chengcheng Wan", "Min Wang", "Xiaodong Gu"], "first_author": "Yuling Shi", "category": ["Technical"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Hierarchical code decomposition", "Subfunction-level unit test generation", "LLM-simulated execution for variable-state tracing", "Bottom-up recursive debugging", "Automated bug localization", "Propagation of fixes across call-tree", "Program repair for LLM-generated and real-world code"], "summary": "本文提出MGDebugger，一种通过将程序分解为子函数树并结合子函数级测试与LLM模拟执行进行自底向上的层级调试方法，显著提高了对LLM生成代码及真实软件缺陷的修复成功率。", "quality": "High", "conference": "ICSE 2026", "pdf_url": "https://arxiv.org/pdf/2410.01215v4", "published": "2024-10-02", "update_time": "2025-11-22", "download_time": "2025-12-16 13:29:02"}
{"id": "2406.18294", "title": "Hierarchical Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs", "abstract": "Some recently developed code large language models (Code LLMs) have been pre-trained on repository-level code data (Repo-Code LLMs), enabling these models to recognize repository structures and utilize cross-file information for code completion. However, in real-world development scenarios, simply concatenating the entire code repository often exceeds the context window limits of these Repo-Code LLMs, leading to significant performance degradation. In this study, we conducted extensive preliminary experiments and analyses on six Repo-Code LLMs. The results indicate that maintaining the topological dependencies of files and increasing the code file content in the completion prompts can improve completion accuracy; pruning the specific implementations of functions in all dependent files does not significantly reduce the accuracy of completions. Based on these findings, we proposed a strategy named Hierarchical Context Pruning (HCP) to construct completion prompts with high informational code content. The HCP models the code repository at the function level, maintaining the topological dependencies between code files while removing a large amount of irrelevant code content, significantly reduces the input length for repository-level code completion. We applied the HCP strategy in experiments with six Repo-Code LLMs, and the results demonstrate that our proposed method can significantly enhance completion accuracy while substantially reducing the length of input. Our code and data are available at https://github.com/Hambaobao/HCP-Coder.", "arxiv_url": "https://arxiv.org/abs/2406.18294", "authors": ["Lei Zhang", "Yunshui Li", "Jiaming Li", "Xiaobo Xia", "Jiaxi Yang", "Run Luo", "Minzheng Wang", "Longze Chen", "Junhao Liu", "Min Yang"], "first_author": "Lei Zhang", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Repository-level context pruning", "Function-level modeling", "Topological dependency preservation", "Prompt construction for code completion", "Context window reduction", "Cross-file information selection", "Token-efficient completion"], "summary": "该论文提出了分层上下文剪枝（HCP）方法，通过在函数级别保留文件间拓扑依赖并移除无关实现，大幅减少输入上下文令牌数以提升仓库级预训练代码模型的跨文件代码补全准确性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2406.18294v2", "published": "2024-06-26", "update_time": "2024-06-27", "download_time": "2025-12-16 13:29:33"}
{"id": "2402.16906", "title": "Debug like a Human: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step", "abstract": "Large language models (LLMs) are leading significant progress in code generation. Beyond one-pass code generation, recent works further integrate unit tests and program verifiers into LLMs to iteratively refine the generated programs. However, these works consider the generated programs as an indivisible entity, which falls short for LLMs in debugging the programs, especially when the programs contain complex logic flows and data operations. In contrast, when human developers debug programs, they typically set breakpoints and selectively examine runtime execution information. The execution flow and the intermediate variables play a crucial role in the debugging process, yet they are underutilized in the existing literature on code generation. In this study, we introduce Large Language Model Debugger (LDB), a novel debugging framework that enables LLMs to refine their generated programs with the runtime execution information. Specifically, LDB segments the programs into basic blocks and tracks the values of intermediate variables after each block throughout the runtime execution. This allows LLMs to concentrate on simpler code units within the overall execution flow, verify their correctness against the task description block by block, and efficiently pinpoint any potential errors. Experiments demonstrate that LDB consistently enhances the baseline performance by up to 9.8% across the HumanEval, MBPP, and TransCoder benchmarks, archiving new state-of-the-art performance in code debugging for various LLM selections.", "arxiv_url": "https://arxiv.org/abs/2402.16906", "authors": ["Li Zhong", "Zilong Wang", "Jingbo Shang"], "first_author": "Li Zhong", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Runtime Execution Traces", "Basic Block Segmentation", "Control-Flow Graph Profiling", "Intermediate Variable Inspection", "Block-wise Correctness Verification", "Iterative Regeneration", "Visible Test Case Guided Debugging"], "summary": "本文提出LDB，一种将运行时执行信息（基于控制流图的基本块与中间变量值）引入大语言模型的调试框架，按块逐步验证并迭代修正生成的程序，显著提升多项代码生成基准的性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2402.16906v6", "published": "2024-02-25", "update_time": "2024-06-06", "download_time": "2025-12-16 13:30:15"}
{"id": "2306.02907", "title": "SelfEvolve: A Code Evolution Framework via Large Language Models", "abstract": "Large language models (LLMs) have already revolutionized code generation, after being pretrained on publicly available code data. However, while various methods have been proposed to augment LLMs with retrieved knowledge and enhance the quality of code generation, the performance of these retrieval-based methods is limited by the strength of the retrievers used. In addition, while LLMs show great emergent ability, they still struggle to produce the correct code in one turn. To address these challenges, we propose a novel two-step pipeline, called \\autoknow, that leverages LLMs as both knowledge providers and self-reflective programmers. Unlike retrieval-based methods, \\autoknow~obtains the knowledge from input prompts and generates intermediate code based on the generated knowledge. After that, \\autoknow~asks LLM to act as an expert programmer to perform debugging for the generated code. This is achieved by receiving the error message from the interpreter, without requiring special test cases for correctness verification. We evaluate \\autoknow~on three code generation datasets, including DS-1000 for data science code, HumanEval for software engineering code, and TransCoder for C++-to-Python translation. Our empirical experiments show that \\autoknow~outperforms strong baselines by a significant margin on all datasets. We also conduct exhaustive analytical experiments to validate the effectiveness of the two stages of \\autoknow, and find that both are superior to other prompting-based methods. Further scalability analysis demonstrates that \\autoknow~can be adapted to other more advanced models, such as GPT-4, and bring consistent efficacy improvement.", "arxiv_url": "https://arxiv.org/abs/2306.02907", "authors": ["Shuyang Jiang", "Yuhao Wang", "Yu Wang"], "first_author": "Shuyang Jiang", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Prompting", "tags": ["Self-generated knowledge extraction", "Interpreter-feedback debugging", "Iterative self-refinement", "Execution-guided program synthesis", "Retrieval-free augmentation", "Prompt-based code evolution", "Error-message-driven repair", "Two-stage generation-and-refinement pipeline"], "summary": "该文提出SELF-EVOLVE，一个不依赖检索器的两阶段代码生成框架：先让模型自我生成问题相关知识并基于其生成初始代码，再通过解释器返回的错误信息让模型自我调试迭代改进，从而在多项代码生成与翻译数据集上显著提升性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2306.02907v1", "published": "2023-06-05", "update_time": "2023-06-05", "download_time": "2025-12-16 13:35:10"}
{"id": "2306.09896", "title": "Is Self-Repair a Silver Bullet for Code Generation?", "abstract": "Large language models have shown remarkable aptitude in code generation, but still struggle to perform complex tasks. Self-repair -- in which the model debugs and repairs its own code -- has recently become a popular way to boost performance in these settings. However, despite its increasing popularity, existing studies of self-repair have been limited in scope; in many settings, its efficacy thus remains poorly understood. In this paper, we analyze Code Llama, GPT-3.5 and GPT-4's ability to perform self-repair on problems taken from HumanEval and APPS. We find that when the cost of carrying out repair is taken into account, performance gains are often modest, vary a lot between subsets of the data, and are sometimes not present at all. We hypothesize that this is because self-repair is bottlenecked by the model's ability to provide feedback on its own code; using a stronger model to artificially boost the quality of the feedback, we observe substantially larger performance gains. Similarly, a small-scale study in which we provide GPT-4 with feedback from human participants suggests that even for the strongest models, self-repair still lags far behind what can be achieved with human-level debugging.", "arxiv_url": "https://arxiv.org/abs/2306.09896", "authors": ["Theo X. Olausson", "Jeevana Priya Inala", "Chenglong Wang", "Jianfeng Gao", "Armando Solar-Lezama"], "first_author": "Theo X. Olausson", "category": ["Empirical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Self-repair pipeline (generate→execute→feedback→repair)", "Feedback quality ablation", "Compute-budget vs sampling tradeoff", "Unit-test-driven debugging", "Cross-model feedback substitution", "Human-in-the-loop debugging comparison", "Pass@k baseline analysis", "Error-message to natural-language feedback"], "summary": "本文在HumanEval和APPS上实证评估了CodeLlama、GPT-3.5与GPT-4的自我修复能力，发现计入修复成本后收益常常有限且不稳定，修复效果高度依赖反馈质量，且人类反馈显著优于自动反馈。", "quality": "High", "conference": "ICLR 2024", "pdf_url": "https://arxiv.org/pdf/2306.09896v5", "published": "2023-06-16", "update_time": "2024-02-02", "download_time": "2025-12-16 13:41:23"}
{"id": "2304.05128", "title": "Teaching Large Language Models to Self-Debug", "abstract": "Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by investigating the execution results and explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, Self-Debugging with code explanation consistently improves the baseline by 2-3%, and improves the prediction accuracy on problems of the hardest level by 9%. On TransCoder and MBPP where unit tests are available, Self-Debugging improves the baseline accuracy by up to 12%. Meanwhile, by leveraging feedback messages and reusing failed predictions, Self-Debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10x candidate programs.", "arxiv_url": "https://arxiv.org/abs/2304.05128", "authors": ["Xinyun Chen", "Maxwell Lin", "Nathanael Schärli", "Denny Zhou"], "first_author": "Xinyun Chen", "category": ["Technical"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Self-debugging prompts", "Rubber-duck code explanation", "Execution-guided feedback", "Unit-test feedback integration", "Iterative program repair", "Sample-efficiency improvement"], "summary": "本文提出SELF-DEBUGGING，通过少样例提示让大模型在执行生成的代码后输出自然语言解释与执行结果作为反馈，并基于此迭代自我修复代码，从而在多项代码生成任务上显著提高准确率与样本效率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2304.05128v2", "published": "2023-04-11", "update_time": "2023-10-05", "download_time": "2025-12-16 13:47:52"}
{"id": "2302.08468", "title": "LEVER: Learning to Verify Language-to-Code Generation with Execution", "abstract": "The advent of large language models trained on code (code LLMs) has led to significant progress in language-to-code generation. State-of-the-art approaches in this area combine LLM decoding with sample pruning and reranking using test cases or heuristics based on the execution results. However, it is challenging to obtain test cases for many real-world language-to-code applications, and heuristics cannot well capture the semantic features of the execution results, such as data type and value range, which often indicates the correctness of the program. In this work, we propose LEVER, a simple approach to improve language-to-code generation by learning to verify the generated programs with their execution results. Specifically, we train verifiers to determine whether a program sampled from the LLMs is correct or not based on the natural language input, the program itself and its execution results. The sampled programs are reranked by combining the verification score with the LLM generation probability, and marginalizing over programs with the same execution results. On four datasets across the domains of table QA, math QA and basic Python programming, LEVER consistently improves over the base code LLMs(4.6% to 10.9% with code-davinci-002) and achieves new state-of-the-art results on all of them.", "arxiv_url": "https://arxiv.org/abs/2302.08468", "authors": ["Ansong Ni", "Srini Iyer", "Dragomir Radev", "Ves Stoyanov", "Wen-tau Yih", "Sida I. Wang", "Xi Victoria Lin"], "first_author": "Ansong Ni", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Execution-based verifier", "Verifier-guided reranking", "Aggregation by execution result", "Sampling with deduplication", "Training with execution-derived labels", "Weakly-supervised verification", "Few-shot language-to-code", "Binary classification reranker"], "summary": "本文提出LEVER，通过训练一个基于自然语言描述、程序文本及其执行结果的验证器来对代码生成候选进行重排序并对相同执行结果的程序进行概率聚合，从而显著提升语言到代码任务的执行准确率并在多个基准上达到新SOTA。", "quality": "High", "conference": "ICML 2023", "pdf_url": "https://arxiv.org/pdf/2302.08468v3", "published": "2023-02-16", "update_time": "2023-09-01", "download_time": "2025-12-16 13:48:26"}
{"id": "2211.16490", "title": "Coder Reviewer Reranking for Code Generation", "abstract": "Sampling diverse programs from a code language model and reranking with model likelihood is a popular method for code generation but it is prone to preferring degenerate solutions. Inspired by collaborative programming, we propose Coder-Reviewer reranking. We augment Coder language models from past work, which generate programs given language instructions, with Reviewer models, which evaluate the likelihood of the instruction given the generated programs. We perform an extensive study across six datasets with eight models from three model families. Experimental results show that Coder-Reviewer reranking leads to consistent and significant improvement (up to 17% absolute accuracy gain) over reranking with the Coder model only. When combined with executability filtering, Coder-Reviewer reranking can often outperform the minimum Bayes risk method. Coder-Reviewer reranking is easy to implement by prompting, can generalize to different programming languages, and works well with off-the-shelf hyperparameters.", "arxiv_url": "https://arxiv.org/abs/2211.16490", "authors": ["Tianyi Zhang", "Tao Yu", "Tatsunori B. Hashimoto", "Mike Lewis", "Wen-tau Yih", "Daniel Fried", "Sida I. Wang"], "first_author": "Tianyi Zhang", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Prompt-based Reviewer Model", "MMI-style bidirectional reranking", "Sampling and reranking", "Length-bias mitigation", "Executable-filtering compatibility", "Zero-/few-shot code generation", "Cross-language generalization"], "summary": "本文提出通过提示构造的 Reviewer 模型估计 p(x|y)，与 Coder 的 p(y|x) 相乘进行重排（Coder-Reviewer reranking），在多数据集、多模型和多语言设置下显著提升代码生成准确率并易于实现与执行性过滤结合。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2211.16490v1", "published": "2022-11-29", "update_time": "2022-11-29", "download_time": "2025-12-16 13:48:58"}
{"id": "2207.10397", "title": "CodeT: Code Generation with Generated Tests", "abstract": "The task of generating code solutions for a given programming problem can benefit from the use of pre-trained language models such as Codex, which can produce multiple diverse samples. However, a major challenge for this task is to select the most appropriate solution from the multiple samples generated by the pre-trained language models. A natural way to evaluate the quality and correctness of a code solution is to run it against a set of test cases, but the manual creation of such test cases is often costly and time-consuming. In this paper, we propose a novel method, CodeT, that leverages the same pre-trained language models to automatically generate test cases for the code samples, thus reducing the human effort and increasing the coverage of the test scenarios. CodeT then executes the code samples using the generated test cases, and performs a dual execution agreement, which considers both the consistency of the outputs against the generated test cases and the agreement of the outputs with other code samples. We conduct comprehensive experiments on four benchmarks, HumanEval, MBPP, APPS and CodeContests, using five different pre-trained language models with varying sizes and capabilities. Our results show that CodeT can significantly improve the performance of code solution selection over previous methods, achieving remarkable and consistent gains across different models and benchmarks. For instance, CodeT improves the pass@1 metric on HumanEval to 65.8%, which represents an absolute improvement of 18.8% over the code-davinci-002 model, and an absolute improvement of more than 20% over the previous state-of-the-art results.", "arxiv_url": "https://arxiv.org/abs/2207.10397", "authors": ["Bei Chen", "Fengji Zhang", "Anh Nguyen", "Daoguang Zan", "Zeqi Lin", "Jian-Guang Lou", "Weizhu Chen"], "first_author": "Bei Chen", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["LM-generated unit tests", "Dual execution agreement", "RANSAC-inspired consensus clustering", "Execution-based code ranking", "Test-driven code selection", "Zero-shot test synthesis"], "summary": "本文提出CODET，一种利用同一预训练语言模型自动生成测试用例并通过受RANSAC启发的双重执行一致性在候选代码中选出最优解的无监督方法，从而在多项基准上显著提升了pass@1性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2207.10397v2", "published": "2022-07-21", "update_time": "2022-11-23", "download_time": "2025-12-16 13:50:06"}
{"id": "2412.05210", "title": "Evaluating and Aligning CodeLLMs on Human Preference", "abstract": "Code large language models (codeLLMs) have made significant strides in code generation. Most previous code-related benchmarks, which consist of various programming exercises along with the corresponding test cases, are used as a common measure to evaluate the performance and capabilities of code LLMs. However, the current code LLMs focus on synthesizing the correct code snippet, ignoring the alignment with human preferences, where the query should be sampled from the practical application scenarios and the model-generated responses should satisfy the human preference. To bridge the gap between the model-generated response and human preference, we present a rigorous human-curated benchmark CodeArena to emulate the complexity and diversity of real-world coding tasks, where 397 high-quality samples spanning 40 categories and 44 programming languages, carefully curated from user queries. Further, we propose a diverse synthetic instruction corpus SynCode-Instruct (nearly 20B tokens) by scaling instructions from the website to verify the effectiveness of the large-scale synthetic instruction fine-tuning, where Qwen2.5-SynCoder totally trained on synthetic instruction data can achieve top-tier performance of open-source code LLMs. The results find performance differences between execution-based benchmarks and CodeArena. Our systematic experiments of CodeArena on 40+ LLMs reveal a notable performance gap between open SOTA code LLMs (e.g. Qwen2.5-Coder) and proprietary LLMs (e.g., OpenAI o1), underscoring the importance of the human preference alignment.\\footnote{\\url{https://codearenaeval.github.io/ }}", "arxiv_url": "https://arxiv.org/abs/2412.05210", "authors": ["Jian Yang", "Jiaxi Yang", "Ke Jin", "Yibo Miao", "Lei Zhang", "Liqun Yang", "Zeyu Cui", "Yichang Zhang", "Binyuan Hui", "Junyang Lin"], "first_author": "Jian Yang", "category": ["Benchmark", "Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Alignment", "tags": ["Human preference evaluation", "Human-curated code QA benchmark", "Synthetic instruction corpus from web", "Instruction fine-tuning at scale", "LLM-as-judge pairwise comparison", "Decontamination and data filtering", "Multi-language and multi-category tasks", "Annotation protocol and quality control"], "summary": "本文提出了面向人类偏好的代码评估基准 CodeArena、构建了近200亿标记的合成指令语料 SynCode‑Instruct，并基于此对40余款模型进行了系统评测与指令微调以研究代码LLM与人类偏好的对齐问题。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2412.05210v1", "published": "2024-12-06", "update_time": "2024-12-06", "download_time": "2025-12-16 13:50:46"}
{"id": "2412.00535", "title": "FullStack Bench: Evaluating LLMs as Full Stack Coders", "abstract": "As the capabilities of code large language models (LLMs) continue to expand, their applications across diverse code intelligence domains are rapidly increasing. However, most existing datasets only evaluate limited application domains. To address this gap, we have developed a comprehensive code evaluation dataset FullStack Bench focusing on full-stack programming, which encompasses a wide range of application domains (e.g., basic programming, data analysis, software engineering, mathematics, and machine learning). Besides, to assess multilingual programming capabilities, in FullStack Bench, we design real-world instructions and corresponding unit test cases from 16 widely-used programming languages to reflect real-world usage scenarios rather than simple translations. Moreover, we also release an effective code sandbox execution tool (i.e., SandboxFusion) supporting various programming languages and packages to evaluate the performance of our FullStack Bench efficiently. Comprehensive experimental results on our FullStack Bench demonstrate the necessity and effectiveness of our FullStack Bench and SandboxFusion.", "arxiv_url": "https://arxiv.org/abs/2412.00535", "authors": ["Bytedance-Seed-Foundation-Code-Team", ":", "Yao Cheng", "Jianfeng Chen", "Jie Chen", "Li Chen", "Liyu Chen", "Wentao Chen", "Zhengyu Chen", "Shijie Geng", "Aoyan Li", "Bo Li", "Bowen Li", "Linyi Li", "Boyi Liu", "Jiaheng Liu", "Kaibo Liu", "Qi Liu", "Shukai Liu", "Siyao Liu", "Tianyi Liu", "Tingkai Liu", "Yongfei Liu", "Rui Long", "Jing Mai", "Guanghan Ning", "Z. Y. Peng", "Kai Shen", "Jiahao Su", "Jing Su", "Tao Sun", "Yifan Sun", "Yunzhe Tao", "Guoyin Wang", "Siwei Wang", "Xuwu Wang", "Yite Wang", "Zihan Wang", "Jinxiang Xia", "Liang Xiang", "Xia Xiao", "Yongsheng Xiao", "Chenguang Xi", "Shulin Xin", "Jingjing Xu", "Shikun Xu", "Hongxia Yang", "Jack Yang", "Yingxiang Yang", "Jianbo Yuan", "Jun Zhang", "Yufeng Zhang", "Yuyu Zhang", "Shen Zheng", "He Zhu", "Ming Zhu"], "first_author": "Bytedance-Seed-Foundation-Code-Team", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Multilingual code evaluation", "Full-stack application domains", "Unit-test-driven assessment", "Sandboxed execution environment", "Cross-language dependency/package support", "Automated grading and feedback", "Difficulty stratification and scaling analysis"], "summary": "本文提出了 FullStack Bench —— 一个覆盖多领域（如基础编程、数据分析、前后端、机器学习等）并支持16种编程语言的全栈代码评测基准，同时发布了支持多语言、多包依赖的 SandboxFusion 沙箱以实现自动化执行与评估。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2412.00535v6", "published": "2024-11-30", "update_time": "2025-05-12", "download_time": "2025-12-16 13:51:21"}
{"id": "2411.05830", "title": "GitChameleon: Unmasking the Version-Switching Capabilities of Code Generation Models", "abstract": "The rapid evolution of software libraries presents a significant challenge for code generation models, which must adapt to frequent version updates while maintaining compatibility with previous versions. Existing code completion benchmarks often overlook this dynamic aspect, and the one that does consider it relies on static code prediction tasks without execution-based evaluation, offering a limited perspective on a model's practical usability. To address this gap, we introduce \\textbf{\\GitChameleon{}}, a novel, manually curated dataset comprising 116 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests. \\GitChameleon{} is designed to rigorously assess the ability of modern large language models (LLMs) to generate version-specific code that is not only syntactically correct but also functionally accurate upon execution. Our comprehensive evaluations reveal that state-of-the-art LLMs struggle with this task; for instance, \\textbf{GPT-4o} achieves a pass@10 of only 39.9\\% (43.7\\% when provided with error feedback), highlighting the complexity of the problem and the limitations of current models. By providing an execution-based benchmark that emphasizes the dynamic nature of code libraries, \\GitChameleon{} serves as a critical tool to advance the development of more adaptable and reliable code generation models. For facilitation for further exploration of version-conditioned code generation, we make our code repository publicly accessible at \\url{https://github.com/NizarIslah/GitChameleon}.", "arxiv_url": "https://arxiv.org/abs/2411.05830", "authors": ["Nizar Islah", "Justine Gehring", "Diganta Misra", "Eilif Muller", "Irina Rish", "Terry Yue Zhuo", "Massimo Caccia"], "first_author": "Nizar Islah", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Version-conditioned code generation", "Executable unit-test evaluation", "API change classification", "Library-version compatibility", "Hand-curated version examples", "Error-log feedback for iterative repair"], "summary": "GitChameleon 提出一个包含116个基于真实库版本变更且附带可执行单元测试的 Python 代码补全基准，用以评估模型生成版本特定且功能正确代码的能力，并展示现有模型在此任务上的局限性及利用错误日志反馈提升性能的可行性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2411.05830v1", "published": "2024-11-05", "update_time": "2024-11-05", "download_time": "2025-12-16 13:51:50"}
{"id": "2408.06450", "title": "Evaluating Language Models for Efficient Code Generation", "abstract": "We introduce Differential Performance Evaluation (DPE), a framework designed to reliably evaluate Large Language Models (LLMs) for efficient code generation. Traditional coding benchmarks often fail to provide reliable insights into code efficiency, due to their reliance on simplistic test inputs and the absence of effective compound metrics. DPE addresses these issues by focusing on efficiency-demanding programming tasks and establishing an insightful compound metric for performance evaluation. DPE operates in two phases: To curate efficiency datasets, it selects efficiency-demanding tasks from existing coding benchmarks and generates computationally expensive inputs to stress the efficiency of LLM solutions. To assess the code efficiency, DPE profiles the new solution and compares it globally against a set of reference solutions that exhibit distinct efficiency levels, where the matched level defines its efficiency score. As a proof of concept, we use DPE to create EvalPerf, a benchmark with 121 performance-challenging coding tasks. Our comprehensive evaluation draws interesting findings on the efficiency impact of model sizes, instruction tuning, and prompting. For example, while the scaling law fails to account for code efficiency, general instruction tuning benefits both code correctness and efficiency. We also evaluate the evaluation by examining the effectiveness of DPE, showing that EvalPerf is reliable and convenient to use even across platforms.", "arxiv_url": "https://arxiv.org/abs/2408.06450", "authors": ["Jiawei Liu", "Songrun Xie", "Junhao Wang", "Yuxiang Wei", "Yifeng Ding", "Lingming Zhang"], "first_author": "Jiawei Liu", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Performance Optimization", "tags": ["Differential Performance Evaluation", "Synthesizing a Synthesizer", "Performance-exercising input generation", "Exponential input sampling", "Performance clustering and reference matching", "Cluster-percentile efficiency scoring", "Chain-of-thought few-shot generator prompting", "Cross-platform profiling and robustness"], "summary": "本文提出 Differential Performance Evaluation (DPE) 框架，通过自动合成性能压力测试输入、对正确解进行性能聚类并以参考解匹配的方式为新生成代码打分，基于此构建了一个用于评估代码效率的基准并开展了实证分析。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2408.06450v1", "published": "2024-08-12", "update_time": "2024-08-12", "download_time": "2025-12-16 13:52:36"}
{"id": "2403.07974", "title": "LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code", "abstract": "Large Language Models (LLMs) applied to code-related applications have emerged as a prominent field, attracting significant interest from both academia and industry. However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient for assessing their capabilities. In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code, which continuously collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our benchmark also focuses on a broader range of code related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation. Currently, LiveCodeBench hosts four hundred high-quality coding problems that were published between May 2023 and May 2024. We have evaluated 18 base LLMs and 34 instruction-tuned LLMs on LiveCodeBench. We present empirical findings on contamination, holistic performance comparisons, potential overfitting in existing benchmarks as well as individual model comparisons. We will release all prompts and model completions for further community analysis, along with a general toolkit for adding new scenarios and model", "arxiv_url": "https://arxiv.org/abs/2403.07974", "authors": ["Naman Jain", "King Han", "Alex Gu", "Wen-Ding Li", "Fanjia Yan", "Tianjun Zhang", "Sida Wang", "Armando Solar-Lezama", "Koushik Sen", "Ion Stoica"], "first_author": "Naman Jain", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Understanding", "tags": ["Contamination-free evaluation", "Time-segmented / cutoff-aware benchmarking", "Contest-problem harvesting (LeetCode/AtCoder/CodeForces)", "Self-repair (debug from execution feedback)", "Test output prediction", "Code execution / program emulation evaluation", "Holistic multi-scenario benchmarking", "Public prompts and completions toolkit"], "summary": "本文提出LiveCodeBench——一个从线上编程比赛实时收集并按发布时间切分以避免训练数据污染的综合代码基准，支持代码生成、自我修复、程序执行与测试输出预测等多场景评估，并对多款模型进行了广泛评测和污染/过拟合分析，同时发布了提示与模型完成结果工具包。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2403.07974v2", "published": "2024-03-12", "update_time": "2024-06-06", "download_time": "2025-12-16 13:53:17"}
{"id": "2403.08604", "title": "Prompting Large Language Models to Tackle the Full Software Development Lifecycle: A Case Study", "abstract": "Recent advancements in large language models (LLMs) have significantly enhanced their coding capabilities. However, existing benchmarks predominantly focused on simplified or isolated aspects of coding, such as single-file code generation or repository issue debugging, falling short of measuring the full spectrum of challenges raised by real-world programming activities. In this case study, we explore the performance of LLMs across the entire software development lifecycle with DevEval, encompassing stages including software design, environment setup, implementation, acceptance testing, and unit testing. DevEval features four programming languages, multiple domains, high-quality data collection, and carefully designed and verified metrics for each task. Empirical studies show that current LLMs, including GPT-4, fail to solve the challenges presented within DevEval. Our findings offer actionable insights for the future development of LLMs toward real-world programming applications.", "arxiv_url": "https://arxiv.org/abs/2403.08604", "authors": ["Bowen Li", "Wenhan Wu", "Ziwei Tang", "Lin Shi", "John Yang", "Jinyang Li", "Shunyu Yao", "Chen Qian", "Binyuan Hui", "Qicheng Zhang", "Zhiyin Yu", "He Du", "Ping Yang", "Dahua Lin", "Chao Peng", "Kai Chen"], "first_author": "Bowen Li", "category": ["Benchmark", "Empirical"], "field": "Requirements & Design", "task": "Specification & Validation", "tags": ["End-to-end lifecycle evaluation", "Repository-level codebase construction", "UML class and sequence diagram generation", "Dependency file and environment setup generation", "Acceptance and unit test generation and execution", "Execution-based metrics and coverage measurement", "LLM-as-a-judge automated design evaluation", "Multi-language benchmark (Python/Java/C/C++/JavaScript)"], "summary": "本文提出了DevEval——一个覆盖软件设计、环境配置、实现与测试等全生命周期、多语言的仓库级评估基准，并通过执行与覆盖率等度量展示了现有大模型在真实软件开发任务上的显著不足与改进方向。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2403.08604v3", "published": "2024-03-13", "update_time": "2024-12-14", "download_time": "2025-12-16 13:54:13"}
{"id": "2310.06770", "title": "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?", "abstract": "Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.", "arxiv_url": "https://arxiv.org/abs/2310.06770", "authors": ["Carlos E. Jimenez", "John Yang", "Alexander Wettig", "Shunyu Yao", "Kexin Pei", "Ofir Press", "Karthik Narasimhan"], "first_author": "Carlos E. Jimenez", "category": ["Benchmark", "Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Repository-Level Coding", "Program Repair", "Code RAG", "Execution-based Evaluation", "Long-Context Reasoning", "Cross-File Edits", "Continual Benchmarking"], "summary": "SWE-bench 提出并公开了一个基于真实 GitHub issue 与已合并 PR 的 Python 仓库级别基准（2,294 个实例）及训练集，并通过补丁应用与运行仓库测试评估模型在跨文件、长上下文场景下修复问题的能力，结果显示现有模型只能解决极少数问题。", "quality": "High", "conference": "ICLR 2024", "pdf_url": "https://arxiv.org/pdf/2310.06770v3", "published": "2023-10-10", "update_time": "2024-11-11", "download_time": "2025-12-17 19:14:26"}
{"id": "2306.03091", "title": "RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems", "abstract": "Large Language Models (LLMs) have greatly advanced code auto-completion systems, with a potential for substantial productivity enhancements for developers. However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios. To fill this gap, we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems. RepoBench supports both Python and Java and consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). Each task respectively measures the system's ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction. RepoBench aims to facilitate a more complete comparison of performance and encouraging continuous improvement in auto-completion systems. RepoBench is publicly available at https://github.com/Leolty/repobench.", "arxiv_url": "https://arxiv.org/abs/2306.03091", "authors": ["Tianyang Liu", "Canwen Xu", "Julian McAuley"], "first_author": "Tianyang Liu", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Repository-level evaluation", "Cross-file retrieval", "Next-line code prediction", "Retrieval–completion pipeline", "Python and Java multi-file projects", "Temporal test split to reduce leakage", "Long-context benchmarking", "Retrieval-augmented completion"], "summary": "本文提出RepoBench，一个针对仓库级（跨文件）代码自动补全的基准，包含检索、补全与端到端流水线三项任务，并在大量 Python 和 Java 仓库上评估检索方法与补全模型以揭示处理长上下文与跨文件依赖的挑战。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2306.03091v2", "published": "2023-06-05", "update_time": "2023-10-04", "download_time": "2025-12-16 13:55:21"}
{"id": "2306.14893", "title": "LongCoder: A Long-Range Pre-trained Language Model for Code Completion", "abstract": "In this paper, we introduce a new task for code completion that focuses on handling long code input and propose a sparse Transformer model, called LongCoder, to address this task. LongCoder employs a sliding window mechanism for self-attention and introduces two types of globally accessible tokens - bridge tokens and memory tokens - to improve performance and efficiency. Bridge tokens are inserted throughout the input sequence to aggregate local information and facilitate global interaction, while memory tokens are included to highlight important statements that may be invoked later and need to be memorized, such as package imports and definitions of classes, functions, or structures. We conduct experiments on a newly constructed dataset that contains longer code context and the publicly available CodeXGLUE benchmark. Experimental results demonstrate that LongCoder achieves superior performance on code completion tasks compared to previous models while maintaining comparable efficiency in terms of computational resources during inference. All the codes and data are available at https://github.com/microsoft/CodeBERT.", "arxiv_url": "https://arxiv.org/abs/2306.14893", "authors": ["Daya Guo", "Canwen Xu", "Nan Duan", "Jian Yin", "Julian McAuley"], "first_author": "Daya Guo", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["sliding-window sparse attention", "bridge tokens for local aggregation", "memory tokens for global memorization", "long-context / file-level code modeling", "linearized attention complexity", "autoregressive pre-training for completion", "construction of long-code completion benchmark", "code-driven dynamic global attention"], "summary": "该文提出一种用于长代码上下文补全的稀疏Transformer预训练模型，通过滑动窗口注意力及两类全局令牌（桥接令牌与记忆令牌）在保持线性复杂度的同时建模文件级长上下文，并构建了面向长代码的补全基准以验证方法有效性。", "quality": "High", "conference": "ICML 2023", "pdf_url": "https://arxiv.org/pdf/2306.14893v1", "published": "2023-06-26", "update_time": "2023-06-26", "download_time": "2025-12-16 13:55:58"}
{"id": "2308.10335", "title": "Can ChatGPT replace StackOverflow? A Study on Robustness and Reliability of Large Language Model Code Generation", "abstract": "Recently, the large language models (LLMs) have shown extraordinary ability in understanding natural language and generating programming code. It has been a common practice of software engineers to consult LLMs when encountering coding questions. Although efforts have been made to avoid syntax errors and align the code with the intended semantics, the reliability and robustness of the code generationfrom LLMs have not yet been thoroughly studied. The executable code is not equivalent to the reliable and robust code, especially in the context of real-world software development. The misuse of APIs in the generated code could lead to severe problem, such as resource leaks, program crashes. To make things worse, the users of LLM code generation services are actually the developers that are most vulnerable to these code that seems right -- They are always novice developers that are not familiar with the APIs that LLMs generate code for them. Therefore, they could hardly tell the misuse in the code generated by LLMs, which further facilitates the incorrect code applied in real-world software. Existing code evaluation benchmark and datasets focus on crafting small tasks such as programming questions in coding interviews, which however deviates from the problem that developers would ask LLM for real-world coding help. To fill the missing piece, in this work, we propose a dataset RobustAPI for evaluating the reliability and robustness of code generated by LLMs. We collect 1208 coding questions from StackOverflow on 24 representative Java APIs. We summarize thecommon misuse patterns of these APIs and evaluate them oncurrent popular LLMs. The evaluation results show that evenfor GPT-4, 62% of the generated code contains API misuses,which would cause unexpected consequences if the code isintroduced into real-world software.", "arxiv_url": "https://arxiv.org/abs/2308.10335", "authors": ["Li Zhong", "Zilong Wang"], "first_author": "Li Zhong", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["API Misuse Detection", "AST-based Evaluator", "Structured Call Sequences", "Stack Overflow-derived Questions", "Java API Usage Patterns", "Robustness and Reliability Benchmarking", "Zero-shot and One-shot Prompting", "Resource Leak and Crash Analysis"], "summary": "本文提出了ROBUSTAPI基准，基于1208条来自Stack Overflow 的Java API问题与AST检测器，系统评估LLM生成代码的API误用与鲁棒性，并发现即使GPT‑4也存在大量API误用（约62%）。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2308.10335v5", "published": "2023-08-20", "update_time": "2024-01-27", "download_time": "2025-12-16 13:56:38"}
{"id": "2308.16458", "title": "BioCoder: A Benchmark for Bioinformatics Code Generation with Large Language Models", "abstract": "Pre-trained large language models (LLMs) have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks and to be appropriately specialized to particular domains. Here, we target bioinformatics due to the amount of domain knowledge, algorithms, and data operations this discipline requires. We present BioCoder, a benchmark developed to evaluate LLMs in generating bioinformatics-specific code. BioCoder spans much of the field, covering cross-file dependencies, class declarations, and global variables. It incorporates 1,026 Python functions and 1,243 Java methods extracted from GitHub, along with 253 examples from the Rosalind Project, all pertaining to bioinformatics. Using topic modeling, we show that the overall coverage of the included code is representative of the full spectrum of bioinformatics calculations. BioCoder incorporates a fuzz-testing framework for evaluation. We have applied it to evaluate various models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT5+, GPT-3.5, and GPT- 4. Furthermore, we fine-tuned one model (StarCoder), demonstrating that our training dataset can enhance the performance on our testing benchmark (by >15% in terms of Pass@K under certain prompt configurations and always >3%). The results highlight two key aspects of successful models: (1) Successful models accommodate a long prompt (> 2,600 tokens) with full context, including functional dependencies. (2) They contain domain-specific knowledge of bioinformatics, beyond just general coding capability. This is evident from the performance gain of GPT-3.5/4 compared to the smaller models on our benchmark (50% vs. up to 25%). Availability and implementation: Code is available at: https://github.com/gersteinlab/biocoder and https://biocoder-benchmark. github.io/.", "arxiv_url": "https://arxiv.org/abs/2308.16458", "authors": ["Xiangru Tang", "Bill Qian", "Rick Gao", "Jiakang Chen", "Xinyun Chen", "Mark Gerstein"], "first_author": "Xiangru Tang", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Domain-specific code generation", "Bioinformatics data formats and workflows", "Cross-file dependency extraction", "AST-based code parsing", "Execution-based fuzz testing", "Dockerized evaluation environment", "Context-rich / repository-level prompts", "Prompt length and full-context effects", "Benchmark curation and anti-memorization filtering"], "summary": "BioCoder 提出并发布了一个面向生物信息学的代码生成基准（含来自 GitHub 与 Rosalind 的 2,269 个问题），并提供 AST 解析工具、上下文构建与 Docker 化的模糊测试框架以基于执行评估 LLM 的域特定代码生成能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2308.16458v5", "published": "2023-08-31", "update_time": "2024-05-20", "download_time": "2025-12-16 13:57:19"}
{"id": "2305.01210", "title": "Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation", "abstract": "Program synthesis has been long studied with recent approaches focused on directly using the power of Large Language Models (LLMs) to generate code. Programming benchmarks, with curated synthesis problems and test-cases, are used to measure the performance of various LLMs on code synthesis. However, these test-cases can be limited in both quantity and quality for fully assessing the functional correctness of the generated code. Such limitation in the existing benchmarks begs the following question: In the era of LLMs, is the code generated really correct? To answer this, we propose EvalPlus -- a code synthesis evaluation framework to rigorously benchmark the functional correctness of LLM-synthesized code. EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies. While EvalPlus is general, we extend the test-cases of the popular HumanEval benchmark by 80x to build HumanEval+. Our extensive evaluation across 26 popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs, reducing the pass@k by up-to 19.3-28.9%. We also surprisingly found that test insufficiency can lead to mis-ranking. For example, both WizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+, while none of them could on HumanEval. Our work not only indicates that prior popular code synthesis evaluation results do not accurately reflect the true performance of LLMs for code synthesis, but also opens up a new direction to improve such programming benchmarks through automated testing. We have open-sourced our tools, enhanced datasets as well as all LLM-generated code at https://github.com/evalplus/evalplus to facilitate and accelerate future LLM-for-code research.", "arxiv_url": "https://arxiv.org/abs/2305.01210", "authors": ["Jiawei Liu", "Chunqiu Steven Xia", "Yuyao Wang", "Lingming Zhang"], "first_author": "Jiawei Liu", "category": ["Benchmark", "Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Automated test-input generation", "LLM-seeded seed input generation", "Type-aware mutation testing", "Differential testing against reference implementation", "Test-suite reduction", "Greedy set-cover selection for tests", "Benchmark augmentation for evaluation", "Detection of evaluation-induced model misranking", "Ground-truth oracle validation"], "summary": "本文提出EvalPlus，通过LLM引导的种子输入与类型感知变异自动生成并精简大量测试用例以扩充程序合成基准，从而更严格地评估LLM生成代码的功能正确性并揭示现有基准高估模型性能与可能导致的模型错排。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2305.01210v3", "published": "2023-05-02", "update_time": "2023-10-30", "download_time": "2025-12-16 13:57:55"}
{"id": "2305.18584", "title": "Coeditor: Leveraging Contextual Changes for Multi-round Code Auto-editing", "abstract": "Developers often dedicate significant time to maintaining and refactoring existing code. However, most prior work on generative models for code focuses solely on creating new code, overlooking the distinctive needs of editing existing code. In this work, we explore a multi-round code auto-editing setting, aiming to predict edits to a code region based on recent changes within the same codebase. Our model, Coeditor, is a fine-tuned language model specifically designed for code editing tasks. We represent code changes using a line diff format and employ static analysis to form large customized model contexts, ensuring the availability of appropriate information for prediction. We collect a code editing dataset from the commit histories of 1650 open-source Python projects for training and evaluation. In a simplified single-round, single-edit task, Coeditor significantly outperforms GPT-3.5 and SOTA open-source code completion models (bringing exact-match accuracy from 34.7 up to 60.4), demonstrating the benefits of incorporating editing history for code completion. In a multi-round, multi-edit setting, we observe substantial gains by iteratively conditioning on additional user edits. We have open-sourced our code, data, and model weights to encourage future research and have released a VSCode extension powered by our model for interactive IDE usage.", "arxiv_url": "https://arxiv.org/abs/2305.18584", "authors": ["Jiayi Wei", "Greg Durrett", "Isil Dillig"], "first_author": "Jiayi Wei", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Multi-round code auto-editing", "Repo-level diff conditioning", "Line-based diff representation", "Masked span infilling for edits", "Static analysis for context retrieval", "Block-sparse attention for long contexts", "Commit-history-derived training data", "Interactive IDE integration"], "summary": "本文提出了面向多轮仓库级代码自动编辑的新任务与评测，设计了基于行差异编码、静态分析和块稀疏注意力的编辑模型，并构建了来自提交历史的新数据集以验证其在编辑自动化上的显著提升。", "quality": "High", "conference": "International Conference on Learning Representations (ICLR 2024)", "pdf_url": "https://arxiv.org/pdf/2305.18584v2", "published": "2023-05-29", "update_time": "2024-04-28", "download_time": "2025-12-16 13:58:36"}
{"id": "2211.11501", "title": "DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation", "abstract": "We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas. Compared to prior works, DS-1000 incorporates three core features. First, our problems reflect diverse, realistic, and practical use cases since we collected them from StackOverflow. Second, our automatic evaluation is highly specific (reliable) -- across all Codex-002-predicted solutions that our evaluation accept, only 1.8% of them are incorrect; we achieve this with multi-criteria metrics, checking both functional correctness by running test cases and surface-form constraints by restricting API usages or keywords. Finally, we proactively defend against memorization by slightly modifying our problems to be different from the original StackOverflow source; consequently, models cannot answer them correctly by memorizing the solutions from pre-training. The current best public system (Codex-002) achieves 43.3% accuracy, leaving ample room for improvement. We release our benchmark at https://ds1000-code-gen.github.io.", "arxiv_url": "https://arxiv.org/abs/2211.11501", "authors": ["Yuhang Lai", "Chengxi Li", "Yiming Wang", "Tianyi Zhang", "Ruiqi Zhong", "Luke Zettlemoyer", "Scott Wen-tau Yih", "Daniel Fried", "Sida Wang", "Tao Yu"], "first_author": "Yuhang Lai", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["StackOverflow-sourced problems", "Execution-based multi-criteria evaluation", "Surface-form constraints", "Memorization mitigation (problem perturbation)", "Executable code contexts", "Library-version controlled environment", "False discovery rate validation"], "summary": "该论文提出了DS-1000——一个包含1000个来源于StackOverflow的真实数据科学代码生成基准，覆盖NumPy/Pandas等库，结合可执行的多准则自动评估并通过题目扰动防止模型记忆化，展示了现有模型在此基准上的性能表现。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2211.11501v1", "published": "2022-11-18", "update_time": "2022-11-18", "download_time": "2025-12-16 13:59:53"}
{"id": "2208.08227", "title": "MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation", "abstract": "Large language models have demonstrated the ability to generate both natural language and programming language text. Such models open up the possibility of multi-language code generation: could code generation models generalize knowledge from one language to another? Although contemporary code generation models can generate semantically correct Python code, little is known about their abilities with other languages. We propose MultiPL-E, a system for translating unit test-driven code generation benchmarks to new languages. We create the first massively multilingual code generation benchmark by using MultiPL-E to translate two popular Python code generation benchmarks to 18 additional programming languages.   We use MultiPL-E to extend the HumanEval benchmark and MBPP benchmark to 18 languages that encompass a range of programming paradigms and popularity. Using these new parallel benchmarks, we evaluate the multi-language performance of three state-of-the-art code generation models: Codex, CodeGen, and InCoder. We find that Codex matches or even exceeds its performance on Python for several other languages. The range of programming languages represented in MultiPL-E allow us to explore the impact of language frequency and language features on model performance. Finally, the MultiPL-E approach of compiling code generation benchmarks to new programming languages is both scalable and extensible, making it straightforward to evaluate new models, benchmarks, and languages.", "arxiv_url": "https://arxiv.org/abs/2208.08227", "authors": ["Federico Cassano", "John Gouwar", "Daniel Nguyen", "Sydney Nguyen", "Luna Phipps-Costin", "Donald Pinckney", "Ming-Ho Yee", "Yangtian Zi", "Carolyn Jane Anderson", "Molly Q Feldman", "Arjun Guha", "Michael Greenberg", "Abhinav Jangda"], "first_author": "Federico Cassano", "category": ["Benchmark", "Empirical", "Technical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Multilingual code‑generation benchmarking", "Unit-test translation and compilation", "Parallel problem instances across languages", "Prompt translation and sensitivity analysis", "Language feature and popularity impact study", "Containerized sandboxed execution and error classification"], "summary": "本文提出 MultiPL‑E，将基于单元测试的 Python 代码生成基准自动翻译到 18 种编程语言，构建大规模多语言并行基准并用其评估现有模型以分析语言特性、流行度、类型注解与提示设计对代码生成正确率的影响。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2208.08227v4", "published": "2022-08-17", "update_time": "2022-12-19", "download_time": "2025-12-16 14:00:43"}
{"id": "2108.07732", "title": "Program Synthesis with Large Language Models", "abstract": "This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages. We evaluate a collection of such models (with between 244M and 137B parameters) on two new benchmarks, MBPP and MathQA-Python, in both the few-shot and fine-tuning regimes. Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers. The MathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914 problems that evaluate the ability of the models to synthesize code from more complex text. On both datasets, we find that synthesis performance scales log-linearly with model size. Our largest models, even without finetuning on a code dataset, can synthesize solutions to 59.6 percent of the problems from MBPP using few-shot learning with a well-designed prompt. Fine-tuning on a held-out portion of the dataset improves performance by about 10 percentage points across most model sizes. On the MathQA-Python dataset, the largest fine-tuned model achieves 83.8 percent accuracy. Going further, we study the model's ability to engage in dialog about code, incorporating human feedback to improve its solutions. We find that natural language feedback from a human halves the error rate compared to the model's initial prediction. Additionally, we conduct an error analysis to shed light on where these models fall short and what types of programs are most difficult to generate. Finally, we explore the semantic grounding of these models by fine-tuning them to predict the results of program execution. We find that even our best models are generally unable to predict the output of a program given a specific input.", "arxiv_url": "https://arxiv.org/abs/2108.07732", "authors": ["Jacob Austin", "Augustus Odena", "Maxwell Nye", "Maarten Bosma", "Henryk Michalewski", "David Dohan", "Ellen Jiang", "Carrie Cai", "Michael Terry", "Quoc Le", "Charles Sutton"], "first_author": "Jacob Austin", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Prompting", "tags": ["Natural-language-to-code synthesis", "Few-shot prompting with exemplars", "Small-scale fine-tuning for code", "Unit-test based functional evaluation", "Model-size scaling laws for synthesis", "Human-in-the-loop natural language feedback", "Semantic grounding / execution-result prediction", "Error analysis of synthesis failures"], "summary": "本文提出用于评估Python程序合成的两个新基准并系统性评估不同规模的Transformer模型在few-shot与微调设置下的代码生成能力，研究尺寸扩展、交互式人类反馈及语义执行能力等问题。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2108.07732v1", "published": "2021-08-16", "update_time": "2021-08-16", "download_time": "2025-12-16 14:01:40"}
{"id": "2105.09938", "title": "Measuring Coding Challenge Competence With APPS", "abstract": "While programming is one of the most broadly applicable skills in modern society, modern machine learning models still cannot code solutions to basic problems. Despite its importance, there has been surprisingly little work on evaluating code generation, and it can be difficult to accurately assess code generation performance rigorously. To meet this challenge, we introduce APPS, a benchmark for code generation. Unlike prior work in more restricted settings, our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code. Similar to how companies assess candidate software developers, we then evaluate models by checking their generated code on test cases. Our benchmark includes 10,000 problems, which range from having simple one-line solutions to being substantial algorithmic challenges. We fine-tune large language models on both GitHub and our training set, and we find that the prevalence of syntax errors is decreasing exponentially as models improve. Recent models such as GPT-Neo can pass approximately 20% of the test cases of introductory problems, so we find that machine learning models are now beginning to learn how to code. As the social significance of automatic code generation increases over the coming years, our benchmark can provide an important measure for tracking advancements.", "arxiv_url": "https://arxiv.org/abs/2105.09938", "authors": ["Dan Hendrycks", "Steven Basart", "Saurav Kadavath", "Mantas Mazeika", "Akul Arora", "Ethan Guo", "Collin Burns", "Samir Puranik", "Horace He", "Dawn Song", "Jacob Steinhardt"], "first_author": "Dan Hendrycks", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Generation from Natural Language Specifications", "tags": ["Natural-language-to-code synthesis", "Algorithmic / programming-contest problems", "Test-case-driven functional evaluation", "Difficulty stratification (intro / interview / competition)", "Python-only benchmark", "Large test-case bank (~130k test cases)"], "summary": "APPS 提出一个包含 10,000 道不同难度的 Python 自然语言到代码生成基准并配有约 130,000 个测试用例，通过测试用例评估模型的功能正确性，展示模型随规模和微调而提升但仍面临挑战的表现。", "quality": "High", "conference": "NeurIPS 2021", "pdf_url": "https://arxiv.org/pdf/2105.09938v3", "published": "2021-05-20", "update_time": "2021-11-08", "download_time": "2025-12-16 14:02:12"}
{"id": "2511.17368", "title": "Exploring Scientific Debt: Harnessing AI for SATD Identification in Scientific Software", "abstract": "Developers often leave behind clues in their code, admitting where it falls short, known as Self-Admitted Technical Debt (SATD). In the world of Scientific Software (SSW), where innovation moves fast and collaboration is key, such debt is not just common but deeply impactful. As research relies on accurate and reproducible results, accumulating SATD can threaten the very foundations of scientific discovery. Yet, despite its significance, the relationship between SATD and SSW remains largely unexplored, leaving a crucial gap in understanding how to manage SATD in this critical domain. This study explores SATD in SSW repositories, comparing SATD in scientific versus general-purpose open-source software and evaluating transformer-based models for SATD identification. We analyzed SATD in 27 scientific and general-purpose repositories across multiple domains and languages. We fine-tuned and compared 10 transformer-based models (100M-7B parameters) on 67,066 labeled code comments. SSW contains 9.25x more Scientific Debt and 4.93x more SATD than general-purpose software due to complex computations, domain constraints, and evolving research needs. Furthermore, our best model outperforms existing ones. This study uncovers how SATD in SSW differs from general software, revealing its impact on quality and scientific validity. By recognizing these challenges, developers and researchers can adopt smarter strategies to manage debt and safeguard the integrity of scientific discovery.", "arxiv_url": "https://arxiv.org/abs/2511.17368", "authors": ["Eric L. Melin", "Ahmed Musa Awon", "Nasir U. Eisty", "Neil A. Ernst", "Shurui Zhou"], "first_author": "Eric L. Melin", "category": ["Technical", "Benchmark", "Empirical"], "field": "Maintenance", "task": "Code Review", "tags": ["Scientific Debt", "Self-Admitted Technical Debt (SATD) detection", "Code comment classification", "Transformer fine-tuning for SATD", "Data augmentation for labeling", "Cross-project evaluation", "Domain-specific analysis of scientific software", "Large-model (100M–7B) performance comparison"], "summary": "本文构建并扩充了针对科学软件中“Scientific Debt”的标注数据集，微调并比较了多种transformer模型用于代码注释中的SATD识别与分类，发现科学软件中SATD和Scientific Debt显著高于通用软件且最佳模型优于现有基线。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.17368v1", "published": "2025-11-21", "update_time": "2025-11-21", "download_time": "2025-12-16 14:02:52"}
{"id": "2511.17330", "title": "Agentic Program Verification", "abstract": "Automatically generated code is gaining traction recently, owing to the prevalence of Large Language Models (LLMs). Further, the AlphaProof initiative has demonstrated the possibility of using AI for general mathematical reasoning. Reasoning about computer programs (software) can be accomplished via general mathematical reasoning; however, it tends to be more structured and richer in contexts. This forms an attractive proposition, since then AI agents can be used to reason about voluminous code that gets generated by AI.   In this work, we present a first LLM agent, AutoRocq, for conducting program verification. Unlike past works, which rely on extensive training of LLMs on proof examples, our agent learns on-the-fly and improves the proof via an iterative refinement loop. The iterative improvement of the proof is achieved by the proof agent communicating with the Rocq (formerly Coq) theorem prover to get additional context and feedback. The final result of the iteration is a proof derivation checked by the Rocq theorem prover. In this way, our proof construction involves autonomous collaboration between the proof agent and the theorem prover. This autonomy facilitates the search for proofs and decision-making in deciding on the structure of the proof tree.   Experimental evaluation on SV-COMP benchmarks and on Linux kernel modules shows promising efficacy in achieving automated program verification. As automation in code generation becomes more widespread, we posit that our proof agent can be potentially integrated with AI coding agents to achieve a generate and validate loop, thus moving closer to the vision of trusted automatic programming.", "arxiv_url": "https://arxiv.org/abs/2511.17330", "authors": ["Haoxin Tu", "Huan Zhao", "Yahui Song", "Mehtab Zafar", "Ruijie Meng", "Abhik Roychoudhury"], "first_author": "Haoxin Tu", "category": ["Technical"], "field": "Formal Verification & Theorem Proving", "task": "Agentic Program Proof Generation", "tags": ["Agentic theorem-prover interaction", "On-the-fly iterative proof refinement", "Tactic prediction guided by proof trees", "Context-aware lemma retrieval", "Tree-structured proof representations", "Autonomous proof search and decision-making", "Generate-and-validate verification loop"], "summary": "本文提出了AutoRocq，一种智能体化的自动程序证明系统，通过与交互式定理证明器循环交互、上下文感知的引理检索与树状证明表示实现逐步精炼的策略，在SV-COMP与Linux内核模块上显著提升了自动化验证能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.17330v1", "published": "2025-11-21", "update_time": "2025-11-21", "download_time": "2025-12-16 14:03:40"}
{"id": "2511.17262", "title": "SlsReuse: LLM-Powered Serverless Function Reuse", "abstract": "Serverless computing has rapidly emerged as a popular cloud computing paradigm. It enables developers to implement function-level tasks, i.e., serverless functions, without managing infrastructure. While reducing operational overhead, it poses challenges, especially for novice developers. Developing functions from scratch requires adapting to heterogeneous, platform-specific programming styles, making the process time-consuming and error-prone. Function reuse offers a promising solution to address these challenges. However, research on serverless computing lacks a dedicated approach for function recommendation. Existing techniques from traditional contexts remain insufficient due to the semantic gap between task descriptions and heterogeneous function implementations. Advances in large language models (LLMs), pre-trained on large-scale corpora, create opportunities to bridge this gap by aligning developer requirements with function semantics.   This paper presents SlsReuse, the first LLM-powered framework for serverless function reuse. Specifically, SlsReuse first constructs a reusable function repository serving as a foundational knowledge base. Then, it learns unified semantic-enhanced representations of heterogeneous functions through effective prompt engineering with few-shot prompting, capturing implicit code intent, target platforms, programming languages, and cloud services. Finally, given a natural language task query, SlsReuse performs intent-aware discovery combined with a multi-level pruning strategy and similarity matching. We evaluate SlsReuse on a curated dataset of 110 task queries. Built on ChatGPT-4o, one of the most representative LLMs, SlsReuse achieves Recall@10 of 91.20%, exceeding the state-of-the-art baseline by 24.53 percentage points.", "arxiv_url": "https://arxiv.org/abs/2511.17262", "authors": ["Jinfeng Wen", "Yuehan Sun"], "first_author": "Jinfeng Wen", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Retrieval", "tags": ["Serverless function retrieval", "Semantic code representation", "Few-shot prompt engineering", "Intent-aware discovery", "Multi-level pruning", "Cross-platform heterogeneity", "Function repository construction", "Similarity-based ranking"], "summary": "本文提出SlsReuse——一个基于大模型、通过少量示例提示抽取语义增强表示并结合意图感知检索与多级剪枝实现无服务器函数重用的框架，并构建了500个函数的仓库和110条评测查询展示其显著效果。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.17262v1", "published": "2025-11-21", "update_time": "2025-11-21", "download_time": "2025-12-16 14:04:08"}
{"id": "2511.17027", "title": "ReVul-CoT: Towards Effective Software Vulnerability Assessment with Retrieval-Augmented Generation and Chain-of-Thought Prompting", "abstract": "Context: Software Vulnerability Assessment (SVA) plays a vital role in evaluating and ranking vulnerabilities in software systems to ensure their security and reliability. Objective: Although Large Language Models (LLMs) have recently shown remarkable potential in SVA, they still face two major limitations. First, most LLMs are trained on general-purpose corpora and thus lack domain-specific knowledge essential for effective SVA. Second, they tend to rely on shallow pattern matching instead of deep contextual reasoning, making it challenging to fully comprehend complex code semantics and their security implications. Method: To alleviate these limitations, we propose a novel framework ReVul-CoT that integrates Retrieval-Augmented Generation (RAG) with Chain-of-Thought (COT) prompting. In ReVul-CoT, the RAG module dynamically retrieves contextually relevant information from a constructed local knowledge base that consolidates vulnerability data from authoritative sources (such as NVD and CWE), along with corresponding code snippets and descriptive information. Building on DeepSeek-V3.1, CoT prompting guides the LLM to perform step-by-step reasoning over exploitability, impact scope, and related factors Results: We evaluate ReVul-CoT on a dataset of 12,070 vulnerabilities. Experimental results show that ReVul-CoT outperforms state-of-the-art SVA baselines by 16.50%-42.26% in terms of MCC, and outperforms the best baseline by 10.43%, 15.86%, and 16.50% in Accuracy, F1-score, and MCC, respectively. Our ablation studies further validate the contributions of considering dynamic retrieval, knowledge integration, and CoT-based reasoning. Conclusion: Our results demonstrate that combining RAG with CoT prompting significantly enhances LLM-based SVA and points out promising directions for future research.", "arxiv_url": "https://arxiv.org/abs/2511.17027", "authors": ["Zhijie Chen", "Xiang Chen", "Ziming Li", "Jiacheng Xue", "Chaoyang Gao"], "first_author": "Zhijie Chen", "category": ["Technical", "Benchmark"], "field": "Quality Management", "task": "Vulnerability Assessment (Severity Scoring)", "tags": ["Retrieval-Augmented Generation (RAG)", "Chain-of-Thought (CoT) prompting", "Local vulnerability knowledge base", "CVSS v3 severity prediction", "Code and vulnerability-description fusion", "Dynamic retrieval strategy", "Exploitability and impact reasoning", "Ablation study of framework components"], "summary": "本文提出ReVul-CoT框架，将动态检索增强生成与链式思维提示相结合，利用构建的本地漏洞知识库对代码与描述进行逐步推理，从而显著提升基于CVSS v3的软件漏洞严重性评估性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.17027v1", "published": "2025-11-21", "update_time": "2025-11-21", "download_time": "2025-12-16 14:04:51"}
{"id": "2511.16858", "title": "Is the Cure Still Worse Than the Disease? Test Overfitting by LLMs in Automated Program Repair", "abstract": "Automated program repair has been shown to be susceptible to generating repaired code that passes on seen tests but fails on a hold-out set of hidden tests. This problem, dubbed test overfitting, has been identified and studied before the rise of large language models. We experimentally study how much test overfitting is still a problem today, using repository-level SWE-bench tasks.", "arxiv_url": "https://arxiv.org/abs/2511.16858", "authors": ["Toufique Ahmed", "Jatin Ganhotra", "Avraham Shinnar", "Martin Hirzel"], "first_author": "Toufique Ahmed", "category": ["Empirical"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Test Overfitting", "White-box vs Black-box Tests", "Repository-level Automated Program Repair", "Reproduction Test Generation", "Test-based Code Refinement Loop", "Reward-guided Patch Selection", "Test Exposure/Hiding Mitigation", "Limit Study with Revealed Tests"], "summary": "本文在仓库级自动程序修复任务上实证量化了基于LLM的方法在使用白盒测试时产生的测试过拟合问题，并评估了基于测试的补丁改进与揭示金标准测试对修复效果的影响。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16858v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-12-16 14:05:25"}
{"id": "2511.16787", "title": "NALA_MAINZ at BLP-2025 Task 2: A Multi-agent Approach for Bangla Instruction to Python Code Generation", "abstract": "This paper presents JGU Mainz's winning system for the BLP-2025 Shared Task on Code Generation from Bangla Instructions. We propose a multi-agent-based pipeline. First, a code-generation agent produces an initial solution from the input instruction. The candidate program is then executed against the provided unit tests (pytest-style, assert-based). Only the failing cases are forwarded to a debugger agent, which reruns the tests, extracts error traces, and, conditioning on the error messages, the current program, and the relevant test cases, generates a revised solution. Using this approach, our submission achieved first place in the shared task with a $Pass@1$ score of 95.4. We also make our code public.", "arxiv_url": "https://arxiv.org/abs/2511.16787", "authors": ["Hossain Shaikh Saadi", "Faria Alam", "Mario Sanz-Guerrero", "Minh Duc Bui", "Manuel Mager", "Katharina von der Wense"], "first_author": "Hossain Shaikh Saadi", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Prompting", "tags": ["Multi-agent pipeline", "Test-driven debugging", "Selective failure-only debugging", "Unit-test execution feedback", "Error-trace-conditioned repair", "Automatic test-case augmentation", "Bangla instruction-to-code"], "summary": "本文提出了一种面向孟加拉语指令到Python代码生成的多智能体流水线：先由生成代理产出代码并运行单元测试，仅将失败样例与错误追踪交给调试代理进行基于错误信息的修复，从而在BLP-2025共享任务中取得第一（Pass@1=95.4%）。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16787v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-12-16 14:06:05"}
{"id": "2511.16395", "title": "CorrectHDL: Agentic HDL Design with LLMs Leveraging High-Level Synthesis as Reference", "abstract": "Large Language Models (LLMs) have demonstrated remarkable potential in hardware front-end design using hardware description languages (HDLs). However, their inherent tendency toward hallucination often introduces functional errors into the generated HDL designs. To address this issue, we propose the framework CorrectHDL that leverages high-level synthesis (HLS) results as functional references to correct potential errors in LLM-generated HDL designs.The input to the proposed framework is a C/C++ program that specifies the target circuit's functionality. The program is provided to an LLM to directly generate an HDL design, whose syntax errors are repaired using a Retrieval-Augmented Generation (RAG) mechanism. The functional correctness of the LLM-generated circuit is iteratively improved by comparing its simulated behavior with an HLS reference design produced by conventional HLS tools, which ensures the functional correctness of the result but can lead to suboptimal area and power efficiency. Experimental results demonstrate that circuits generated by the proposed framework achieve significantly better area and power efficiency than conventional HLS designs and approach the quality of human-engineered circuits. Meanwhile, the correctness of the resulting HDL implementation is maintained, highlighting the effectiveness and potential of agentic HDL design leveraging the generative capabilities of LLMs and the rigor of traditional correctness-driven IC design flows.", "arxiv_url": "https://arxiv.org/abs/2511.16395", "authors": ["Kangwei Xu", "Grace Li Zhang", "Ulf Schlichtmann", "Bing Li"], "first_author": "Kangwei Xu", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["HLS-guided differential verification", "RAG-based HDL syntax repair", "C/C++ submodule decomposition", "Submodule boundary instrumentation", "Backward slicing for debugging", "LLM-driven HDL generation", "HDL testbench translation", "Area and power (PPA) optimization", "Toolchain integration (HLS/RTL sim/synthesis)"], "summary": "本文提出CorrectHDL，一种以HLS生成的HDL作为功能金标准、结合LLM生成、RAG语法修复、差分验证与子模块切片的自动化HDL设计与调试框架，可在保证功能正确性的同时显著降低面积与功耗并接近人工设计质量。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16395v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-12-16 14:06:52"}
{"id": "2511.16383", "title": "An Agent-Based Framework for the Automatic Validation of Mathematical Optimization Models", "abstract": "Recently, using Large Language Models (LLMs) to generate optimization models from natural language descriptions has became increasingly popular. However, a major open question is how to validate that the generated models are correct and satisfy the requirements defined in the natural language description. In this work, we propose a novel agent-based method for automatic validation of optimization models that builds upon and extends methods from software testing to address optimization modeling . This method consists of several agents that initially generate a problem-level testing API, then generate tests utilizing this API, and, lastly, generate mutations specific to the optimization model (a well-known software testing technique assessing the fault detection power of the test suite). In this work, we detail this validation framework and show, through experiments, the high quality of validation provided by this agent ensemble in terms of the well-known software testing measure called mutation coverage.", "arxiv_url": "https://arxiv.org/abs/2511.16383", "authors": ["Alexander Zadorojniy", "Segev Wasserkrug", "Eitan Farchi"], "first_author": "Alexander Zadorojniy", "category": ["Technical", "Empirical"], "field": "Software Testing", "task": "Test Generation", "tags": ["Agent-based validation", "Problem-level testing API", "Unit-test generation for optimization models", "Optimization-model mutation operators", "Auxiliary verifier model generation", "Mutation coverage evaluation"], "summary": "本文提出一个基于多智能体的自动化验证框架，通过构建问题级测试接口、自动生成面向优化建模的单元测试与变异体（mutation）来对自然语言生成的线性规划模型进行模型不可知的正确性验证，并以变异覆盖率评估其有效性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16383v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-12-16 14:07:36"}
{"id": "2511.16224", "title": "Beyond Code Similarity: Benchmarking the Plausibility, Efficiency, and Complexity of LLM-Generated Smart Contracts", "abstract": "Smart Contracts are critical components of blockchain ecosystems, with Solidity as the dominant programming language. While LLMs excel at general-purpose code generation, the unique constraints of Smart Contracts, such as gas consumption, security, and determinism, raise open questions about the reliability of LLM-generated Solidity code. Existing studies lack a comprehensive evaluation of these critical functional and non-functional properties. We benchmark four state-of-the-art models under zero-shot and retrieval-augmented generation settings across 500 real-world functions. Our multi-faceted assessment employs code similarity metrics, semantic embeddings, automated test execution, gas profiling, and cognitive and cyclomatic complexity analysis. Results show that while LLMs produce code with high semantic similarity to real contracts, their functional correctness is low: only 20% to 26% of zero-shot generations behave identically to ground-truth implementations under testing. The generated code is consistently simpler, with significantly lower complexity and gas consumption, often due to omitted validation logic. Retrieval-Augmented Generation markedly improves performance, boosting functional correctness by up to 45% and yielding more concise and efficient code. Our findings reveal a significant gap between semantic similarity and functional plausibility in LLM-generated Smart Contracts. We conclude that while RAG is a powerful enhancer, achieving robust, production-ready code generation remains a substantial challenge, necessitating careful expert validation.", "arxiv_url": "https://arxiv.org/abs/2511.16224", "authors": ["Francesco Salzano", "Simone Scalabrino", "Rocco Oliveto", "Remo Pareschi"], "first_author": "Francesco Salzano", "category": ["Empirical", "Benchmark"], "field": "Coding Assistant", "task": "Code Prompting", "tags": ["Solidity smart contracts", "Gas profiling", "Functional plausibility testing", "Retrieval-augmented generation", "AST structural similarity (Tree Edit Distance)", "Automated test execution", "Cyclomatic and cognitive complexity", "Semantic code embeddings"], "summary": "本文在500个真实Solidity函数上比较四种LLM的零样本与检索增强生成，系统评估语义/结构相似度、自动化测试通过率、燃气消耗与代码复杂度，发现语义相似高但功能正确率低，且检索增强显著提升可行性与效率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16224v2", "published": "2025-11-20", "update_time": "2025-11-21", "download_time": "2025-12-16 14:08:20"}
{"id": "2511.16123", "title": "Domain-constrained Synthesis of Inconsistent Key Aspects in Textual Vulnerability Descriptions", "abstract": "Textual Vulnerability Descriptions (TVDs) are crucial for security analysts to understand and address software vulnerabilities. However, the key aspect inconsistencies in TVDs from different repositories pose challenges for achieving a comprehensive understanding of vulnerabilities. Existing approaches aim to mitigate inconsistencies by aligning TVDs with external knowledge bases, but they often discard valuable information and fail to synthesize comprehensive representations. In this paper, we propose a domain-constrained LLM-based synthesis framework for unifying key aspects of TVDs. Our framework consists of three stages: 1) Extraction, guided by rule-based templates to ensure all critical details are captured; 2) Self-evaluation, using domain-specific anchor words to assess semantic variability across sources; and 3) Fusion, leveraging information entropy to reconcile inconsistencies and prioritize relevant details. This framework improves synthesis performance, increasing the F1 score for key aspect augmentation from 0.82 to 0.87, while enhancing comprehension and efficiency by over 30\\%. We further develop Digest Labels, a practical tool for visualizing TVDs, which human evaluations show significantly boosts usability.", "arxiv_url": "https://arxiv.org/abs/2511.16123", "authors": ["Linyi Han", "Shidong Pan", "Zhenchang Xing", "Sofonias Yitagesu", "Xiaowang Zhang", "Zhiyong Feng", "Jiamou Sun", "Qing Huang"], "first_author": "Linyi Han", "category": ["Technical"], "field": "Vulnerability Analysis & Documentation", "task": "Textual Vulnerability Description Synthesis", "tags": ["Domain-constrained synthesis", "Rule-based extraction templates", "Domain-specific anchor words", "Information-entropy based fusion", "Digest Labels visualization", "Cross-repository TVD integration", "Key-aspect reconciliation", "Human usability evaluation"], "summary": "本文提出一种基于领域约束的合成框架，通过规则化提取、基于领域锚词的自评估和信息熵驱动的融合，统一不同漏洞库中不一致的文本漏洞描述关键要素并设计了可视化的 Digest Labels 系统以提升理解与效率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16123v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-12-16 14:09:03"}
{"id": "2511.16092", "title": "The Future of Development Environments with AI Foundation Models: NII Shonan Meeting 222 Report", "abstract": "Generative Artificial Intelligence (GenAI) models are achieving remarkable performance in various tasks, including code generation, testing, code review, and program repair. The ability to increase the level of abstraction away from writing code has the potential to change the Human-AI interaction within the integrated development environment (IDE). To explore the impact of GenAI on IDEs, 33 experts from the Software Engineering, Artificial Intelligence, and Human-Computer Interaction domains gathered to discuss challenges and opportunities at Shonan Meeting 222. This is the report", "arxiv_url": "https://arxiv.org/abs/2511.16092", "authors": ["Xing Hu", "Raula Gaikovina Kula", "Christoph Treude"], "first_author": "Xing Hu", "category": ["Survey"], "field": "Coding Assistant", "task": "Code Prompting", "tags": ["IDE–FM integration", "Human–AI interaction in IDEs", "Prompt language design for code", "Foundation model engineering", "AI dataset management for code", "FM debugging and validation", "Cross-platform FM deployment", "AI agent orchestration"], "summary": "本文报告总结了33位专家在Shonan会议上关于将AI基础模型整合进开发环境的讨论，涵盖了人机交互、提示语言、模型与数据工程、调试与部署等挑战与机遇。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16092v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-12-16 14:09:36"}
{"id": "2511.16708", "title": "Multi-Agent Code Verification via Information Theory", "abstract": "LLMs generate buggy code: 29.6% of SWE-bench solved patches fail, 62% of BaxBench solutions have vulnerabilities, and existing tools only catch 65% of bugs with 35% false positives. We built CodeX-Verify, a multi-agent system that uses four specialized agents to detect different types of bugs. We prove mathematically that combining agents with different detection patterns finds more bugs than any single agent when the agents look for different problems, using submodularity of mutual information under conditional independence. Measuring agent correlation of rho = 0.05 to 0.25 confirms they detect different bugs. Testing on 99 code samples with verified labels shows our system catches 76.1% of bugs, matching the best existing method (Meta Prompt Testing: 75%) while running faster and without test execution. We tested all 15 agent combinations and found that using multiple agents improves accuracy by 39.7 percentage points (from 32.8% to 72.4%) compared to single agents, with diminishing returns of +14.9pp, +13.5pp, and +11.2pp for agents 2, 3, and 4, validating our theoretical model. The best two-agent combination (Correctness + Performance) reaches 79.3% accuracy. Testing on 300 real patches from Claude Sonnet 4.5 runs in under 200ms per sample, making this practical for production use.", "arxiv_url": "https://arxiv.org/abs/2511.16708", "authors": ["Shreshth Rajan"], "first_author": "Shreshth Rajan", "category": ["Technical", "Benchmark"], "field": "Code Verification & Analysis", "task": "Multi-Agent Code Verification", "tags": ["Multi-agent static analysis", "Information-theoretic submodularity", "Mutual information aggregation", "Agent specialization (Correctness/Security/Performance/Style)", "All-combinations ablation study", "Zero-execution verification", "Heuristic weight selection", "Released annotated verification dataset", "Diminishing returns of agents", "Low inter-agent correlation measurement"], "summary": "本文提出CodeX-Verify，一种由正确性、安全、性能与风格四个专门代理组成的静态多代理代码验证系统，并用互信息子模性质给出理论证明、穷尽性消融实验验证性能提升且公开了99个标注样本的数据集。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16708v3", "published": "2025-11-20", "update_time": "2025-12-03", "download_time": "2025-12-16 14:10:11"}
{"id": "2511.16005", "title": "InfCode-C++: Intent-Guided Semantic Retrieval and AST-Structured Search for C++ Issue Resolution", "abstract": "Large language model (LLM) agents have recently shown strong performance on repository-level issue resolution, but existing systems are almost exclusively designed for Python and rely heavily on lexical retrieval and shallow code navigation. These approaches transfer poorly to C++ projects, where overloaded identifiers, nested namespaces, template instantiations, and deep control-flow structures make context retrieval and fault localization substantially more difficult. As a result, state-of-the-art Python-oriented agents show a drastic performance drop on the C++ subset of MultiSWE-bench. We introduce INFCODE-C++, the first C++-aware autonomous system for end-to-end issue resolution. The system combines two complementary retrieval mechanisms -- semantic code-intent retrieval and deterministic AST-structured querying -- to construct accurate, language-aware context for repair.These components enable precise localization and robust patch synthesis in large, statically typed C++ repositories. Evaluated on the \\texttt{MultiSWE-bench-CPP} benchmark, INFCODE-C++ achieves a resolution rate of 25.58\\%, outperforming the strongest prior agent by 10.85 percentage points and more than doubling the performance of MSWE-agent. Ablation and behavioral studies further demonstrate the critical role of semantic retrieval, structural analysis, and accurate reproduction in C++ issue resolution. INFCODE-C++ highlights the need for language-aware reasoning in multi-language software agents and establishes a foundation for future research on scalable, LLM-driven repair for complex, statically typed ecosystems.", "arxiv_url": "https://arxiv.org/abs/2511.16005", "authors": ["Qingao Dong", "Mengfei Wang", "Hengzhi Zhang", "Zhichao Li", "Yuan Yuan", "Mu Li", "Xiang Gao", "Hailong Sun", "Chunming Hu", "Weifeng Lv"], "first_author": "Qingao Dong", "category": ["Technical", "Benchmark"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Intent-guided semantic retrieval", "AST-structured query engine", "C++ overload and namespace disambiguation", "Deterministic code navigation tools", "Multi-file feature-level context aggregation", "Repository-level patch synthesis", "Language-aware agent design"], "summary": "本文提出InfCode-C++，通过意图驱动的语义检索与基于AST的结构化查询，为大型C++代码库构建精确语义上下文，从而显著提升自动化问题定位与修复效果并开源了系统与评测基准。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16005v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-12-16 14:11:05"}
{"id": "2511.16004", "title": "InfCode: Adversarial Iterative Refinement of Tests and Patches for Reliable Software Issue Resolution", "abstract": "Large language models have advanced software engineering automation, yet resolving real-world software issues remains difficult because it requires repository-level reasoning, accurate diagnostics, and strong verification signals. Existing agent-based and pipeline-based methods often rely on insufficient tests, which can lead to patches that satisfy verification but fail to fix the underlying defect. We present InfCode, an adversarial multi-agent framework for automated repository-level issue resolution. InfCode iteratively refines both tests and patches through adversarial interaction between a Test Patch Generator and a Code Patch Generator, while a Selector agent identifies the most reliable fix. The framework runs inside a containerized environment that supports realistic repository inspection, modification, and validation. Experiments on SWE-bench Lite and SWE-bench Verified using models such as DeepSeek-V3 and Claude 4.5 Sonnet show that InfCode consistently outperforms strong baselines. It achieves 79.4% performance on SWE-bench Verified, establishing a new state-of-the-art. We have released InfCode as an open-source project at https://github.com/Tokfinity/InfCode.", "arxiv_url": "https://arxiv.org/abs/2511.16004", "authors": ["KeFan Li", "Mengfei Wang", "Hengzhi Zhang", "Zhichao Li", "Yuan Yuan", "Mu Li", "Xiang Gao", "Hailong Sun", "Chunming Hu", "Weifeng Lv"], "first_author": "KeFan Li", "category": ["Technical"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Adversarial Test-Patch Iteration", "Multi-Agent Repair Framework", "Test Strengthening", "Patch Selection and Ranking", "Containerized Repository Execution", "Repository-aware Code Manipulation", "Robustness-driven Patch Generation"], "summary": "本文提出 InfCode：在容器化仓库环境中通过测试生成器与代码生成器的对抗式迭代来强化测试并改进补丁，辅以选择器挑选最可靠修复，从而提升仓库级别问题修复的可靠性并在基准上取得SOTA结果。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16004v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-12-16 14:11:48"}
{"id": "2511.15817", "title": "A Causal Perspective on Measuring, Explaining and Mitigating Smells in LLM-Generated Code", "abstract": "Recent advances in large language models (LLMs) have accelerated their adoption in software engineering contexts. However, concerns persist about the structural quality of the code they produce. In particular, LLMs often replicate poor coding practices, introducing code smells (i.e., patterns that hinder readability, maintainability, or design integrity). Although prior research has examined the detection or repair of smells, we still lack a clear understanding of how and when these issues emerge in generated code.   This paper addresses this gap by systematically measuring, explaining and mitigating smell propensity in LLM-generated code. We build on the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating particular smell types, and establish its robustness as a signal of structural quality. Using PSC as an instrument for causal analysis, we identify how generation strategy, model size, model architecture and prompt formulation shape the structural properties of generated code. Our findings show that prompt design and architectural choices play a decisive role in smell propensity and motivate practical mitigation strategies that reduce its occurrence. A user study further demonstrates that PSC helps developers interpret model behavior and assess code quality, providing evidence that smell propensity signals can support human judgement. Taken together, our work lays the groundwork for integrating quality-aware assessments into the evaluation and deployment of LLMs for code.", "arxiv_url": "https://arxiv.org/abs/2511.15817", "authors": ["Alejandro Velasco", "Daniel Rodriguez-Cardenas", "Dipin Khati", "David N. Palacio", "Luftar Rahman Alif", "Denys Poshyvanyk"], "first_author": "Alejandro Velasco", "category": ["Technical", "Benchmark", "Empirical"], "field": "Quality Management", "task": "Code Smell Measurement & Mitigation", "tags": ["Propensity Smelly Score (PSC)", "Causal inference on generation factors", "Prompt-based mitigation", "Decoding strategy effects", "Model architecture impact", "Semantic-preserving code transformations (SECT)", "Robustness and information-gain analysis", "Developer interpretability user study"], "summary": "本文从因果视角验证并扩展了PSC度量，用以量化并解释LLM生成代码中的代码异味，评估生成策略、模型规模与架构及提示对异味倾向的影响，提出基于提示的缓解策略并通过用户研究证明其实用性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.15817v3", "published": "2025-11-19", "update_time": "2025-12-09", "download_time": "2025-12-16 14:12:57"}
{"id": "2511.15293", "title": "A Viable Paradigm of Software Automation: Iterative End-to-End Automated Software Development", "abstract": "Software development automation is a long-term goal in software engineering. With the development of artificial intelligence (AI), more and more researchers are exploring approaches to software automation. They view AI systems as tools or assistants in software development, still requiring significant human involvement. Another initiative is ``vibe coding'', where AI systems write and repeatedly revise most (or even all) of the code. We foresee these two development paths will converge towards the same destination: AI systems participate in throughout the software development lifecycle, expanding boundaries of full-stack software development. In this paper, we present a vision of an iterative end-to-end automated software development paradigm AutoSW. It operates in an analyze-plan-implement-deliver loop, where AI systems as human partners become first-class actors, translating human intentions expressed in natural language into executable software. We explore a lightweight prototype across the paradigm and initially execute various representative cases. The results indicate that AutoSW can successfully deliver executable software, providing a feasible direction for truly end-to-end automated software development.", "arxiv_url": "https://arxiv.org/abs/2511.15293", "authors": ["Jia Li", "Zhi Jin", "Huangzhao Zhang", "Kechi Zhang", "Jiaru Qian", "Tiankuo Zhao"], "first_author": "Jia Li", "category": ["Technical"], "field": "Software Automation", "task": "End-to-End Automated Software Development (analyze-plan-implement-deliver)", "tags": ["Orchestral agent system", "Iterative analyze-plan-implement-deliver loop", "Conversational requirement elicitation", "Plan generation with traceability", "Automated system design", "Task-level implementation orchestration", "Automated test generation and vulnerability checking", "Deployment and delivery automation", "Human-in-the-loop physical-constraint specification", "Software evolution and change propagation"], "summary": "本文提出了AutoSW——一种以编排式智能体为核心的迭代端到端软件自动化范式，通过分析-规划-实现-交付循环，将自然语言意图自动转化为可部署软件，并提供了轻量原型与示例验证其可行性。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.15293v2", "published": "2025-11-19", "update_time": "2025-11-23", "download_time": "2025-12-16 14:13:35"}
{"id": "2511.15757", "title": "Rethinking Kernel Program Repair: Benchmarking and Enhancing LLMs with RGym", "abstract": "Large Language Models (LLMs) have revolutionized automated program repair (APR) but current benchmarks like SWE-Bench predominantly focus on userspace applications and overlook the complexities of kernel-space debugging and repair. The Linux kernel poses unique challenges due to its monolithic structure, concurrency, and low-level hardware interactions. Prior efforts such as KGym and CrashFixer have highlighted the difficulty of APR in this domain, reporting low success rates or relying on costly and complex pipelines and pricey cloud infrastructure. In this work, we introduce RGym, a lightweight, platform-agnostic APR evaluation framework for the Linux kernel designed to operate on local commodity hardware. Built on RGym, we propose a simple yet effective APR pipeline leveraging specialized localization techniques (e.g., call stacks and blamed commits) to overcome the unrealistic usage of oracles in KGym. We test on a filtered and verified dataset of 143 bugs. Our method achieves up to a 43.36% pass rate with GPT-5 Thinking while maintaining a cost of under $0.20 per bug. We further conduct an ablation study to analyze contributions from our proposed localization strategy, prompt structure, and model choice, and demonstrate that feedback-based retries can significantly enhance success rates.", "arxiv_url": "https://arxiv.org/abs/2511.15757", "authors": ["Kareem Shehada", "Yifan Wu", "Wyatt D. Feng", "Adithya Iyer", "Gryphon Kumfert", "Yangruibo Ding", "Zhiyun Qian"], "first_author": "Kareem Shehada", "category": ["Technical", "Benchmark", "Empirical"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Kernel-space automated repair", "KASAN crash bug reproduction", "Bug-inducing-commit (BIC) localization", "Call-stack-based localization", "Function-wise patching", "Feedback-driven retry loop", "Local Docker+QEMU test harness", "Cost-efficient LLM APR evaluation", "Ablation of localization and prompt components"], "summary": "本文提出了RGym——一个可在本地运行的轻量级Linux内核自动修复评测框架，整理并验证了143个KASAN内核漏洞，并通过基于错误诱发提交与调用栈的现实定位策略、函数级补丁生成与反馈重试流程，使用大模型在低成本下显著提升内核程序修复成功率。", "quality": "High", "conference": "NeurIPS 2025 Workshop: Evaluating the Evolving LLM Lifecycle: Benchmarks, Emergent Abilities, and Scaling 2025", "pdf_url": "https://arxiv.org/pdf/2511.15757v1", "published": "2025-11-19", "update_time": "2025-11-19", "download_time": "2025-12-16 14:14:25"}
{"id": "2511.15168", "title": "Finetuning LLMs for Automatic Form Interaction on Web-Browser in Selenium Testing Framework", "abstract": "Automated web application testing is a critical component of modern software development, with frameworks like Selenium widely adopted for validating functionality through browser automation. Among the essential aspects of such testing is the ability to interact with and validate web forms, a task that requires syntactically correct, executable scripts with high coverage of input fields. Despite its importance, this task remains underexplored in the context of large language models (LLMs), and no public benchmark or dataset exists to evaluate LLMs on form interaction generation systematically. This paper introduces a novel method for training LLMs to generate high-quality test cases in Selenium, specifically targeting form interaction testing. We curate both synthetic and human-annotated datasets for training and evaluation, covering diverse real-world forms and testing scenarios. We define clear metrics for syntax correctness, script executability, and input field coverage. Our empirical study demonstrates that our approach significantly outperforms strong baselines, including GPT-4o and other popular LLMs, across all evaluation metrics. Our work lays the groundwork for future research on LLM-based web testing and provides resources to support ongoing progress in this area.", "arxiv_url": "https://arxiv.org/abs/2511.15168", "authors": ["Nguyen-Khang Le", "Hiep Nguyen", "Ngoc-Minh Nguyen", "Son T. Luu", "Trung Vo", "Quan Minh Bui", "Shoshin Nomura", "Le-Minh Nguyen"], "first_author": "Nguyen-Khang Le", "category": ["Technical", "Benchmark"], "field": "Software Testing", "task": "Test Generation", "tags": ["Selenium script generation", "Form interaction automation", "Synthetic HTML field pool", "Human-annotated form scenarios", "Prompted scenario-to-code pipeline", "Executable-script filtering", "Input-field coverage metric"], "summary": "本文提出一种基于合成与人工标注数据、通过提示生成并微调模型以自动生成可执行的Selenium表单交互测试脚本的方法，并构建了首个针对表单交互的测试数据集和评估指标，在语法正确性、可执行性与字段覆盖率上显著优于强基线。", "quality": "High", "conference": "Proceedings of KSE 2025", "pdf_url": "https://arxiv.org/pdf/2511.15168v2", "published": "2025-11-19", "update_time": "2025-11-20", "download_time": "2025-12-16 14:15:12"}
{"id": "2511.15755", "title": "Multi-Agent LLM Orchestration Achieves Deterministic, High-Quality Decision Support for Incident Response", "abstract": "Large language models (LLMs) promise to accelerate incident response in production systems, yet single-agent approaches generate vague, unusable recommendations. We present MyAntFarm.ai, a reproducible containerized framework demonstrating that multi-agent orchestration fundamentally transforms LLM-based incident response quality. Through 348 controlled trials comparing single-agent copilot versus multi-agent systems on identical incident scenarios, we find that multi-agent orchestration achieves 100% actionable recommendation rate versus 1.7% for single-agent approaches, an 80 times improvement in action specificity and 140 times improvement in solution correctness. Critically, multi-agent systems exhibit zero quality variance across all trials, enabling production SLA commitments impossible with inconsistent single-agent outputs. Both architectures achieve similar comprehension latency (approx.40s), establishing that the architectural value lies in deterministic quality, not speed. We introduce Decision Quality (DQ), a novel metric capturing validity, specificity, and correctness properties essential for operational deployment that existing LLM metrics do not address. These findings reframe multi-agent orchestration from a performance optimization to a production-readiness requirement for LLM-based incident response. All code, Docker configurations, and trial data are publicly available for reproduction.", "arxiv_url": "https://arxiv.org/abs/2511.15755", "authors": ["Philip Drammeh"], "first_author": "Philip Drammeh", "category": ["Technical", "Empirical"], "field": "AIOps", "task": "Incident Response / Decision Support", "tags": ["Multi-agent orchestration", "Prompt decomposition", "Specialized agent pipeline (diagnosis/planning/risk)", "Decision Quality (DQ) metric", "Time-to-Usable-Understanding (T2U)", "Containerized reproducible evaluation framework", "Deterministic output guarantees", "Regex-based specificity scoring", "Token-overlap correctness scoring", "Actionable remediation command generation"], "summary": "本文提出并评估了 MyAntFarm.ai：通过将 incident response 分解为诊断、修复计划和风险评估的多代理LLM编排，结合新引入的 Decision Quality 和 T2U 指标，在 348 次可复现仿真实验中实现了对事故响应可执行性、特异性和正确性的显著且确定性的提升。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.15755v1", "published": "2025-11-19", "update_time": "2025-11-19", "download_time": "2025-12-16 14:15:53"}
{"id": "2511.19427", "title": "Prompt Less, Smile More: MTP with Semantic Engineering in Lieu of Prompt Engineering", "abstract": "AI-Integrated programming is emerging as a foundational paradigm for building intelligent systems with large language models (LLMs). Recent approaches such as Meaning Typed Programming (MTP) automate prompt generation by leveraging the semantics already present in code. However, many real-world applications depend on contextual cues, developer intent, and domain-specific reasoning that extend beyond what static code semantics alone can express. To address this limitation, we introduce Semantic Engineering, a lightweight method for enriching program semantics so that LLM-based systems can more accurately reflect developer intent without requiring full manual prompt design. We present Semantic Context Annotations (SemTexts), a language-level mechanism that allows developers to embed natural-language context directly into program constructs. Integrated into the Jac programming language, Semantic Engineering extends MTP to incorporate these enriched semantics during prompt generation. We further introduce a benchmark suite designed to reflect realistic AI-Integrated application scenarios. Our evaluation shows that Semantic Engineering substantially improves prompt fidelity, achieving performance comparable to Prompt Engineering while requiring significantly less developer effort.", "arxiv_url": "https://arxiv.org/abs/2511.19427", "authors": ["Jayanaka L. Dantanarayana", "Savini Kashmira", "Thakee Nathees", "Zichen Zhang", "Krisztian Flautner", "Lingjia Tang", "Jason Mars"], "first_author": "Jayanaka L. Dantanarayana", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Prompting", "tags": ["Semantic Context Annotations", "Semantic Engineering", "MTP integration", "MT-IR enrichment", "Compiler pass for semantic binding", "Prompt generation from code semantics", "Jac language integration", "AI-integrated application benchmark", "Developer effort vs. prompt fidelity evaluation"], "summary": "本文提出Semantic Engineering与SemText语义上下文注释，通过在Jac语言中将自然语言语义绑定到程序构件并扩展MTP的MT-IR，从而自动生成更贴合开发者意图的提示，显著提升AI集成应用的性能并提供现实场景基准评测，同时大幅降低开发者工作量。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.19427v1", "published": "2025-11-24", "update_time": "2025-11-24", "download_time": "2025-12-16 14:16:31"}
{"id": "2511.19132", "title": "LLMs-Powered Real-Time Fault Injection: An Approach Toward Intelligent Fault Test Cases Generation", "abstract": "A well-known testing method for the safety evaluation and real-time validation of automotive software systems (ASSs) is Fault Injection (FI). In accordance with the ISO 26262 standard, the faults are introduced artificially for the purpose of analyzing the safety properties and verifying the safety mechanisms during the development phase. However, the current FI method and tools have a significant limitation in that they require manual identification of FI attributes, including fault type, location and time. The more complex the system, the more expensive, time-consuming and labour-intensive the process. To address the aforementioned challenge, a novel Large Language Models (LLMs)-assisted fault test cases (TCs) generation approach for utilization during real-time FI tests is proposed in this paper. To this end, considering the representativeness and coverage criteria, the applicability of various LLMs to create fault TCs from the functional safety requirements (FSRs) has been investigated. Through the validation results of LLMs, the superiority of the proposed approach utilizing gpt-4o in comparison to other state-of-the-art models has been demonstrated. Specifically, the proposed approach exhibits high performance in terms of FSRs classification and fault TCs generation with F1-score of 88% and 97.5%, respectively. To illustrate the proposed approach, the generated fault TCs were executed in real time on a hardware-in-the-loop system, where a high-fidelity automotive system model served as a case study. This novel approach offers a means of optimizing the real-time testing process, thereby reducing costs while simultaneously enhancing the safety properties of complex safety-critical ASSs.", "arxiv_url": "https://arxiv.org/abs/2511.19132", "authors": ["Mohammad Abboush", "Ahmad Hatahet", "Andreas Rausch"], "first_author": "Mohammad Abboush", "category": ["Technical"], "field": "Software Testing", "task": "Test Generation", "tags": ["Requirements-to-fault-testcase generation", "LLM-assisted requirements classification", "Real-time hardware-in-the-loop validation", "Sensor and actuator fault modeling", "Concurrent fault injection", "ISO 26262-aligned testing", "CAN-layer fault types", "JSON fault-vector encoding", "Coverage-driven fault selection"], "summary": "本文提出一种基于大型语言模型的实时故障注入测试用例生成方法，从功能安全需求自动分类并生成传感器/执行器故障用例，并在高保真硬件在环汽车模型上验证了其有效性与高F1得分。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.19132v1", "published": "2025-11-24", "update_time": "2025-11-24", "download_time": "2025-12-16 14:17:10"}
{"id": "2511.19422", "title": "SLMFix: Leveraging Small Language Models for Error Fixing with Reinforcement Learning", "abstract": "Recent advancements in large language models (LLMs) have shown very impressive capabilities in code generation across many programming languages. However, even state-of-the-art LLMs generate programs that contains syntactic errors and fail to complete the given tasks, especially for low-resource programming languages (LRPLs). In addition, high training cost makes finetuning LLMs unaffordable with constrained computational resources, further undermining the effectiveness of LLMs for code generation. In this work, we propose SLMFix, a novel code generation pipeline that leverages a small language model (SLM) finetuned using reinforcement learning (RL) techniques to fix syntactic errors in LLM-generated programs to improve the quality of LLM-generated programs for domain-specific languages (DSLs). In specific, we applied RL on the SLM for the program repair task using a reward calculated using both a static validator and a static semantic similarity metric. Our experimental results demonstrate the effectiveness and generalizability of our approach across multiple DSLs, achieving more than 95% pass rate on the static validator. Notably, SLMFix brings substantial improvement to the base model and outperforms supervised finetuning approach even for 7B models on a LRPL, showing the potential of our approach as an alternative to traditional finetuning approaches.", "arxiv_url": "https://arxiv.org/abs/2511.19422", "authors": ["David Jiahao Fu", "Aryan Gupta", "Aaron Councilman", "David Grove", "Yu-Xiong Wang", "Vikram Adve"], "first_author": "David Jiahao Fu", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["SLM post-editing", "Reinforcement learning for program repair", "AST-based semantic reward", "Static validator reward", "DSL-focused error fixing", "Ansible evaluation corpus", "Low-resource language adaptation", "Post-hoc fixer pipeline"], "summary": "本文提出SLMFix：在不微调大模型的前提下，用强化学习微调的小型语言模型对LLM生成的DSL代码进行静态错误修复，并以静态验证器与基于AST的相似性作为奖励显著提升生成代码质量（在Ansible、Bash、SQL上验证）。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.19422v1", "published": "2025-11-24", "update_time": "2025-11-24", "download_time": "2025-12-16 14:17:53"}
{"id": "2511.19130", "title": "Can LLMs Recover Program Semantics? A Systematic Evaluation with Symbolic Execution", "abstract": "Obfuscation poses a persistent challenge for software engineering tasks such as program comprehension, maintenance, testing, and vulnerability detection. While compiler optimizations and third-party code often introduce transformations that obscure program intent, existing analysis tools and large language models (LLMs) struggle to recover the original semantics. In this work, we investigate whether LLMs, when fine-tuned with symbolic execution artifacts, can effectively deobfuscate programs and restore analyzability. We construct a benchmark by applying four widely studied transformations-control-flow flattening, opaque predicates, arithmetic encoding, and branch encoding-across diverse C programs from TUM Obfuscation Benchmarks, the LLVM test suite, and algorithmic repositories. We then compare three state-of-the-art LLMs under two training configurations: baseline fine-tuning on obfuscated/original code pairs, and enhanced fine-tuning with additional KLEE artifacts such as SMT constraints, path statistics, and test cases. Our evaluation examines syntactic correctness (compilation success), semantic fidelity (behavioral equivalence under symbolic execution), and code quality (readability and structure). Results show that GPT-4.1-mini achieves the strongest deobfuscation overall, and that incorporating KLEE artifacts consistently improves semantic preservation and compilation success across models. These findings highlight deobfuscation as a broader software engineering concern, demonstrating that combining LLMs with symbolic execution can strengthen automated testing, static analysis, and program comprehension in the presence of obfuscation.", "arxiv_url": "https://arxiv.org/abs/2511.19130", "authors": ["Rong Feng", "Suman Saha"], "first_author": "Rong Feng", "category": ["Technical", "Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Understanding", "tags": ["Deobfuscation", "Symbolic execution artifacts", "SMT constraints", "Path statistics", "Test-case guided fine-tuning", "Obfuscation transformations", "Semantic equivalence evaluation", "Compilation-aware deobfuscation"], "summary": "本文构建了一个包含四类混淆变换的去混淆基准，并系统评估了将符号执行产物（如SMT约束、路径统计、测试用例）融入LLM微调以提升生成代码的可编译性与语义保真性的效果，结果表明混合方法显著优于仅用代码的微调。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.19130v1", "published": "2025-11-24", "update_time": "2025-11-24", "download_time": "2025-12-16 14:18:32"}
{"id": "2511.20403", "title": "LLMs for Automated Unit Test Generation and Assessment in Java: The AgoneTest Framework", "abstract": "Unit testing is an essential but resource-intensive step in software development, ensuring individual code units function correctly. This paper introduces AgoneTest, an automated evaluation framework for Large Language Model-generated (LLM) unit tests in Java. AgoneTest does not aim to propose a novel test generation algorithm; rather, it supports researchers and developers in comparing different LLMs and prompting strategies through a standardized end-to-end evaluation pipeline under realistic conditions. We introduce the Classes2Test dataset, which maps Java classes under test to their corresponding test classes, and a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment. Experimental results show that, for the subset of tests that compile, LLM-generated tests can match or exceed human-written tests in terms of coverage and defect detection. Our findings also demonstrate that enhanced prompting strategies contribute to test quality. AgoneTest clarifies the potential of LLMs in software testing and offers insights for future improvements in model design, prompt engineering, and testing practices.", "arxiv_url": "https://arxiv.org/abs/2511.20403", "authors": ["Andrea Lops", "Fedelucio Narducci", "Azzurra Ragone", "Michelantonio Trizio", "Claudio Bartolini"], "first_author": "Andrea Lops", "category": ["Technical", "Benchmark", "Empirical"], "field": "Software Testing", "task": "Test Generation", "tags": ["End-to-end evaluation pipeline", "Class-level unit test generation", "Annotated class-to-test mappings", "Automated project setup and build integration", "Mutation score integration", "Test smells detection", "Prompt engineering comparison", "Comparative LLM test compilability and coverage"], "summary": "本文提出AGONETEST——一个用于评估LLM生成的Java单元测试的端到端自动化框架，并发布了面向类级测试的注释数据集，结合覆盖率、变异测试与测试气味等指标比较不同模型和提示策略的性能。", "quality": "High", "conference": "IEEE/ACM International Conference on Automated Software Engineering (ASE 2025)", "pdf_url": "https://arxiv.org/pdf/2511.20403v2", "published": "2025-11-25", "update_time": "2025-11-26", "download_time": "2025-12-16 14:19:46"}
{"id": "2511.21382", "title": "Large Language Models for Unit Test Generation: Achievements, Challenges, and the Road Ahead", "abstract": "Unit testing is an essential yet laborious technique for verifying software and mitigating regression risks. Although classic automated methods effectively explore program structures, they often lack the semantic information required to produce realistic inputs and assertions. Large Language Models (LLMs) address this limitation by utilizing by leveraging their data-driven knowledge of code semantics and programming patterns. To analyze the state of the art in this domain, we conducted a systematic literature review of 115 publications published between May 2021 and August 2025. We propose a unified taxonomy based on the unit test generation lifecycle that treats LLMs as stochastic generators requiring systematic engineering constraints. This framework analyzes the literature regarding core generative strategies and a set of enhancement techniques ranging from pre-generation context enrichment to post-generation quality assurance. Our analysis reveals that prompt engineering has emerged as the dominant utilization strategy and accounts for 89% of the studies due to its flexibility. We find that iterative validation and repair loops have become the standard mechanism to ensure robust usability and lead to significant improvements in compilation and execution pass rates. However, critical challenges remain regarding the weak fault detection capabilities of generated tests and the lack of standardized evaluation benchmarks. We conclude with a roadmap for future research that emphasizes the progression towards autonomous testing agents and hybrid systems combining LLMs with traditional software engineering tools. This survey provides researchers and practitioners with a comprehensive perspective on converting the potential of LLMs into industrial-grade testing solutions.", "arxiv_url": "https://arxiv.org/abs/2511.21382", "authors": ["Bei Chu", "Yang Feng", "Kui Liu", "Zifan Nan", "Zhaoqiang Guo", "Baowen Xu"], "first_author": "Bei Chu", "category": ["Survey"], "field": "Software Testing", "task": "Test Generation", "tags": ["Prompt engineering for test generation", "Context enrichment via program analysis", "Iterative validation and repair loops", "Mocking and external dependency synthesis", "Usability vs. fault-detection tradeoff", "Autonomous testing agents", "Hybrid LLM–classical tooling integration", "Lack of standardized evaluation benchmarks"], "summary": "本文对2021–2025年间115篇关于使用大型语言模型生成单元测试的工作进行了系统综述，提出基于测试生成生命周期的统一分类，概述了以提示工程为主的生成策略、上下文增强与后处理修复技术，指出了生成测试可执行性虽有提升但故障检测能力薄弱与评估基准缺失等关键挑战，并给出未来研究路线图。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.21382v1", "published": "2025-11-26", "update_time": "2025-11-26", "download_time": "2025-12-16 14:21:09"}
{"id": "2511.21380", "title": "Multi-Agent Systems for Dataset Adaptation in Software Engineering: Capabilities, Limitations, and Future Directions", "abstract": "Automating the adaptation of software engineering (SE) research artifacts across datasets is essential for scalability and reproducibility, yet it remains largely unstudied. Recent advances in large language model (LLM)-based multi-agent systems, such as GitHub Copilot's agent mode, promise to automate complex development workflows through coordinated reasoning, code generation, and tool interaction. This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks. We evaluate Copilot, backed by GPT-4.1 and Claude Sonnet 4, on adapting SE research artifacts from benchmark repositories including ROCODE and LogHub2.0. Through a five-stage evaluation pipeline (file comprehension, code editing, command generation, validation, and final execution), we measure success rates, analyze failure patterns, and assess prompt-based interventions designed to enhance agent performance. Results show that current systems can identify key files and generate partial adaptations but rarely produce functionally correct implementations. Prompt-level interventions, especially providing execution error messages and reference code, substantially improve structural similarity to ground truth (from 7.25% to 67.14%), highlighting the importance of contextual and feedback-driven guidance. Our findings reveal both the promise and limitations of today's multi-agent LLM systems for dataset adaptation, and suggest concrete directions for building more reliable, self-correcting agents in future SE research.", "arxiv_url": "https://arxiv.org/abs/2511.21380", "authors": ["Jingyi Chen", "Xiaoyan Guo", "Songqiang Chen", "Shing-Chi Cheung", "Jiasi Shen"], "first_author": "Jingyi Chen", "category": ["Empirical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Multi-agent orchestration", "Dataset adaptation automation", "Repository-level file comprehension", "Automated command generation and execution", "Prompt-based intervention and feedback", "Failure-mode analysis", "Structural-similarity evaluation"], "summary": "本文首度通过五阶段评估流水线，实证研究基于多智能体的大语言模型系统在软件工程研究工件的数据集适配任务中的表现、失败模式及提示干预效果，发现系统能生成部分结构正确的改动但很少能直接产出功能正确的实现，且带有执行错误信息与参考代码的提示能显著提升结构相似度。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.21380v1", "published": "2025-11-26", "update_time": "2025-11-26", "download_time": "2025-12-16 14:21:41"}
{"id": "2511.21022", "title": "Lightweight Model Editing for LLMs to Correct Deprecated API Recommendations", "abstract": "Pre-trained or fine-tuned on large code corpora, Large Language Models (LLMs) have demonstrated strong performance in code completion tasks. However, their embedded knowledge is constrained by the timeliness of training data, which often includes code using deprecated APIs. Consequently, LLMs frequently generate deprecated APIs that will no longer be supported in future versions of third-party libraries. While retraining LLMs on updated codebases could refresh their API knowledge, this approach is computationally expensive. Recently, lightweight model editing methods have emerged to efficiently correct specific knowledge in LLMs. However, it remains unclear whether these methods can effectively update deprecated API knowledge and enable edited models to generate up-to-date APIs. To address this gap, we conduct the first systematic study applying 10 state-of-the-art model editing techniques to update deprecated API knowledge in three LLMs: Qwen2.5-Coder, StarCoder2, and DeepSeek-Coder. We introduce EDAPIBench, a dedicated benchmark featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances. Our results show that the parameter-efficient fine-tuning method AdaLoRA achieves the best performance in enabling edited models to generate correct, up-to-date APIs, but falls short in Specificity (i.e., the editing influences untargeted knowledge). To resolve this, we propose AdaLoRA-L, which defines \"Common API Layers\" (layers within the LLMs with high importance across all APIs, storing general knowledge and excluded from editing) and restricts edits exclusively to \"Specific API Layers\" (layers with high importance only for the target API, storing the API-specific knowledge). Experimental results demonstrate that AdaLoRA-L significantly improves Specificity while maintaining comparable performance across other evaluation metrics.", "arxiv_url": "https://arxiv.org/abs/2511.21022", "authors": ["Guancheng Lin", "Xiao Yu", "Jacky Keung", "Xing Hu", "Xin Xia", "Alex X. Liu"], "first_author": "Guancheng Lin", "category": ["Technical", "Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Alignment", "tags": ["Deprecated API knowledge editing", "Automated benchmark construction from API mappings", "Parameter-efficient fine-tuning (low-rank adapters)", "Gradient-based layer importance scoring", "Selective layer editing (Common vs Specific layers)", "Specificity-aware editing metrics", "Evaluation on real-world GitHub function contexts"], "summary": "本文构建了首个用于评估过时API知识编辑的基准并提出了一种基于梯度层重要性限制编辑范围的轻量模型编辑方法，以在保持效果与泛化能力的同时显著提升编辑的特异性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.21022v1", "published": "2025-11-26", "update_time": "2025-11-26", "download_time": "2025-12-16 14:22:33"}
{"id": "2511.20933", "title": "Hierarchical Evaluation of Software Design Capabilities of Large Language Models of Code", "abstract": "Large language models (LLMs) are being increasingly adopted in the software engineering domain, yet the robustness of their grasp on core software design concepts remains unclear. We conduct an empirical study to systematically evaluate their understanding of cohesion (intra-module) and coupling (inter-module). We programmatically generate poorly designed code fragments and test the DeepSeek-R1 model family ($14$B, $32$B, $70$B) under varying levels of guidance, from simple \\textit{Verification} to \\textit{Guided} and \\textit{Open-ended Generation}, while varying contextual noise by injecting distractor elements. While models exhibit a solid baseline understanding of both concepts in ideal conditions, their practical knowledge is fragile and highly asymmetrical. Reasoning about coupling proves brittle; performance collapses in noisy, open-ended scenarios, with F1 scores dropping by over $50\\%$. In contrast, the models' analysis of cohesion is remarkably robust to internal noise in guided tasks, showing little performance degradation. However, this resilience also fails when all guidance is removed. Reasoning-trace analysis confirms these failure modes, revealing \\textit{cognitive shortcutting} for coupling versus a more exhaustive (yet still failing) analysis for cohesion. To summarize, while LLMs can provide reliable assistance for recognizing design flaws, their ability to reason autonomously in noisy, realistic contexts is limited, highlighting the critical need for more scalable and robust program understanding capabilities.", "arxiv_url": "https://arxiv.org/abs/2511.20933", "authors": ["Mootez Saad", "Boqi Chen", "José Antonio Hernández López", "Dániel Varró", "Tushar Sharma"], "first_author": "Mootez Saad", "category": ["Empirical", "Benchmark"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Cohesion evaluation", "Coupling evaluation", "Synthetic code generation for design flaws", "Controlled distractor/noise injection", "Hierarchical prompting (verification/guided/open-ended)", "Chain-of-Thought trace analysis", "Robustness to contextual noise", "Design-quality benchmarking"], "summary": "本文提出了一个可控的基准与评估框架，通过生成带内聚性与耦合性缺陷的代码并注入干扰，系统实证评估大规模代码模型在不同提示层级下对软件设计原则的识别与推理能力，揭示了对耦合推理的高度脆弱性与对内聚分析的相对鲁棒性及其失败机制。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.20933v1", "published": "2025-11-25", "update_time": "2025-11-25", "download_time": "2025-12-16 14:23:03"}
{"id": "2511.21197", "title": "Bug Detective and Quality Coach: Developers' Mental Models of AI-Assisted IDE Tools", "abstract": "AI-assisted tools support developers in performing cognitively demanding tasks such as bug detection and code readability assessment. Despite the advancements in the technical characteristics of these tools, little is known about how developers mentally model them and how mismatches affect trust, control, and adoption. We conducted six co-design workshops with 58 developers to elicit their mental models about AI-assisted bug detection and readability features. It emerged that developers conceive bug detection tools as \\textit{bug detectives}, which warn users only in case of critical issues, guaranteeing transparency, actionable feedback, and confidence cues. Readability assessment tools, on the other hand, are envisioned as \\textit{quality coaches}, which provide contextual, personalized, and progressive guidance. Trust, in both tasks, depends on the clarity of explanations, timing, and user control. A set of design principles for Human-Centered AI in IDEs has been distilled, aiming to balance disruption with support, conciseness with depth, and automation with human agency.", "arxiv_url": "https://arxiv.org/abs/2511.21197", "authors": ["Paolo Buono", "Mary Cerullo", "Stefano Cirillo", "Giuseppe Desolda", "Francesco Greco", "Emanuela Guglielmi", "Grazia Margarella", "Giuseppe Polese", "Simone Scalabrino", "Cesare Tucci"], "first_author": "Paolo Buono", "category": ["Empirical"], "field": "Requirements & Design", "task": "Elicitation", "tags": ["Developer Mental Models", "Co-design Workshops", "Bug Detective (metaphor)", "Quality Coach (metaphor)", "Design Principles for Human-Centered IDEs", "Explanation Transparency", "User Control and Timing", "Actionable Contextual Feedback", "Personalized Readability Guidance"], "summary": "本文通过对58名开发者开展六次共创研讨会，探讨了AI辅助IDE中错误检测与代码可读性评估的心智模型，提出“Bug Detective”和“Quality Coach”两种核心模型并归纳了以透明性、可操作性、时机与用户控制为核心的设计原则。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.21197v1", "published": "2025-11-26", "update_time": "2025-11-26", "download_time": "2025-12-16 14:23:51"}
{"id": "2511.19875", "title": "CodeFuse-CommitEval: Towards Benchmarking LLM's Power on Commit Message and Code Change Inconsistency Detection", "abstract": "Version control relies on commit messages to convey the rationale for code changes, but these messages are often low quality and, more critically, inconsistent with their diffs-known as message-code inconsistency (MCI). MCIs mislead reviewers, hinder maintenance, contaminate research datasets, and may obscure security patches. Yet, no dedicated benchmark exists to evaluate models for MCI detection. We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models (LLMs). Built on the ApacheCM dataset for diversity and quality, we generate seven types of inconsistent messages through rule-guided mutations of originally consistent commits and apply two-fold validation to verify both positive and negative samples. Using this labeled dataset of message-diff pairs, we evaluate six state-of-the-art open-source LLMs under a vanilla setting and with three augmentation strategies: few-shot prompting, chain-of-thought, and extended context. Results show models detect inconsistent commits more reliably than consistent ones (average Recall 85.95%, Precision 80.28%, Specificity 63.8%); gpt-oss-20B performs best overall but uses over twice the tokens of others. Augmentation effects vary: adjacent context helps larger models but adds noise for smaller ones; few-shot improves accuracy and reduces token use, yet increases universally incorrect predictions; chain-of-thought boosts precision and specificity at the cost of recall and higher token consumption. Type-wise analysis reveals higher detectability for component, file-path, and operation inconsistencies, but lower accuracy and higher token cost for intent-level \"purpose\" inconsistencies. CODEFUSE-COMMITEVAL provides a rigorous foundation for measuring, comparing, and advancing MCI detection, highlighting the need for richer context and balanced data to capture high-level semantic gaps.", "arxiv_url": "https://arxiv.org/abs/2511.19875", "authors": ["Qingyu Zhang", "Puzhuo Liu", "Peng Di", "Chenxiong Qian"], "first_author": "Qingyu Zhang", "category": ["Benchmark", "Empirical"], "field": "Version Control & Collaboration", "task": "Git VCS", "tags": ["Message-Code Inconsistency Detection", "Synthetic Mutation Rules", "Commit-Diff Pair Labeling", "Two-fold Validation", "Prompting Augmentations (few-shot, CoT, extended context)", "Context Window Sensitivity", "Inconsistency-Type Analysis", "Token Consumption Trade-offs"], "summary": "本文提出了CodeFuse-CommitEval基准，通过对高质量提交进行七类规则化变异并经双重验证构建标签化的提交消息—代码差异对，进而评估不同开源大模型及多种提示增强策略在检测提交消息与代码不一致（MCI）任务上的性能与类型差异。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.19875v1", "published": "2025-11-25", "update_time": "2025-11-25", "download_time": "2025-12-16 14:24:25"}
{"id": "2511.20709", "title": "DUALGUAGE: Automated Joint Security-Functionality Benchmarking for Secure Code Generation", "abstract": "Large language models (LLMs) and autonomous coding agents are increasingly used to generate software across a wide range of domains. Yet a core requirement remains unmet: ensuring that generated code is secure without compromising its functional correctness. Existing benchmarks and evaluations for secure code generation fall short-many measure only vulnerability reduction, disregard correctness preservation, or evaluate security and functionality on separate datasets, violating the fundamental need for simultaneous joint evaluation. We present DUALGAUGE, the first fully automated benchmarking framework designed to rigorously evaluate the security and correctness of LLM-generated code in unison. Given the lack of datasets enabling joint evaluation of secure code generation, we also present DUALGAUGE-BENCH, a curated benchmark suite of diverse coding tasks, each paired with manually validated test suites for both security and functionality, designed for full coverage of specification requirements. At the core of DUALGAUGE is an agentic program executor, which runs a program against given tests in sandboxed environments, and an LLM-based evaluator, which assesses both correctness and vulnerability behavior against expected outcomes. We rigorously evaluated and ensured the quality of DUALGAUGE-BENCH and the accuracy of DUALGAUGE, and applied DUALGAUGE to benchmarking ten leading LLMs on DUALGAUGE-BENCH across thousands of test scenarios. Our results reveal critical gaps in correct and secure code generation by these LLMs, for which our open-source system and datasets help accelerate progress via reproducible, scalable, and rigorous evaluation.", "arxiv_url": "https://arxiv.org/abs/2511.20709", "authors": ["Abhijeet Pathak", "Suvadra Barua", "Dinesh Gudimetla", "Rupam Patir", "Jiawei Guo", "Hongxin Hu", "Haipeng Cai"], "first_author": "Abhijeet Pathak", "category": ["Technical", "Benchmark", "Empirical"], "field": "Software Testing", "task": "Testing automation", "tags": ["Joint security–functionality benchmarking", "Runtime exploitability/security tests", "Agentic execution and dependency resolution", "LLM-based semantic test evaluator", "Coverage-enforced test suites", "Human-and-LLM co-creation of tests", "Sandboxed program execution"], "summary": "本文提出DUALGAUGE及DUALGAUGE-BENCH：一个自动化的基准与系统，通过沙箱执行、覆盖驱动的功能/安全测试及LLM评估器对生成代码进行联合的功能正确性与安全性评估，并在大规模模型上揭示多项安全-功能权衡与失败模式。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.20709v1", "published": "2025-11-24", "update_time": "2025-11-24", "download_time": "2025-12-16 14:25:05"}
{"id": "2511.19635", "title": "Agint: Agentic Graph Compilation for Software Engineering Agents", "abstract": "LLM-based coding agents are increasingly common but still face challenges in context management, latency, reliability, reproducibility, and scalability. We present Agint, an agentic graph compiler, interpreter, and runtime that incrementally and hierarchically converts natural-language instructions into typed, effect-aware code DAGs. Agint introduces explicit type floors (text to data to spec to code) grounded in semantic graph transformations and a hybrid LLM and function-based JIT runtime. This enables dynamic graph refinement, reproducible and optimizable execution, speculative evaluation, and interoperability with existing developer tools. Agint's typed graph bindings improve reliability and allow concurrent composition of concurrent codebases by construction, supporting accelerated development with smaller and faster models, lower latency, efficient context utilization, and higher throughput. Hierarchical compilation allows scalable graph edits, while the graph structure supports reproducibility and efficient parallel generation. Agint provides a composable unix-style toolchain: dagify (DAG compiler), dagent (hybrid JIT runtime), schemagin (schema generator), and datagin (data transformer) for realtime, low-latency code and dataflow creation. Human developers and coding agents refine graphs through the Agint CLI, while non-technical users use Agint Flow GUI for visual editing, conversational refinement, and debugging to promote prototype agentic workflows to production code. This continuous co-creation model allows teams to prototype quickly, refine seamlessly, and deploy reliably, bridging natural language, compiler methods, and developer tooling to enable a new generation of composable, team-centric coding agents at scale.", "arxiv_url": "https://arxiv.org/abs/2511.19635", "authors": ["Abhi Chivukula", "Jay Somasundaram", "Vijay Somasundaram"], "first_author": "Abhi Chivukula", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Prompting", "tags": ["Agentic Graph Compilation", "Typed Intermediate Representations", "Effect-aware DAGs", "Hybrid LLM/function JIT Runtime", "Speculative Parallel Execution", "Virtual Shim / Virtual Function Abstraction", "Locality-preserving Graph Transformations", "Incremental Resolution and Refinement", "Human-in-the-loop Visual Editing"], "summary": "本文提出Agint，一种将自然语言意图编译为带类型与副作用感知的可执行DAG的编译器、解释器与运行时，通过分层类型地板、混合LLM/函数JIT、并行与推测执行实现低延迟、可复现且可组合的软件工程代理流水线。", "quality": "High", "conference": "NeurIPS (Deep Learning for Code workshop) 2025", "pdf_url": "https://arxiv.org/pdf/2511.19635v1", "published": "2025-11-24", "update_time": "2025-11-24", "download_time": "2025-12-16 14:26:11"}
{"id": "2511.23408", "title": "Evaluating LLMs for One-Shot Patching of Real and Artificial Vulnerabilities", "abstract": "Automated vulnerability patching is crucial for software security, and recent advancements in Large Language Models (LLMs) present promising capabilities for automating this task. However, existing research has primarily assessed LLMs using publicly disclosed vulnerabilities, leaving their effectiveness on related artificial vulnerabilities largely unexplored. In this study, we empirically evaluate the patching effectiveness and complementarity of several prominent LLMs, such as OpenAI's GPT variants, LLaMA, DeepSeek, and Mistral models, using both real and artificial vulnerabilities. Our evaluation employs Proof-of-Vulnerability (PoV) test execution to concretely assess whether LLM-generated source code successfully patches vulnerabilities. Our results reveal that LLMs patch real vulnerabilities more effectively compared to artificial ones. Additionally, our analysis reveals significant variability across LLMs in terms of overlapping (multiple LLMs patching the same vulnerabilities) and complementarity (vulnerabilities patched exclusively by a single LLM), emphasizing the importance of selecting appropriate LLMs for effective vulnerability patching.", "arxiv_url": "https://arxiv.org/abs/2511.23408", "authors": ["Aayush Garg", "Zanis Ali Khan", "Renzo Degiovanni", "Qiang Tang"], "first_author": "Aayush Garg", "category": ["Empirical"], "field": "Quality Management", "task": "Vulnerability Repair", "tags": ["One-shot vulnerability patching", "Proof-of-Vulnerability test execution", "Mutation-derived artificial vulnerabilities", "Patch overlap and complementarity analysis", "LLM comparative evaluation across architectures", "Execution-based patch validation", "Java/CWE vulnerability cases", "Uniform prompting strategy"], "summary": "本文通过对多款大型语言模型在15个真实Java漏洞及其41个人工构造变体上的一次性补丁生成进行实证评估，使用PoV测试执行验证补丁有效性并分析模型间的重叠与互补性，发现模型对真实漏洞的修补率普遍高于人工漏洞且不同模型在可修补漏洞上存在显著差异。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.23408v1", "published": "2025-11-28", "update_time": "2025-11-28", "download_time": "2025-12-16 14:26:58"}
{"id": "2511.23321", "title": "Chart2Code-MoLA: Efficient Multi-Modal Code Generation via Adaptive Expert Routing", "abstract": "Chart-to-code generation is a critical task in automated data visualization, translating complex chart structures into executable programs. While recent Multi-modal Large Language Models (MLLMs) improve chart representation, existing approaches still struggle to achieve cross-type generalization, memory efficiency, and modular design. To address these challenges, this paper proposes C2C-MoLA, a multimodal framework that synergizes Mixture of Experts (MoE) with Low-Rank Adaptation (LoRA). The MoE component uses a complexity-aware routing mechanism with domain-specialized experts and load-balanced sparse gating, dynamically allocating inputs based on learnable structural metrics like element count and chart complexity. LoRA enables parameter-efficient updates for resource-conscious tuning, further supported by a tailored training strategy that aligns routing stability with semantic accuracy. Experiments on Chart2Code-160k show that the proposed model improves generation accuracy by up to 17%, reduces peak GPU memory by 18%, and accelerates convergence by 20%, when compared to standard fine-tuning and LoRA-only baselines, particularly on complex charts. Ablation studies validate optimal designs, such as 8 experts and rank-8 LoRA, and confirm scalability for real-world multimodal code generation.", "arxiv_url": "https://arxiv.org/abs/2511.23321", "authors": ["Yifei Wang", "Jacky Keung", "Zhenyu Mao", "Jingyu Zhang", "Yuchen Cao"], "first_author": "Yifei Wang", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Mixture-of-Experts", "Complexity-aware routing", "Low-Rank Adaptation", "Multimodal chart-to-code generation", "Structure-aware sparse gating", "Load-balancing regularization", "Memory-efficient training", "Syntax-and-semantics reconstruction loss", "Dual-stream visual encoder"], "summary": "本文提出 C2C-MoLA，将可学习的结构复杂度引导的专家混合（MoE）与低秩适配（LoRA）结合，用于高效的多模态图表到可执行代码生成，在Chart2Code-160k上显著提高准确率、降低显存占用并加速收敛。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.23321v1", "published": "2025-11-28", "update_time": "2025-11-28", "download_time": "2025-12-16 14:27:31"}
{"id": "2401.12554", "title": "Can Large Language Models Write Parallel Code?", "abstract": "Large language models are increasingly becoming a popular tool for software development. Their ability to model and generate source code has been demonstrated in a variety of contexts, including code completion, summarization, translation, and lookup. However, they often struggle to generate code for complex programs. In this paper, we study the capabilities of state-of-the-art language models to generate parallel code. In order to evaluate language models, we create a benchmark, ParEval, consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing. We use ParEval to evaluate the effectiveness of several state-of-the-art open- and closed-source language models on these tasks. We introduce novel metrics for evaluating the performance of generated code, and use them to explore how well each large language model performs for 12 different computational problem types and six different parallel programming models.", "arxiv_url": "https://arxiv.org/abs/2401.12554", "authors": ["Daniel Nichols", "Joshua H. Davis", "Zhaojun Xie", "Arjun Rajaram", "Abhinav Bhatele"], "first_author": "Daniel Nichols", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Parallel code generation", "HPC benchmarking", "Execution-model translation", "Performance and scaling metrics", "Shared- and distributed-memory models", "Automated compile-and-run evaluation", "Sparse/unstructured problem difficulties"], "summary": "本文提出了ParEval基准并引入新的性能与可扩展性评估指标，用以自动化评测多种大型语言模型生成和翻译并行/HPC代码的正确性、性能与伸缩性。", "quality": "High", "conference": "The 33rd International Symposium on High-Performance Parallel and Distributed Computing (HPDC '24) 2024", "pdf_url": "https://arxiv.org/pdf/2401.12554v3", "published": "2024-01-23", "update_time": "2024-05-14", "download_time": "2025-12-16 14:28:06"}
{"id": "2401.01062", "title": "Experimenting a New Programming Practice with LLMs", "abstract": "The recent development on large language models makes automatically constructing small programs possible. It thus has the potential to free software engineers from low-level coding and allow us to focus on the perhaps more interesting parts of software development, such as requirement engineering and system testing. In this project, we develop a prototype named AISD (AI-aided Software Development), which is capable of taking high-level (potentially vague) user requirements as inputs, generates detailed use cases, prototype system designs, and subsequently system implementation. Different from existing attempts, AISD is designed to keep the user in the loop, i.e., by repeatedly taking user feedback on use cases, high-level system designs, and prototype implementations through system testing. AISD has been evaluated with a novel benchmark of non-trivial software projects. The experimental results suggest that it might be possible to imagine a future where software engineering is reduced to requirement engineering and system testing only.", "arxiv_url": "https://arxiv.org/abs/2401.01062", "authors": ["Simiao Zhang", "Jiaping Wang", "Guoliang Dong", "Jun Sun", "Yueling Zhang", "Geguang Pu"], "first_author": "Simiao Zhang", "category": ["Technical", "Benchmark"], "field": "Requirements & Design", "task": "Elicitation", "tags": ["Human-in-the-loop requirement elicitation", "Use-case centric requirement generation", "LLM-based multi-agent orchestration", "Iterative prototype generation and refinement", "System-level automated code synthesis", "Test-driven validation and feedback loop", "System-level evaluation benchmark", "Token-efficient interaction"], "summary": "本文提出AISD——一个以用户为中心的LLM驱动软件开发框架，通过交互式需求获取、生成简洁高层设计并自动分解实现原型，结合系统级测试和用户反馈迭代改进，同时构建了用于评估此类系统的系统级基准以验证效果。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2401.01062v1", "published": "2024-01-02", "update_time": "2024-01-02", "download_time": "2025-12-16 14:28:39"}
{"id": "2512.01010", "title": "Chain of Unit-Physics: A Primitive-Centric Approach to Scientific Code Synthesis", "abstract": "Agentic large language models are proposed as autonomous code generators for scientific computing, yet their reliability in high-stakes problems remains unclear. Developing computational scientific software from natural-language queries remains challenging broadly due to (a) sparse representation of domain codes during training and (b) the limited feasibility of RLHF with a small expert community. To address these limitations, this work conceptualizes an inverse approach to code design, embodied in the Chain of Unit-Physics framework: a first-principles (or primitives)-centric, multi-agent system in which human expert knowledge is encoded as unit-physics tests that explicitly constrain code generation. The framework is evaluated on a nontrivial combustion task, used here as a representative benchmark for scientific problem with realistic physical constraints. Closed-weight systems and code-focused agentic variants fail to produce correct end-to-end solvers, despite tool and web access, exhibiting four recurrent error classes: interface (syntax/API) hallucinations, overconfident assumptions, numerical/physical incoherence, and configuration fragility. Open-weight models with chain-of-thought (CoT) decoding reduce interface errors but still yield incorrect solutions. On the benchmark task, the proposed framework converges within 5-6 iterations, matches the human-expert implementation (mean error of $3.1\\times10^{-3}$ %), with a $\\sim$33.4 % faster runtime and a $\\sim$30 % efficient memory usage at a cost comparable to mid-sized commercial APIs, yielding a practical template for physics-grounded scientific code generation. As datasets and models evolve, zero-shot code accuracy will improve; however, the Chain of Unit-Physics framework goes further by embedding first-principles analysis that is foundational to scientific codes.", "arxiv_url": "https://arxiv.org/abs/2512.01010", "authors": ["Vansh Sharma", "Venkat Raman"], "first_author": "Vansh Sharma", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Unit-physics tests", "Test-driven scientific code synthesis", "Multi-agent orchestration for code generation", "Branching top-k decoding with confidence-margin scoring", "Sandboxed execution and diagnostic agent", "Physics-grounded verification and numerical consistency checks", "Combustion solver case study", "Iterative human-in-the-loop verification"], "summary": "本文提出 Chain of Unit-Physics 框架：通过人类专家编写的基于物理第一性原理的“单元物理”测试、分支解码与多智能体（诊断/验证）流水线，引导并验证科学计算代码生成，在一个真实的燃烧求解器任务中收敛到接近人工实现的高精度解并提升运行与内存效率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.01010v1", "published": "2025-11-30", "update_time": "2025-11-30", "download_time": "2025-12-16 14:29:30"}
{"id": "2512.00867", "title": "The AI Attribution Paradox: Transparency as Social Strategy in Open-Source Software Development", "abstract": "AI coding assistants have transformed software development, raising questions about transparency and attribution practices. We examine the \"AI attribution paradox\": how developers strategically balance acknowledging AI assistance with managing community scrutiny. Analyzing 14,300 GitHub commits across 7,393 repositories from 2023-2025, we investigated attribution strategies and community responses across eight major AI tools. Results reveal widespread AI usage (95.2% of commits) but strategic attribution: only 29.5% employ explicit disclosure, with dramatic tool variation (Claude 80.5% versus Copilot 9.0%). Explicit attribution triggers modest scrutiny (23% more questions and 21% more comments) but tool choice matters 20-30 times more for predicting reception. Community sentiment remains neutral regardless of attribution type, suggesting curiosity rather than hostility. Temporal analyses show rapid norm evolution: explicit attribution increased from near-zero in early 2024 to 40% by late 2025, indicating community adaptation. These findings illuminate attribution as strategic communication rather than simple transparency, advancing understanding of algorithmic accountability and norm formation during technological transitions. We discuss implications for developers navigating disclosure decisions, platforms designing attribution mechanisms, and researchers studying emergent practices in AI-augmented collaborative work.", "arxiv_url": "https://arxiv.org/abs/2512.00867", "authors": ["Obada Kraishan"], "first_author": "Obada Kraishan", "category": ["Empirical"], "field": "Version Control & Collaboration", "task": "Git VCS", "tags": ["AI attribution practices", "Commit-message disclosure", "Impression management", "Tool-specific attribution norms", "Community engagement metrics", "Temporal norm evolution", "Emoji and comment sentiment analysis", "Open-source governance"], "summary": "本文通过对2023–2025年14,300次GitHub提交的实证分析，揭示了“AI归属悖论”——开发者在公开源码中对AI协助的披露具有策略性（显式署名比例较低且随工具差异显著），显式披露会带来略增的审查但无明显负面情绪，且披露规范在短期内快速演进。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.00867v1", "published": "2025-11-30", "update_time": "2025-11-30", "download_time": "2025-12-16 14:30:03"}
{"id": "2512.01939", "title": "An Empirical Study of Agent Developer Practices in AI Agent Frameworks", "abstract": "The rise of large language models (LLMs) has sparked a surge of interest in agents, leading to the rapid growth of agent frameworks. Agent frameworks are software toolkits and libraries that provide standardized components, abstractions, and orchestration mechanisms to simplify agent development. Despite widespread use of agent frameworks, their practical applications and how they influence the agent development process remain underexplored. Different agent frameworks encounter similar problems during use, indicating that these recurring issues deserve greater attention and call for further improvements in agent framework design. Meanwhile, as the number of agent frameworks continues to grow and evolve, more than 80% of developers report difficulties in identifying the frameworks that best meet their specific development requirements. In this paper, we conduct the first empirical study of LLM-based agent frameworks, exploring real-world experiences of developers in building AI agents. To compare how well the agent frameworks meet developer needs, we further collect developer discussions for the ten previously identified agent frameworks, resulting in a total of 11,910 discussions. Finally, by analyzing these discussions, we compare the frameworks across five dimensions: development efficiency, functional abstraction, learning cost, performance optimization, and maintainability, which refers to how easily developers can update and extend both the framework itself and the agents built upon it over time. Our comparative analysis reveals significant differences among frameworks in how they meet the needs of agent developers. Overall, we provide a set of findings and implications for the LLM-driven AI agent framework ecosystem and offer insights for the design of future LLM-based agent frameworks and agent developers.", "arxiv_url": "https://arxiv.org/abs/2512.01939", "authors": ["Yanlin Wang", "Xinyi Xu", "Jiachi Chen", "Tingting Bi", "Wenchao Gu", "Zibin Zheng"], "first_author": "Yanlin Wang", "category": ["Empirical"], "field": "Agent Frameworks & Developer Practices", "task": "Developer practices and SDLC challenges in LLM-based agent frameworks", "tags": ["Mining GitHub discussions", "Agent framework adoption patterns", "SDLC challenges taxonomy", "Logic control & task termination", "Tool integration and API compatibility", "Context retention and memory management", "Version and dependency compatibility", "Comparative framework evaluation (learning cost, efficiency, abstraction, performance, maintainability)", "Multi-framework composition"], "summary": "本文通过分析1,575个基于LLM的代理项目及近1.2万条开发者讨论，构建了代理开发面向SDLC的挑战分类、识别十个代表性框架并基于五维度比较其对开发者需求的满足情况，揭示常见问题并提出改进建议。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.01939v1", "published": "2025-12-01", "update_time": "2025-12-01", "download_time": "2025-12-16 14:30:54"}
{"id": "2512.01690", "title": "Generating REST API Tests With Descriptive Names", "abstract": "Automated test generation has become a key technique for ensuring software quality, particularly in modern API-based architectures. However, automatically generated test cases are typically assigned non-descriptive names (e.g., test0, test1), which reduces their readability and hinders their usefulness during comprehension and maintenance. In this work, we present three novel deterministic techniques to generate REST API test names. We then compare eight techniques in total for generating descriptive names for REST API tests automatically produced by the fuzzer EvoMaster, using 10 test cases generated for 9 different open-source APIs. The eight techniques include rule-based heuristics and large language model (LLM)-based approaches. Their effectiveness was empirically evaluated through two surveys (involving up to 39 people recruited via LinkedIn). Our results show that a rule-based approach achieves the highest clarity ratings among deterministic methods, performs on par with state-of-the-art LLM-based models such as Gemini and GPT-4o, and significantly outperforms GPT-3.5.   To further evaluate the practical impact of our results, an industrial case study was carried out with practitioners who actively use EvoMaster at Volkswagen AG. A developer questionnaire was then carried out based on the use of EvoMaster on four different APIs by four different users, for a total of 74 evaluated test cases. Feedback from practitioners further confirms that descriptive names produced by this approach improve test suite readability.   These findings highlight that lightweight, deterministic techniques can serve as effective alternatives to computationally expensive and security-sensitive LLM-based approaches for automated system-level test naming, providing a practical step toward more developer-friendly API test generation.", "arxiv_url": "https://arxiv.org/abs/2512.01690", "authors": ["Philip Garrett", "Juan P. Galeotti", "Andrea Arcuri", "Alexander Poth", "Olsi Rrjolli"], "first_author": "Philip Garrett", "category": ["Technical", "Empirical"], "field": "Software Testing", "task": "Test Generation", "tags": ["Test case naming", "Deterministic naming heuristics", "REST API semantics (endpoints, methods, params, status codes)", "EvoMaster integration", "Human readability evaluation", "LLM-based naming comparison", "Test-suite organization and sorting", "Industrial case study / practitioner feedback"], "summary": "本文提出三种确定性方法为自动生成的REST API测试用例生成描述性名称，并通过问卷用户研究与大众及工业案例将这些规则方法与多种基于LLM的方法对比，结果表明规则方法在可读性上优于GPT‑3.5、与更先进模型相当且已集成到EvoMaster中提升了测试套件的可读性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.01690v1", "published": "2025-12-01", "update_time": "2025-12-01", "download_time": "2025-12-16 14:31:28"}
{"id": "2503.01449", "title": "Benchmarking Large Language Models for Multi-Language Software Vulnerability Detection", "abstract": "Recent advancements in generative AI have led to the widespread adoption of large language models (LLMs) in software engineering, addressing numerous long-standing challenges. However, a comprehensive study examining the capabilities of LLMs in software vulnerability detection (SVD), a crucial aspect of software security, is currently lacking. Existing research primarily focuses on evaluating LLMs using C/C++ datasets. It typically explores only one or two strategies among prompt engineering, instruction tuning, and sequence classification fine-tuning for open-source LLMs. Consequently, there is a significant knowledge gap regarding the effectiveness of diverse LLMs in detecting vulnerabilities across various programming languages. To address this knowledge gap, we present a comprehensive empirical study evaluating the performance of LLMs on the SVD task. We have compiled a comprehensive dataset comprising 8,260 vulnerable functions in Python, 7,505 in Java, and 28,983 in JavaScript. We assess five open-source LLMs using multiple approaches, including prompt engineering, instruction tuning, and sequence classification fine-tuning. These LLMs are benchmarked against five fine-tuned small language models and two open-source static application security testing tools. Furthermore, we explore two avenues to improve LLM performance on SVD: a) Data perspective: Retraining models using downsampled balanced datasets. b) Model perspective: Investigating ensemble learning methods that combine predictions from multiple LLMs. Our comprehensive experiments demonstrate that SVD remains a challenging task for LLMs. This study provides a thorough understanding of the role of LLMs in SVD and offers practical insights for future advancements in leveraging generative AI to enhance software security practices.", "arxiv_url": "https://arxiv.org/abs/2503.01449", "authors": ["Ting Zhang", "Chengran Yang", "Yindu Su", "Martin Weyssow", "Hung Nguyen", "Tan Bui", "Hong Jin Kang", "Yikun Li", "Eng Lieh Ouh", "Lwin Khin Shar", "David Lo"], "first_author": "Ting Zhang", "category": ["Empirical", "Benchmark"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Function-level multi-language vulnerability dataset", "Python/Java/JavaScript vulnerable functions", "Prompt engineering vs instruction tuning vs sequence-classification fine-tuning", "Comparison with small language models and SAST tools", "Ensemble methods for combining LLM predictions", "Class imbalance mitigation via downsampling", "Empirical benchmarking of open-source LLMs for SVD", "Cross-language vulnerability detection evaluation"], "summary": "本文构建了包含 Python、Java 和 JavaScript 函数级漏洞的综合数据集，并系统评估多种开源 LLM（通过提示工程、指令微调和序列分类微调）在漏洞检测任务中的效果，比较了小模型和静态分析工具，同时探索了下采样平衡数据与模型集成以提升性能的策略。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2503.01449v1", "published": "2025-03-03", "update_time": "2025-03-03", "download_time": "2025-12-16 14:32:02"}
{"id": "2512.01609", "title": "GPTrace: Effective Crash Deduplication Using LLM Embeddings", "abstract": "Fuzzing is a highly effective method for uncovering software vulnerabilities, but analyzing the resulting data typically requires substantial manual effort. This is amplified by the fact that fuzzing campaigns often find a large number of crashing inputs, many of which share the same underlying bug. Crash deduplication is the task of finding such duplicate crashing inputs and thereby reducing the data that needs to be examined. Many existing deduplication approaches rely on comparing stack traces or other information that is collected when a program crashes. Although various metrics for measuring the similarity of such pieces of information have been proposed, many do not yield satisfactory deduplication results. In this work, we present GPTrace, a deduplication workflow that leverages a large language model to evaluate the similarity of various data sources associated with crashes by computing embedding vectors and supplying those as input to a clustering algorithm. We evaluate our approach on over 300 000 crashing inputs belonging to 50 ground truth labels from 14 different targets. The deduplication results produced by GPTrace show a noticeable improvement over hand-crafted stack trace comparison methods and even more complex state-of-the-art approaches that are less flexible.", "arxiv_url": "https://arxiv.org/abs/2512.01609", "authors": ["Patrick Herter", "Vincent Ahlrichs", "Ridvan Açilan", "Julian Horsch"], "first_author": "Patrick Herter", "category": ["Technical"], "field": "Software Testing", "task": "Testing automation", "tags": ["Crash Deduplication", "Fuzzing Crash Triage", "Stack Trace Preprocessing", "ASan Report Sanitization", "Embedding-based Crash Similarity", "Summed Normalized Embeddings", "HDBSCAN-DBSCAN Hybrid Clustering"], "summary": "GPTrace提出通过对栈跟踪和ASan报告计算并归一化求和的嵌入向量，结合密度聚类实现模糊测试崩溃去重，从而显著优于传统手工栈跟踪比对方法。", "quality": "High", "conference": "IEEE/ACM International Conference on Software Engineering (ICSE 2026)", "pdf_url": "https://arxiv.org/pdf/2512.01609v1", "published": "2025-12-01", "update_time": "2025-12-01", "download_time": "2025-12-16 14:32:38"}
{"id": "2512.01396", "title": "BackportBench: A Multilingual Benchmark for Automated Backporting of Patches", "abstract": "Many modern software projects evolve rapidly to incorporate new features and security patches. It is important for users to update their dependencies to safer versions, but many still use older, vulnerable package versions because upgrading can be difficult and may break their existing codebase. Software developers can mitigate this problem by backporting security patches to older releases. However, manually backporting is time-consuming and error-prone. The effectiveness of existing automated backporting techniques on general software remains unclear since they typically target only code-hunk or function-level patch porting scenarios and are evaluated with imperfect metrics.   To facilitate the development and evaluation of automated backporting techniques, we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem. BackportBench is a multilingual benchmark that contains 202 patch backporting problems from PyPI, Maven, and npm, each with executable Docker environments and relevant test cases. We evaluated existing patch porting methods and LLM-based techniques that have the potential to adapt to this task using BackportBench. The results show that the agentic method has outperformed traditional patch porting methods, especially on cases that require logical and structural changes. However, the performance varies across different programming languages. Based on the findings, we draw several implications for researchers and software practitioners in future work on automated backporting.", "arxiv_url": "https://arxiv.org/abs/2512.01396", "authors": ["Zhiqing Zhong", "Jiaming Huang", "Pinjia He"], "first_author": "Zhiqing Zhong", "category": ["Benchmark", "Empirical"], "field": "Quality Management", "task": "Vulnerability Repair", "tags": ["Patch Backporting", "Multilingual (Python/Java/JavaScript)", "Repository-level Patch Adaptation", "Executable Docker Environments", "Test-suite Validation", "Agentic LLM Workflows", "Cross-file Incompatibility Handling", "Evaluation Metrics for Backporting"], "summary": "本文提出BackportBench——一个包含来自PyPI、Maven和npm的202个多语言可执行补丁回移任务的基准，并用测试用例评估传统补丁迁移方法与多种（包括agentic）LLM驱动方法的效果与差异。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.01396v1", "published": "2025-12-01", "update_time": "2025-12-01", "download_time": "2025-12-16 14:33:18"}
{"id": "2512.01356", "title": "LAURA: Enhancing Code Review Generation with Context-Enriched Retrieval-Augmented LLM", "abstract": "Code review is critical for ensuring software quality and maintainability. With the rapid growth in software scale and complexity, code review has become a bottleneck in the development process because of its time-consuming and knowledge-intensive nature and the shortage of experienced developers willing to review code. Several approaches have been proposed for automatically generating code reviews based on retrieval, neural machine translation, pre-trained models, or large language models (LLMs). These approaches mainly leverage historical code changes and review comments. However, a large amount of crucial information for code review, such as the context of code changes and prior review knowledge, has been overlooked. This paper proposes an LLM-based review knowledge-augmented, context-aware framework for code review generation, named LAURA. The framework integrates review exemplar retrieval, context augmentation, and systematic guidance to enhance the performance of ChatGPT-4o and DeepSeek v3 in generating code review comments. Besides, given the extensive low-quality reviews in existing datasets, we also constructed a high-quality dataset. Experimental results show that for both models, LAURA generates review comments that are either completely correct or at least helpful to developers in 42.2% and 40.4% of cases, respectively, significantly outperforming SOTA baselines. Furthermore, our ablation studies demonstrate that all components of LAURA contribute positively to improving comment quality.", "arxiv_url": "https://arxiv.org/abs/2512.01356", "authors": ["Yuxin Zhang", "Yuxia Zhang", "Zeyu Sun", "Yanjie Jiang", "Hui Liu"], "first_author": "Yuxin Zhang", "category": ["Technical", "Benchmark"], "field": "Maintenance", "task": "Code Review", "tags": ["Context-extended diffs", "PR metadata augmentation", "Review exemplar retrieval", "Retrieval-augmented generation (RAG) for reviews", "Transformer-based code embeddings", "AST-based context expansion", "Systematic review prompting", "High-quality diff-comment dataset construction", "Human + LLM dual evaluation (I-Score/IH-Score)", "Ablation study of components"], "summary": "本文提出LAURA框架，通过补充PR上下文、检索相似变更及其审查范例并结合系统化提示来增强LLM生成代码审查注释，并构建并公开了一个高质量的diff‑comment检索数据库以验证性能提升。", "quality": "High", "conference": "IEEE/ACM International Conference on Automated Software Engineering (ASE 2025)", "pdf_url": "https://arxiv.org/pdf/2512.01356v1", "published": "2025-12-01", "update_time": "2025-12-01", "download_time": "2025-12-16 14:33:59"}
{"id": "2512.01255", "title": "Large Language Models Cannot Reliably Detect Vulnerabilities in JavaScript: The First Systematic Benchmark and Evaluation", "abstract": "Researchers have proposed numerous methods to detect vulnerabilities in JavaScript, especially those assisted by Large Language Models (LLMs). However, the actual capability of LLMs in JavaScript vulnerability detection remains questionable, necessitating systematic evaluation and comprehensive benchmarks. Unfortunately, existing benchmarks suffer from three critical limitations: (1) incomplete coverage, such as covering a limited subset of CWE types; (2) underestimation of LLM capabilities caused by unreasonable ground truth labeling; and (3) overestimation due to unrealistic cases such as using isolated vulnerable files rather than complete projects.   In this paper, we introduce, for the first time, three principles for constructing a benchmark for JavaScript vulnerability detection that directly address these limitations: (1) comprehensiveness, (2) no underestimation, and (3) no overestimation. Guided by these principles, we propose FORGEJS, the first automatic benchmark generation framework for evaluating LLMs' capability in JavaScript vulnerability detection. Then, we use FORGEJS to construct ARENAJS-the first systematic benchmark for LLM-based JavaScript vulnerability detection-and further propose JUDGEJS, an automatic evaluation framework.   We conduct the first systematic evaluation of LLMs for JavaScript vulnerability detection, leveraging JUDGEJS to assess seven popular commercial LLMs on ARENAJS. The results show that LLMs not only exhibit limited reasoning capabilities, but also suffer from severe robustness defects, indicating that reliable JavaScript vulnerability detection with LLMs remains an open challenge.", "arxiv_url": "https://arxiv.org/abs/2512.01255", "authors": ["Qingyuan Fei", "Xin Liu", "Song Li", "Shujiang Wu", "Jianwei Hou", "Ping Chen", "Zifeng Kang"], "first_author": "Qingyuan Fei", "category": ["Benchmark", "Empirical", "Technical"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["JavaScript vulnerability benchmark", "Comprehensive CWE coverage", "Project-level vs. function-level evaluation", "Automatic benchmark generation", "Automated evaluation pipeline", "Semantic equivalence / fuzzy label alignment", "Vulnerability localization (file/function/line)", "Taint-flow and dependency reasoning assessment", "Robustness testing via code augmentations", "False positive rate and deployment-oriented metrics"], "summary": "本文提出遵循全面性、避免低估和避免高估三项原则的自动化基准生成与评估体系，并基于该体系构建了系统性JavaScript漏洞检测基准与评估框架，实证评估多款主流商业大模型后发现其在推理能力、鲁棒性和可部署性方面表现不足且不可靠。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.01255v1", "published": "2025-12-01", "update_time": "2025-12-01", "download_time": "2025-12-16 14:34:37"}
{"id": "2512.02795", "title": "Towards Observation Lakehouses: Living, Interactive Archives of Software Behavior", "abstract": "Code-generating LLMs are trained largely on static artifacts (source, comments, specifications) and rarely on materializations of run-time behavior. As a result, they readily internalize buggy or mislabeled code. Since non-trivial semantic properties are undecidable in general, the only practical way to obtain ground-truth functionality is by dynamic observation of executions. In prior work, we addressed representation with Sequence Sheets, Stimulus-Response Matrices (SRMs), and Stimulus-Response Cubes (SRCs) to capture and compare behavior across tests, implementations, and contexts. These structures make observation data analyzable offline and reusable, but they do not by themselves provide persistence, evolution, or interactive analytics at scale. In this paper, therefore, we introduce observation lakehouses that operationalize continual SRCs: a tall, append-only observations table storing every actuation (stimulus, response, context) and SQL queries that materialize SRC slices on demand. Built on Apache Parquet + Iceberg + DuckDB, the lakehouse ingests data from controlled pipelines (LASSO) and CI pipelines (e.g., unit test executions), enabling n-version assessment, behavioral clustering, and consensus oracles without re-execution. On a 509-problem benchmark, we ingest $\\approx$8.6M observation rows ($<$51MiB) and reconstruct SRM/SRC views and clusters in $<$100ms on a laptop, demonstrating that continual behavior mining is practical without a distributed cluster of machines. This makes behavioral ground truth first-class alongside other run-time data and provides an infrastructure path toward behavior-aware evaluation and training. The Observation Lakehouse, together with the accompanying dataset, is publicly available as an open-source project on GitHub: https://github.com/SoftwareObservatorium/observation-lakehouse", "arxiv_url": "https://arxiv.org/abs/2512.02795", "authors": ["Marcus Kessel"], "first_author": "Marcus Kessel", "category": ["Technical", "Benchmark"], "field": "Software Testing", "task": "Testing automation", "tags": ["Observation Lakehouse", "Stimulus-Response Matrix (SRM)", "Stimulus-Response Cube (SRC)", "Sequence Sheets / Invocation step records", "Append-only observations table", "Parquet + Iceberg + DuckDB stack", "Schema evolution for continual observations", "Partitioned SRM reconstruction", "Behavioral clustering", "Consensus oracle / n-version assessment", "Interactive SQL-based analytics"], "summary": "本文提出Observation Lakehouse，一种基于Parquet/Iceberg/DuckDB的数据湖仓架构，用细粒度调用步记录持续存储并按需重建SRM/SRC视图，从而在单机上高效完成行为聚类、n版评估与共识判定，推动运行时行为成为训练与评估的一等公民。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.02795v1", "published": "2025-12-02", "update_time": "2025-12-02", "download_time": "2025-12-16 14:35:14"}
{"id": "2512.02750", "title": "\"Can you feel the vibes?\": An exploration of novice programmer engagement with vibe coding", "abstract": "Emerging alongside generative AI and the broader trend of AI-assisted coding, the term \"vibe coding\" refers to creating software via natural language prompts rather than direct code authorship. This approach promises to democratize software development, but its educational implications remain underexplored. This paper reports on a one-day educational hackathon investigating how novice programmers and mixed-experience teams engage with vibe coding. We organized an inclusive event at a Brazilian public university with 31 undergraduate participants from computing and non-computing disciplines, divided into nine teams. Through observations, an exit survey, and semi-structured interviews, we examined creative processes, tool usage patterns, collaboration dynamics, and learning outcomes. Findings reveal that vibe coding enabled rapid prototyping and cross-disciplinary collaboration, with participants developing prompt engineering skills and delivering functional demonstrations within time constraints. However, we observed premature convergence in ideation, uneven code quality requiring rework, and limited engagement with core software engineering practices. Teams adopted sophisticated workflows combining multiple AI tools in pipeline configurations, with human judgment remaining essential for critical refinement. The short format (9 hours) proved effective for confidence-building among newcomers while accommodating participants with limited availability. We conclude that vibe coding hackathons can serve as valuable low-stakes learning environments when coupled with explicit scaffolds for divergent thinking, critical evaluation of AI outputs, and realistic expectations about production quality.", "arxiv_url": "https://arxiv.org/abs/2512.02750", "authors": ["Kiev Gama", "Filipe Calegario", "Victoria Jackson", "Alexander Nolte", "Luiz Augusto Morais", "Vinicius Garcia"], "first_author": "Kiev Gama", "category": ["Empirical"], "field": "Coding Assistant", "task": "Code Prompting", "tags": ["Vibe Coding", "Hackathon Design", "Prompt Engineering", "Novice Programmer Engagement", "Cross-disciplinary Collaboration", "Toolchain Orchestration", "Premature Ideation Convergence", "Human-in-the-loop Validation"], "summary": "本文通过对一次为期一天、面向新手的vibe coding黑客松的观察、问卷和访谈，探讨参与者如何用自然语言提示与多种AI工具协作快速原型、学到的提示工程技能、协作动态与学习成效，发现活动能提高信心与跨学科合作但也带来早熟收敛、代码质量参差及对软件工程实践的有限参与。", "quality": "Middle", "conference": "International Conference on Software Engineering, Education Track (SEET) 2026", "pdf_url": "https://arxiv.org/pdf/2512.02750v1", "published": "2025-12-02", "update_time": "2025-12-02", "download_time": "2025-12-16 14:35:58"}
{"id": "2511.15665", "title": "Quantum-Guided Test Case Minimization for LLM-Based Code Generation", "abstract": "Precisely controlling Large Language Models (LLMs) to generate efficient and concise code is a central challenge in software engineering. We introduce a framework based on Test-Driven Development (TDD) that transforms code specification into a combinatorial optimization task. The framework first prompts an LLM to generate a test suite, then formulates the Test Case Minimization (TCM) problem as a Quadratic Unconstrained Binary Optimization (QUBO) model. This QUBO paradigm is compatible with both classical solvers and emerging hardware such as quantum annealers. Experimentally, quantum annealing solves the core TCM task 16 times faster than simulated annealing. This performance underpins our end-to-end framework, which reduces total token consumption by 36.5\\% and significantly improves code quality. This work demonstrates a powerful synergy between generative AI and combinatorial optimization in software engineering, highlighting the critical importance of precise model formulation.", "arxiv_url": "https://arxiv.org/abs/2511.15665", "authors": ["Huixiang Zhang", "Mahzabeen Emu"], "first_author": "Huixiang Zhang", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Prompting", "tags": ["Test-Driven Development for LLMs", "Test Case Minimization", "QUBO formulation for test selection", "Quantum annealing acceleration", "LLM-generated test-suite pruning", "Token-cost aware specification", "LLM-guided code refactoring", "CI/CD per-commit optimization"], "summary": "本文提出一个基于测试驱动开发和QUBO优化的端到端框架，通过对LLM生成的冗余测试用例进行量子退火加速的最小化，显著降低token消耗并提升生成代码的质量与生成效率。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.15665v1", "published": "2025-11-19", "update_time": "2025-11-19", "download_time": "2025-12-16 14:36:47"}
{"id": "2512.03421", "title": "Exploring the Potential and Limitations of Large Language Models for Novice Program Fault Localization", "abstract": "Novice programmers often face challenges in fault localization due to their limited experience and understanding of programming syntax and logic. Traditional methods like Spectrum-Based Fault Localization (SBFL) and Mutation-Based Fault Localization (MBFL) help identify faults but often lack the ability to understand code context, making them less effective for beginners. In recent years, Large Language Models (LLMs) have shown promise in overcoming these limitations by utilizing their ability to understand program syntax and semantics. LLM-based fault localization provides more accurate and context-aware results than traditional techniques. This study evaluates six closed-source and seven open-source LLMs using the Codeflaws, Condefects, and BugT datasets, with BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns. Advanced models with reasoning capabilities, such as OpenAI o3 and DeepSeekR1, achieve superior accuracy with minimal reliance on prompt engineering. In contrast, models without reasoning capabilities, like GPT-4, require carefully designed prompts to maintain performance. While LLMs perform well in simple fault localization, their accuracy decreases as problem difficulty increases, though top models maintain robust performance in the BugT dataset. Over-reasoning is another challenge, where some models generate excessive explanations that hinder fault localization clarity. Additionally, the computational cost of deploying LLMs remains a significant barrier for real-time debugging. LLM's explanations demonstrate significant value for novice programmer assistance, with one-year experience participants consistently rating them highly. Our findings demonstrate the potential of LLMs to improve debugging efficiency while stressing the need for further refinement in their reasoning and computational efficiency for practical adoption.", "arxiv_url": "https://arxiv.org/abs/2512.03421", "authors": ["Hexiang Xu", "Hengyuan Liu", "Yonghao Wu", "Xiaolan Kang", "Xiang Chen", "Yong Liu"], "first_author": "Hexiang Xu", "category": ["Empirical", "Benchmark"], "field": "Quality Management", "task": "Bug Localization", "tags": ["Novice-program fault dataset construction", "LLM vs SBFL/MBFL comparison", "Prompt engineering sensitivity", "Difficulty-level performance analysis", "Over-reasoning in model explanations", "User study on explanation usefulness", "Model size versus performance analysis", "Computational cost and real-time feasibility"], "summary": "本文通过构建防止数据泄露的新数据集并在多套公开与自建数据上系统评估多款闭源/开源大模型，分析其在新手程序错误定位中的性能、提示工程敏感性、问题难度影响、过度推理与计算成本等局限，并通过用户研究验证解释性输出对初学者的教学价值。", "quality": "High", "conference": "Journal of Systems and Software", "pdf_url": "https://arxiv.org/pdf/2512.03421v1", "published": "2025-12-03", "update_time": "2025-12-03", "download_time": "2025-12-16 14:37:42"}
{"id": "2512.03420", "title": "HarnessAgent: Scaling Automatic Fuzzing Harness Construction with Tool-Augmented LLM Pipelines", "abstract": "Large language model (LLM)-based techniques have achieved notable progress in generating harnesses for program fuzzing. However, applying them to arbitrary functions (especially internal functions) \\textit{at scale} remains challenging due to the requirement of sophisticated contextual information, such as specification, dependencies, and usage examples. State-of-the-art methods heavily rely on static or incomplete context provisioning, causing failure of generating functional harnesses. Furthermore, LLMs tend to exploit harness validation metrics, producing plausible yet logically useless code. % Therefore, harness generation across large and diverse projects continues to face challenges in reliable compilation, robust code retrieval, and comprehensive validation.   To address these challenges, we present HarnessAgent, a tool-augmented agentic framework that achieves fully automated, scalable harness construction over hundreds of OSS-Fuzz targets. HarnessAgent introduces three key innovations: 1) a rule-based strategy to identify and minimize various compilation errors; 2) a hybrid tool pool for precise and robust symbol source code retrieval; and 3) an enhanced harness validation pipeline that detects fake definitions. We evaluate HarnessAgent on 243 target functions from OSS-Fuzz projects (65 C projects and 178 C++ projects). It improves the three-shot success rate by approximately 20\\% compared to state-of-the-art techniques, reaching 87\\% for C and 81\\% for C++. Our one-hour fuzzing results show that more than 75\\% of the harnesses generated by HarnessAgent increase the target function coverage, surpassing the baselines by over 10\\%. In addition, the hybrid tool-pool system of HarnessAgent achieves a response rate of over 90\\% for source code retrieval, outperforming Fuzz Introspector by more than 30\\%.", "arxiv_url": "https://arxiv.org/abs/2512.03420", "authors": ["Kang Yang", "Yunhang Zhang", "Zichuan Li", "Guanhong Tao", "Jun Xu", "Xiaojing Liao"], "first_author": "Kang Yang", "category": ["Technical", "Empirical"], "field": "Software Testing", "task": "Test Generation", "tags": ["Fuzzing harness generation", "Compilation-error triage and routing", "Hybrid symbol/source retrieval", "LSP- and AST-based retrieval backend", "Agentic tool-augmented pipeline", "Detection of fake definitions during validation", "Scalable internal-function targeting", "Coverage-driven harness evaluation"], "summary": "HarnessAgent 提出了一种工具增强的 agent 框架，通过编译错误分流、混合符号/源码检索与加强的验证管道，实现对大型 C/C++ 代码库中内部函数的可扩展自动化模糊测试 harness 生成，并在 243 个目标上显著提升生成成功率与 fuzz 覆盖率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.03420v3", "published": "2025-12-03", "update_time": "2025-12-11", "download_time": "2025-12-16 14:38:19"}
{"id": "2512.05073", "title": "David vs. Goliath: Can Small Models Win Big with Agentic AI in Hardware Design?", "abstract": "Large Language Model(LLM) inference demands massive compute and energy, making domain-specific tasks expensive and unsustainable. As foundation models keep scaling, we ask: Is bigger always better for hardware design? Our work tests this by evaluating Small Language Models coupled with a curated agentic AI framework on NVIDIA's Comprehensive Verilog Design Problems(CVDP) benchmark. Results show that agentic workflows: through task decomposition, iterative feedback, and correction - not only unlock near-LLM performance at a fraction of the cost but also create learning opportunities for agents, paving the way for efficient, adaptive solutions in complex design tasks.", "arxiv_url": "https://arxiv.org/abs/2512.05073", "authors": ["Shashwat Shankar", "Subhranshu Pandey", "Innocent Dengkhw Mochahari", "Bhabesh Mali", "Animesh Basak Chowdhury", "Sukanta Bhattacharjee", "Chandan Karfa"], "first_author": "Shashwat Shankar", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Agentic task decomposition", "SLM-aware prompt engineering", "Verilog RTL generation", "I/O port usage checking", "Iterative validation and rollback", "Token-budgeted context management", "CocoTB-based functional testing", "Energy‑efficiency (intelligence-per-watt) tradeoffs"], "summary": "该论文提出了一种面向小模型（SLM）的异构 agentic AI 框架，通过任务分解、结构化提示和迭代校验在 NVIDIA 的 CVDP Verilog 设计基准上实现了接近大型模型的硬件设计生成与理解性能，从而显著降低计算与能耗成本。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.05073v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-16 14:39:06"}
{"id": "2512.04680", "title": "Generative AI for Self-Adaptive Systems: State of the Art and Research Roadmap", "abstract": "Self-adaptive systems (SASs) are designed to handle changes and uncertainties through a feedback loop with four core functionalities: monitoring, analyzing, planning, and execution. Recently, generative artificial intelligence (GenAI), especially the area of large language models, has shown impressive performance in data comprehension and logical reasoning. These capabilities are highly aligned with the functionalities required in SASs, suggesting a strong potential to employ GenAI to enhance SASs. However, the specific benefits and challenges of employing GenAI in SASs remain unclear. Yet, providing a comprehensive understanding of these benefits and challenges is complex due to several reasons: limited publications in the SAS field, the technological and application diversity within SASs, and the rapid evolution of GenAI technologies. To that end, this paper aims to provide researchers and practitioners a comprehensive snapshot that outlines the potential benefits and challenges of employing GenAI's within SAS. Specifically, we gather, filter, and analyze literature from four distinct research fields and organize them into two main categories to potential benefits: (i) enhancements to the autonomy of SASs centered around the specific functions of the MAPE-K feedback loop, and (ii) improvements in the interaction between humans and SASs within human-on-the-loop settings. From our study, we outline a research roadmap that highlights the challenges of integrating GenAI into SASs. The roadmap starts with outlining key research challenges that need to be tackled to exploit the potential for applying GenAI in the field of SAS. The roadmap concludes with a practical reflection, elaborating on current shortcomings of GenAI and proposing possible mitigation strategies.", "arxiv_url": "https://arxiv.org/abs/2512.04680", "authors": ["Jialong Li", "Mingyue Zhang", "Nianyu Li", "Danny Weyns", "Zhi Jin", "Kenji Tei"], "first_author": "Jialong Li", "category": ["Survey"], "field": "Requirements & Design", "task": "Analysis", "tags": ["MAPE-K enhancement", "LLM-assisted analysis", "LLM-assisted planning", "LLM-assisted monitoring", "Human-on-the-loop (HOTL)", "Explainability and transparency", "Adaptation plan synthesis", "Safety and robustness mitigation"], "summary": "本文综述了将大型语言模型等生成式AI应用于自适应系统（基于MAPE‑K）的研究现状，归纳其在监测、分析、规划、执行及人机协作方面的潜在增益与挑战，并提出了面向集成与实践的研究路线图与缓解策略。", "quality": "High", "conference": "ACM Transactions on Autonomous and Adaptive Systems 2024", "pdf_url": "https://arxiv.org/pdf/2512.04680v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-16 14:39:58"}
{"id": "2512.04702", "title": "POLARIS: Is Multi-Agentic Reasoning the Next Wave in Engineering Self-Adaptive Systems?", "abstract": "The growing scale, complexity, interconnectivity, and autonomy of modern software ecosystems introduce unprecedented uncertainty, challenging the foundations of traditional self-adaptation. Existing approaches, typically rule-driven controllers or isolated learning components, struggle to generalize to novel contexts or coordinate responses across distributed subsystems, leaving them ill-equipped for emergent unknown unknowns. Recent discussions on Self-Adaptation 2.0 emphasize an equal partnership between AI and adaptive systems, merging learning-driven intelligence with adaptive control for predictive and proactive behavior. Building on this foundation, we introduce POLARIS, a three-layer multi-agentic self-adaptation framework that advances beyond reactive adaptation. POLARIS integrates: (1) a low-latency Adapter layer for monitoring and safe execution, (2) a transparent Reasoning layer that generates and verifies plans using tool-aware, explainable agents, and (3) a Meta layer that records experiences and meta-learns improved adaptation policies over time. Through shared knowledge and predictive models, POLARIS handles uncertainty, learns from past actions, and evolves its strategies, enabling systems that anticipate change and maintain resilient, goal-directed behavior. Preliminary evaluation on two self-adaptive exemplars, SWIM and SWITCH, shows that POLARIS consistently outperforms state-of-the-art baselines. We argue this marks a shift toward Self-Adaptation 3.0, akin to Software 3.0: a paradigm where systems not only learn from their environment but also reason about and evolve their own adaptation processes, continuously improving to meet novel challenges.", "arxiv_url": "https://arxiv.org/abs/2512.04702", "authors": ["Divyansh Pandey", "Vyakhya Gupta", "Prakhar Singhal", "Karthik Vaidhyanathan"], "first_author": "Divyansh Pandey", "category": ["Technical"], "field": "Self-Adaptive Systems", "task": "Multi-Agentic Reasoning & Meta-Learning for Runtime Adaptation", "tags": ["Multi-agent orchestration for adaptation", "Tool-aware explainable reasoning agents", "Meta-learning of adaptation policies", "World-model based what-if simulation", "Verifier agent for plan validation", "Low-latency reactive controller (stabilization)", "Episodic knowledge base for experience replay"], "summary": "本文提出POLARIS——一个三层多代理自适应框架，通过低延迟执行器、可解释的推理代理与记录并元学习的元层，结合世界模型与验证器实现主动、可演化的运行时自适应，并在SWIM与SWITCH示例上优于现有基线。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.04702v2", "published": "2025-12-04", "update_time": "2025-12-07", "download_time": "2025-12-16 14:40:38"}
{"id": "2512.04673", "title": "Cross-Task Benchmarking and Evaluation of General-Purpose and Code-Specific Large Language Models", "abstract": "Large Language Models (LLMs) have revolutionized both general natural language processing and domain-specific applications such as code synthesis, legal reasoning, and finance. However, while prior studies have explored individual model capabilities, a systematic cross-domain comparison that unifies linguistic, reasoning, and code understanding abilities remains underexplored. In this work, we present a comprehensive evaluation of five general-purpose and three code-specific state-of-the-art LLMs across six diverse benchmarks encompassing linguistic competence, mathematical reasoning, and trustworthiness. Additionally, we analyze model behavior on the CoNaLa dataset for code explanation, comparing natural language and code-specialized LLMs. Our findings reveal that models optimized for code (e.g., CodeLLaMA variants) exhibit strong reasoning and syntactic precision, that even for non-coding tasks can show measurable performance gains, in contrast to general-purpose models like Mistral-7B and Llama-3-8B.", "arxiv_url": "https://arxiv.org/abs/2512.04673", "authors": ["Gunjan Das", "Paheli Bhattacharya", "Rishabh Gupta"], "first_author": "Gunjan Das", "category": ["Empirical"], "field": "Coding Assistant", "task": "Code Summarization", "tags": ["Cross-domain benchmarking", "Code vs general-purpose model comparison", "Code explanation / intent generation", "Trustworthiness evaluation", "Commonsense and mathematical reasoning assessment", "Consolidation of lexical and embedding-based metrics"], "summary": "本文系统比较评估了多款通用与代码专用大型语言模型在多项自然语言与代码解释基准上的表现，发现代码专用模型在推理能力与语法精确性上具有显著优势并在某些非代码任务上也能带来提升。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.04673v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-16 14:41:13"}
{"id": "2512.05100", "title": "Structured Document Translation via Format Reinforcement Learning", "abstract": "Recent works on structured text translation remain limited to the sentence level, as they struggle to effectively handle the complex document-level XML or HTML structures. To address this, we propose \\textbf{Format Reinforcement Learning (FormatRL)}, which employs Group Relative Policy Optimization on top of a supervised fine-tuning model to directly optimize novel structure-aware rewards: 1) TreeSim, which measures structural similarity between predicted and reference XML trees and 2) Node-chrF, which measures translation quality at the level of XML nodes. Additionally, we apply StrucAUC, a fine-grained metric distinguishing between minor errors and major structural failures. Experiments on the SAP software-documentation benchmark demonstrate improvements across six metrics and an analysis further shows how different reward functions contribute to improvements in both structural and translation quality.", "arxiv_url": "https://arxiv.org/abs/2512.05100", "authors": ["Haiyue Song", "Johannes Eschbach-Dymanus", "Hour Kaing", "Sumire Honda", "Hideki Tanaka", "Bianka Buschbeck", "Masao Utiyama"], "first_author": "Haiyue Song", "category": ["Technical"], "field": "Structured Document Translation", "task": "Structured Document Translation", "tags": ["Format-aware Reinforcement Learning", "Group Relative Policy Optimization", "Tree edit-distance reward (TreeSim)", "Node-level chrF reward", "Structure-Aware AUC (StrucAUC)", "XML/HTML structure preservation", "Synthetic markup injection for data augmentation", "Document-level localization"], "summary": "本文提出FORMATRL，一种在监督微调基础上使用GRPO并通过TreeSim和Node-chrF等结构感知奖励直接优化XML/HTML文档结构保真度与节点级翻译质量的强化学习方法，并在软件文档数据集上显著提升了结构与翻译评估指标。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.05100v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-16 14:41:56"}
{"id": "2512.04785", "title": "ASTRIDE: A Security Threat Modeling Platform for Agentic-AI Applications", "abstract": "AI agent-based systems are becoming increasingly integral to modern software architectures, enabling autonomous decision-making, dynamic task execution, and multimodal interactions through large language models (LLMs). However, these systems introduce novel and evolving security challenges, including prompt injection attacks, context poisoning, model manipulation, and opaque agent-to-agent communication, that are not effectively captured by traditional threat modeling frameworks. In this paper, we introduce ASTRIDE, an automated threat modeling platform purpose-built for AI agent-based systems. ASTRIDE extends the classical STRIDE framework by introducing a new threat category, A for AI Agent-Specific Attacks, which encompasses emerging vulnerabilities such as prompt injection, unsafe tool invocation, and reasoning subversion, unique to agent-based applications. To automate threat modeling, ASTRIDE combines a consortium of fine-tuned vision-language models (VLMs) with the OpenAI-gpt-oss reasoning LLM to perform end-to-end analysis directly from visual agent architecture diagrams, such as data flow diagrams(DFDs). LLM agents orchestrate the end-to-end threat modeling automation process by coordinating interactions between the VLM consortium and the reasoning LLM. Our evaluations demonstrate that ASTRIDE provides accurate, scalable, and explainable threat modeling for next-generation intelligent systems. To the best of our knowledge, ASTRIDE is the first framework to both extend STRIDE with AI-specific threats and integrate fine-tuned VLMs with a reasoning LLM to fully automate diagram-driven threat modeling in AI agent-based applications.", "arxiv_url": "https://arxiv.org/abs/2512.04785", "authors": ["Eranga Bandara", "Amin Hass", "Ross Gore", "Sachin Shetty", "Ravi Mukkamala", "Safdar H. Bouk", "Xueping Liang", "Ng Wee Keong", "Kasun De Zoysa", "Aruna Withanage", "Nilaan Loganathan"], "first_author": "Eranga Bandara", "category": ["Technical", "Benchmark"], "field": "Requirements & Design", "task": "Analysis", "tags": ["Automated diagram-driven threat modeling", "STRIDE extension for AI agents", "AI agent-specific threats", "Vision-language ensemble for diagram parsing", "Prompt injection and instruction-manipulation detection", "Reasoning-based threat synthesis", "Quantized low-latency fine-tuning for edge inference", "Explainable security assessments"], "summary": "本文提出ASTRIDE，一种将STRIDE扩展以包含AI代理特有攻击并结合微调视觉—语言模型集群与推理LLM，从架构图（如数据流图）自动生成可解释威胁模型的平台。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.04785v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-16 14:42:40"}
{"id": "2512.04611", "title": "PBFuzz: Agentic Directed Fuzzing for PoV Generation", "abstract": "Proof-of-Vulnerability (PoV) input generation is a critical task in software security and supports downstream applications such as path generation and validation. Generating a PoV input requires solving two sets of constraints: (1) reachability constraints for reaching vulnerable code locations, and (2) triggering constraints for activating the target vulnerability. Existing approaches, including directed greybox fuzzing and LLM-assisted fuzzing, struggle to efficiently satisfy these constraints. This work presents an agentic method that mimics human experts. Human analysts iteratively study code to extract semantic reachability and triggering constraints, form hypotheses about PoV triggering strategies, encode them as test inputs, and refine their understanding using debugging feedback. We automate this process with an agentic directed fuzzing framework called PBFuzz. PBFuzz tackles four challenges in agentic PoV generation: autonomous code reasoning for semantic constraint extraction, custom program-analysis tools for targeted inference, persistent memory to avoid hypothesis drift, and property-based testing for efficient constraint solving while preserving input structure. Experiments on the Magma benchmark show strong results. PBFuzz triggered 57 vulnerabilities, surpassing all baselines, and uniquely triggered 17 vulnerabilities not exposed by existing fuzzers. PBFuzz achieved this within a 30-minute budget per target, while conventional approaches use 24 hours. Median time-to-exposure was 339 seconds for PBFuzz versus 8680 seconds for AFL++ with CmpLog, giving a 25.6x efficiency improvement with an API cost of 1.83 USD per vulnerability.", "arxiv_url": "https://arxiv.org/abs/2512.04611", "authors": ["Haochen Zeng", "Andrew Bao", "Jiajun Cheng", "Chengyu Song"], "first_author": "Haochen Zeng", "category": ["Technical"], "field": "Software Testing", "task": "Test Generation", "tags": ["Agentic reasoning", "Directed fuzzing", "Proof-of-vulnerability generation", "Property-based test generation", "Semantic reachability and triggering constraint extraction", "Persistent agent memory", "On-demand program analysis tooling", "Fine-grained execution feedback"], "summary": "本文提出PBFuzz，一种将自治式代码推理、按需程序分析与基于属性的测试相结合的定向模糊测试框架，用于高效自动生成Proof‑of‑Vulnerability输入并在Magma基准上显著超越现有方法。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.04611v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-16 14:43:21"}
{"id": "2512.04538", "title": "Completion by Comprehension: Guiding Code Generation with Multi-Granularity Understanding", "abstract": "As code completion task from function-level to repository-level, leveraging contextual information from large-scale codebases becomes a core challenge. However, existing retrieval-augmented generation (RAG) methods typically treat code as plain natural language, relying primarily on shallow semantic matching while overlooking structural semantics and code-specific dependencies. This limits their ability to capture control flow and underlying intent, ultimately constraining the quality of generated code. Therefore, we propose CoCo, a novel framework that enables code Completion by Comprehension of multi-granularity context from large-scale code repositories. CoCo employs static code analysis to extract structured context at the function, file, and project levels, capturing execution logic and semantic dependencies. It then adopts an graph-based multi-granularity context selection mechanism to filter out redundant information and remove noise. Consequently, the information is converted into natural language in a consistent manner, thereby functioning as explicit contextual prompts to guide subsequent code completion. Additionally, a structure-aware code re-ranker mechanism ensures alignment at both semantic and structural levels. Extensive experiments on CrossCodeEval and RepoEval benchmarks demonstrate that CoCo consistently surpasses state-of-the-art baselines, achieving up to 20.2% gains in EM. Moreover, the framework is model-agnostic and can be seamlessly integrated into existing methods, leading to significant performance.", "arxiv_url": "https://arxiv.org/abs/2512.04538", "authors": ["Xinkui Zhao", "Rongkai Liu", "Yifan Zhang", "Chen Zhi", "Lufei Zhang", "Guanjie Cheng", "Yueshen Xu", "Shuiguang Deng", "Jianwei Yin"], "first_author": "Xinkui Zhao", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Static code analysis for multi-granularity context", "Function-file-repository context modeling", "Graph-based context selection", "Structure-aware code re-ranking", "Natural-language prompt synthesis from structural context", "Repository-level dependency and control-flow modeling", "Retrieval-augmented generation integration"], "summary": "本文提出CoCo，通过静态分析提取函数/文件/仓库三级结构化上下文、用图筛选相关信息并将其转化为自然语言提示，结合结构感知的代码重排序，显著改进检索增强的仓库级代码补全性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.04538v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-16 14:44:03"}
{"id": "2512.04419", "title": "Solving LLM Repetition Problem in Production: A Comprehensive Study of Multiple Solutions", "abstract": "The repetition problem, where Large Language Models (LLMs) continuously generate repetitive content without proper termination, poses a critical challenge in production deployments, causing severe performance degradation and system stalling. This paper presents a comprehensive investigation and multiple practical solutions for the repetition problem encountered in real-world batch code interpretation tasks.   We identify three distinct repetition patterns: (1) business rule generation repetition, (2) method call relationship analysis repetition, and (3) PlantUML diagram syntax generation repetition. Through rigorous theoretical analysis based on Markov models, we establish that the root cause lies in greedy decoding's inability to escape repetitive loops, exacerbated by self-reinforcement effects.   Our comprehensive experimental evaluation demonstrates three viable solutions: (1) Beam Search decoding with early_stopping=True serves as a universal post-hoc mechanism that effectively resolves all three repetition patterns; (2) presence_penalty hyperparameter provides an effective solution specifically for BadCase 1; and (3) Direct Preference Optimization (DPO) fine-tuning offers a universal model-level solution for all three BadCases.   The primary value of this work lies in combining first-hand production experience with extensive experimental validation. Our main contributions include systematic theoretical analysis of repetition mechanisms, comprehensive evaluation of multiple solutions with task-specific applicability mapping, identification of early_stopping as the critical parameter for Beam Search effectiveness, and practical production-ready solutions validated in real deployment environments.", "arxiv_url": "https://arxiv.org/abs/2512.04419", "authors": ["Weiwei Wang", "Weijie Zou", "Jiyong Min"], "first_author": "Weiwei Wang", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Understanding", "tags": ["Generation repetition (repeater)", "Greedy decoding loop", "Beam search with early_stopping", "Presence_penalty tuning", "Direct Preference Optimization (DPO) fine‑tuning", "Markov-chain theoretical analysis", "Production batch code interpretation", "Framework parameter integration (FastChat↔vLLM)"], "summary": "本文基于批量代码解释的生产实践，利用马尔可夫模型分析LLM重复生成的根因，并通过大规模实验验证三种可行解决方案（启用early_stopping的束搜索、presence_penalty调优与DPO微调），给出任务适配与生产部署建议。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.04419v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-16 14:44:59"}
{"id": "2512.05908", "title": "Natural Language Summarization Enables Multi-Repository Bug Localization by LLMs in Microservice Architectures", "abstract": "Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code, LLM context limitations, and the need to first identify the correct repository. We propose reframing this as a natural language reasoning task by transforming codebases into hierarchical NL summaries and performing NL-to-NL search instead of cross-modal retrieval. Our approach builds context-aware summaries at file, directory, and repository levels, then uses a two-phase search: first routing bug reports to relevant repositories, then performing top-down localization within those repositories. Evaluated on DNext, an industrial system with 46 repositories and 1.1M lines of code, our method achieves Pass@10 of 0.82 and MRR of 0.50, significantly outperforming retrieval baselines and agentic RAG systems like GitHub Copilot and Cursor. This work demonstrates that engineered natural language representations can be more effective than raw source code for scalable bug localization, providing an interpretable repository -> directory -> file search path, which is vital for building trust in enterprise AI tools by providing essential transparency.", "arxiv_url": "https://arxiv.org/abs/2512.05908", "authors": ["Amirkia Rafiei Oskooei", "S. Selcan Yukcu", "Mehmet Cevheri Bozoglan", "Mehmet S. Aktas"], "first_author": "Amirkia Rafiei Oskooei", "category": ["Technical", "Empirical"], "field": "Quality Management", "task": "Bug Localization", "tags": ["Context-aware code summarization", "Hierarchical NL knowledge base", "Repository routing", "Top-down directory→file localization", "NL-to-NL retrieval", "Explainable repository→directory→file search", "Chain-of-Thought prompting", "Multi-repository microservice bug localization"], "summary": "本文提出将多仓库微服务代码库转换为层次化、上下文感知的自然语言摘要，并通过先路由到候选仓库再自顶向下目录与文件检索的双阶段搜索，将错误定位转化为NL-to-NL推理，从而在工业规模数据上显著优于基于原始代码的检索与RAG方法。", "quality": "High", "conference": "LLM4Code Workshop, ICSE 2026", "pdf_url": "https://arxiv.org/pdf/2512.05908v1", "published": "2025-12-05", "update_time": "2025-12-05", "download_time": "2025-12-16 14:45:35"}
{"id": "2512.05887", "title": "Bootstrapping Fuzzers for Compilers of Low-Resource Language Dialects Using Language Models", "abstract": "Modern extensible compiler frameworks-such as MLIR-enable rapid creation of domain-specific language dialects. This flexibility, however, makes correctness harder to ensure as the same extensibility that accelerates development also complicates maintaining the testing infrastructure. Extensible languages require automated test generation that is both dialect-agnostic (works across dialects without manual adaptation) and dialect-effective (targets dialect-specific features to find bugs). Existing approaches typically sacrifice one of these goals by either requiring manually constructed seed corpora for each dialect, or by failing to be effective. We present a dialect-agnostic and dialect-effective grammar-based and coverage-guided fuzzing approach for extensible compilers that combines two key insights from existing work: (i) the grammars of dialects, which already encode the structural and type constraints, can often be extracted automatically from the dialect specification; and (ii) these grammars can be used in combination with pre-trained large language models to automatically generate representative and diverse seed inputs from the full dialect space without requiring any manual input or training data. These seeds can then be used to bootstrap coverage-guided fuzzers. We built this approach into a tool, Germinator. When evaluated on six MLIR projects spanning 91 dialects, Germinator generated seeds improve line coverage by 10-120% over grammar-based baselines. We compare against grammar-based baselines because they are the only class of existing automatic seed generators that can be applied uniformly across MLIR's heterogeneous dialect ecosystem. Germinator discovers 88 previously unknown bugs (40 confirmed), including 23 in dialects with no prior automated test generators, demonstrating effective and controllable testing of low-resource dialects at scale.", "arxiv_url": "https://arxiv.org/abs/2512.05887", "authors": ["Sairam Vaidya", "Marcel Böhme", "Loris D'Antoni"], "first_author": "Sairam Vaidya", "category": ["Technical"], "field": "Software Testing", "task": "Test Generation", "tags": ["TableGen grammar extraction", "Grammar-constrained decoding", "Seed program generation", "Coverage-guided fuzzing integration", "Dialect-agnostic fuzzing", "Low-resource MLIR dialects", "Compiler fuzzing", "Bug discovery"], "summary": "本文提出 Germinator，通过从 MLIR 的 TableGen 自动提取语法并对预训练语言模型进行语法约束采样生成多样化种子，以引导覆盖驱动的模糊测试，从而在 91 个方言上显著提升覆盖率并发现大量新缺陷。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.05887v1", "published": "2025-12-05", "update_time": "2025-12-05", "download_time": "2025-12-16 14:46:15"}
{"id": "2311.07989", "title": "Unifying the Perspectives of NLP and Software Engineering: A Survey on Language Models for Code", "abstract": "In this work we systematically review the recent advancements in software engineering with language models, covering 70+ models, 40+ evaluation tasks, 180+ datasets, and 900 related works. Unlike previous works, we integrate software engineering (SE) with natural language processing (NLP) by discussing the perspectives of both sides: SE applies language models for development automation, while NLP adopts SE tasks for language model evaluation. We break down code processing models into general language models represented by the GPT family and specialized models that are specifically pretrained on code, often with tailored objectives. We discuss the relations and differences between these models, and highlight the historical transition of code modeling from statistical models and RNNs to pretrained Transformers and LLMs, which is exactly the same course that had been taken by NLP. We also go beyond programming and review LLMs' application in other software engineering activities including requirement engineering, testing, deployment, and operations in an endeavor to provide a global view of NLP in SE, and identify key challenges and potential future directions in this domain. We keep the survey open and updated on GitHub at https://github.com/codefuse-ai/Awesome-Code-LLM.", "arxiv_url": "https://arxiv.org/abs/2311.07989", "authors": ["Ziyin Zhang", "Chaoyu Chen", "Bingchang Liu", "Cong Liao", "Zi Gong", "Hang Yu", "Jianguo Li", "Rui Wang"], "first_author": "Ziyin Zhang", "category": ["Survey"], "field": "Coding Assistant", "task": "Code Pre-Training", "tags": ["Taxonomy of Code LMs", "SE–NLP Integration", "Code Modeling History", "Full Software Lifecycle Coverage"], "summary": "该论文系统综述代码语言模型及其在软件工程全生命周期中的应用，并统一了NLP与SE两个社区的视角。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2311.07989v7", "published": "2023-11-14", "update_time": "2024-06-26", "download_time": "2025-12-10 15:34:24"}
{"id": "2509.14856", "title": "CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End Code Review Evaluation in Python Projects", "abstract": "Automated code review (CR) is a key application for Large Language Models (LLMs), but progress is hampered by a \"reality gap\": existing benchmarks evaluate models on isolated sub-tasks using simplified, context-poor data. This fails to reflect the holistic context-rich nature of real-world CR. To bridge this gap, we introduce CodeFuse-CR-Bench, the first comprehensiveness-aware benchmark for repository-level CR evaluation. CodeFuse-CR-Bench comprises 601 high-quality instances from 70 Python projects covering nine Pull-Request (PR) problem domains, where each instance provides rich, multi-faceted context including the associated issue, PR details, and repository state, enabling end-to-end evaluation. Beyond superficial metrics, we also propose a novel evaluation framework that combines rule-based checks for location and syntax with model-based judgments of review quality. We present the first large-scale assessment of state-of-the-art LLMs on this comprehensive CR task. Our results establish crucial baselines and reveal that (1) no single LLM dominates all aspects of CR; (2) Gemini 2.5 Pro achieves the highest comprehensive performance; and (3) different LLMs exhibit varying robustness to redundant context. These findings highlight the necessity of holistic, multi-dimensional evaluation and provide actionable insights for advancing truly intelligent yet practical CR assistants.", "arxiv_url": "https://arxiv.org/abs/2509.14856", "authors": ["Hanyang Guo", "Xunjin Zheng", "Zihan Liao", "Hang Yu", "Peng DI", "Ziyin Zhang", "Hong-Ning Dai"], "first_author": "Hanyang Guo", "category": ["Benchmark"], "field": "Maintenance", "task": "Code Review", "tags": ["Repository-level Context", "End-to-End Evaluation", "Comprehensive Code Review", "Holistic Evaluation Framework"], "summary": "该论文提出首个面向端到端代码审查的全面性基准，通过提供丰富仓库级上下文并结合规则与模型评估框架，更真实地衡量LLM在实际代码审查任务中的能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2509.14856v3", "published": "2025-09-18", "update_time": "2025-10-23", "download_time": "2025-12-10 15:34:47"}
{"id": "2505.16901", "title": "Code Graph Model (CGM): A Graph-Integrated Large Language Model for Repository-Level Software Engineering Tasks", "abstract": "Recent advances in Large Language Models (LLMs) have shown promise in function-level code generation, yet repository-level software engineering tasks remain challenging. Current solutions predominantly rely on proprietary LLM agents, which introduce unpredictability and limit accessibility, raising concerns about data privacy and model customization. This paper investigates whether open-source LLMs can effectively address repository-level tasks without requiring agent-based approaches. We demonstrate this is possible by enabling LLMs to comprehend functions and files within codebases through their semantic information and structural dependencies. To this end, we introduce Code Graph Models (CGMs), which integrate repository code graph structures into the LLM's attention mechanism and map node attributes to the LLM's input space using a specialized adapter. When combined with an agentless graph RAG framework, our approach achieves a 43.00% resolution rate on the SWE-bench Lite benchmark using the open-source Qwen2.5-72B model. This performance ranks first among open weight models, second among methods with open-source systems, and eighth overall, surpassing the previous best open-source model-based method by 12.33%.", "arxiv_url": "https://arxiv.org/abs/2505.16901", "authors": ["Hongyuan Tao", "Ying Zhang", "Zhenhao Tang", "Hongen Peng", "Xukun Zhu", "Bingchang Liu", "Yingguang Yang", "Ziyin Zhang", "Zhaogui Xu", "Haipeng Zhang", "Linchao Zhu", "Rui Wang", "Hang Yu", "Jianguo Li", "Peng Di"], "first_author": "Hongyuan Tao", "category": ["Technical"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Repository-level Issue Fixing", "Graph-Integrated Attention", "Code Graph Representation", "Agentless RAG"], "summary": "该论文提出通过将代码图结构融入LLM并结合无代理的图检索框架，实现开源模型在仓库级缺陷修复任务中的大幅性能提升。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2505.16901v4", "published": "2025-05-22", "update_time": "2025-06-23", "download_time": "2025-12-10 15:35:12"}
{"id": "2409.04183", "title": "GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding", "abstract": "Programming languages possess rich semantic information - such as data flow - that is represented by graphs and not available from the surface form of source code. Recent code language models have scaled to billions of parameters, but model source code solely as text tokens while ignoring any other structural information. Conversely, models that do encode structural information of code make modifications to the Transformer architecture, limiting their scale and compatibility with pretrained LLMs. In this work, we take the best of both worlds with GALLa - Graph Aligned Large Language Models. GALLa utilizes graph neural networks and cross-modal alignment technologies to inject the structural information of code into LLMs as an auxiliary task during finetuning. This framework is both model-agnostic and task-agnostic, as it can be applied to any code LLM for any code downstream task, and requires the structural graph data only at training time from a corpus unrelated to the finetuning data, while incurring no cost at inference time over the baseline LLM. Experiments on five code tasks with seven different baseline LLMs ranging in size from 350M to 14B validate the effectiveness of GALLa, demonstrating consistent improvement over the baseline, even for powerful models such as LLaMA3 and Qwen2.5-Coder.", "arxiv_url": "https://arxiv.org/abs/2409.04183", "authors": ["Ziyin Zhang", "Hang Yu", "Shijie Li", "Peng Di", "Jianguo Li", "Rui Wang"], "first_author": "Ziyin Zhang", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Graph Alignment", "AST Integration", "DFG Integration", "GNN Adapters", "Cross‑Modal Representation"], "summary": "该论文提出GALLa框架利用图结构信息对代码LLM进行对齐，从而在多种代码理解与生成任务中提升模型性能。", "quality": "High", "conference": "ACL 2025", "pdf_url": "https://arxiv.org/pdf/2409.04183v4", "published": "2024-09-06", "update_time": "2025-09-23", "download_time": "2025-12-10 15:35:38"}
{"id": "2212.09420", "title": "Large Language Models Meet NL2Code: A Survey", "abstract": "The task of generating code from a natural language description, or NL2Code, is considered a pressing and significant challenge in code intelligence. Thanks to the rapid development of pre-training techniques, surging large language models are being proposed for code, sparking the advances in NL2Code. To facilitate further research and applications in this field, in this paper, we present a comprehensive survey of 27 existing large language models for NL2Code, and also review benchmarks and metrics. We provide an intuitive comparison of all existing models on the HumanEval benchmark. Through in-depth observation and analysis, we provide some insights and conclude that the key factors contributing to the success of large language models for NL2Code are \"Large Size, Premium Data, Expert Tuning\". In addition, we discuss challenges and opportunities regarding the gap between models and humans. We also create a website https://nl2code.github.io to track the latest progress through crowd-sourcing. To the best of our knowledge, this is the first survey of large language models for NL2Code, and we believe it will contribute to the ongoing development of the field.", "arxiv_url": "https://arxiv.org/abs/2212.09420", "authors": ["Daoguang Zan", "Bei Chen", "Fengji Zhang", "Dianjie Lu", "Bingchao Wu", "Bei Guan", "Yongji Wang", "Jian-Guang Lou"], "first_author": "Daoguang Zan", "category": ["Survey"], "field": "Coding Assistant", "task": "NL2Code", "tags": ["NL2Code", "Code Generation", "Model Comparison", "Benchmark Analysis"], "summary": "该论文系统综述了27种面向NL2Code任务的大语言模型，总结其技术特征、性能表现与未来挑战。", "quality": "High", "conference": "ACL 2023", "pdf_url": "https://arxiv.org/pdf/2212.09420v2", "published": "2022-12-19", "update_time": "2023-05-08", "download_time": "2025-12-10 15:35:55"}
{"id": "2212.10079", "title": "A Survey on Pretrained Language Models for Neural Code Intelligence", "abstract": "As the complexity of modern software continues to escalate, software engineering has become an increasingly daunting and error-prone endeavor. In recent years, the field of Neural Code Intelligence (NCI) has emerged as a promising solution, leveraging the power of deep learning techniques to tackle analytical tasks on source code with the goal of improving programming efficiency and minimizing human errors within the software industry. Pretrained language models have become a dominant force in NCI research, consistently delivering state-of-the-art results across a wide range of tasks, including code summarization, generation, and translation. In this paper, we present a comprehensive survey of the NCI domain, including a thorough review of pretraining techniques, tasks, datasets, and model architectures. We hope this paper will serve as a bridge between the natural language and programming language communities, offering insights for future research in this rapidly evolving field.", "arxiv_url": "https://arxiv.org/abs/2212.10079", "authors": ["Yichen Xu", "Yanqiao Zhu"], "first_author": "Yichen Xu", "category": ["Survey"], "field": "Coding Assistant", "task": "Code Pre-Training", "tags": ["Tokenization Strategies", "Structure Extraction", "Pretraining Paradigms", "Neural Code Intelligence"], "summary": "该论文系统综述了面向神经代码智能的预训练语言模型在预处理、模型设计与下游任务中的方法与进展。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2212.10079v1", "published": "2022-12-20", "update_time": "2022-12-20", "download_time": "2025-12-10 15:36:12"}
{"id": "2302.04026", "title": "An Empirical Comparison of Pre-Trained Models of Source Code", "abstract": "While a large number of pre-trained models of source code have been successfully developed and applied to a variety of software engineering (SE) tasks in recent years, our understanding of these pre-trained models is arguably fairly limited. With the goal of advancing our understanding of these models, we perform the first systematic empirical comparison of 19 recently-developed pre-trained models of source code on 13 SE tasks. To gain additional insights into these models, we adopt a recently-developed 4-dimensional categorization of pre-trained models, and subsequently investigate whether there are correlations between different categories of pre-trained models and their performances on different SE tasks.", "arxiv_url": "https://arxiv.org/abs/2302.04026", "authors": ["Changan Niu", "Chuanyi Li", "Vincent Ng", "Dongxiao Chen", "Jidong Ge", "Bin Luo"], "first_author": "Changan Niu", "category": ["Empirical"], "field": "New Field: Model Evaluation for SE", "task": "New Task: Pre-trained Model Comparison", "tags": ["Empirical Comparison", "Code Pre-trained Models", "Cross-task Evaluation", "Model Categorization"], "summary": "该论文系统性地对19种源码预训练模型在13项软件工程任务上的表现进行了实证比较，并分析其类别与性能之间的关联。", "quality": "High", "conference": "ICSE 2023", "pdf_url": "https://arxiv.org/pdf/2302.04026v1", "published": "2023-02-08", "update_time": "2023-02-08", "download_time": "2025-12-10 15:36:31"}
{"id": "2512.07814", "title": "Understanding Privacy Risks in Code Models Through Training Dynamics: A Causal Approach", "abstract": "Large language models for code (LLM4Code) have greatly improved developer productivity but also raise privacy concerns due to their reliance on open-source repositories containing abundant personally identifiable information (PII). Prior work shows that commercial models can reproduce sensitive PII, yet existing studies largely treat PII as a single category and overlook the heterogeneous risks among different types. We investigate whether distinct PII types vary in their likelihood of being learned and leaked by LLM4Code, and whether this relationship is causal. Our methodology includes building a dataset with diverse PII types, fine-tuning representative models of different scales, computing training dynamics on real PII data, and formulating a structural causal model to estimate the causal effect of learnability on leakage. Results show that leakage risks differ substantially across PII types and correlate with their training dynamics: easy-to-learn instances such as IP addresses exhibit higher leakage, while harder types such as keys and passwords leak less frequently. Ambiguous types show mixed behaviors. This work provides the first causal evidence that leakage risks are type-dependent and offers guidance for developing type-aware and learnability-aware defenses for LLM4Code.", "arxiv_url": "https://arxiv.org/abs/2512.07814", "authors": ["Hua Yang", "Alejandro Velasco", "Sen Fang", "Bowen Xu", "Denys Poshyvanyk"], "first_author": "Hua Yang", "category": ["Empirical", "Benchmark"], "field": "Privacy & Security", "task": "PII Leakage Analysis (causal)", "tags": ["PII Types", "Training Dynamics", "Memorization and Leakage", "Structural Causal Model", "PII Dataset from Code", "LLM4Code Fine-tuning", "Type-aware Privacy Risk", "Leakage Attack Evaluation"], "summary": "本文构建多类型PII数据集并在不同规模和架构的代码模型上计算训练动态，基于结构因果模型实证证明不同PII类型的可学性与推理时泄露风险存在因果关联，为类型感知的防护策略提供依据。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.07814v1", "published": "2025-12-08", "update_time": "2025-12-08", "download_time": "2025-12-10 01:51:03"}
{"id": "2512.07666", "title": "Bridging Code Graphs and Large Language Models for Better Code Understanding", "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in code intelligence tasks such as code generation, summarization, and translation. However, their reliance on linearized token sequences limits their ability to understand the structural semantics of programs. While prior studies have explored graphaugmented prompting and structure-aware pretraining, they either suffer from prompt length constraints or require task-specific architectural changes that are incompatible with large-scale instructionfollowing LLMs. To address these limitations, this paper proposes CGBridge, a novel plug-and-play method that enhances LLMs with Code Graph information through an external, trainable Bridge module. CGBridge first pre-trains a code graph encoder via selfsupervised learning on a large-scale dataset of 270K code graphs to learn structural code semantics. It then trains an external module to bridge the modality gap among code, graph, and text by aligning their semantics through cross-modal attention mechanisms. Finally, the bridge module generates structure-informed prompts, which are injected into a frozen LLM, and is fine-tuned for downstream code intelligence tasks. Experiments show that CGBridge achieves notable improvements over both the original model and the graphaugmented prompting method. Specifically, it yields a 16.19% and 9.12% relative gain in LLM-as-a-Judge on code summarization, and a 9.84% and 38.87% relative gain in Execution Accuracy on code translation. Moreover, CGBridge achieves over 4x faster inference than LoRA-tuned models, demonstrating both effectiveness and efficiency in structure-aware code understanding.", "arxiv_url": "https://arxiv.org/abs/2512.07666", "authors": ["Zeqi Chen", "Zhaoyang Chu", "Yi Gui", "Feng Guo", "Yao Wan", "Chuan Shi"], "first_author": "Zeqi Chen", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Summarization", "tags": ["Code Property Graph (CPG)", "GNN-based code graph encoder", "Self-supervised graph pretraining", "Cross-modal attention / alignment", "Bridge module (plug-and-play)", "Structure-informed soft prompts", "Graph-text contrastive learning", "Code translation"], "summary": "本文提出CGBridge——一种可插拔的桥接模块，通过对约27万条异构代码图进行自监督预训练、采用跨模态对齐（图-文本对比与匹配）并生成结构感知提示注入冻结的LLM，从而在代码摘要与翻译等代码理解任务上显著提升性能并提高推理效率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.07666v1", "published": "2025-12-08", "update_time": "2025-12-08", "download_time": "2025-12-10 01:51:28"}
{"id": "2512.07631", "title": "The Agent Capability Problem: Predicting Solvability Through Information-Theoretic Bounds", "abstract": "When should an autonomous agent commit resources to a task? We introduce the Agent Capability Problem (ACP), a framework for predicting whether an agent can solve a problem under resource constraints. Rather than relying on empirical heuristics, ACP frames problem-solving as information acquisition: an agent requires $\\Itotal$ bits to identify a solution and gains $\\Istep$ bits per action at cost $\\Cstep$, yielding an effective cost $\\Ceff = (\\Itotal/\\Istep), \\Cstep$ that predicts resource requirements before search. We prove that $\\Ceff$ lower-bounds expected cost and provide tight probabilistic upper bounds. Experimental validation shows that ACP predictions closely track actual agent performance, consistently bounding search effort while improving efficiency over greedy and random strategies. The framework generalizes across LLM-based and agentic workflows, linking principles from active learning, Bayesian optimization, and reinforcement learning through a unified information-theoretic lens. \\", "arxiv_url": "https://arxiv.org/abs/2512.07631", "authors": ["Shahar Lutati"], "first_author": "Shahar Lutati", "category": ["Technical"], "field": "Agent Decision & Planning", "task": "Solvability Prediction / Resource Allocation", "tags": ["Agent Capability Problem", "Solvability Prediction", "Information-Theoretic Bounds", "Effective Cost (Ceffective)", "Mutual Information per Action", "Stopping Time Analysis", "Lorden's Inequality", "Gaussian Process Approximation", "Resource Allocation for LLM-based Agents"], "summary": "本文提出了代理能力问题（ACP），用信息论视角将解决所需信息量 Itotal 与每步信息增益 Is 及每步代价 Cs 结合成 Ceffective，并给出该量的下界与概率性上界以在资源受限下预测并引导代理求解可行性，实验以高斯过程近似验证了理论预测的有效性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.07631v1", "published": "2025-12-08", "update_time": "2025-12-08", "download_time": "2025-12-10 01:55:01"}
{"id": "2308.10620", "title": "Large Language Models for Software Engineering: A Systematic Literature Review", "abstract": "Large Language Models (LLMs) have significantly impacted numerous domains, including Software Engineering (SE). Many recent publications have explored LLMs applied to various SE tasks. Nevertheless, a comprehensive understanding of the application, effects, and possible limitations of LLMs on SE is still in its early stages. To bridge this gap, we conducted a systematic literature review (SLR) on LLM4SE, with a particular focus on understanding how LLMs can be exploited to optimize processes and outcomes. We select and analyze 395 research papers from January 2017 to January 2024 to answer four key research questions (RQs). In RQ1, we categorize different LLMs that have been employed in SE tasks, characterizing their distinctive features and uses. In RQ2, we analyze the methods used in data collection, preprocessing, and application, highlighting the role of well-curated datasets for successful LLM for SE implementation. RQ3 investigates the strategies employed to optimize and evaluate the performance of LLMs in SE. Finally, RQ4 examines the specific SE tasks where LLMs have shown success to date, illustrating their practical contributions to the field. From the answers to these RQs, we discuss the current state-of-the-art and trends, identifying gaps in existing research, and flagging promising areas for future study. Our artifacts are publicly available at https://github.com/xinyi-hou/LLM4SE_SLR.", "arxiv_url": "https://arxiv.org/abs/2308.10620", "authors": ["Xinyi Hou", "Yanjie Zhao", "Yue Liu", "Zhou Yang", "Kailong Wang", "Li Li", "Xiapu Luo", "David Lo", "John Grundy", "Haoyu Wang"], "first_author": "Xinyi Hou", "category": ["Survey"], "field": "LLM4SE", "task": "Systematic Literature Review", "tags": ["Systematic literature review", "LLM categorization for software engineering", "Data sourcing and preprocessing practices", "Prompt engineering strategies", "Instruction‑tuning & parameter‑efficient adaptation", "Evaluation metrics and benchmarking practices", "Mapping LLMs to 85 specific SE tasks", "Challenges and future research directions in LLM4SE"], "summary": "本文对2017至2024年间395篇将大型语言模型应用于软件工程的研究进行了系统文献综述，归类LLM类型与SE任务、分析数据处理与调优/评估方法，并总结应用场景、挑战与未来研究方向。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2308.10620v6", "published": "2023-08-21", "update_time": "2024-04-10", "download_time": "2025-12-10 16:14:25"}
{"id": "2308.11396", "title": "Towards an Understanding of Large Language Models in Software Engineering Tasks", "abstract": "Large Language Models (LLMs) have drawn widespread attention and research due to their astounding performance in text generation and reasoning tasks. Derivative products, like ChatGPT, have been extensively deployed and highly sought after. Meanwhile, the evaluation and optimization of LLMs in software engineering tasks, such as code generation, have become a research focus. However, there is still a lack of systematic research on applying and evaluating LLMs in software engineering. Therefore, this paper comprehensively investigate and collate the research and products combining LLMs with software engineering, aiming to answer two questions: (1) What are the current integrations of LLMs with software engineering? (2) Can LLMs effectively handle software engineering tasks? To find the answers, we have collected related literature as extensively as possible from seven mainstream databases and selected 123 timely papers published starting from 2022 for analysis. We have categorized these papers in detail and reviewed the current research status of LLMs from the perspective of seven major software engineering tasks, hoping this will help researchers better grasp the research trends and address the issues when applying LLMs. Meanwhile, we have also organized and presented papers with evaluation content to reveal the performance and effectiveness of LLMs in various software engineering tasks, guiding researchers and developers to optimize.", "arxiv_url": "https://arxiv.org/abs/2308.11396", "authors": ["Zibin Zheng", "Kaiwen Ning", "Qingyuan Zhong", "Jiachi Chen", "Wenqing Chen", "Lianghong Guo", "Weicheng Wang", "Yanlin Wang"], "first_author": "Zibin Zheng", "category": ["Survey", "Empirical"], "field": "LLM for Software Engineering", "task": "Survey & Cross-task Evaluation", "tags": ["Systematic Literature Review", "Cross-task Evaluation", "Code Generation", "Test Case Generation", "Vulnerability Detection", "Tool/Product Survey", "Research Trend Analysis"], "summary": "本文系统收集并分析了自2022年起的123篇相关论文，综述了大模型在代码生成、测试、缺陷与安全检测等软件工程各类任务中的应用、评估结果与研究趋势并总结了存在的挑战。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2308.11396v3", "published": "2023-08-22", "update_time": "2024-12-10", "download_time": "2025-12-10 16:15:12"}
{"id": "2310.17903", "title": "Pitfalls in Language Models for Code Intelligence: A Taxonomy and Survey", "abstract": "Modern language models (LMs) have been successfully employed in source code generation and understanding, leading to a significant increase in research focused on learning-based code intelligence, such as automated bug repair, and test case generation. Despite their great potential, language models for code intelligence (LM4Code) are susceptible to potential pitfalls, which hinder realistic performance and further impact their reliability and applicability in real-world deployment. Such challenges drive the need for a comprehensive understanding - not just identifying these issues but delving into their possible implications and existing solutions to build more reliable language models tailored to code intelligence. Based on a well-defined systematic research approach, we conducted an extensive literature review to uncover the pitfalls inherent in LM4Code. Finally, 67 primary studies from top-tier venues have been identified. After carefully examining these studies, we designed a taxonomy of pitfalls in LM4Code research and conducted a systematic study to summarize the issues, implications, current solutions, and challenges of different pitfalls for LM4Code systems. We developed a comprehensive classification scheme that dissects pitfalls across four crucial aspects: data collection and labeling, system design and learning, performance evaluation, and deployment and maintenance. Through this study, we aim to provide a roadmap for researchers and practitioners, facilitating their understanding and utilization of LM4Code in reliable and trustworthy ways.", "arxiv_url": "https://arxiv.org/abs/2310.17903", "authors": ["Xinyu She", "Yue Liu", "Yanjie Zhao", "Yiling He", "Li Li", "Chakkrit Tantithamthavorn", "Zhan Qin", "Haoyu Wang"], "first_author": "Xinyu She", "category": ["Survey"], "field": "Model Reliability & Trustworthiness", "task": "Pitfalls Taxonomy & Evaluation", "tags": ["Pitfall Taxonomy", "Data collection and labeling issues", "Dataset contamination and leakage", "Evaluation bias and metric validity", "Model robustness and reliability", "Deployment and maintenance risks", "Mitigation strategies and best practices", "Reproducibility and transparency"], "summary": "本文基于系统文献综述构建了面向代码智能的语言模型陷阱分类法，系统梳理了数据、模型设计、性能评估与部署维护等方面的常见问题、影响及现有应对策略，并提出未来研究方向与实践建议。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2310.17903v1", "published": "2023-10-27", "update_time": "2023-10-27", "download_time": "2025-12-10 16:15:54"}
{"id": "2312.15223", "title": "A Survey on Large Language Models for Software Engineering", "abstract": "Software Engineering (SE) is the systematic design, development, maintenance, and management of software applications underpinning the digital infrastructure of our modern world. Very recently, the SE community has seen a rapidly increasing number of techniques employing Large Language Models (LLMs) to automate a broad range of SE tasks. Nevertheless, existing information of the applications, effects, and possible limitations of LLMs within SE is still not well-studied.   In this paper, we provide a systematic survey to summarize the current state-of-the-art research in the LLM-based SE community. We summarize 62 representative LLMs of Code across three model architectures, 15 pre-training objectives across four categories, and 16 downstream tasks across five categories. We then present a detailed summarization of the recent SE studies for which LLMs are commonly utilized, including 947 studies for 112 specific code-related tasks across five crucial phases within the SE workflow. We also discuss several critical aspects during the integration of LLMs into SE, such as empirical evaluation, benchmarking, security and reliability, domain tuning, compressing and distillation. Finally, we highlight several challenges and potential opportunities on applying LLMs for future SE studies, such as exploring domain LLMs and constructing clean evaluation datasets. Overall, our work can help researchers gain a comprehensive understanding about the achievements of the existing LLM-based SE studies and promote the practical application of these techniques. Our artifacts are publicly available and will be continuously updated at the living repository: https://github.com/iSEngLab/AwesomeLLM4SE.", "arxiv_url": "https://arxiv.org/abs/2312.15223", "authors": ["Quanjun Zhang", "Chunrong Fang", "Yang Xie", "Yaxin Zhang", "Yun Yang", "Weisong Sun", "Shengcheng Yu", "Zhenyu Chen"], "first_author": "Quanjun Zhang", "category": ["Survey"], "field": "LLMs for Software Engineering", "task": "Comprehensive survey of LLM applications across the SE workflow", "tags": ["Model architecture taxonomy for code LLMs", "Pre-training objective categorization", "Downstream code-task taxonomy", "Mapping of 112 code-related tasks to SE phases", "Evaluation and benchmarking practices for code LLMs", "Security and reliability concerns in LLM4SE", "Domain adaptation / domain tuning for code models", "Model compression and distillation for code LLMs", "Living repository for tracking LLM4SE research"], "summary": "本文对将大语言模型应用于软件工程的研究进行了系统综述，汇总了代码领域的模型类别、预训练目标、下游任务与112项具体任务，讨论了评估、基准、可靠性、领域调优与模型压缩等整合问题并提供持续更新的资源库与未来研究方向。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2312.15223v2", "published": "2023-12-23", "update_time": "2024-09-08", "download_time": "2025-12-10 16:16:35"}
{"id": "2401.00288", "title": "Deep Learning for Code Intelligence: Survey, Benchmark and Toolkit", "abstract": "Code intelligence leverages machine learning techniques to extract knowledge from extensive code corpora, with the aim of developing intelligent tools to improve the quality and productivity of computer programming. Currently, there is already a thriving research community focusing on code intelligence, with efforts ranging from software engineering, machine learning, data mining, natural language processing, and programming languages. In this paper, we conduct a comprehensive literature review on deep learning for code intelligence, from the aspects of code representation learning, deep learning techniques, and application tasks. We also benchmark several state-of-the-art neural models for code intelligence, and provide an open-source toolkit tailored for the rapid prototyping of deep-learning-based code intelligence models. In particular, we inspect the existing code intelligence models under the basis of code representation learning, and provide a comprehensive overview to enhance comprehension of the present state of code intelligence. Furthermore, we publicly release the source code and data resources to provide the community with a ready-to-use benchmark, which can facilitate the evaluation and comparison of existing and future code intelligence models (https://xcodemind.github.io). At last, we also point out several challenging and promising directions for future research.", "arxiv_url": "https://arxiv.org/abs/2401.00288", "authors": ["Yao Wan", "Yang He", "Zhangqian Bi", "Jianguo Zhang", "Hongyu Zhang", "Yulei Sui", "Guandong Xu", "Hai Jin", "Philip S. Yu"], "first_author": "Yao Wan", "category": ["Survey", "Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Representation Learning", "tags": ["Code Representation Learning", "Pre-trained Code Language Models", "Open-source Benchmarking Toolkit", "Cross-task Evaluation (summarization, search, completion, type inference)", "AST-path and Structural Code Features", "Evaluation Protocols and Metrics"], "summary": "本文全面综述了基于深度学习的代码智能研究，提出并开源了用于快速原型和评测的工具平台（NaturalCC），并对多种代码预训练模型在代码摘要、代码检索、代码补全与类型推断等任务上进行了统一基准评测。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2401.00288v1", "published": "2023-12-30", "update_time": "2023-12-30", "download_time": "2025-12-10 16:17:26"}
{"id": "2512.08867", "title": "SimpleDevQA: Benchmarking Large Language Models on Development Knowledge QA", "abstract": "The Development Knowledge Question Answering (Dev Knowledge QA) task aims to provide natural language answers to knowledge-seeking questions during software development. To investigate its importance and to what extent it has been explored, we analyze real user-LLM dialogues from WildChat and find that: (1) The Dev Knowledge QA task accounts for 39.6% of interactions(highest among all tasks), revealing broad knowledge needs beyond code generation (32.3%). (2) Only 27.5% of real Dev Knowledge QA dialogues focus on code understanding, leaving out development knowledge-seeking. (3) Only 17.1% of real-world Dev Knowledge QA dialogues can be used for constructing a benchmark. Existing benchmarks have two primary limitations for evaluating the Dev Knowledge QA capability of LLMs. First, existing benchmarks offer a limited development knowledge scope, mainly focusing on code understanding and neglecting broader knowledge during development. Second, some benchmarks are not built from real user queries. To bridge this gap, we design a three-phase pipeline that transforms real-world dialogue into simple development knowledge-seeking QA pairs. Through this pipeline, we introduce SimpleDevQA, a multilingual benchmark derived from real user dialogues. It contains 2,740 QA pairs in three languages (English, Chinese, and Russian), and focuses on questions with unique, short, and verifiable answers for accurate and simple evaluation. Experiments show that: Code LLMs generally outperform general LLMs of similar scale; Knowledge injection with the Retrieval-Augmented Generation (RAG) strategy can boost LLM accuracy by 11.3% on average; LLMs show systematic overconfidence in Dev Knowledge QA, and the answering accuracy of LLMs shows a positive correlation with their stated confidence; Generally, LLMs with stronger code generation performance also exhibit stronger performance in Dev Knowledge QA.", "arxiv_url": "https://arxiv.org/abs/2512.08867", "authors": ["Jing Zhang", "Lianghong Guo", "Yanlin Wang", "Mingwei Liu", "Jiachi Chen", "Yuchi Ma", "Ensheng Shi", "Terry Yue Zhuo", "Hongyu Zhang", "Zibin Zheng"], "first_author": "Jing Zhang", "category": ["Benchmark", "Empirical"], "field": "Development Knowledge", "task": "Development Knowledge QA", "tags": ["Benchmark Construction", "Dev Knowledge QA", "Multilingual Dataset", "WildChat Dialogue Mining", "Data Pipeline", "Reference Retrieval", "RAG (Retrieval-Augmented Generation)", "LLM Evaluation", "Factuality Verification", "Human Annotation", "Closed-source vs Open-source Comparison", "Code LLM vs General LLM Performance", "Overconfidence Analysis"], "summary": "本文提出SimpleDevQA——一个基于真实用户对话、覆盖英中俄三语的开发知识问答基准及其三阶段构建流水线，并通过对18种主流LLM的评测揭示了RAG可提升性能、闭源/代码模型普遍优于开源/通用模型以及模型过度自信等现象。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.08867v1", "published": "2025-12-09", "update_time": "2025-12-09", "download_time": "2025-12-11 01:52:20"}
{"id": "2512.08810", "title": "Multicalibration for LLM-based Code Generation", "abstract": "As AI-based code generation becomes widespread, researchers are investigating the calibration of code LLMs - ensuring their confidence scores faithfully represent the true likelihood of code correctness. To do so, we investigate multicalibration, which can capture additional factors about a coding problem, such as complexity, code length, or programming language used. We study four multicalibration approaches on three function synthesis benchmarks, using latest-generation code LLMs (Qwen3 Coder, GPT-OSS, DeepSeek-R1-Distill). Our results demonstrate that multicalibration can yield distinct improvements over both uncalibrated token likelihoods (+1.03 in skill score) and baseline calibrations (+0.37 in skill score). We study the influence of the aforementioned factors in ablations, and make our dataset (consisting of code generations, likelihoods, and correctness labels) available for future research on code LLM calibration.", "arxiv_url": "https://arxiv.org/abs/2512.08810", "authors": ["Viola Campos", "Robin Kuschnereit", "Adrian Ulges"], "first_author": "Viola Campos", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Uncertainty Estimation & Calibration", "tags": ["Multicalibration", "Post-hoc calibration", "Uncertainty estimation", "Group-aware calibration (complexity/code-length/prompt-length/language)", "Brier Skill Score", "Expected Calibration Error", "Token likelihood aggregation", "CALIBRI dataset", "Function synthesis benchmarks", "Code LLM evaluation (Qwen3/GPT-OSS/DeepSeek)"], "summary": "本文首次将多组校准(multicalibration)方法应用于代码生成LLM，通过按复杂度、代码/提示长度及编程语言分组的后验校准显著提高置信度估计的可靠性，并发布了包含生成代码、似然与正确性标签的CALIBRI数据集以促进后续研究。", "quality": "High", "conference": "AI-SQE 2026 (The 1st International Workshop on AI for Software Quality Evaluation: Judgment, Metrics, Benchmarks, and Beyond) 2026", "pdf_url": "https://arxiv.org/pdf/2512.08810v1", "published": "2025-12-09", "update_time": "2025-12-09", "download_time": "2025-12-11 01:52:53"}
{"id": "2512.08769", "title": "A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows", "abstract": "Agentic AI marks a major shift in how autonomous systems reason, plan, and execute multi-step tasks. Unlike traditional single model prompting, agentic workflows integrate multiple specialized agents with different Large Language Models(LLMs), tool-augmented capabilities, orchestration logic, and external system interactions to form dynamic pipelines capable of autonomous decision-making and action. As adoption accelerates across industry and research, organizations face a central challenge: how to design, engineer, and operate production-grade agentic AI workflows that are reliable, observable, maintainable, and aligned with safety and governance requirements. This paper provides a practical, end-to-end guide for designing, developing, and deploying production-quality agentic AI systems. We introduce a structured engineering lifecycle encompassing workflow decomposition, multi-agent design patterns, Model Context Protocol(MCP), and tool integration, deterministic orchestration, Responsible-AI considerations, and environment-aware deployment strategies. We then present nine core best practices for engineering production-grade agentic AI workflows, including tool-first design over MCP, pure-function invocation, single-tool and single-responsibility agents, externalized prompt management, Responsible-AI-aligned model-consortium design, clean separation between workflow logic and MCP servers, containerized deployment for scalable operations, and adherence to the Keep it Simple, Stupid (KISS) principle to maintain simplicity and robustness. To demonstrate these principles in practice, we present a comprehensive case study: a multimodal news-analysis and media-generation workflow. By combining architectural guidance, operational patterns, and practical implementation insights, this paper offers a foundational reference to build robust, extensible, and production-ready agentic AI workflows.", "arxiv_url": "https://arxiv.org/abs/2512.08769", "authors": ["Eranga Bandara", "Ross Gore", "Peter Foytik", "Sachin Shetty", "Ravi Mukkamala", "Abdul Rahman", "Xueping Liang", "Safdar H. Bouk", "Amin Hass", "Sachini Rajapakse", "Ng Wee Keong", "Kasun De Zoysa", "Aruna Withanage", "Nilaan Loganathan"], "first_author": "Eranga Bandara", "category": ["Survey", "Technical"], "field": "Requirements & Design", "task": "Management", "tags": ["Agentic AI", "Multi-Agent Workflows", "Model Context Protocol (MCP)", "Tool Integration", "Orchestration", "Deterministic Execution", "Responsible AI", "Deployment & Containerization", "Prompt Management", "Single-Responsibility Agents", "Case Study: Multimodal News Analysis"], "summary": "本文提供了一套面向生产的 agentic AI 工作流的端到端工程指南，包含工作流分解、多代理设计模式、Model Context Protocol、工具集成、确定性编排、责任化 AI 考量及部署策略，并通过多模态新闻分析案例演示最佳实践。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.08769v1", "published": "2025-12-09", "update_time": "2025-12-09", "download_time": "2025-12-11 01:57:12"}
{"id": "2204.02311", "title": "PaLM: Scaling Language Modeling with Pathways", "abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.", "arxiv_url": "https://arxiv.org/abs/2204.02311", "authors": ["Aakanksha Chowdhery", "Sharan Narang", "Jacob Devlin", "Maarten Bosma", "Gaurav Mishra", "Adam Roberts", "Paul Barham", "Hyung Won Chung", "Charles Sutton", "Sebastian Gehrmann", "Parker Schuh", "Kensen Shi", "Sasha Tsvyashchenko", "Joshua Maynez", "Abhishek Rao", "Parker Barnes", "Yi Tay", "Noam Shazeer", "Vinodkumar Prabhakaran", "Emily Reif", "Nan Du", "Ben Hutchinson", "Reiner Pope", "James Bradbury", "Jacob Austin", "Michael Isard", "Guy Gur-Ari", "Pengcheng Yin", "Toju Duke", "Anselm Levskaya", "Sanjay Ghemawat", "Sunipa Dev", "Henryk Michalewski", "Xavier Garcia", "Vedant Misra", "Kevin Robinson", "Liam Fedus", "Denny Zhou", "Daphne Ippolito", "David Luan", "Hyeontaek Lim", "Barret Zoph", "Alexander Spiridonov", "Ryan Sepassi", "David Dohan", "Shivani Agrawal", "Mark Omernick", "Andrew M. Dai", "Thanumalayan Sankaranarayana Pillai", "Marie Pellat", "Aitor Lewkowycz", "Erica Moreira", "Rewon Child", "Oleksandr Polozov", "Katherine Lee", "Zongwei Zhou", "Xuezhi Wang", "Brennan Saeta", "Mark Diaz", "Orhan Firat", "Michele Catasta", "Jason Wei", "Kathy Meier-Hellstern", "Douglas Eck", "Jeff Dean", "Slav Petrov", "Noah Fiedel"], "first_author": "Aakanksha Chowdhery", "category": ["Technical"], "field": "Pretraining & Scaling", "task": "Autoregressive pretraining and multi‑pod training infrastructure", "tags": ["Multi-Pod TPU v4 scaling", "Pipeline-free distributed training", "Autoregressive pretraining at 540B parameters", "Few-shot generalization evaluation", "Multi-step mathematical reasoning", "Code generation and evaluation", "Memorization and data contamination analysis", "Bias and toxicity analysis", "Training stability and hyperparameter techniques", "FLOPs and hardware utilization optimization"], "summary": "本文提出并训练了一个5400亿参数的自回归语言模型，利用跨多Pod的高效分布式训练基础设施进行规模化预训练，在少样本学习、多步骤推理、代码生成与多语种任务上取得了显著进展，并对记忆、数据污染、偏见与毒性等进行了系统分析。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2204.02311v5", "published": "2022-04-05", "update_time": "2022-10-05", "download_time": "2025-12-11 15:35:20"}
{"id": "2211.05100", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model", "abstract": "Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.", "arxiv_url": "https://arxiv.org/abs/2211.05100", "authors": ["BigScience Workshop", ":", "Teven Le Scao", "Angela Fan", "Christopher Akiki", "Ellie Pavlick", "Suzana Ilić", "Daniel Hesslow", "Roman Castagné", "Alexandra Sasha Luccioni", "François Yvon", "Matthias Gallé", "Jonathan Tow", "Alexander M. Rush", "Stella Biderman", "Albert Webson", "Pawan Sasanka Ammanamanchi", "Thomas Wang", "Benoît Sagot", "Niklas Muennighoff", "Albert Villanova del Moral", "Olatunji Ruwase", "Rachel Bawden", "Stas Bekman", "Angelina McMillan-Major", "Iz Beltagy", "Huu Nguyen", "Lucile Saulnier", "Samson Tan", "Pedro Ortiz Suarez", "Victor Sanh", "Hugo Laurençon", "Yacine Jernite", "Julien Launay", "Margaret Mitchell", "Colin Raffel", "Aaron Gokaslan", "Adi Simhi", "Aitor Soroa", "Alham Fikri Aji", "Amit Alfassy", "Anna Rogers", "Ariel Kreisberg Nitzav", "Canwen Xu", "Chenghao Mou", "Chris Emezue", "Christopher Klamm", "Colin Leong", "Daniel van Strien", "David Ifeoluwa Adelani", "Dragomir Radev", "Eduardo González Ponferrada", "Efrat Levkovizh", "Ethan Kim", "Eyal Bar Natan", "Francesco De Toni", "Gérard Dupont", "Germán Kruszewski", "Giada Pistilli", "Hady Elsahar", "Hamza Benyamina", "Hieu Tran", "Ian Yu", "Idris Abdulmumin", "Isaac Johnson", "Itziar Gonzalez-Dios", "Javier de la Rosa", "Jenny Chim", "Jesse Dodge", "Jian Zhu", "Jonathan Chang", "Jörg Frohberg", "Joseph Tobing", "Joydeep Bhattacharjee", "Khalid Almubarak", "Kimbo Chen", "Kyle Lo", "Leandro Von Werra", "Leon Weber", "Long Phan", "Loubna Ben allal", "Ludovic Tanguy", "Manan Dey", "Manuel Romero Muñoz", "Maraim Masoud", "María Grandury", "Mario Šaško", "Max Huang", "Maximin Coavoux", "Mayank Singh", "Mike Tian-Jian Jiang", "Minh Chien Vu", "Mohammad A. Jauhar", "Mustafa Ghaleb", "Nishant Subramani", "Nora Kassner", "Nurulaqilla Khamis", "Olivier Nguyen", "Omar Espejel", "Ona de Gibert", "Paulo Villegas", "Peter Henderson", "Pierre Colombo", "Priscilla Amuok", "Quentin Lhoest", "Rheza Harliman", "Rishi Bommasani", "Roberto Luis López", "Rui Ribeiro", "Salomey Osei", "Sampo Pyysalo", "Sebastian Nagel", "Shamik Bose", "Shamsuddeen Hassan Muhammad", "Shanya Sharma", "Shayne Longpre", "Somaieh Nikpoor", "Stanislav Silberberg", "Suhas Pai", "Sydney Zink", "Tiago Timponi Torrent", "Timo Schick", "Tristan Thrush", "Valentin Danchev", "Vassilina Nikoulina", "Veronika Laippala", "Violette Lepercq", "Vrinda Prabhu", "Zaid Alyafeai", "Zeerak Talat", "Arun Raja", "Benjamin Heinzerling", "Chenglei Si", "Davut Emre Taşar", "Elizabeth Salesky", "Sabrina J. Mielke", "Wilson Y. Lee", "Abheesht Sharma", "Andrea Santilli", "Antoine Chaffin", "Arnaud Stiegler", "Debajyoti Datta", "Eliza Szczechla", "Gunjan Chhablani", "Han Wang", "Harshit Pandey", "Hendrik Strobelt", "Jason Alan Fries", "Jos Rozen", "Leo Gao", "Lintang Sutawika", "M Saiful Bari", "Maged S. Al-shaibani", "Matteo Manica", "Nihal Nayak", "Ryan Teehan", "Samuel Albanie", "Sheng Shen", "Srulik Ben-David", "Stephen H. Bach", "Taewoon Kim", "Tali Bers", "Thibault Fevry", "Trishala Neeraj", "Urmish Thakker", "Vikas Raunak", "Xiangru Tang", "Zheng-Xin Yong", "Zhiqing Sun", "Shaked Brody", "Yallow Uri", "Hadar Tojarieh", "Adam Roberts", "Hyung Won Chung", "Jaesung Tae", "Jason Phang", "Ofir Press", "Conglong Li", "Deepak Narayanan", "Hatim Bourfoune", "Jared Casper", "Jeff Rasley", "Max Ryabinin", "Mayank Mishra", "Minjia Zhang", "Mohammad Shoeybi", "Myriam Peyrounette", "Nicolas Patry", "Nouamane Tazi", "Omar Sanseviero", "Patrick von Platen", "Pierre Cornette", "Pierre François Lavallée", "Rémi Lacroix", "Samyam Rajbhandari", "Sanchit Gandhi", "Shaden Smith", "Stéphane Requena", "Suraj Patil", "Tim Dettmers", "Ahmed Baruwa", "Amanpreet Singh", "Anastasia Cheveleva", "Anne-Laure Ligozat", "Arjun Subramonian", "Aurélie Névéol", "Charles Lovering", "Dan Garrette", "Deepak Tunuguntla", "Ehud Reiter", "Ekaterina Taktasheva", "Ekaterina Voloshina", "Eli Bogdanov", "Genta Indra Winata", "Hailey Schoelkopf", "Jan-Christoph Kalo", "Jekaterina Novikova", "Jessica Zosa Forde", "Jordan Clive", "Jungo Kasai", "Ken Kawamura", "Liam Hazan", "Marine Carpuat", "Miruna Clinciu", "Najoung Kim", "Newton Cheng", "Oleg Serikov", "Omer Antverg", "Oskar van der Wal", "Rui Zhang", "Ruochen Zhang", "Sebastian Gehrmann", "Shachar Mirkin", "Shani Pais", "Tatiana Shavrina", "Thomas Scialom", "Tian Yun", "Tomasz Limisiewicz", "Verena Rieser", "Vitaly Protasov", "Vladislav Mikhailov", "Yada Pruksachatkun", "Yonatan Belinkov", "Zachary Bamberger", "Zdeněk Kasner", "Alice Rueda", "Amanda Pestana", "Amir Feizpour", "Ammar Khan", "Amy Faranak", "Ana Santos", "Anthony Hevia", "Antigona Unldreaj", "Arash Aghagol", "Arezoo Abdollahi", "Aycha Tammour", "Azadeh HajiHosseini", "Bahareh Behroozi", "Benjamin Ajibade", "Bharat Saxena", "Carlos Muñoz Ferrandis", "Daniel McDuff", "Danish Contractor", "David Lansky", "Davis David", "Douwe Kiela", "Duong A. Nguyen", "Edward Tan", "Emi Baylor", "Ezinwanne Ozoani", "Fatima Mirza", "Frankline Ononiwu", "Habib Rezanejad", "Hessie Jones", "Indrani Bhattacharya", "Irene Solaiman", "Irina Sedenko", "Isar Nejadgholi", "Jesse Passmore", "Josh Seltzer", "Julio Bonis Sanz", "Livia Dutra", "Mairon Samagaio", "Maraim Elbadri", "Margot Mieskes", "Marissa Gerchick", "Martha Akinlolu", "Michael McKenna", "Mike Qiu", "Muhammed Ghauri", "Mykola Burynok", "Nafis Abrar", "Nazneen Rajani", "Nour Elkott", "Nour Fahmy", "Olanrewaju Samuel", "Ran An", "Rasmus Kromann", "Ryan Hao", "Samira Alizadeh", "Sarmad Shubber", "Silas Wang", "Sourav Roy", "Sylvain Viguier", "Thanh Le", "Tobi Oyebade", "Trieu Le", "Yoyo Yang", "Zach Nguyen", "Abhinav Ramesh Kashyap", "Alfredo Palasciano", "Alison Callahan", "Anima Shukla", "Antonio Miranda-Escalada", "Ayush Singh", "Benjamin Beilharz", "Bo Wang", "Caio Brito", "Chenxi Zhou", "Chirag Jain", "Chuxin Xu", "Clémentine Fourrier", "Daniel León Periñán", "Daniel Molano", "Dian Yu", "Enrique Manjavacas", "Fabio Barth", "Florian Fuhrimann", "Gabriel Altay", "Giyaseddin Bayrak", "Gully Burns", "Helena U. Vrabec", "Imane Bello", "Ishani Dash", "Jihyun Kang", "John Giorgi", "Jonas Golde", "Jose David Posada", "Karthik Rangasai Sivaraman", "Lokesh Bulchandani", "Lu Liu", "Luisa Shinzato", "Madeleine Hahn de Bykhovetz", "Maiko Takeuchi", "Marc Pàmies", "Maria A Castillo", "Marianna Nezhurina", "Mario Sänger", "Matthias Samwald", "Michael Cullan", "Michael Weinberg", "Michiel De Wolf", "Mina Mihaljcic", "Minna Liu", "Moritz Freidank", "Myungsun Kang", "Natasha Seelam", "Nathan Dahlberg", "Nicholas Michio Broad", "Nikolaus Muellner", "Pascale Fung", "Patrick Haller", "Ramya Chandrasekhar", "Renata Eisenberg", "Robert Martin", "Rodrigo Canalli", "Rosaline Su", "Ruisi Su", "Samuel Cahyawijaya", "Samuele Garda", "Shlok S Deshmukh", "Shubhanshu Mishra", "Sid Kiblawi", "Simon Ott", "Sinee Sang-aroonsiri", "Srishti Kumar", "Stefan Schweter", "Sushil Bharati", "Tanmay Laud", "Théo Gigant", "Tomoya Kainuma", "Wojciech Kusa", "Yanis Labrak", "Yash Shailesh Bajaj", "Yash Venkatraman", "Yifan Xu", "Yingxin Xu", "Yu Xu", "Zhe Tan", "Zhongli Xie", "Zifan Ye", "Mathilde Bras", "Younes Belkada", "Thomas Wolf"], "first_author": "BigScience Workshop", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Pre-Training", "tags": ["Decoder-only Transformer", "Multilingual pretraining corpus", "Mixed natural-language and programming-language pretraining", "Multitask prompted fine-tuning", "Responsible open-access licensing", "Collaborative large-scale training engineering"], "summary": "本文介绍了由全球协作开发的1760亿参数开源多语种解码器Transformer模型及其用于混合自然语言与编程语言的大规模语料的预训练、随后通过多任务提示微调提升性能，并以负责任许可公开发布模型与代码。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2211.05100v4", "published": "2022-11-09", "update_time": "2023-06-27", "download_time": "2025-12-11 15:36:19"}
{"id": "2412.18843", "title": "Improving the Readability of Automatically Generated Tests using Large Language Models", "abstract": "Search-based test generators are effective at producing unit tests with high coverage. However, such automatically generated tests have no meaningful test and variable names, making them hard to understand and interpret by developers. On the other hand, large language models (LLMs) can generate highly readable test cases, but they are not able to match the effectiveness of search-based generators, in terms of achieved code coverage.   In this paper, we propose to combine the effectiveness of search-based generators with the readability of LLM generated tests. Our approach focuses on improving test and variable names produced by search-based tools, while keeping their semantics (i.e., their coverage) unchanged.   Our evaluation on nine industrial and open source LLMs show that our readability improvement transformations are overall semantically-preserving and stable across multiple repetitions. Moreover, a human study with ten professional developers, show that our LLM-improved tests are as readable as developer-written tests, regardless of the LLM employed.", "arxiv_url": "https://arxiv.org/abs/2412.18843", "authors": ["Matteo Biagiola", "Gianluca Ghislotti", "Paolo Tonella"], "first_author": "Matteo Biagiola", "category": ["Technical", "Empirical"], "field": "Software Testing", "task": "Test Generation", "tags": ["Automated unit-test readability improvement", "Identifier and test-name renaming", "Semantic-preserving transformations", "Multi-step prompting to limit context", "Coverage-preservation validation", "Stability analysis across LLM runs", "Human developer readability study", "Integration with search-based generators (Evosuite)"], "summary": "本文提出一种基于多步提示的技术，利用大型语言模型对搜索驱动（如Evosuite）自动生成的单元测试进行语义保留的标识符与测试名重命名以提升可读性，并通过九个LLM的实验与人工评估验证了其在保持覆盖率的同时可读性和稳定性。", "quality": "High", "conference": "IEEE Conference on Software Testing, Verification and Validation (ICST) 2025", "pdf_url": "https://arxiv.org/pdf/2412.18843v1", "published": "2024-12-25", "update_time": "2024-12-25", "download_time": "2025-12-11 15:40:05"}
{"id": "2501.00217", "title": "The Potential of LLMs in Automating Software Testing: From Generation to Reporting", "abstract": "Having a high quality software is essential in software engineering, which requires robust validation and verification processes during testing activities. Manual testing, while effective, can be time consuming and costly, leading to an increased demand for automated methods. Recent advancements in Large Language Models (LLMs) have significantly influenced software engineering, particularly in areas like requirements analysis, test automation, and debugging. This paper explores an agent-oriented approach to automated software testing, using LLMs to reduce human intervention and enhance testing efficiency. The proposed framework integrates LLMs to generate unit tests, visualize call graphs, and automate test execution and reporting. Evaluations across multiple applications in Python and Java demonstrate the system's high test coverage and efficient operation. This research underscores the potential of LLM-powered agents to streamline software testing workflows while addressing challenges in scalability and accuracy.", "arxiv_url": "https://arxiv.org/abs/2501.00217", "authors": ["Betim Sherifi", "Khaled Slhoub", "Fitzroy Nembhard"], "first_author": "Betim Sherifi", "category": ["Technical"], "field": "Software Testing", "task": "Test Generation", "tags": ["Multi-agent testing framework", "Automated unit test generation", "Natural-language prompt-driven testing", "Call-graph DOT visualization", "Automated test execution and PDF reporting", "Code file locator and extraction", "Cross-language evaluation (Python, Java)", "Test rationale generation"], "summary": "本文提出了一个由多智能体与大型语言模型驱动的自动化软件测试框架，能够基于自然语言输入自动生成单元测试、可视化调用图并执行测试与生成报告，并在多个 Python 与 Java 应用上进行案例评估。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2501.00217v1", "published": "2024-12-31", "update_time": "2024-12-31", "download_time": "2025-12-11 15:40:40"}
{"id": "2501.01329", "title": "The Prompt Alchemist: Automated LLM-Tailored Prompt Optimization for Test Case Generation", "abstract": "Test cases are essential for validating the reliability and quality of software applications. Recent studies have demonstrated the capability of Large Language Models (LLMs) to generate useful test cases for given source code. However, the existing work primarily relies on human-written plain prompts, which often leads to suboptimal results since the performance of LLMs can be highly influenced by the prompts. Moreover, these approaches use the same prompt for all LLMs, overlooking the fact that different LLMs might be best suited to different prompts. Given the wide variety of possible prompt formulations, automatically discovering the optimal prompt for each LLM presents a significant challenge. Although there are methods on automated prompt optimization in the natural language processing field, they are hard to produce effective prompts for the test case generation task. First, the methods iteratively optimize prompts by simply combining and mutating existing ones without proper guidance, resulting in prompts that lack diversity and tend to repeat the same errors in the generated test cases. Second, the prompts are generally lack of domain contextual knowledge, limiting LLMs' performance in the task.", "arxiv_url": "https://arxiv.org/abs/2501.01329", "authors": ["Shuzheng Gao", "Chaozheng Wang", "Cuiyun Gao", "Xiaoqian Jiao", "Chun Yong Chong", "Shan Gao", "Michael Lyu"], "first_author": "Shuzheng Gao", "category": ["Technical"], "field": "Software Testing", "task": "Test Generation", "tags": ["Prompt Optimization for Testing", "Diversity-Guided Prompt Generation", "Failure-Driven Rule Induction", "Domain Contextual Knowledge Extraction", "Tailored Prompts per LLM", "Reflection-based Error Mitigation", "Cross-file Inheritance and Invocation Context"], "summary": "本文提出MAPS，一种用于测试用例生成的自动化LLM定制提示优化方法，结合多样性引导、基于失败的规则归纳与领域上下文提取，为不同LLM生成定制化提示并显著提升线覆盖与分支覆盖率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2501.01329v1", "published": "2025-01-02", "update_time": "2025-01-02", "download_time": "2025-12-11 15:41:12"}
{"id": "2501.07425", "title": "Enhancing LLM's Ability to Generate More Repository-Aware Unit Tests Through Precise Contextual Information Injection", "abstract": "Though many learning-based approaches have been proposed for unit test generation and achieved remarkable performance, they still have limitations in relying on task-specific datasets. Recently, Large Language Models (LLMs) guided by prompt engineering have gained attention for their ability to handle a broad range of tasks, including unit test generation. Despite their success, LLMs may exhibit hallucinations when generating unit tests for focal methods or functions due to their lack of awareness regarding the project's global context. These hallucinations may manifest as calls to non-existent methods, as well as incorrect parameters or return values, such as mismatched parameter types or numbers. While many studies have explored the role of context, they often extract fixed patterns of context for different models and focal methods, which may not be suitable for all generation processes (e.g., excessive irrelevant context could lead to redundancy, preventing the model from focusing on essential information). To overcome this limitation, we propose RATester, which enhances the LLM's ability to generate more repository-aware unit tests through global contextual information injection. To equip LLMs with global knowledge similar to that of human testers, we integrate the language server gopls, which provides essential features (e.g., definition lookup) to assist the LLM. When RATester encounters an unfamiliar identifier (e.g., an unfamiliar struct name), it first leverages gopls to fetch relevant definitions and documentation comments, and then uses this global knowledge to guide the LLM. By utilizing gopls, RATester enriches the LLM's knowledge of the project's global context, thereby reducing hallucinations during unit test generation.", "arxiv_url": "https://arxiv.org/abs/2501.07425", "authors": ["Xin Yin", "Chao Ni", "Xinrui Li", "Liushan Chen", "Guojun Ma", "Xiaohu Yang"], "first_author": "Xin Yin", "category": ["Technical", "Benchmark"], "field": "Software Testing", "task": "Test Generation", "tags": ["Repository-aware context injection", "Language-server-assisted definition lookup", "Prompt augmentation for unit tests", "Hallucination mitigation", "Golang unit test generation", "Mutation testing evaluation", "Line-coverage improvement", "Model-agnostic testing framework"], "summary": "提出RATester，通过集成语言服务器以注入仓库级全局上下文信息，增强LLM生成更具仓库感的Golang单元测试，从而减少幻觉并显著提升覆盖率与变异检测效果。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2501.07425v1", "published": "2025-01-13", "update_time": "2025-01-13", "download_time": "2025-12-11 15:41:48"}
{"id": "2501.10200", "title": "Test Wars: A Comparative Study of SBST, Symbolic Execution, and LLM-Based Approaches to Unit Test Generation", "abstract": "Generating tests automatically is a key and ongoing area of focus in software engineering research. The emergence of Large Language Models (LLMs) has opened up new opportunities, given their ability to perform a wide spectrum of tasks. However, the effectiveness of LLM-based approaches compared to traditional techniques such as search-based software testing (SBST) and symbolic execution remains uncertain. In this paper, we perform an extensive study of automatic test generation approaches based on three tools: EvoSuite for SBST, Kex for symbolic execution, and TestSpark for LLM-based test generation. We evaluate tools performance on the GitBug Java dataset and compare them using various execution-based and feature-based metrics. Our results show that while LLM-based test generation is promising, it falls behind traditional methods in terms of coverage. However, it significantly outperforms them in mutation scores, suggesting that LLMs provide a deeper semantic understanding of code. LLM-based approach also performed worse than SBST and symbolic execution-based approaches w.r.t. fault detection capabilities. Additionally, our feature-based analysis shows that all tools are primarily affected by the complexity and internal dependencies of the class under test (CUT), with LLM-based approaches being especially sensitive to the CUT size.", "arxiv_url": "https://arxiv.org/abs/2501.10200", "authors": ["Azat Abdullin", "Pouria Derakhshanfar", "Annibale Panichella"], "first_author": "Azat Abdullin", "category": ["Empirical"], "field": "Software Testing", "task": "Test Generation", "tags": ["Comparative evaluation of SBST, symbolic execution, and LLM-based tests", "Coverage vs. mutation-score tradeoffs", "Statistical analysis of nondeterministic LLM outputs", "Data-leakage-aware benchmark selection", "Sensitivity to class-under-test complexity and size", "Automated, extensible test-generation evaluation pipeline", "Execution- and feature-based metrics", "Repeatability via multiple seeds and independent sessions"], "summary": "本文在为避免数据泄露的GitBug Java基准上，使用多次随机重复与统计检验比较了EvoSuite（SBST）、Kex（符号执行）和基于LLM的TestSpark的单元测试生成性能，发现LLM在变异得分上表现优异但在覆盖率与缺陷检测上落后，并分析了类复杂度等对方法效果的影响。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2501.10200v1", "published": "2025-01-17", "update_time": "2025-01-17", "download_time": "2025-12-11 15:42:22"}
{"id": "2501.11086", "title": "Can LLM Generate Regression Tests for Software Commits?", "abstract": "Large Language Models (LLMs) have shown tremendous promise in automated software engineering. In this paper, we investigate the opportunities of LLMs for automatic regression test generation for programs that take highly structured, human-readable inputs, such as XML parsers or JavaScript interpreters. Concretely, we explore the following regression test generation scenarios for such programs that have so far been difficult to test automatically in the absence of corresponding input grammars:   $\\bullet$ Bug finding. Given a code change (e.g., a commit or pull request), our LLM-based approach generates a test case with the objective of revealing any bugs that might be introduced if that change is applied.   $\\bullet$ Patch testing. Given a patch, our LLM-based approach generates a test case that fails before but passes after the patch. This test can be added to the regression test suite to catch similar bugs in the future.   We implement Cleverest, a feedback-directed, zero-shot LLM-based regression test generation technique, and evaluate its effectiveness on 22 commits to three subject programs: Mujs, Libxml2, and Poppler. For programs using more human-readable file formats, like XML or JavaScript, we found Cleverest performed very well. It generated easy-to-understand bug-revealing or bug-reproduction test cases for the majority of commits in just under three minutes -- even when only the code diff or commit message (unless it was too vague) was given. For programs with more compact file formats, like PDF, as expected, it struggled to generate effective test cases. However, the LLM-supplied test cases are not very far from becoming effective (e.g., when used as a seed by a greybox fuzzer or as a starting point by the developer).", "arxiv_url": "https://arxiv.org/abs/2501.11086", "authors": ["Jing Liu", "Seongmin Lee", "Eleonora Losiouk", "Marcel Böhme"], "first_author": "Jing Liu", "category": ["Technical", "Empirical"], "field": "Software Testing", "task": "Test Generation", "tags": ["Feedback-directed zero-shot test generation", "Commit-diff and commit-message prompting", "Regression test generation for structured human-readable inputs", "Patch testing and bug reproduction", "Seed generation for greybox fuzzers", "Comparison to directed greybox fuzzing", "Hyperparameter and ablation analysis", "Limitations on compact/binary input formats"], "summary": "本文提出 Cleverest——一种基于大语言模型的反馈驱动零样本回归测试生成技术，能够从提交的 diff 或提交信息自动合成针对人类可读结构化输入（如 XML/JavaScript）的回归测试，并在多数案例中快速找到或复现 bug，且可作为灰盒模糊测试的种子提升发现能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2501.11086v1", "published": "2025-01-19", "update_time": "2025-01-19", "download_time": "2025-12-11 15:43:00"}
{"id": "2504.08703", "title": "SWE-PolyBench: A multi-language benchmark for repository level evaluation of coding agents", "abstract": "Coding agents powered by large language models have shown impressive capabilities in software engineering tasks, but evaluating their performance across diverse programming languages and real-world scenarios remains challenging. We introduce SWE-PolyBench, a new multi-language benchmark for repository-level, execution-based evaluation of coding agents. SWE-PolyBench contains 2110 instances from 21 repositories and includes tasks in Java (165), JavaScript (1017), TypeScript (729) and Python (199), covering bug fixes, feature additions, and code refactoring. We provide a task and repository-stratified subsample (SWE-PolyBench500) and release an evaluation harness allowing for fully automated evaluation. To enable a more comprehensive comparison of coding agents, this work also presents a novel set of metrics rooted in syntax tree analysis. We evaluate leading open source coding agents on SWE-PolyBench, revealing their strengths and limitations across languages, task types, and complexity classes. Our experiments show that current agents exhibit uneven performances across languages and struggle with complex problems while showing higher performance on simpler tasks. SWE-PolyBench aims to drive progress in developing more versatile and robust AI coding assistants for real-world software engineering. Our datasets and code are available at: https://github.com/amazon-science/SWE-PolyBench", "arxiv_url": "https://arxiv.org/abs/2504.08703", "authors": ["Muhammad Shihab Rashid", "Christian Bock", "Yuan Zhuang", "Alexander Buchholz", "Tim Esler", "Simon Valentin", "Luca Franceschi", "Martin Wistuba", "Prabhu Teja Sivaprasad", "Woo Jung Kim", "Anoop Deoras", "Giovanni Zappella", "Laurent Callot"], "first_author": "Muhammad Shihab Rashid", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Repository-level code edits", "Pull-request-derived tasks", "Multi-language benchmark", "Execution-based evaluation with unit tests", "Concrete Syntax Tree (CST) node metrics", "File- and node-retrieval metrics", "Stratified 500-sample subset for fast iteration", "Bug/feature/refactor task taxonomy", "Cross-language robustness analysis", "Automated evaluation harness"], "summary": "本文提出SWE-PolyBench，一个包含2110个基于Pull Request的多语言（Java/JavaScript/TypeScript/Python）仓库级执行型基准，并引入基于语法树的文件与节点检索度量与自动化评估工具以评测编码代理在不同语言与复杂度下的表现。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2504.08703v3", "published": "2025-04-11", "update_time": "2025-04-23", "download_time": "2025-12-11 15:46:42"}
{"id": "2504.21205", "title": "SecRepoBench: Benchmarking Code Agents for Secure Code Completion in Real-World Repositories", "abstract": "This paper introduces SecRepoBench, a benchmark to evaluate code agents on secure code completion in real-world repositories. SecRepoBench has 318 code completion tasks in 27 C/C++ repositories, covering 15 CWEs. We evaluate 28 standalone LLMs and 13 code agents across 3 state-of-the-art agent frameworks using our benchmark. We find that state-of-the-art LLMs struggle with generating correct and secure code completions. However, code agents significantly outperform standalone LLMs. We show that SecRepoBench is more difficult than the prior state-of-the-art benchmark. Finally, our comprehensive analysis provides insights into potential directions for enhancing the ability of code agents to write correct and secure code in real-world repositories.", "arxiv_url": "https://arxiv.org/abs/2504.21205", "authors": ["Chihao Shen", "Connor Dilgren", "Purva Chiniya", "Luke Griffith", "Yu Ding", "Yizheng Chen"], "first_author": "Chihao Shen", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Repository-level secure code completion", "C/C++ memory-safety and CWE coverage", "Developer-written unit-test correctness evaluation", "OSS-Fuzz PoC crash-based security testing", "AST-based masked region generation", "Semantic-preserving code mutation to prevent memorization", "Context retriever vs. agent-framework comparison", "Automated compile-and-test evaluation pipeline"], "summary": "本文提出了SecRepoBench——一个包含27个真实C/C++仓库、318个安全敏感代码补全任务的基准，结合开发者单元测试与OSS‑Fuzz PoC同时评估代码正确性与安全性，并用于比较多种独立LLM与代码代理的表现，发现代理显著优于独立模型但整体仍不足以保证正确与安全。 ", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2504.21205v2", "published": "2025-04-29", "update_time": "2025-11-05", "download_time": "2025-12-11 15:47:14"}
{"id": "2505.04606", "title": "OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue Resolution", "abstract": "The GitHub issue resolution task aims to resolve issues reported in repositories automatically. With advances in large language models (LLMs), this task has gained increasing attention, and several benchmarks are proposed to evaluate the issue resolution ability of LLMs. However, existing benchmarks have three main limitations. First, current benchmarks focus on a single programming language, limiting the evaluation of issues from repositories across different languages. Second, they usually cover a narrow range of domains, which may fail to represent the diversity of real-world issues. Third, existing benchmarks rely solely on textual information in issue descriptions, overlooking multimodal information such as images in issues. In this paper, we propose OmniGIRL, a GitHub Issue ResoLution benchmark that is multilingual, multimodal, and multi-domain. OmniGIRL includes 959 task instances, which are collected from repositories across four programming languages (i.e., Python, JavaScript, TypeScript, and Java) and eight different domains. Our evaluation shows that current LLMs show limited performances on OmniGIRL. Notably, the best-performing model, GPT-4o, resolves only 8.6% of the issues. Besides, we find that current LLMs struggle to resolve issues requiring understanding images. The best performance is achieved by Claude-3.5-Sonnet, which resolves only 10.5% of the issues with image information. Finally, we analyze the reasons behind current LLMs' failure on OmniGIRL, providing insights for future improvements.", "arxiv_url": "https://arxiv.org/abs/2505.04606", "authors": ["Lianghong Guo", "Wei Tao", "Runhan Jiang", "Yanlin Wang", "Jiachi Chen", "Xilin Liu", "Yuchi Ma", "Mingzhi Mao", "Hongyu Zhang", "Zibin Zheng"], "first_author": "Lianghong Guo", "category": ["Benchmark", "Empirical"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Multilingual issue resolution", "Multimodal (image) issue cases", "Cross‑language evaluation", "Multi‑domain repository selection", "Test-driven patch application", "Multi-file modification analysis", "LLM failure modes: formatting & parsing"], "summary": "OmniGIRL 提出一个包含 959 条实例的多语言、多模态、多领域的 GitHub 问题修复基准，并评估与分析现有大模型在跨语言、多文件和含图像问题上的有限表现与失败原因。", "quality": "High", "conference": "ISSTA 2025", "pdf_url": "https://arxiv.org/pdf/2505.04606v1", "published": "2025-05-07", "update_time": "2025-05-07", "download_time": "2025-12-11 15:48:00"}
{"id": "2505.16975", "title": "SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software Development", "abstract": "Large Language Models (LLMs) have shown strong capability in diverse software engineering tasks, e.g. code completion, bug fixing, and document generation. However, feature-driven development (FDD), a highly prevalent real-world task that involves developing new functionalities for large, existing codebases, remains underexplored. We therefore introduce SWE-Dev, the first large-scale dataset (with 14,000 training and 500 test samples) designed to evaluate and train autonomous coding systems on real-world feature development tasks. To ensure verifiable and diverse training, SWE-Dev uniquely provides all instances with a runnable environment and its developer-authored executable unit tests. This collection not only provides high-quality data for Supervised Fine-Tuning (SFT), but also enables Reinforcement Learning (RL) by delivering accurate reward signals from executable unit tests. Our extensive evaluations on SWE-Dev, covering 17 chatbot LLMs, 10 reasoning models, and 10 Multi-Agent Systems (MAS), reveal that FDD is a profoundly challenging frontier for current AI (e.g., Claude-3.7-Sonnet achieves only 22.45\\% Pass@3 on the hard test split). Crucially, we demonstrate that SWE-Dev serves as an effective platform for model improvement: fine-tuning on training set enabled a 7B model comparable to GPT-4o on \\textit{hard} split, underscoring the value of its high-quality training data. Code is available here \\href{https://github.com/DorothyDUUU/SWE-Dev}{https://github.com/DorothyDUUU/SWE-Dev}.", "arxiv_url": "https://arxiv.org/abs/2505.16975", "authors": ["Yaxin Du", "Yuzhu Cai", "Yifan Zhou", "Cheng Wang", "Yu Qian", "Xianghe Pang", "Qian Liu", "Yue Hu", "Siheng Chen"], "first_author": "Yaxin Du", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Feature-driven development", "Executable unit-test supervision", "Runnable repository environments", "Cross-file refactoring and additions", "Test-based reinforcement learning rewards", "Supervised fine-tuning for repository tasks", "Multi-agent collaboration for coding", "Functional correctness evaluation", "Long-context code generation"], "summary": "该论文构建并公开了SWE-Dev——一个含1.45万可执行训练样本与500测试样本的仓库级功能开发基准，提供可运行环境与单元测试以评估并用于SFT、RL和多智能体训练，展示了在真实特性开发任务中对模型评估与训练的有效性与挑战。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2505.16975v2", "published": "2025-05-22", "update_time": "2025-06-19", "download_time": "2025-12-11 15:48:35"}
{"id": "2505.20411", "title": "SWE-rebench: An Automated Pipeline for Task Collection and Decontaminated Evaluation of Software Engineering Agents", "abstract": "LLM-based agents have shown promising capabilities in a growing range of software engineering (SWE) tasks. However, advancing this field faces two critical challenges. First, high-quality training data is scarce, especially data that reflects real-world SWE scenarios, where agents must interact with development environments, execute code and adapt behavior based on the outcomes of their actions. Existing datasets are either limited to one-shot code generation or comprise small, manually curated collections of interactive tasks, lacking both scale and diversity. Second, the lack of fresh interactive SWE tasks affects evaluation of rapidly improving models, as static benchmarks quickly become outdated due to contamination issues. To address these limitations, we introduce a novel, automated, and scalable pipeline to continuously extract real-world interactive SWE tasks from diverse GitHub repositories. Using this pipeline, we construct SWE-rebench, a public dataset comprising over 21,000 interactive Python-based SWE tasks, suitable for reinforcement learning of SWE agents at scale. Additionally, we use continuous supply of fresh tasks collected using SWE-rebench methodology to build a contamination-free benchmark for agentic software engineering. We compare results of various LLMs on this benchmark to results on SWE-bench Verified and show that performance of some language models might be inflated due to contamination issues.", "arxiv_url": "https://arxiv.org/abs/2505.20411", "authors": ["Ibragim Badertdinov", "Alexander Golubev", "Maksim Nekrashevich", "Anton Shevtsov", "Simon Karasik", "Andrei Andriushchenko", "Maria Trofimova", "Daria Litvintseva", "Boris Yangel"], "first_author": "Ibragim Badertdinov", "category": ["Benchmark", "Technical"], "field": "Software Testing", "task": "Testing automation", "tags": ["Automated task mining", "Executable environment configuration", "Test-driven verification", "Decontamination-aware benchmarking", "Continuous dataset collection", "Reinforcement-learning-ready tasks", "GitHub PR/issue mining", "Distributed pipeline for large-scale processing"], "summary": "本文提出一个可扩展的自动化管道从 GitHub 挖掘可执行的交互式软件工程任务，发布了包含 21,000+ Python 任务的 SWE-rebench 数据集并建立了持续更新且去污染的基准与排行榜以评估软件工程代理。", "quality": "High", "conference": "NeurIPS 2025", "pdf_url": "https://arxiv.org/pdf/2505.20411v2", "published": "2025-05-26", "update_time": "2025-11-04", "download_time": "2025-12-11 15:49:05"}
{"id": "2505.20749", "title": "Can Agents Fix Agent Issues?", "abstract": "LLM-based agent systems are emerging as a new software paradigm and have been widely adopted across diverse domains such as medicine, robotics, and programming. However, maintaining these systems requires substantial effort, as they are inevitably prone to bugs and continually evolve to meet changing external requirements. Therefore, automatically resolving agent issues (i.e., bug reports or feature requests) is a crucial and challenging task. While recent software engineering (SE) agents (e.g., SWE-agent) have shown promise in addressing issues in traditional software systems, it remains unclear how effectively they can resolve real-world issues in agent systems, which differ significantly from traditional software. To fill this gap, we first manually analyze 201 real-world agent issues and identify common categories of agent issues. We then spend 500 person-hours constructing AgentIssue-Bench, a reproducible benchmark comprising 50 agent issue resolution tasks (each with an executable environment and failure-triggering tests). We further evaluate state-of-the-art SE agents on AgentIssue-Bench and reveal their limited effectiveness (i.e., with only 0.67% - 4.67% resolution rates). These results underscore the unique challenges of maintaining agent systems compared to traditional software, highlighting the need for further research to develop advanced SE agents for resolving agent issues. Data and code are available at https://github.com/alfin06/AgentIssue-Bench.", "arxiv_url": "https://arxiv.org/abs/2505.20749", "authors": ["Alfin Wijaya Rahardja", "Junwei Liu", "Weitong Chen", "Zhenpeng Chen", "Yiling Lou"], "first_author": "Alfin Wijaya Rahardja", "category": ["Empirical", "Benchmark"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Agent issue taxonomy", "Reproducible agent-issue benchmark", "Dockerized failure reproduction", "Failure-triggering tests", "SE agent empirical evaluation", "LLM nondeterminism and volatility", "Agent-specific failures (model binding, memory, tool use)", "Qualitative resolution analysis"], "summary": "本文通过对201个真实Agent系统问题的人工分析构建了包含50个可复现任务的AGENTISSUE-BENCH基准，并评估多种现有SE agents，结果显示其在修复Agent问题上成功率极低，从而揭示了维护Agent系统的独特挑战。", "quality": "High", "conference": "NeurIPS 2025", "pdf_url": "https://arxiv.org/pdf/2505.20749v4", "published": "2025-05-27", "update_time": "2025-10-24", "download_time": "2025-12-11 15:49:37"}
{"id": "2505.22583", "title": "GitGoodBench: A Novel Benchmark For Evaluating Agentic Performance On Git", "abstract": "Benchmarks for Software Engineering (SE) AI agents, most notably SWE-bench, have catalyzed progress in programming capabilities of AI agents. However, they overlook critical developer workflows such as Version Control System (VCS) operations. To address this issue, we present GitGoodBench, a novel benchmark for evaluating AI agent performance on VCS tasks. GitGoodBench covers three core Git scenarios extracted from permissive open-source Python, Java, and Kotlin repositories. Our benchmark provides three datasets: a comprehensive evaluation suite (900 samples), a rapid prototyping version (120 samples), and a training corpus (17,469 samples). We establish baseline performance on the prototyping version of our benchmark using GPT-4o equipped with custom tools, achieving a 21.11% solve rate overall. We expect GitGoodBench to serve as a crucial stepping stone toward truly comprehensive SE agents that go beyond mere programming.", "arxiv_url": "https://arxiv.org/abs/2505.22583", "authors": ["Tobias Lindenbauer", "Egor Bogomolov", "Yaroslav Zharov"], "first_author": "Tobias Lindenbauer", "category": ["Benchmark"], "field": "Version Control & Collaboration", "task": "Git VCS", "tags": ["Agentic VCS evaluation", "Merge conflict resolution benchmark", "Interactive rebase planning", "Iterative commit reconstruction", "File-Commit Chain sampling", "Repository selection & filtering heuristics", "Agent trajectory training corpus", "Tool-enabled agent baseline evaluation"], "summary": "本文提出 GitGoodBench，一个用于评估 AI 代理在 Git 版本控制任务（合并冲突解决、交互式变基、迭代提交重构）上的端到端基准，发布了评估集、轻量集与用于收集轨迹的训练集并给出基线结果。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2505.22583v1", "published": "2025-05-28", "update_time": "2025-05-28", "download_time": "2025-12-11 15:50:19"}
{"id": "2505.23932", "title": "SwingArena: Competitive Programming Arena for Long-context GitHub Issue Solving", "abstract": "We present SwingArena, a competitive evaluation framework for Large Language Models (LLMs) that closely mirrors real-world software development workflows. Unlike traditional static benchmarks, SwingArena models the collaborative process of software iteration by pairing LLMs as submitters, who generate patches, and reviewers, who create test cases and verify the patches through continuous integration (CI) pipelines. To support these interactive evaluations, we introduce a retrieval-augmented code generation (RACG) module that efficiently handles long-context challenges by providing syntactically and semantically relevant code snippets from large codebases, supporting multiple programming languages (C++, Python, Rust, and Go). This enables the framework to scale across diverse tasks and contexts while respecting token limitations. Our experiments, using over 400 high-quality real-world GitHub issues selected from a pool of 2,300 issues, show that models like GPT-4o excel at aggressive patch generation, whereas DeepSeek and Gemini prioritize correctness in CI validation. SwingArena presents a scalable and extensible methodology for evaluating LLMs in realistic, CI-driven software development settings. More details are available on our project page: swing-bench.github.io", "arxiv_url": "https://arxiv.org/abs/2505.23932", "authors": ["Wendong Xu", "Jing Xiong", "Chenyang Zhao", "Qiujiang Chen", "Haoran Wang", "Hui Shen", "Zhongwei Wan", "Jianbo Dai", "Taiqiang Wu", "He Xiao", "Chaofan Tao", "Z. Morley Mao", "Ying Sheng", "Zhijiang Guo", "Hongxia Yang", "Bei Yu", "Lingpeng Kong", "Quanquan Gu", "Ngai Wong"], "first_author": "Wendong Xu", "category": ["Benchmark", "Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["CI-driven evaluation", "Submitter–reviewer interaction", "Retrieval-augmented code generation", "BM25 sparse retrieval with dense reranking", "Syntax-aware code chunking", "Adversarial patch and test generation", "Multi-language repository support", "GitHub Actions pipeline simulation", "Long-context code understanding"], "summary": "SWINGARENA 提出一个通过模拟完整 CI 流水线、提交者—评审循环和检索增强的长上下文代码检索来评估 LLM 在真实多语言 GitHub issue 修复场景中表现的可扩展基准与框架。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2505.23932v2", "published": "2025-05-29", "update_time": "2025-06-02", "download_time": "2025-12-11 15:50:55"}
{"id": "2507.05281", "title": "CoreCodeBench: A Configurable Multi-Scenario Repository-Level Benchmark", "abstract": "As Large Language Models (LLMs) demonstrate increasingly sophisticated code processing capabilities, evaluating their performance on engineering-level code remains challenging. Existing repository-level benchmarks primarily focus on single scenarios, such as code generation or bug fixing, without adequately capturing the diversity and complexity of real-world software or project engineering workflows. Furthermore, these benchmarks suffer from limited controllability in question positioning and reliability issues in their generated test cases. To address these limitations, we present CorePipe, a fully automated pipeline that converts repositories into comprehensive test cases, and introduce CoreCodeBench, a configurable multi-scenario repository-level benchmark. To simulate real engineering scenarios, CorePipe generates three types of atomic questions (Development, BugFix, and Test-Driven Development) specifically targeting core code segments. These atomic questions are further combined into three types of composite questions, with difficulty levels flexibly adjusted through hyperparameter tuning. CoreCodeBench provides a comprehensive and extensive repository-level benchmark to investigate the applicability of LLMs in real-world engineering projects. Experiments with 16 LLMs across diverse scenarios reveal varying capabilities and offer multi-dimensional insights into LLM performance in engineering contexts. The code for CorePipe is available at https://github.com/AGI-Eval-Official/CoreCodeBench, and the data for CoreCodeBench can be accessed at https://huggingface.co/collections/tubehhh/corecodebench-68256d2faabf4b1610a08caa.", "arxiv_url": "https://arxiv.org/abs/2507.05281", "authors": ["Lingyue Fu", "Hao Guan", "Bolun Zhang", "Haowei Yuan", "Yaoming Zhu", "Jun Xu", "Zongyu Wang", "Lin Qiu", "Xunliang Cai", "Xuezhi Cao", "Weiwen Liu", "Weinan Zhang", "Yong Yu"], "first_author": "Lingyue Fu", "category": ["Benchmark", "Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Repository-level benchmark generation", "Automated repository-to-test pipeline", "Core code segment identification", "Atomic and composite problem composition", "Configurable difficulty and position control", "Test-driven development scenario synthesis", "Quality inspection of generated tests", "Cross-file contextual reasoning evaluation"], "summary": "本文提出了自动化管道CorePipe并发布CORECODEBENCH，一个可配置的多场景仓库级基准，用于从真实代码库生成高质量的开发、修复和TDD测试用例并评估LLM在工程级代码任务中的表现。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2507.05281v1", "published": "2025-07-04", "update_time": "2025-07-04", "download_time": "2025-12-11 15:51:30"}
{"id": "2507.09866", "title": "Turning the Tide: Repository-based Code Reflection", "abstract": "Code large language models (LLMs) enhance programming by understanding and generating code across languages, offering intelligent feedback, bug detection, and code updates through reflection, improving development efficiency and accessibility. While benchmarks (e.g. HumanEval/LiveCodeBench) evaluate code generation and real-world relevance, previous works ignore the scenario of modifying code in repositories. Considering challenges remaining in improving reflection capabilities and avoiding data contamination in dynamic benchmarks, we introduce LiveRepoReflection, a challenging benchmark for evaluating code understanding and generation in multi-file repository contexts, featuring 1,888 rigorously filtered test cases across $6$ programming languages to ensure diversity, correctness, and high difficulty. Further, we create RepoReflection-Instruct, a large-scale, quality-filtered instruction-tuning dataset derived from diverse sources, used to train RepoReflectionCoder through a two-turn dialogue process involving code generation and error-driven repair. The leaderboard evaluates over 40 LLMs to reflect the model performance of repository-based code reflection.", "arxiv_url": "https://arxiv.org/abs/2507.09866", "authors": ["Wei Zhang", "Jian Yang", "Jiaxi Yang", "Ya Wang", "Zhoujun Li", "Zeyu Cui", "Binyuan Hui", "Junyang Lin"], "first_author": "Wei Zhang", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Repository-level code reflection", "Multi-file dependency reasoning", "Unit-test-driven problem selection", "Cross-execution verification", "Dynamic decontamination and deduplication", "Simulated multi-turn instruction generation", "Difficulty filtering via multi-model pass rates"], "summary": "本文提出了 LiveRepoReflection——一个面向多文件仓库情境的动态高难度代码反思基准，构建了用于指令微调的 RepoReflection-Instruct 数据集并训练出 RepoReflectionCoder，同时通过自动化管线、沙箱交叉执行和多模型评估建立排行榜以量化仓库级别的代码理解与修复能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2507.09866v1", "published": "2025-07-14", "update_time": "2025-07-14", "download_time": "2025-12-11 15:52:08"}
{"id": "2507.12415", "title": "SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories?", "abstract": "Code performance optimization is paramount in real-world software engineering and critical for production-level systems. While Large Language Models (LLMs) have demonstrated impressive capabilities in code generation and bug fixing, their proficiency in enhancing code performance at the repository level remains largely unexplored. To address this gap, we introduce SWE-Perf, the first benchmark specifically designed to systematically evaluate LLMs on code performance optimization tasks within authentic repository contexts. SWE-Perf comprises 140 carefully curated instances, each derived from performance-improving pull requests from popular GitHub repositories. Each benchmark instance includes the relevant codebase, target functions, performance-related tests, expert-authored patches, and executable environments. Through a comprehensive evaluation of representative methods that span file-level and repo-level approaches (e.g., Agentless and OpenHands), we reveal a substantial capability gap between existing LLMs and expert-level optimization performance, highlighting critical research opportunities in this emerging field.", "arxiv_url": "https://arxiv.org/abs/2507.12415", "authors": ["Xinyi He", "Qian Liu", "Mingzhe Du", "Lin Yan", "Zhijie Fan", "Yiming Huang", "Zejian Yuan", "Zejun Ma"], "first_author": "Xinyi He", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Performance Optimization", "tags": ["Repository-level performance benchmark", "Pull-request mining for performance improvements", "Runtime-based evaluation metrics", "Performance-oriented unit tests", "Executable Docker environments", "Expert-patch gold standards", "Oracle (file-level) vs realistic (repo-level) evaluations", "Repository-scale optimization challenges"], "summary": "SWE-Perf 提出并发布了第一个评估语言模型在真实代码仓库中进行代码性能优化能力的基准数据集，包含 140 个基于性能改进的 PR 实例、可执行环境、性能测试与专家补丁，并在文件级与仓库级设置下对多种方法进行了系统评估，揭示模型与专家之间显著的性能差距。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2507.12415v1", "published": "2025-07-16", "update_time": "2025-07-16", "download_time": "2025-12-11 15:52:42"}
{"id": "2509.04078", "title": "RepoDebug: Repository-Level Multi-Task and Multi-Language Debugging Evaluation of Large Language Models", "abstract": "Large Language Models (LLMs) have exhibited significant proficiency in code debugging, especially in automatic program repair, which may substantially reduce the time consumption of developers and enhance their efficiency. Significant advancements in debugging datasets have been made to promote the development of code debugging. However, these datasets primarily focus on assessing the LLM's function-level code repair capabilities, neglecting the more complex and realistic repository-level scenarios, which leads to an incomplete understanding of the LLM's challenges in repository-level debugging. While several repository-level datasets have been proposed, they often suffer from limitations such as limited diversity of tasks, languages, and error types. To mitigate this challenge, this paper introduces RepoDebug, a multi-task and multi-language repository-level code debugging dataset with 22 subtypes of errors that supports 8 commonly used programming languages and 3 debugging tasks. Furthermore, we conduct evaluation experiments on 10 LLMs, where Claude 3.5 Sonnect, the best-performing model, still cannot perform well in repository-level debugging.", "arxiv_url": "https://arxiv.org/abs/2509.04078", "authors": ["Jingjing Liu", "Zeming Liu", "Zihao Cheng", "Mengliang He", "Xiaoming Shi", "Yuhang Guo", "Xiangrong Zhu", "Yuanfang Guo", "Yunhong Wang", "Haifeng Wang"], "first_author": "Jingjing Liu", "category": ["Benchmark", "Empirical"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Repository-level debugging", "Multi-language benchmark", "Multi-task evaluation (identification, localization, repair)", "22 bug subtypes taxonomy", "AST-based bug injection (tree-sitter)", "Cross-repository train/test split to avoid data leakage", "Automated filtering plus manual validation", "Metrics distinguishing single vs. multiple error localization", "Empirical LLM comparison across languages and error types"], "summary": "本文提出了RepoDebug，一个覆盖8种语言、3类调试任务和22种错误子类型的仓库级多语言多任务代码调试基准，并用多款大模型评估显示现有模型在仓库级调试上仍存在显著不足。", "quality": "High", "conference": "EMNLP 2025", "pdf_url": "https://arxiv.org/pdf/2509.04078v2", "published": "2025-09-04", "update_time": "2025-09-08", "download_time": "2025-12-11 15:53:22"}
{"id": "2509.16941", "title": "SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?", "abstract": "We introduce SWE-Bench Pro, a substantially more challenging benchmark that builds upon the best practices of SWE-BENCH [25], but is explicitly designed to capture realistic, complex, enterprise-level problems beyond the scope of SWE-BENCH. SWE-BENCH PRO contains 1,865 problems sourced from a diverse set of 41 actively maintained repositories spanning business applications, B2B services, and developer tools. The benchmark is partitioned into a public set with open access to problems sourced from 11 repositories, a held-out set of 12 repositories and a commercial set of 18 proprietary repositories where we have formal partnership agreements with early-stage startups. Problems in the held-out and the commercial set are not publicly accessible, but we release results on the commercial set. Our benchmark features long-horizon tasks that may require hours to days for a professional software engineer to complete, often involving patches across multiple files and substantial code modifications. All tasks are human-verified and augmented with sufficient context to ensure resolvability. To better understand these limitations, we cluster the failure modes observed in the collected agent trajectories for a clearer characterization of the error patterns exhibited by current models. Overall, SWE-BENCH PRO provides a contamination-resistant testbed that more faithfully captures the complexity and diversity of real-world software development, advancing the pursuit of truly autonomous software engineering agents at a professional level.", "arxiv_url": "https://arxiv.org/abs/2509.16941", "authors": ["Xiang Deng", "Jeff Da", "Edwin Pan", "Yannis Yiming He", "Charles Ide", "Kanak Garg", "Niklas Lauffer", "Andrew Park", "Nitin Pasari", "Chetan Rane", "Karmini Sampath", "Maya Krishnan", "Srivatsa Kundurthy", "Sean Hendryx", "Zifan Wang", "Vijay Bharadwaj", "Jeff Holm", "Raja Aluri", "Chen Bo Calvin Zhang", "Noah Jacobson", "Bing Liu", "Brad Kenstler"], "first_author": "Xiang Deng", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Contamination-resistant curation", "Copyleft (GPL) licensing strategy", "Private commercial repositories", "Long-horizon multi-file code patches", "Human-in-the-loop task augmentation", "Unit-test-based verification", "Held-out and commercial test partitions", "Trajectory-level failure clustering", "Repository-level evaluation protocols", "Diagnostic analyses of agent failures"], "summary": "本文提出了一个面向企业级、长时程、多文件修改的污染抵抗软件工程基准，通过仅选用强制开源许可仓库并引入私有商用代码库、人工增强与单元测试验证来构建和评估能解决真实工程问题的代码代理，并对失败轨迹进行聚类分析以诊断当前模型局限。 ", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2509.16941v2", "published": "2025-09-21", "update_time": "2025-11-14", "download_time": "2025-12-11 15:53:52"}
{"id": "2510.14509", "title": "E2Edev: Benchmarking Large Language Models in End-to-End Software Development Task", "abstract": "The rapid advancement in large language models (LLMs) has demonstrated significant potential in End-to-End Software Development (E2ESD). However, existing E2ESD benchmarks are limited by coarse-grained requirement specifications and unreliable evaluation protocols, hindering a true understanding of current framework capabilities. To address these limitations, we present E2EDev, a novel benchmark grounded in the principles of Behavior-Driven Development (BDD), which evaluates the capabilities of E2ESD frameworks by assessing whether the generated software meets user needs through mimicking real user interactions (Figure 1). E2EDev comprises (i) a fine-grained set of user requirements, (ii) multiple BDD test scenarios with corresponding Python step implementations for each requirement, and (iii) a fully automated testing pipeline built on the Behave framework. To ensure its quality while reducing the annotation effort, E2EDev leverages our proposed Human-in-the-Loop Multi-Agent Annotation Framework (HITL-MAA). By evaluating various E2ESD frameworks and LLM backbones with E2EDev, our analysis reveals a persistent struggle to effectively solve these tasks, underscoring the critical need for more effective and cost-efficient E2ESD solutions. Our codebase and benchmark are publicly available at https://github.com/SCUNLP/E2EDev.", "arxiv_url": "https://arxiv.org/abs/2510.14509", "authors": ["Jingyao Liu", "Chen Huang", "Zhizhao Guan", "Wenqiang Lei", "Yang Deng"], "first_author": "Jingyao Liu", "category": ["Benchmark", "Technical"], "field": "Requirements & Design", "task": "Specification & Validation", "tags": ["Behavior-Driven Development", "Executable BDD test scenarios", "Human-in-the-Loop Multi-Agent Annotation", "Behave framework integration", "Fine-grained user requirement decomposition", "End-to-end software generation evaluation", "Multi-agent vs single-agent E2ESD comparison", "Error analysis and token-cost evaluation"], "summary": "该论文提出了基于BDD的E2EDev基准与人机协同多代理注释框架，通过可执行的BDD测试和自动化管道评估端到端软件生成框架的需求满足情况并分析其性能与开销。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2510.14509v2", "published": "2025-10-16", "update_time": "2025-10-24", "download_time": "2025-12-11 15:54:37"}
{"id": "2511.02352", "title": "SWE-Sharp-Bench: A Reproducible Benchmark for C# Software Engineering Tasks", "abstract": "AI coding agents have shown great progress on Python software engineering benchmarks like SWE-Bench, and for other languages like Java and C in benchmarks like Multi-SWE-Bench. However, C# -- a prominent enterprise language ranking #5 in the TIOBE index -- remains absent from such benchmarks. We introduce SWE-Sharp-Bench, a reproducible software engineering benchmark for C# featuring 150 instances from 17 repositories. Evaluating identical model-agent configurations across languages reveals a significant performance gap: while 70% of Python tasks in SWE-Bench Verified are solved, only 40% of our C# tasks are resolved. We open-source SWE-Sharp-Bench and our entire curation pipeline.", "arxiv_url": "https://arxiv.org/abs/2511.02352", "authors": ["Sanket Mhatre", "Yasharth Bajpai", "Sumit Gulwani", "Emerson Murphy-Hill", "Gustavo Soares"], "first_author": "Sanket Mhatre", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["C#/.NET ecosystem", "Reproducible containerized environments", "NuGet/MSBuild dependency resolution", "Execution-based filtering (pass→fail→pass)", "Patch complexity analysis", "Repository-level pull-request tasks", "Agent resolution-rate evaluation", "Multi-file coordinated edits"], "summary": "本文提出SWE-Sharp-Bench——首个面向C#/.NET生态的可复现软件工程基准（150个实例、17个仓库）并开源构建管道，实验证明当前大模型代理在C#上解决率显著低于Python，主要受复杂多文件补丁和依赖/构建管理影响。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.02352v3", "published": "2025-11-04", "update_time": "2025-11-18", "download_time": "2025-12-11 15:55:19"}
{"id": "2511.03404", "title": "Towards Realistic Project-Level Code Generation via Multi-Agent Collaboration and Semantic Architecture Modeling", "abstract": "In recent years, Large Language Models (LLMs) have achieved remarkable progress in automated code generation. In real-world software engineering, the growing demand for rapid iteration and continuous delivery underscores the importance of project-level code generation, where LLMs are expected to generate complete software projects directly from complex user requirements. Although existing studies have made initial explorations, they still face key limitations, including unrealistic datasets and unreliable evaluation metrics that fail to reflect real-world complexity, the semantic gap between human-written requirements and machine-interpretable structures, and difficulties in managing hierarchical dependencies and maintaining quality throughout the generation process. To address these limitations, we first introduce CodeProjectEval, a project-level code generation dataset built from 18 real-world repositories with 12.7 files and 2,388.6 lines of code per task on average, supplemented with documentation and executable test cases for automatic evaluation. We further propose ProjectGen, a multi-agent framework that decomposes projects into architecture design, skeleton generation, and code filling stages with iterative refinement and memory-based context management. Within this framework, we introduce the Semantic Software Architecture Tree (SSAT), a structured and semantically rich representation that effectively bridges user requirements and source code implementation. Experiments show that ProjectGen achieves state-of-the-art performance, passing 52/124 test cases on the small-scale project-level code generation dataset DevBench, a 57% improvement over the baseline approaches, and 310 test cases on CodeProjectEval, representing an improvement of roughly tenfold compared to the baselines.", "arxiv_url": "https://arxiv.org/abs/2511.03404", "authors": ["Qianhui Zhao", "Li Zhang", "Fang Liu", "Junhang Cheng", "Chengru Wu", "Junchen Ai", "Qiaoyuanhe Meng", "Lichen Zhang", "Xiaoli Lian", "Shubin Song", "Yuanping Guo"], "first_author": "Qianhui Zhao", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Project-level Code Generation", "Multi-agent Collaboration", "Semantic Software Architecture Tree", "Architecture-aware Synthesis", "Skeleton-and-fill Pipeline", "Iterative Refinement", "Memory-based Context Management", "Executable Test-case Evaluation", "Hierarchical Dependency Management"], "summary": "该论文提出了真实项目级代码生成数据集CodeProjectEval，并设计了多智能体ProjectGen框架与语义软件架构树（SSAT）作为中间表征，通过架构设计、骨架生成与代码填充的分阶段迭代及记忆化上下文管理显著提升了项目级代码生成的可执行性与质量。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.03404v1", "published": "2025-11-05", "update_time": "2025-11-05", "download_time": "2025-12-11 15:55:52"}
{"id": "2511.06090", "title": "SWE-fficiency: Can Language Models Optimize Real-World Repositories on Real Workloads?", "abstract": "Optimizing the performance of large-scale software repositories demands expertise in code reasoning and software engineering (SWE) to reduce runtime while preserving program correctness. However, most benchmarks emphasize what to fix rather than how to fix code. We introduce SWE-fficiency, a benchmark for evaluating repository-level performance optimization on real workloads. Our suite contains 498 tasks across nine widely used data-science, machine-learning, and HPC repositories (e.g., numpy, pandas, scipy): given a complete codebase and a slow workload, an agent must investigate code semantics, localize bottlenecks and relevant tests, and produce a patch that matches or exceeds expert speedup while passing the same unit tests. To enable this how-to-fix evaluation, our automated pipeline scrapes GitHub pull requests for performance-improving edits, combining keyword filtering, static analysis, coverage tooling, and execution validation to both confirm expert speedup baselines and identify relevant repository unit tests. Empirical evaluation of state-of-the-art agents reveals significant underperformance. On average, agents achieve less than 0.15x the expert speedup: agents struggle in localizing optimization opportunities, reasoning about execution across functions, and maintaining correctness in proposed edits. We release the benchmark and accompanying data pipeline to facilitate research on automated performance engineering and long-horizon software reasoning.", "arxiv_url": "https://arxiv.org/abs/2511.06090", "authors": ["Jeffrey Jian Ma", "Milad Hashemi", "Amir Yazdanbakhsh", "Kevin Swersky", "Ofir Press", "Enhui Li", "Vijay Janapa Reddi", "Parthasarathy Ranganathan"], "first_author": "Jeffrey Jian Ma", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Repository-level performance optimization", "Workload-driven patch generation", "Test localization via coverage", "Automated PR scraping pipeline", "Oracle-free evaluation", "Speedup ratio metric", "Real-world Python scientific libraries", "Correctness-preserving edits", "Long-horizon codebase reasoning"], "summary": "本文提出SWE-FFICIENCY基准与可复现的数据采集管道，包含498个来自真实Python科学计算仓库的性能优化任务，要求在不破坏仓库单元测试的前提下通过代码修改加速指定工作负载，并发现现有语言模型在定位瓶颈、跨函数执行推理和保持正确性方面远落后于专家（平均仅达专家提速的0.15倍）。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.06090v2", "published": "2025-11-08", "update_time": "2025-11-11", "download_time": "2025-12-11 15:56:27"}
{"id": "1706.03762", "title": "Attention Is All You Need", "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.", "arxiv_url": "https://arxiv.org/abs/1706.03762", "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin"], "first_author": "Ashish Vaswani", "category": ["Technical"], "field": "Model Architecture", "task": "Self-Attention Transformer for Sequence Transduction", "tags": ["Self-Attention", "Multi-Head Attention", "Scaled Dot-Product Attention", "Positional Encoding", "Encoder-Decoder Stack", "Position-wise Feed-Forward", "Parallelizable Training", "Autoregressive Decoding & Masking"], "summary": "本文提出了Transformer——一种完全基于自注意力的序列转换模型，通过多头注意力、位置编码与逐点前馈网络替代循环和卷积，从而显著提升并行性与翻译性能。", "quality": "High", "conference": "NeurIPS 2017", "pdf_url": "https://arxiv.org/pdf/1706.03762v7", "published": "2017-06-12", "update_time": "2023-08-02", "download_time": "2025-12-11 15:57:25"}
{"id": "1704.04856", "title": "A Neural Architecture for Generating Natural Language Descriptions from Source Code Changes", "abstract": "We propose a model to automatically describe changes introduced in the source code of a program using natural language. Our method receives as input a set of code commits, which contains both the modifications and message introduced by an user. These two modalities are used to train an encoder-decoder architecture. We evaluated our approach on twelve real world open source projects from four different programming languages. Quantitative and qualitative results showed that the proposed approach can generate feasible and semantically sound descriptions not only in standard in-project settings, but also in a cross-project setting.", "arxiv_url": "https://arxiv.org/abs/1704.04856", "authors": ["Pablo Loyola", "Edison Marrese-Taylor", "Yutaka Matsuo"], "first_author": "Pablo Loyola", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Summarization", "tags": ["Commit Message Generation", "Diff-level Summarization", "Attention-based Encoder-Decoder", "LSTM Decoder with Beam Search", "Line-level Code Tokenization", "Atomic-commit Assumption", "Cross-project Evaluation", "Neural MT Baseline Comparison"], "summary": "本文提出一种基于注意力的编码-解码神经网络，用于从源码变更(diff/commit)自动生成自然语言描述，并在12个开源项目上进行原位与跨项目实证评估以验证效果。", "quality": "Middle", "conference": "ACL 2017", "pdf_url": "https://arxiv.org/pdf/1704.04856v1", "published": "2017-04-17", "update_time": "2017-04-17", "download_time": "2025-12-11 16:26:23"}
{"id": "1708.09492", "title": "Automatically Generating Commit Messages from Diffs using Neural Machine Translation", "abstract": "Commit messages are a valuable resource in comprehension of software evolution, since they provide a record of changes such as feature additions and bug repairs. Unfortunately, programmers often neglect to write good commit messages. Different techniques have been proposed to help programmers by automatically writing these messages. These techniques are effective at describing what changed, but are often verbose and lack context for understanding the rationale behind a change. In contrast, humans write messages that are short and summarize the high level rationale. In this paper, we adapt Neural Machine Translation (NMT) to automatically \"translate\" diffs into commit messages. We trained an NMT algorithm using a corpus of diffs and human-written commit messages from the top 1k Github projects. We designed a filter to help ensure that we only trained the algorithm on higher-quality commit messages. Our evaluation uncovered a pattern in which the messages we generate tend to be either very high or very low quality. Therefore, we created a quality-assurance filter to detect cases in which we are unable to produce good messages, and return a warning instead.", "arxiv_url": "https://arxiv.org/abs/1708.09492", "authors": ["Siyuan Jiang", "Ameer Armaly", "Collin McMillan"], "first_author": "Siyuan Jiang", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Summarization", "tags": ["Diff-to-commit translation", "Diff-level summarization", "Neural Machine Translation for code changes", "Verb-Direct-Object filtering", "Quality-assurance filter for generation", "Attentional RNN encoder-decoder", "Human evaluation"], "summary": "本文提出将 git diff 作为输入、使用注意力 RNN 的神经机器翻译模型自动生成一行高层次的提交信息，并通过动词-直接宾语过滤与质量保证过滤器在约2M 对提交数据上进行训练与人工评估。", "quality": "High", "conference": "ASE (IEEE/ACM International Conference on Automated Software Engineering) 2017", "pdf_url": "https://arxiv.org/pdf/1708.09492v1", "published": "2017-08-30", "update_time": "2017-08-30", "download_time": "2025-12-11 16:27:07"}
{"id": "1912.02972", "title": "ATOM: Commit Message Generation Based on Abstract Syntax Tree and Hybrid Ranking", "abstract": "Commit messages record code changes (e.g., feature modifications and bug repairs) in natural language, and are useful for program comprehension. Due to the frequent updates of software and time cost, developers are generally unmotivated to write commit messages for code changes. Therefore, automating the message writing process is necessitated. Previous studies on commit message generation have been benefited from generation models or retrieval models, but the code structure of changed code, i.e., AST, which can be important for capturing code semantics, has not been explicitly involved. Moreover, although generation models have the advantages of synthesizing commit messages for new code changes, they are not easy to bridge the semantic gap between code and natural languages which could be mitigated by retrieval models. In this paper, we propose a novel commit message generation model, named ATOM, which explicitly incorporates the abstract syntax tree for representing code changes and integrates both retrieved and generated messages through hybrid ranking. Specifically, the hybrid ranking module can prioritize the most accurate message from both retrieved and generated messages regarding one code change. We evaluate the proposed model ATOM on our dataset crawled from 56 popular Java repositories. Experimental results demonstrate that ATOM increases the state-of-the-art models by 30.72% in terms of BLEU-4 (an accuracy measure that is widely used to evaluate text generation systems). Qualitative analysis also demonstrates the effectiveness of ATOM in generating accurate code commit messages.", "arxiv_url": "https://arxiv.org/abs/1912.02972", "authors": ["Shangqing Liu", "Cuiyun Gao", "Sen Chen", "Lun Yiu Nie", "Yang Liu"], "first_author": "Shangqing Liu", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Summarization", "tags": ["AST-based code representation", "AST-to-sequence encoding", "Hybrid retrieval-generation ranking", "Commit/diff-level summarization", "BiLSTM with attention for AST paths", "Function-level dataset cleaning"], "summary": "本文提出ATOM，一种显式利用抽象语法树进行代码变更表示并通过检索与生成结果的混合排序优先选取最准确提交信息的方法，同时发布了约16万条Java提交的基准数据集，显著提升了BLEU-4成绩。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/1912.02972v2", "published": "2019-12-06", "update_time": "2020-11-11", "download_time": "2025-12-11 16:27:38"}
{"id": "2105.14242", "title": "CommitBERT: Commit Message Generation Using Pre-Trained Programming Language Model", "abstract": "Commit message is a document that summarizes source code changes in natural language. A good commit message clearly shows the source code changes, so this enhances collaboration between developers. Therefore, our work is to develop a model that automatically writes the commit message.   To this end, we release 345K datasets consisting of code modification and commit messages in six programming languages (Python, PHP, Go, Java, JavaScript, and Ruby). Similar to the neural machine translation (NMT) model, using our dataset, we feed the code modification to the encoder input and the commit message to the decoder input and measure the result of the generated commit message with BLEU-4.   Also, we propose the following two training methods to improve the result of generating the commit message: (1) A method of preprocessing the input to feed the code modification to the encoder input. (2) A method that uses an initial weight suitable for the code domain to reduce the gap in contextual representation between programming language (PL) and natural language (NL). Training code, dataset, and pre-trained weights are available at https://github.com/graykode/commit-autosuggestions", "arxiv_url": "https://arxiv.org/abs/2105.14242", "authors": ["Tae-Hwan Jung"], "first_author": "Tae-Hwan Jung", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Summarization", "tags": ["Commit message generation", "Git diff added/deleted pair representation", "Cross-language dataset (6 languages)", "Pretrained code→NL transfer", "Transformer encoder-decoder fine-tuning", "Dataset curation and license filtering", "Evaluation with BLEU-4"], "summary": "本文发布了一个包含345K对添加/删除代码片段与提交消息的多语言数据集，并通过在预训练的代码域语言模型上进行微调，以区分新增/删除部分的二元输入显著提升了提交消息生成效果。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2105.14242v1", "published": "2021-05-29", "update_time": "2021-05-29", "download_time": "2025-12-11 16:28:08"}
{"id": "2107.05373", "title": "On the Evaluation of Commit Message Generation Models: An Experimental Study", "abstract": "Commit messages are natural language descriptions of code changes, which are important for program understanding and maintenance. However, writing commit messages manually is time-consuming and laborious, especially when the code is updated frequently. Various approaches utilizing generation or retrieval techniques have been proposed to automatically generate commit messages. To achieve a better understanding of how the existing approaches perform in solving this problem, this paper conducts a systematic and in-depth analysis of the state-of-the-art models and datasets. We find that: (1) Different variants of the BLEU metric are used in previous works, which affects the evaluation and understanding of existing methods. (2) Most existing datasets are crawled only from Java repositories while repositories in other programming languages are not sufficiently explored. (3) Dataset splitting strategies can influence the performance of existing models by a large margin. Some models show better performance when the datasets are split by commit, while other models perform better when the datasets are split by timestamp or by project. Based on our findings, we conduct a human evaluation and find the BLEU metric that best correlates with the human scores for the task. We also collect a large-scale, information-rich, and multi-language commit message dataset MCMD and evaluate existing models on this dataset. Furthermore, we conduct extensive experiments under different dataset splitting strategies and suggest the suitable models under different scenarios. Based on the experimental results and findings, we provide feasible suggestions for comprehensively evaluating commit message generation models and discuss possible future research directions. We believe this work can help practitioners and researchers better evaluate and select models for automatic commit message generation.", "arxiv_url": "https://arxiv.org/abs/2107.05373", "authors": ["Wei Tao", "Yanlin Wang", "Ensheng Shi", "Lun Du", "Shi Han", "Hongyu Zhang", "Dongmei Zhang", "Wenqiang Zhang"], "first_author": "Wei Tao", "category": ["Empirical", "Benchmark", "Survey"], "field": "Coding Assistant", "task": "Code Summarization", "tags": ["BLEU variant comparison", "Metric–human correlation", "Dataset splitting strategies (commit/timestamp/project)", "Multi‑language, information‑rich commit corpus", "Just‑In‑Time evaluation scenario", "Cross‑project generalization", "Retrieval vs generation vs hybrid approaches", "Commit metadata usage (timestamp, repo, SHA)"], "summary": "本文系统性评估了现有的提交消息生成模型与数据集，比对不同BLEU变体与人工评分的相关性，分析数据切分策略对模型性能的影响，并发布了一个大规模多语言提交消息数据集MCMD及相应评估建议。", "quality": "High", "conference": "International Conference on Software Maintenance and Evolution (ICSME 2021)", "pdf_url": "https://arxiv.org/pdf/2107.05373v3", "published": "2021-07-12", "update_time": "2021-07-26", "download_time": "2025-12-11 16:28:38"}
{"id": "2308.00147", "title": "Delving into Commit-Issue Correlation to Enhance Commit Message Generation Models", "abstract": "Commit message generation (CMG) is a challenging task in automated software engineering that aims to generate natural language descriptions of code changes for commits. Previous methods all start from the modified code snippets, outputting commit messages through template-based, retrieval-based, or learning-based models. While these methods can summarize what is modified from the perspective of code, they struggle to provide reasons for the commit. The correlation between commits and issues that could be a critical factor for generating rational commit messages is still unexplored.   In this work, we delve into the correlation between commits and issues from the perspective of dataset and methodology. We construct the first dataset anchored on combining correlated commits and issues. The dataset consists of an unlabeled commit-issue parallel part and a labeled part in which each example is provided with human-annotated rational information in the issue. Furthermore, we propose \\tool (\\underline{Ex}traction, \\underline{Gro}unding, \\underline{Fi}ne-tuning), a novel paradigm that can introduce the correlation between commits and issues into the training phase of models. To evaluate whether it is effective, we perform comprehensive experiments with various state-of-the-art CMG models. The results show that compared with the original models, the performance of \\tool-enhanced models is significantly improved.", "arxiv_url": "https://arxiv.org/abs/2308.00147", "authors": ["Liran Wang", "Xunzhu Tang", "Yichen He", "Changyu Ren", "Shuhua Shi", "Chaoran Yan", "Zhoujun Li"], "first_author": "Liran Wang", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Summarization", "tags": ["Commit-Issue Correlation", "Rational Information Extraction", "Issue-Grounded Code Representation", "Extraction-Grounding-Fine-tuning Paradigm", "Commit-Issue Parallel Dataset", "Human-Annotated Rationale Labels", "Training-Time Issue Augmentation"], "summary": "本文构建了首个提交—问题并行数据集并提出ExGroFi（提取—落地—微调）范式，通过将问题报告中的理性信息引入训练显著提升了自动生成提交信息的合理性与质量。", "quality": "High", "conference": "ASE (IEEE/ACM International Conference on Automated Software Engineering) 2023", "pdf_url": "https://arxiv.org/pdf/2308.00147v2", "published": "2023-07-31", "update_time": "2023-09-28", "download_time": "2025-12-11 16:29:10"}
{"id": "2308.07655", "title": "From Commit Message Generation to History-Aware Commit Message Completion", "abstract": "Commit messages are crucial to software development, allowing developers to track changes and collaborate effectively. Despite their utility, most commit messages lack important information since writing high-quality commit messages is tedious and time-consuming. The active research on commit message generation (CMG) has not yet led to wide adoption in practice. We argue that if we could shift the focus from commit message generation to commit message completion and use previous commit history as additional context, we could significantly improve the quality and the personal nature of the resulting commit messages.   In this paper, we propose and evaluate both of these novel ideas. Since the existing datasets lack historical data, we collect and share a novel dataset called CommitChronicle, containing 10.7M commits across 20 programming languages. We use this dataset to evaluate the completion setting and the usefulness of the historical context for state-of-the-art CMG models and GPT-3.5-turbo. Our results show that in some contexts, commit message completion shows better results than generation, and that while in general GPT-3.5-turbo performs worse, it shows potential for long and detailed messages. As for the history, the results show that historical information improves the performance of CMG models in the generation task, and the performance of GPT-3.5-turbo in both generation and completion.", "arxiv_url": "https://arxiv.org/abs/2308.07655", "authors": ["Aleksandra Eliseeva", "Yaroslav Sokolov", "Egor Bogomolov", "Yaroslav Golubev", "Danny Dig", "Timofey Bryksin"], "first_author": "Aleksandra Eliseeva", "category": ["Technical", "Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Summarization", "tags": ["Commit message completion", "History-aware generation", "Prefix-conditioned suggestion", "Diff-based textual representation", "Large-scale multilingual commit history", "Dataset filter bias analysis", "Evaluation of generation vs completion", "Personalization of commit messages"], "summary": "本文将提交信息生成重新表述为基于已输入前缀的提交信息补全并将历史提交作为上下文，构建并发布包含约1070万提交的多语言历史数据集，对多种CMG模型与LLM进行了系统评估，证明在若干场景下补全与历史信息能提升性能。", "quality": "High", "conference": "ASE (IEEE/ACM International Conference on Automated Software Engineering) 2023", "pdf_url": "https://arxiv.org/pdf/2308.07655v1", "published": "2023-08-15", "update_time": "2023-08-15", "download_time": "2025-12-11 16:29:48"}
{"id": "2303.12570", "title": "RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation", "abstract": "The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline. RepoCoder makes effective utilization of repository-level information for code completion and has the ability to generate code at various levels of granularity. Moreover, we propose a new benchmark RepoEval, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion scenarios. Experimental results indicate that RepoCoder significantly improves the In-File completion baseline by over 10% in all settings and consistently outperforms the vanilla retrieval-augmented code completion approach. Furthermore, we validate the effectiveness of RepoCoder through comprehensive analysis, providing valuable insights for future research. Our source code and benchmark are publicly available: https://github.com/microsoft/CodeT/tree/main/RepoCoder", "arxiv_url": "https://arxiv.org/abs/2303.12570", "authors": ["Fengji Zhang", "Bei Chen", "Yue Zhang", "Jacky Keung", "Jin Liu", "Daoguang Zan", "Yi Mao", "Jian-Guang Lou", "Weizhu Chen"], "first_author": "Fengji Zhang", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Iterative retrieval-generation", "Repository-level context retrieval", "Retrieval-augmented code completion", "Sliding-window code snippet indexing", "Query grounding with model-generated code", "Multi-granularity completion (line/API/function)", "Unit-test-based evaluation"], "summary": "本文提出RepoCoder，一种通过迭代检索-生成管道利用仓库级上下文进行代码补全的框架，并构建了包含行、API 调用与函数体多粒度且借助单元测试评估的RepoEval基准，实验证明显著提升补全性能。", "quality": "High", "conference": "EMNLP 2023", "pdf_url": "https://arxiv.org/pdf/2303.12570v3", "published": "2023-03-22", "update_time": "2023-10-20", "download_time": "2025-12-11 16:30:21"}
{"id": "2306.10763", "title": "Guiding Language Models of Code with Global Context using Monitors", "abstract": "Language models of code (LMs) work well when the surrounding code provides sufficient context. This is not true when it becomes necessary to use types, functionality or APIs defined elsewhere in the repository or a linked library, especially those not seen during training. LMs suffer from limited awareness of such global context and end up hallucinating.   Integrated development environments (IDEs) assist developers in understanding repository context using static analysis. We extend this assistance, enjoyed by developers, to LMs. We propose monitor-guided decoding (MGD) where a monitor uses static analysis to guide the decoding. We construct a repository-level dataset PragmaticCode for method-completion in Java and evaluate MGD on it. On models of varying parameter scale, by monitoring for type-consistent object dereferences, MGD consistently improves compilation rates and agreement with ground truth. Further, LMs with fewer parameters, when augmented with MGD, can outperform larger LMs. With MGD, SantaCoder-1.1B achieves better compilation rate and next-identifier match than the much larger text-davinci-003 model.   We also conduct a generalizability study to evaluate the ability of MGD to generalize to multiple programming languages (Java, C# and Rust), coding scenarios (e.g., correct number of arguments to method calls), and to enforce richer semantic constraints (e.g., stateful API protocols). Our data and implementation are available at https://github.com/microsoft/monitors4codegen .", "arxiv_url": "https://arxiv.org/abs/2306.10763", "authors": ["Lakshya A Agrawal", "Aditya Kanade", "Navin Goyal", "Shuvendu K. Lahiri", "Sriram K. Rajamani"], "first_author": "Lakshya A Agrawal", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Monitor-guided decoding", "Static-analysis-driven token masking", "Language Server Protocol integration", "Type-aware identifier resolution", "Decoding-time semantic constraints", "Repository-level context", "Stateful API protocol enforcement", "Compilation-focused evaluation"], "summary": "本文提出监视器引导解码（MGD），在解码阶段调用静态分析（基于LSP）对模型生成的令牌进行约束，从而利用仓库级全局上下文提升类型一致性、编译率和与真实代码的匹配，并公开了相应的仓库级数据集与工具实现。", "quality": "High", "conference": "NeurIPS 2023", "pdf_url": "https://arxiv.org/pdf/2306.10763v2", "published": "2023-06-19", "update_time": "2023-11-03", "download_time": "2025-12-11 16:30:53"}
{"id": "2306.10998", "title": "RepoFusion: Training Code Models to Understand Your Repository", "abstract": "Despite the huge success of Large Language Models (LLMs) in coding assistants like GitHub Copilot, these models struggle to understand the context present in the repository (e.g., imports, parent classes, files with similar names, etc.), thereby producing inaccurate code completions. This effect is more pronounced when using these assistants for repositories that the model has not seen during training, such as proprietary software or work-in-progress code projects. Recent work has shown the promise of using context from the repository during inference. In this work, we extend this idea and propose RepoFusion, a framework to train models to incorporate relevant repository context. Experiments on single-line code completion show that our models trained with repository context significantly outperform much larger code models as CodeGen-16B-multi ($\\sim73\\times$ larger) and closely match the performance of the $\\sim 70\\times$ larger StarCoderBase model that was trained with the Fill-in-the-Middle objective. We find these results to be a novel and compelling demonstration of the gains that training with repository context can bring. We carry out extensive ablation studies to investigate the impact of design choices such as context type, number of contexts, context length, and initialization within our framework. Lastly, we release Stack-Repo, a dataset of 200 Java repositories with permissive licenses and near-deduplicated files that are augmented with three types of repository contexts. Additionally, we are making available the code and trained checkpoints for our work. Our released resources can be found at \\url{https://huggingface.co/RepoFusion}.", "arxiv_url": "https://arxiv.org/abs/2306.10998", "authors": ["Disha Shrivastava", "Denis Kocetkov", "Harm de Vries", "Dzmitry Bahdanau", "Torsten Scholak"], "first_author": "Disha Shrivastava", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Repository-level context integration", "Fusion-in-Decoder adaptation", "Repo-level prompt proposals", "Context retrieval (BM25 and embedding-based)", "Surrounding-context augmentation", "Single-line code completion", "Ablation studies on context design", "Java repository corpus release"], "summary": "本文提出RepoFusion，通过将多个仓库级上下文用Fusion-in-Decoder训练融入代码模型以提升单行代码补全性能，并发布了带上下文增强的Java仓库语料与训练检查点，且小模型在多项评测中显著优于更大的基线模型。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2306.10998v1", "published": "2023-06-19", "update_time": "2023-06-19", "download_time": "2025-12-11 16:31:32"}
{"id": "2309.12499", "title": "CodePlan: Repository-level Coding using LLMs and Planning", "abstract": "Software engineering activities such as package migration, fixing errors reports from static analysis or testing, and adding type annotations or other specifications to a codebase, involve pervasively editing the entire repository of code. We formulate these activities as repository-level coding tasks.   Recent tools like GitHub Copilot, which are powered by Large Language Models (LLMs), have succeeded in offering high-quality solutions to localized coding problems. Repository-level coding tasks are more involved and cannot be solved directly using LLMs, since code within a repository is inter-dependent and the entire repository may be too large to fit into the prompt. We frame repository-level coding as a planning problem and present a task-agnostic framework, called CodePlan to solve it. CodePlan synthesizes a multi-step chain of edits (plan), where each step results in a call to an LLM on a code location with context derived from the entire repository, previous code changes and task-specific instructions. CodePlan is based on a novel combination of an incremental dependency analysis, a change may-impact analysis and an adaptive planning algorithm.   We evaluate the effectiveness of CodePlan on two repository-level tasks: package migration (C#) and temporal code edits (Python). Each task is evaluated on multiple code repositories, each of which requires inter-dependent changes to many files (between 2-97 files). Coding tasks of this level of complexity have not been automated using LLMs before. Our results show that CodePlan has better match with the ground truth compared to baselines. CodePlan is able to get 5/6 repositories to pass the validity checks (e.g., to build without errors and make correct code edits) whereas the baselines (without planning but with the same type of contextual information as CodePlan) cannot get any of the repositories to pass them.", "arxiv_url": "https://arxiv.org/abs/2309.12499", "authors": ["Ramakrishna Bairi", "Atharv Sonwane", "Aditya Kanade", "Vageesh D C", "Arun Iyer", "Suresh Parthasarathy", "Sriram Rajamani", "B. Ashok", "Shashank Shet"], "first_author": "Ramakrishna Bairi", "category": ["Technical"], "field": "Maintenance", "task": "Refactoring", "tags": ["Repository-level planning", "Incremental dependency analysis", "Change may-impact analysis", "Adaptive planning algorithm", "Edit-obligation plan graph", "Seed vs derived edit specification synthesis", "Oracle-driven validation (build/tests/type)", "Multi-file API migration"], "summary": "本文提出CodePlan，将仓库级代码编辑建模为规划问题，结合增量依赖分析、变更影响分析与自适应规划，生成多步LLM驱动的跨文件编辑以完成包迁移和时间性代码修改等仓库级维护任务并通过构建/验证保证正确性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2309.12499v1", "published": "2023-09-21", "update_time": "2023-09-21", "download_time": "2025-12-11 16:32:15"}
{"id": "2310.11248", "title": "CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code Completion", "abstract": "Code completion models have made significant progress in recent years, yet current popular evaluation datasets, such as HumanEval and MBPP, predominantly focus on code completion tasks within a single file. This over-simplified setting falls short of representing the real-world software development scenario where repositories span multiple files with numerous cross-file dependencies, and accessing and understanding cross-file context is often required to complete the code correctly.   To fill in this gap, we propose CrossCodeEval, a diverse and multilingual code completion benchmark that necessitates an in-depth cross-file contextual understanding to complete the code accurately. CrossCodeEval is built on a diverse set of real-world, open-sourced, permissively-licensed repositories in four popular programming languages: Python, Java, TypeScript, and C#. To create examples that strictly require cross-file context for accurate completion, we propose a straightforward yet efficient static-analysis-based approach to pinpoint the use of cross-file context within the current file.   Extensive experiments on state-of-the-art code language models like CodeGen and StarCoder demonstrate that CrossCodeEval is extremely challenging when the relevant cross-file context is absent, and we see clear improvements when adding these context into the prompt. However, despite such improvements, the pinnacle of performance remains notably unattained even with the highest-performing model, indicating that CrossCodeEval is also capable of assessing model's capability in leveraging extensive context to make better code completion. Finally, we benchmarked various methods in retrieving cross-file context, and show that CrossCodeEval can also be used to measure the capability of code retrievers.", "arxiv_url": "https://arxiv.org/abs/2310.11248", "authors": ["Yangruibo Ding", "Zijian Wang", "Wasi Uddin Ahmad", "Hantian Ding", "Ming Tan", "Nihal Jain", "Murali Krishna Ramanathan", "Ramesh Nallapati", "Parminder Bhatia", "Dan Roth", "Bing Xiang"], "first_author": "Yangruibo Ding", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Cross-file Context", "Cross-file Code Completion Benchmark", "Static-analysis-based Example Selection", "Undefined-name Detection", "Multilingual (Python/Java/TypeScript/C#)", "Repository-level Context Retrieval", "Data-leakage Mitigation", "Prompt Context Augmentation"], "summary": "该论文提出了CROSSCODEEVAL——一个来自真实开源仓库的多语言跨文件代码补全基准，使用静态分析自动构造必须依赖跨文件上下文的样例，并评估了多种代码模型和检索方法以展示跨文件上下文对补全性能的重要性。", "quality": "High", "conference": "NeurIPS", "pdf_url": "https://arxiv.org/pdf/2310.11248v2", "published": "2023-10-17", "update_time": "2023-11-17", "download_time": "2025-12-11 16:32:51"}
{"id": "2404.00599", "title": "EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories", "abstract": "How to evaluate Large Language Models (LLMs) in code generation is an open question. Existing benchmarks demonstrate poor alignment with real-world code repositories and are insufficient to evaluate the coding abilities of LLMs. This paper proposes a new benchmark - EvoCodeBench to address the preceding problems, which has three primary advances. (1) EvoCodeBench aligns with real-world repositories in multiple dimensions, e.g., code distributions and dependency distributions. (2) EvoCodeBench offers comprehensive annotations (e.g., requirements, reference code, and reference dependencies), and robust evaluation metrics (e.g., Pass@k and Recall@k). (3) EvoCodeBench is an evolving benchmark to avoid data leakage. We build an automatic pipeline to update EvoCodeBench from the latest repositories. We release the first version - EvoCodeBench-2403, containing 275 samples from 25 real-world repositories. Based on EvoCodeBench, we propose repository-level code generation and evaluate 10 popular LLMs (e.g., gpt-4, gpt-3.5, DeepSeek Coder, StarCoder 2, CodeLLaMa, Gemma, and Qwen 1.5). Our experiments reveal the coding abilities of these LLMs in real-world repositories. For example, the highest Pass@1 of gpt-4 only is 20.73% in our experiments. We also analyze failed cases and summarize the shortcomings of existing LLMs in EvoCodeBench. We release EvoCodeBench, all prompts, and LLMs' completions for further community analysis.", "arxiv_url": "https://arxiv.org/abs/2404.00599", "authors": ["Jia Li", "Ge Li", "Xuanming Zhang", "Yihong Dong", "Zhi Jin"], "first_author": "Jia Li", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Repository-level code generation", "Evolving benchmark", "Dependency-aware evaluation", "Reference-dependency Recall@k", "Pass@k functional testing", "Automatic update pipeline", "Real-world repository alignment", "Standalone vs non-standalone functions"], "summary": "本文提出了EvoCodeBench——一个与真实开源仓库分布对齐、可周期更新的仓库级代码生成基准，提供依赖注释与测试用例并用以评估多款LLM在实际仓库情境下的代码生成能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2404.00599v1", "published": "2024-03-31", "update_time": "2024-03-31", "download_time": "2025-12-11 16:33:24"}
{"id": "2405.19856", "title": "DevEval: A Manually-Annotated Code Generation Benchmark Aligned with Real-World Code Repositories", "abstract": "How to evaluate the coding abilities of Large Language Models (LLMs) remains an open question. We find that existing benchmarks are poorly aligned with real-world code repositories and are insufficient to evaluate the coding abilities of LLMs.   To address the knowledge gap, we propose a new benchmark named DevEval, which has three advances. (1) DevEval aligns with real-world repositories in multiple dimensions, e.g., code distributions and dependency distributions. (2) DevEval is annotated by 13 developers and contains comprehensive annotations (e.g., requirements, original repositories, reference code, and reference dependencies). (3) DevEval comprises 1,874 testing samples from 117 repositories, covering 10 popular domains (e.g., Internet, Database). Based on DevEval, we propose repository-level code generation and evaluate 8 popular LLMs on DevEval (e.g., gpt-4, gpt-3.5, StarCoder 2, DeepSeek Coder, CodeLLaMa). Our experiments reveal these LLMs' coding abilities in real-world code repositories. For example, in our experiments, the highest Pass@1 of gpt-4-turbo is only 53.04%. We also analyze LLMs' failed cases and summarize their shortcomings. We hope DevEval can facilitate the development of LLMs in real code repositories. DevEval, prompts, and LLMs' predictions have been released.", "arxiv_url": "https://arxiv.org/abs/2405.19856", "authors": ["Jia Li", "Ge Li", "Yunfei Zhao", "Yongmin Li", "Huanyu Liu", "Hao Zhu", "Lecheng Wang", "Kaibo Liu", "Zheng Fang", "Lanshen Wang", "Jiazheng Ding", "Xuanming Zhang", "Yuqi Zhu", "Yihong Dong", "Zhi Jin", "Binhua Li", "Fei Huang", "Yongbin Li"], "first_author": "Jia Li", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Repository-level code generation", "Dependency-aware evaluation", "Recall@k for dependency recall", "Pass@k functional testing", "Manual developer annotations", "Real-world code & dependency distribution alignment", "Cross-file and intra-file dependency tracking", "Context-aware function synthesis"], "summary": "本文提出 DevEval —— 一个由13名开发者手工标注、与真实代码库分布对齐的代码生成基准（1,874个样例），定义了仓库级代码生成任务并引入依赖召回指标，对多款大模型进行了评测与失败案例分析。", "quality": "High", "conference": "ACL 2024", "pdf_url": "https://arxiv.org/pdf/2405.19856v1", "published": "2024-05-30", "update_time": "2024-05-30", "download_time": "2025-12-11 16:33:53"}
{"id": "2406.12902", "title": "JavaBench: A Benchmark of Object-Oriented Code Generation for Evaluating Large Language Models", "abstract": "Code generation benchmarks such as HumanEval are widely adopted to evaluate LLMs' capabilities. However, after consolidating the latest 24 benchmarks, we noticed three significant imbalances. First, imbalanced programming language. 95.8% of benchmarks involve Python, while only 5 benchmarks involve Java. Second, imbalanced code granularity. Function-/statement-level benchmarks account for over 83.3% of benchmarks. Only a mere handful extends to class-/project-levels, and all are limited to Python. Third, lacking advanced features. Existing benchmarks primarily assess basic coding skills, while overlooking advanced Object-Oriented Programming (OOP) features (i.e., encapsulation, inheritance, and polymorphism).   To fill these gaps, we propose JavaBench, a project-level Java benchmark that exercises OOP features. It comprises four Java projects with 389 methods in 106 Java classes. The test coverage is up to 92%, and JavaBench is attested by 282 undergraduate students, reaching a 90.93/100 average score (i.e., pass rate against the test suite), ensuring the quality of documentation, code skeleton, and tests. To better evaluate LLM's capability against JavaBench, we introduce a systematic evaluation design covering three context settings and five synthesis strategies at two granularities using three hierarchical metrics. Our extensive experiment yields several interesting findings. First, we noticed that regarding project-level Java programming, LLMs are far behind undergraduate students (no project can be correctly completed by any studied LLMs, and at most 41.17% Pass@5 in a more relaxed evaluation). Second, using method signature as prompt context may strike an ideal balance for project-level code generation. JavaBench is publicly available at https://github.com/java-bench/JavaBench.", "arxiv_url": "https://arxiv.org/abs/2406.12902", "authors": ["Jialun Cao", "Zhiyong Chen", "Jiarong Wu", "Shing-chi Cheung", "Chang Xu"], "first_author": "Jialun Cao", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Project-level Java generation", "Object-oriented features (encapsulation, inheritance, polymorphism)", "Method-signature context prompting", "Context ablation (max/min/selected)", "Synthesis strategies (holistic/independent/incremental)", "Hierarchical evaluation metrics (Completion/Compilation/Pass@k)", "Class-wise vs test-wise evaluation granularity", "Student-validated canonical solutions and human baseline", "High test coverage with mocking-based tests"], "summary": "本文提出了JavaBench——一个包含四个面向对象Java项目、由学生验证并带高覆盖测试的项目级基准，并给出系统化的多上下文、多合成策略评估，揭示当前LLM在面向对象Java生成上明显落后于本科生且在仅提供方法签名时表现最好。", "quality": "High", "conference": "ASE", "pdf_url": "https://arxiv.org/pdf/2406.12902v2", "published": "2024-06-10", "update_time": "2024-10-11", "download_time": "2025-12-11 16:34:30"}
{"id": "2406.06918", "title": "HumanEvo: An Evolution-aware Benchmark for More Realistic Evaluation of Repository-level Code Generation", "abstract": "To evaluate the repository-level code generation capabilities of Large Language Models (LLMs) in complex real-world software development scenarios, many evaluation methods have been developed. These methods typically leverage contextual code from the latest version of a project to assist LLMs in accurately generating the desired function. However, such evaluation methods fail to consider the dynamic evolution of software projects over time, which we refer to as evolution-ignored settings. This in turn results in inaccurate evaluation of LLMs' performance. In this paper, we conduct an empirical study to deeply understand LLMs' code generation performance within settings that reflect the evolution nature of software development. To achieve this, we first construct an evolution-aware repository-level code generation dataset, namely HumanEvo, equipped with an automated execution-based evaluation tool. Second, we manually categorize HumanEvo according to dependency levels to more comprehensively analyze the model's performance in generating functions with different dependency levels. Third, we conduct extensive experiments on HumanEvo with seven representative and diverse LLMs to verify the effectiveness of the proposed benchmark. We obtain several important findings through our experimental study. For example, we find that previous evolution-ignored evaluation methods result in inflated performance of LLMs, with performance overestimations ranging from 10.0% to 61.1% under different context acquisition methods, compared to the evolution-aware evaluation approach. Based on the findings, we give actionable suggestions for more realistic evaluation of LLMs on code generation. We also build a shared evolution-aware code generation toolbox to facilitate future research.", "arxiv_url": "https://arxiv.org/abs/2406.06918", "authors": ["Dewu Zheng", "Yanlin Wang", "Ensheng Shi", "Ruikai Zhang", "Yuchi Ma", "Hongyu Zhang", "Zibin Zheng"], "first_author": "Dewu Zheng", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Evolution-aware evaluation", "Future context leakage", "Useful context missing", "Repository rollback to base commit", "Dependency-level categorization", "Execution-based automated testing", "Temporal/context acquisition analysis", "Cross-language (Python and Java) benchmark"], "summary": "本文提出了HumanEvo——一个考虑代码库随时间演化的存储库级代码生成基准（含400个Python/Java任务与自动化执行评估），并实证表明忽略演化会导致LLM性能被显著高估，同时分析了依赖级别与上下文检索策略的影响。", "quality": "High", "conference": "International Conference on Software Engineering (ICSE 2025)", "pdf_url": "https://arxiv.org/pdf/2406.06918v2", "published": "2024-06-11", "update_time": "2025-03-18", "download_time": "2025-12-11 16:34:58"}
{"id": "2406.11927", "title": "On the Impacts of Contexts on Repository-Level Code Generation", "abstract": "CodeLLMs have gained widespread adoption for code generation tasks, yet their capacity to handle repository-level code generation with complex contextual dependencies remains underexplored. Our work underscores the critical importance of leveraging repository-level contexts to generate executable and functionally correct code. We present RepoExec, a novel benchmark designed to evaluate repository-level code generation, with a focus on three key aspects: executability, functional correctness through comprehensive test case generation, and accurate utilization of cross-file contexts. Our study examines a controlled scenario where developers specify essential code dependencies (contexts), challenging models to integrate them effectively. Additionally, we introduce an instruction-tuned dataset that enhances CodeLLMs' ability to leverage dependencies, along with a new metric, Dependency Invocation Rate (DIR), to quantify context utilization. Experimental results reveal that while pretrained LLMs demonstrate superior performance in terms of correctness, instruction-tuned models excel in context utilization and debugging capabilities. RepoExec offers a comprehensive evaluation framework for assessing code functionality and alignment with developer intent, thereby advancing the development of more reliable CodeLLMs for real-world applications. The dataset and source code are available at https://github.com/FSoft-AI4Code/RepoExec.", "arxiv_url": "https://arxiv.org/abs/2406.11927", "authors": ["Nam Le Hai", "Dung Manh Nguyen", "Nghi D. Q. Bui"], "first_author": "Nam Le Hai", "category": ["Benchmark", "Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Repository-level dependency handling", "Executable evaluation environment", "Dependency Invocation Rate (DIR)", "Automated high-coverage unit-test generation", "Dependency extraction tool", "Dependency-aware instruction tuning", "Multi-round test-driven debugging", "Context-size ablation (full/medium/small)"], "summary": "该论文提出了一个可执行的仓库级代码生成基准与评估范式（包含高覆盖单元测试与依赖提取工具）、引入依赖调用率（DIR）度量并通过对18个模型的实证评估和依赖增强的指令微调与多轮调试展示了依赖上下文对生成正确性与依赖利用的影响。", "quality": "High", "conference": "NAACL 2025", "pdf_url": "https://arxiv.org/pdf/2406.11927v4", "published": "2024-06-17", "update_time": "2025-02-09", "download_time": "2025-12-11 16:35:35"}
{"id": "2406.16801", "title": "RES-Q: Evaluating Code-Editing Large Language Model Systems at the Repository Scale", "abstract": "The instruction-following ability of Large Language Models (LLMs) has cultivated a class of LLM-based systems capable of approaching complex tasks such as making edits to large code repositories. Due to the high sensitivity and unpredictability of LLM behavior in response to changes in prompting, robust evaluation tools are needed to drive future iteration of these systems. We propose RES-Q, a natural language instruction-based benchmark for evaluating $\\textbf{R}$epository $\\textbf{E}$diting $\\textbf{S}$ystems, which consists of 100 handcrafted repository editing tasks derived from real GitHub commits. Given an edit instruction and a code repository, RES-Q evaluates an LLM system's ability to interpret the instruction, navigate the repository to gather relevant information, and construct an appropriate edit that satisfies the specified criteria. We argue that evaluating LLMs in this way addresses issues with traditional benchmarks and provides a more holistic assessment of a model's abilities. We evaluate various state-of-the-art LLMs as language agents in a repository-editing system built on Qurrent OS, our language agent development software. Despite their 1% pass@1 performance difference on HumanEval, we find Claude Sonnet 3.5 outperforms GPT-4o by 12% pass@1 on RES-Q, indicating RES-Q's capacity to differentiate model capability as traditional benchmarks approach saturation. We further investigate token efficiency, performance relationships with existing benchmarks, and interesting disparities between closed and open-source LLMs. Code and dataset are available at https://github.com/Qurrent-AI/RES-Q.", "arxiv_url": "https://arxiv.org/abs/2406.16801", "authors": ["Beck LaBash", "August Rosedale", "Alex Reents", "Lucas Negritto", "Colin Wiel"], "first_author": "Beck LaBash", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Repository-scale code editing", "Handcrafted test suites for verification", "Lazy (ambiguous) natural-language edit instructions", "Cross-file and multi-file patches", "Commit-derived realistic tasks", "Language-agent (LLM agent) evaluation", "Submission/evaluation harness for automated scoring", "Token-efficiency and benchmark saturation analysis"], "summary": "本文提出了RES-Q——由100个基于真实GitHub提交的仓库级代码编辑任务及手工测试套件组成的基准，并在语言代理系统中评估多种大型模型的仓库编辑能力且发布了提交评测环境。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2406.16801v2", "published": "2024-06-24", "update_time": "2024-06-25", "download_time": "2025-12-11 16:36:10"}
{"id": "2408.14354", "title": "SWE-bench-java: A GitHub Issue Resolving Benchmark for Java", "abstract": "GitHub issue resolving is a critical task in software engineering, recently gaining significant attention in both industry and academia. Within this task, SWE-bench has been released to evaluate issue resolving capabilities of large language models (LLMs), but has so far only focused on Python version. However, supporting more programming languages is also important, as there is a strong demand in industry. As a first step toward multilingual support, we have developed a Java version of SWE-bench, called SWE-bench-java. We have publicly released the dataset, along with the corresponding Docker-based evaluation environment and leaderboard, which will be continuously maintained and updated in the coming months. To verify the reliability of SWE-bench-java, we implement a classic method SWE-agent and test several powerful LLMs on it. As is well known, developing a high-quality multi-lingual benchmark is time-consuming and labor-intensive, so we welcome contributions through pull requests or collaboration to accelerate its iteration and refinement, paving the way for fully automated programming.", "arxiv_url": "https://arxiv.org/abs/2408.14354", "authors": ["Daoguang Zan", "Zhirong Huang", "Ailun Yu", "Shaoxin Lin", "Yifan Shi", "Wei Liu", "Dong Chen", "Zongshuai Qi", "Hao Yu", "Lei Yu", "Dezhi Ran", "Muhan Zeng", "Bo Shen", "Pan Bian", "Guangtai Liang", "Bei Guan", "Pengjie Huang", "Tao Xie", "Yongji Wang", "Qianxiang Wang"], "first_author": "Daoguang Zan", "category": ["Benchmark", "Empirical"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Automated Program Repair", "Issue-to-Patch Generation", "Fail-to-Pass Test Extraction", "Docker-based Reproducible Evaluation", "Build Tool & JDK Inference", "Questionnaire-based Manual Verification", "Repository and Patch Mining", "Dependency Caching and Evaluation Optimization"], "summary": "本文提出并开源SWE-bench-java-verified——一个经手工验证的Java语言GitHub issue修复基准（91个可复现问题）、相应的Docker评测环境与排行榜，并基于该基准评估了多种模型与代理的修复能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2408.14354v1", "published": "2024-08-26", "update_time": "2024-08-26", "download_time": "2025-12-11 16:36:39"}
{"id": "2410.01353", "title": "Codev-Bench: How Do LLMs Understand Developer-Centric Code Completion?", "abstract": "Code completion, a key downstream task in code generation, is one of the most frequent and impactful methods for enhancing developer productivity in software development. As intelligent completion tools evolve, we need a robust evaluation benchmark that enables meaningful comparisons between products and guides future advancements. However, existing benchmarks focus more on coarse-grained tasks without industrial analysis resembling general code generation rather than the real-world scenarios developers encounter. Moreover, these benchmarks often rely on costly and time-consuming human annotation, and the standalone test cases fail to leverage minimal tests for maximum repository-level understanding and code coverage. To address these limitations, we first analyze business data from an industrial code completion tool and redefine the evaluation criteria to better align with the developer's intent and desired completion behavior throughout the coding process. Based on these insights, we introduce Codev-Agent, an agent-based system that automates repository crawling, constructs execution environments, extracts dynamic calling chains from existing unit tests, and generates new test samples to avoid data leakage, ensuring fair and effective comparisons. Using Codev-Agent, we present the Code-Development Benchmark (Codev-Bench), a fine-grained, real-world, repository-level, and developer-centric evaluation framework. Codev-Bench assesses whether a code completion tool can capture a developer's immediate intent and suggest appropriate code across diverse contexts, providing a more realistic benchmark for code completion in modern software development.", "arxiv_url": "https://arxiv.org/abs/2410.01353", "authors": ["Zhenyu Pan", "Rongyu Cao", "Yongchang Cao", "Yingwei Ma", "Binhua Li", "Fei Huang", "Han Liu", "Yongbin Li"], "first_author": "Zhenyu Pan", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Developer-centric evaluation", "Repository-level context", "Agent-based pipeline", "Dynamic call-chain extraction", "Automated test generation", "Execution environment reconstruction", "Data-leakage-aware sampling", "Industrial product feedback analysis"], "summary": "本文基于工业代码补全产品的使用反馈提出Codev-Agent自动化系统并构建了Codev-Bench，一个面向开发者、仓库级且细粒度的代码补全评测基准，通过仓库爬取、执行环境搭建、动态调用链提取与自动生成测试样例实现更真实和公平的模型评估。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2410.01353v3", "published": "2024-10-02", "update_time": "2024-10-24", "download_time": "2025-12-11 16:37:15"}
{"id": "2410.03859", "title": "SWE-bench Multimodal: Do AI Systems Generalize to Visual Software Domains?", "abstract": "Autonomous systems for software engineering are now capable of fixing bugs and developing features. These systems are commonly evaluated on SWE-bench (Jimenez et al., 2024a), which assesses their ability to solve software issues from GitHub repositories. However, SWE-bench uses only Python repositories, with problem statements presented predominantly as text and lacking visual elements such as images. This limited coverage motivates our inquiry into how existing systems might perform on unrepresented software engineering domains (e.g., front-end, game development, DevOps), which use different programming languages and paradigms. Therefore, we propose SWE-bench Multimodal (SWE-bench M), to evaluate systems on their ability to fix bugs in visual, user-facing JavaScript software. SWE-bench M features 617 task instances collected from 17 JavaScript libraries used for web interface design, diagramming, data visualization, syntax highlighting, and interactive mapping. Each SWE-bench M task instance contains at least one image in its problem statement or unit tests. Our analysis finds that top-performing SWE-bench systems struggle with SWE-bench M, revealing limitations in visual problem-solving and cross-language generalization. Lastly, we show that SWE-agent's flexible language-agnostic features enable it to substantially outperform alternatives on SWE-bench M, resolving 12% of task instances compared to 6% for the next best system.", "arxiv_url": "https://arxiv.org/abs/2410.03859", "authors": ["John Yang", "Carlos E. Jimenez", "Alex L. Zhang", "Kilian Lieret", "Joyce Yang", "Xindi Wu", "Ori Press", "Niklas Muennighoff", "Gabriel Synnaeve", "Karthik R. Narasimhan", "Diyi Yang", "Sida I. Wang", "Ofir Press"], "first_author": "John Yang", "category": ["Benchmark", "Empirical"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Multimodal bug fixing", "UI screenshot comprehension", "Frontend JavaScript ecosystems", "Repository-level fail-to-pass evaluation", "Image-required task instances", "Cross-language generalization challenges", "Agent toolchain language-agnosticism"], "summary": "本文提出 SWE-bench Multimodal，一个包含图像/视频问题的 JavaScript 前端仓库级别缺陷修复基准，揭示现有自动化 LLM 系统在视觉问题理解与跨语言泛化上的不足并提供分析与改进建议。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2410.03859v1", "published": "2024-10-04", "update_time": "2024-10-04", "download_time": "2025-12-11 16:37:50"}
{"id": "2410.06992", "title": "SWE-Bench+: Enhanced Coding Benchmark for LLMs", "abstract": "Large Language Models (LLMs) in Software Engineering (SE) can offer assistance for coding. To facilitate a rigorous evaluation of LLMs in practical coding contexts, Carlos et al. introduced the SWE-bench dataset, which comprises 2,294 real-world GitHub issues and their corresponding pull requests, collected from 12 widely used Python repositories. Several impressive LLM-based toolkits recently are developed and evaluated on this dataset. However, a systematic evaluation of the quality of SWE-bench remains missing. In this paper, we addressed this gap by presenting an empirical analysis of the SWE-bench dataset. We conducted a manual screening of instances where SWEAgent + GPT-4 successfully resolved issues by comparing the model-generated patches with the actual pull requests. SWE-Agent+GPT-4 was at the top of SWE-bench leaderboard during the time of our study. Our analysis reveals some critical issues with the SWE-bench dataset: 1) 32.67% of the successful patches involve cheating as the solutions were directly provided in the issue report or the comments. We refer to as solution leakage problem. 2) 31.08% of the passed patches are suspicious patches due to weak test cases, i.e., the tests were not adequate to verify the correctness of a patch. When we filtered out these problematic issues, the resolution rate of SWE-Agent+GPT-4 dropped from 12.47% to 3.97%. We also observed that the same data quality issues also exist in the two variants of SWE-bench, i.e., SWE-bench Lite and SWE-Bench Verified. In addition, over 94% of the issues were created before LLM's knowledge cutoff dates, posing potential data leakage issues.", "arxiv_url": "https://arxiv.org/abs/2410.06992", "authors": ["Reem Aleithan", "Haoran Xue", "Mohammad Mahdi Mohajer", "Elijah Nnorom", "Gias Uddin", "Song Wang"], "first_author": "Reem Aleithan", "category": ["Empirical", "Benchmark"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Solution leakage detection", "Test adequacy / weak tests", "Temporal cutoff filtering", "Issue-to-patch evaluation", "Patch-level gold vs generated comparison", "Dataset curation for robust evaluation", "Leaderboard/result auditing"], "summary": "本文通过对现有SWE‑bench基准进行人工与实证分析，发现大量“解答泄露”和弱测试导致的可疑通过补丁并据此构建了避开模型训练截止期且去除泄露的新基准SWE‑Bench+，在新基准上模型的修复通过率显著下降。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2410.06992v2", "published": "2024-10-09", "update_time": "2024-10-10", "download_time": "2025-12-11 16:38:28"}
{"id": "2410.07331", "title": "DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models", "abstract": "We introduce DA-Code, a code generation benchmark specifically designed to assess LLMs on agent-based data science tasks. This benchmark features three core elements: First, the tasks within DA-Code are inherently challenging, setting them apart from traditional code generation tasks and demanding advanced coding skills in grounding and planning. Second, examples in DA-Code are all based on real and diverse data, covering a wide range of complex data wrangling and analytics tasks. Third, to solve the tasks, the models must utilize complex data science programming languages, to perform intricate data processing and derive the answers. We set up the benchmark in a controllable and executable environment that aligns with real-world data analysis scenarios and is scalable. The annotators meticulously design the evaluation suite to ensure the accuracy and robustness of the evaluation. We develop the DA-Agent baseline. Experiments show that although the baseline performs better than other existing frameworks, using the current best LLMs achieves only 30.5% accuracy, leaving ample room for improvement. We release our benchmark at https://da-code-bench.github.io.", "arxiv_url": "https://arxiv.org/abs/2410.07331", "authors": ["Yiming Huang", "Jianwen Luo", "Yan Yu", "Yitong Zhang", "Fangyu Lei", "Yifan Wei", "Shizhu He", "Lifu Huang", "Xiao Liu", "Jun Zhao", "Kang Liu"], "first_author": "Yiming Huang", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Agent-based data science", "Interactive executable sandbox", "Data wrangling / EDA / ML pipeline", "Multi-source data (files, databases, documents)", "SQL+Python+Bash multi-language execution", "Autonomous iterative code editing", "Evaluation suite with red-teaming", "Baseline code agent framework"], "summary": "本文提出了DA-Code，一个包含500个真实复杂代理式数据科学任务的可执行基准与交互沙箱，并提供评估套件及基线代理以衡量大型语言模型在数据清洗、探索性分析与整个机器学习流水线上的自动化代码生成与推理能力。", "quality": "High", "conference": "EMNLP 2024", "pdf_url": "https://arxiv.org/pdf/2410.07331v2", "published": "2024-10-09", "update_time": "2024-10-11", "download_time": "2025-12-11 16:39:06"}
{"id": "2410.21647", "title": "Can Language Models Replace Programmers for Coding? REPOCOD Says 'Not Yet'", "abstract": "Recently, a number of repository-level code generation benchmarks-such as CoderEval, DevEval, RepoEval, RepoBench, and LongCodeArena-have emerged to evaluate the capabilities of large language models (LLMs) beyond standalone benchmarks like HumanEval and MBPP. Thus, a natural question is, would LLMs have similar performance in real world coding tasks as their performance in these benchmarks? Unfortunately, one cannot answer this question, since these benchmarks consist of short completions, synthetic examples, or focus on limited scale repositories, failing to represent real-world coding tasks.   To address these challenges, we create REPOCOD, a Python code-generation benchmark containing complex tasks with realistic dependencies in real-world large projects and appropriate metrics for evaluating source code. It includes 980 whole-function generation tasks from 11 popular projects, 50.8% of which require repository-level context. REPOCOD includes 314 developer-written test cases per instance for better evaluation. We evaluate ten LLMs on REPOCOD and find that none achieves more than 30% pass@1 on REPOCOD, indicating the necessity of building stronger LLMs that can help developers in real-world software development. In addition, we found that retrieval-augmented generation achieves better results than using target function dependencies as context.", "arxiv_url": "https://arxiv.org/abs/2410.21647", "authors": ["Shanchao Liang", "Yiran Hu", "Nan Jiang", "Lin Tan"], "first_author": "Shanchao Liang", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Repository-level dependencies", "Whole-function generation", "Developer-written unit tests for validation", "Static and dynamic analysis for test-function mapping", "Targeted test selection to reduce evaluation cost", "Retrieval-augmented generation vs dependency-context", "Large-scale real-world Python repositories", "Execution-based evaluation (unit-test based)"], "summary": "本文提出REPOCOD，一个包含来自11个大型Python项目的980个复杂仓库级全函数生成任务的基准，使用开发者编写的单元测试进行执行式评估并展示现有LLM在真实软件开发场景下表现仍然有限。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2410.21647v4", "published": "2024-10-29", "update_time": "2025-06-24", "download_time": "2025-12-11 16:39:44"}
{"id": "2410.21157", "title": "M2rc-Eval: Massively Multilingual Repository-level Code Completion Evaluation", "abstract": "Repository-level code completion has drawn great attention in software engineering, and several benchmark datasets have been introduced. However, existing repository-level code completion benchmarks usually focus on a limited number of languages (<5), which cannot evaluate the general code intelligence abilities across different languages for existing code Large Language Models (LLMs). Besides, the existing benchmarks usually report overall average scores of different languages, where the fine-grained abilities in different completion scenarios are ignored. Therefore, to facilitate the research of code LLMs in multilingual scenarios, we propose a massively multilingual repository-level code completion benchmark covering 18 programming languages (called M2RC-EVAL), and two types of fine-grained annotations (i.e., bucket-level and semantic-level) on different completion scenarios are provided, where we obtain these annotations based on the parsed abstract syntax tree. Moreover, we also curate a massively multilingual instruction corpora M2RC- INSTRUCT dataset to improve the repository-level code completion abilities of existing code LLMs. Comprehensive experimental results demonstrate the effectiveness of our M2RC-EVAL and M2RC-INSTRUCT.", "arxiv_url": "https://arxiv.org/abs/2410.21157", "authors": ["Jiaheng Liu", "Ken Deng", "Congnan Liu", "Jian Yang", "Shukai Liu", "He Zhu", "Peng Zhao", "Linzheng Chai", "Yanan Wu", "Ke Jin", "Ge Zhang", "Zekun Wang", "Guoan Zhang", "Bangyu Xiang", "Wenbo Su", "Bo Zheng"], "first_author": "Jiaheng Liu", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Repository-level completion", "Multilingual evaluation (18 languages)", "AST-based cursor selection", "Bucket-level difficulty labeling", "Semantic-level code labels", "Cross-file context retrieval", "Multilingual instruction corpora for tuning"], "summary": "该论文提出了一个包含18种编程语言、基于AST提供桶级与语义级精细标注的仓库级多语言代码补全基准并发布配套的多语言指令语料，以评估并提升代码大模型的跨语言仓库级补全能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2410.21157v1", "published": "2024-10-28", "update_time": "2024-10-28", "download_time": "2025-12-11 16:40:18"}
{"id": "2411.18019", "title": "A Real-World Benchmark for Evaluating Fine-Grained Issue Solving Capabilities of Large Language Models", "abstract": "Automatically resolving software issues is crucial for software development in practice, impacting the software quality and user experience. The process of resolving real-world issues encompasses tasks such as question-answering (QA), fault localization, and code editing. Existing benchmarks such as HumanEval fall short in their ability to assess LLMs' proficiency in solving issues within a codebase. Although benchmarks like SWE-Bench are designed to evaluate the LLMs' capability to handle real-world GitHub issues, the end-to-end evaluation method cannot provide granular insights on the performance of subtasks involved in issue solving. To address existing deficiencies in benchmarking LLMs for practical software engineering tasks, we introduce FAUN-Eval, a benchmark specifically designed to evaluate the Fine-grAined issUe solviNg capabilities of LLMs. FAUN-Eval systematically assesses LLMs across three distinct tasks: QA, fault localization, and code editing. This benchmark is constructed using a dataset curated from 30 well-known GitHub repositories. For each entry, issue and pull request (PR) pairs are meticulously compiled and validated using cross-referencing and keyword verification methods. FAUN-Eval includes 300 entries and employs both LLM and manual checks to ensure data quality. We evaluate ten LLMs with FAUN-Eval, including four closed-source and six open-source models. Our experimental results reveal several key findings. We find that the top-performing LLMs differ across the different tasks. Additionally, features in issues may lead LLMs to generate incorrect information. Moreover, models may vary in their proficiency with texts of different lengths.", "arxiv_url": "https://arxiv.org/abs/2411.18019", "authors": ["Ruida Hu", "Chao Peng", "Jingyi Ren", "Bo Jiang", "Xiangxin Meng", "Qinyun Wu", "Pengfei Gao", "Xinchen Wang", "Cuiyun Gao"], "first_author": "Ruida Hu", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Fine-grained issue decomposition", "Real-world GitHub issue–PR pairing", "Fault localization evaluation", "Code edit/patch generation assessment", "Question-answering for issue triage", "Data quality verification (LLM + manual checks)", "Multi-task performance metrics (EM, CodeBLEU, Edit Similarity)", "Sensitivity to issue text features and length"], "summary": "本文提出FAUN-Eval，一个从真实GitHub仓库构建的细粒度基准与评估框架，用于分别评测大模型在问题问答、故障定位与代码编辑三项子任务上的能力并对多款模型进行对比分析。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2411.18019v1", "published": "2024-11-27", "update_time": "2024-11-27", "download_time": "2025-12-11 16:40:55"}
{"id": "2412.01769", "title": "Commit0: Library Generation from Scratch", "abstract": "With the goal of benchmarking generative systems beyond expert software development ability, we introduce Commit0, a benchmark that challenges AI agents to write libraries from scratch. Agents are provided with a specification document outlining the library's API as well as a suite of interactive unit tests, with the goal of producing an implementation of this API accordingly. The implementation is validated through running these unit tests. As a benchmark, Commit0 is designed to move beyond static one-shot code generation towards agents that must process long-form natural language specifications, adapt to multi-stage feedback, and generate code with complex dependencies. Commit0 also offers an interactive environment where models receive static analysis and execution feedback on the code they generate. Our experiments demonstrate that while current agents can pass some unit tests, none can yet fully reproduce full libraries. Results also show that interactive feedback is quite useful for models to generate code that passes more unit tests, validating the benchmarks that facilitate its use.", "arxiv_url": "https://arxiv.org/abs/2412.01769", "authors": ["Wenting Zhao", "Nan Jiang", "Celine Lee", "Justin T Chiu", "Claire Cardie", "Matthias Gallé", "Alexander M Rush"], "first_author": "Wenting Zhao", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Repository-level code generation", "Interactive test-driven synthesis", "Long-form specification processing", "Dependency-aware implementation ordering", "Execution & static-analysis feedback loop", "Starter-repo hole-filling", "Automated unit-test evaluation"], "summary": "COMMIT0 提出一个从零生成完整软件库的基准，提供长篇规范、起始仓库与交互式单元测试以评估和促进模型在多轮反馈、依赖管理与长上下文下的代码生成能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2412.01769v1", "published": "2024-12-02", "update_time": "2024-12-02", "download_time": "2025-12-11 16:41:31"}
{"id": "2412.11990", "title": "ExecRepoBench: Multi-level Executable Code Completion Evaluation", "abstract": "Code completion has become an essential tool for daily software development. Existing evaluation benchmarks often employ static methods that do not fully capture the dynamic nature of real-world coding environments and face significant challenges, including limited context length, reliance on superficial evaluation metrics, and potential overfitting to training datasets. In this work, we introduce a novel framework for enhancing code completion in software development through the creation of a repository-level benchmark ExecRepoBench and the instruction corpora Repo-Instruct, aim at improving the functionality of open-source large language models (LLMs) in real-world coding scenarios that involve complex interdependencies across multiple files. ExecRepoBench includes 1.2K samples from active Python repositories. Plus, we present a multi-level grammar-based completion methodology conditioned on the abstract syntax tree to mask code fragments at various logical units (e.g. statements, expressions, and functions). Then, we fine-tune the open-source LLM with 7B parameters on Repo-Instruct to produce a strong code completion baseline model Qwen2.5-Coder-Instruct-C based on the open-source model. Qwen2.5-Coder-Instruct-C is rigorously evaluated against existing benchmarks, including MultiPL-E and ExecRepoBench, which consistently outperforms prior baselines across all programming languages. The deployment of \\ourmethod{} can be used as a high-performance, local service for programming development\\footnote{\\url{https://execrepobench.github.io/}}.", "arxiv_url": "https://arxiv.org/abs/2412.11990", "authors": ["Jian Yang", "Jiajun Zhang", "Jiaxi Yang", "Ke Jin", "Lei Zhang", "Qiyao Peng", "Ken Deng", "Yibo Miao", "Tianyu Liu", "Zeyu Cui", "Binyuan Hui", "Junyang Lin"], "first_author": "Jian Yang", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Executable repository benchmark", "Unit-test-based evaluation", "AST-based multi-level masking", "Repository-level (cross-file) completion", "Instruction corpus for repo completion", "Data decontamination (leakage removal)", "Supervised fine-tuning for completion"], "summary": "本文提出了可执行的仓库级代码补全基准 EXECREPOBENCH 与基于抽象语法树的多级语法掩码指令语料 REPO-INSTRUCT，并在此基础上微调开源模型以实现基于单元测试的跨文件代码补全评测与改进。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2412.11990v1", "published": "2024-12-16", "update_time": "2024-12-16", "download_time": "2025-12-11 16:42:03"}
{"id": "2501.13699", "title": "DI-BENCH: Benchmarking Large Language Models on Dependency Inference with Testable Repositories at Scale", "abstract": "Large Language Models have advanced automated software development, however, it remains a challenge to correctly infer dependencies, namely, identifying the internal components and external packages required for a repository to successfully run. Existing studies highlight that dependency-related issues cause over 40\\% of observed runtime errors on the generated repository. To address this, we introduce DI-BENCH, a large-scale benchmark and evaluation framework specifically designed to assess LLMs' capability on dependency inference. The benchmark features 581 repositories with testing environments across Python, C#, Rust, and JavaScript. Extensive experiments with textual and execution-based metrics reveal that the current best-performing model achieves only a 42.9% execution pass rate, indicating significant room for improvement. DI-BENCH establishes a new viewpoint for evaluating LLM performance on repositories, paving the way for more robust end-to-end software synthesis.", "arxiv_url": "https://arxiv.org/abs/2501.13699", "authors": ["Linghao Zhang", "Junhao Wang", "Shilin He", "Chaoyun Zhang", "Yu Kang", "Bowen Li", "Jiaheng Wen", "Chengxing Xie", "Maoquan Wang", "Yufan Huang", "Elsie Nallipogu", "Qingwei Lin", "Yingnong Dang", "Saravan Rajmohan", "Dongmei Zhang", "Qi Zhang"], "first_author": "Linghao Zhang", "category": ["Benchmark", "Empirical"], "field": "Dependency & Build Management", "task": "Dependency Inference", "tags": ["CI-based execution evaluation", "Repository-level dependency extraction", "Masked build configuration reconstruction", "Cross-language coverage (Python, C#, Rust, JavaScript)", "Execution pass rate metric", "Dependency version resolution and mismatch", "Scalable curated testable repositories", "Precision/recall for dependency lists"], "summary": "DI-BENCH 提出一个包含581个可执行仓库、跨 Python/C#/Rust/JavaScript 的依赖推断基准，并通过复用 CI 流水线进行文本与执行级评估，揭示当前 LLM 在依赖推断上的显著不足（最佳执行率仅 42.9%）。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2501.13699v1", "published": "2025-01-23", "update_time": "2025-01-23", "download_time": "2025-12-11 16:42:35"}
{"id": "2502.00226", "title": "HackerRank-ASTRA: Evaluating Correctness & Consistency of Large Language Models on cross-domain multi-file project problems", "abstract": "Evaluating the real-world applicability of large language models (LLMs) provides valuable insights for their development and use in software development tasks. Existing benchmarks often focus on standalone coding problems or specific libraries, overlooking multi-file, project-based scenarios and lacking a rigorous evaluation of consistency. The HackerRank-ASTRA Benchmark introduces project-based coding problems that mirror real-world scenarios. It evaluates model consistency through 32 runs (k = 32) and median standard deviation while incorporating taxonomy-level analysis to assess sub-skill capabilities. Initial evaluations on 65 problems show that the top three models -- o1, o1-preview, and Claude-3.5-Sonnet-1022 -- achieved comparable average scores of 75%, with no statistically significant differences in performance. Notably, Claude-3.5-Sonnet-1022 demonstrated the highest consistency across problems, with low variability (SD = 0.0497), which was statistically significant compared to other models, highlighting its reliability for real-world software development tasks.", "arxiv_url": "https://arxiv.org/abs/2502.00226", "authors": ["Jun Xing", "Mayur Bhatia", "Sahil Phulwani", "Darshan Suresh", "Rafik Matta"], "first_author": "Jun Xing", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Multi-file project problems", "Cross-framework frontend tasks", "Consistency evaluation across runs", "Median standard deviation (k=32)", "Mean pass@1 and mean score metrics", "Taxonomy-level subskill analysis", "Long-context code generation", "High test-case coverage", "Industry-aligned proprietary problem bank"], "summary": "HackerRank-ASTRA 提出并发布了一个包含65个多文件、跨框架项目级前端开发问题的基准，并通过32次独立运行的中位标准差与多项正确性指标评估LLM在正确性与一致性及子技能层面的表现。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2502.00226v1", "published": "2025-01-31", "update_time": "2025-01-31", "download_time": "2025-12-11 16:43:08"}
{"id": "2502.12115", "title": "SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?", "abstract": "We introduce SWE-Lancer, a benchmark of over 1,400 freelance software engineering tasks from Upwork, valued at \\$1 million USD total in real-world payouts. SWE-Lancer encompasses both independent engineering tasks--ranging from \\$50 bug fixes to \\$32,000 feature implementations--and managerial tasks, where models choose between technical implementation proposals. Independent tasks are graded with end-to-end tests triple-verified by experienced software engineers, while managerial decisions are assessed against the choices of the original hired engineering managers. We evaluate model performance and find that frontier models are still unable to solve the majority of tasks. To facilitate future research, we open-source a unified Docker image and a public evaluation split, SWE-Lancer Diamond (https://github.com/openai/SWELancer-Benchmark). By mapping model performance to monetary value, we hope SWE-Lancer enables greater research into the economic impact of AI model development.", "arxiv_url": "https://arxiv.org/abs/2502.12115", "authors": ["Samuel Miserendino", "Michele Wang", "Tejal Patwardhan", "Johannes Heidecke"], "first_author": "Samuel Miserendino", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Freelance job-derived benchmark", "End-to-end browser automation tests", "Economic payout mapping", "Managerial proposal selection evaluation", "Full-stack repository-level patching", "Triple-verified test validation", "Public/private evaluation splits"], "summary": "本文构建并公开了一个基于真实Upwork自由职业软件工程任务（共1,488个、总计约100万美元报酬）的基准，采用专业工程师编写并三重验证的端到端测试评估模型对真实全栈修补与管理决策的能力，并报告前沿模型在该基准上的表现。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2502.12115v4", "published": "2025-02-17", "update_time": "2025-05-29", "download_time": "2025-12-11 16:43:43"}
{"id": "2503.06680", "title": "FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation for Feature Implementation", "abstract": "Implementing new features in repository-level codebases is a crucial application of code generation models. However, current benchmarks lack a dedicated evaluation framework for this capability. To fill this gap, we introduce FEA-Bench, a benchmark designed to assess the ability of large language models (LLMs) to perform incremental development within code repositories. We collect pull requests from 83 GitHub repositories and use rule-based and intent-based filtering to construct task instances focused on new feature development. Each task instance containing code changes is paired with relevant unit test files to ensure that the solution can be verified. The feature implementation requires LLMs to simultaneously possess code completion capabilities for new components and code editing abilities for other relevant parts in the code repository, providing a more comprehensive evaluation method of LLMs' automated software engineering capabilities. Experimental results show that LLMs perform significantly worse in the FEA-Bench, highlighting considerable challenges in such repository-level incremental code development.", "arxiv_url": "https://arxiv.org/abs/2503.06680", "authors": ["Wei Li", "Xin Zhang", "Zhongxin Guo", "Shaoguang Mao", "Wen Luo", "Guangyue Peng", "Yangyu Huang", "Houfeng Wang", "Scarlett Li"], "first_author": "Wei Li", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Repository-level incremental development", "Feature implementation from pull requests", "Unit-test-based execution validation", "Cross-file edits and new-file addition", "Automated PR-to-task collection pipeline", "Large-patch long-form code generation"], "summary": "本文提出FEA-Bench——一个从GitHub pull request构建的基准，用以评估LLM在仓库级别实现新功能（包括新增组件与跨文件编辑）能力，并以单元测试执行结果作为验证，实验证明当前模型在此任务上表现较差。", "quality": "High", "conference": "ACL 2025", "pdf_url": "https://arxiv.org/pdf/2503.06680v2", "published": "2025-03-09", "update_time": "2025-06-19", "download_time": "2025-12-11 16:44:20"}
{"id": "2503.06689", "title": "DependEval: Benchmarking LLMs for Repository Dependency Understanding", "abstract": "While large language models (LLMs) have shown considerable promise in code generation, real-world software development demands advanced repository-level reasoning. This includes understanding dependencies, project structures, and managing multi-file changes. However, the ability of LLMs to effectively comprehend and handle complex code repositories has yet to be fully explored. To address challenges, we introduce a hierarchical benchmark designed to evaluate repository dependency understanding (DependEval). Benchmark is based on 15,576 repositories collected from real-world websites. It evaluates models on three core tasks: Dependency Recognition, Repository Construction, and Multi-file Editing, across 8 programming languages from actual code repositories. Our evaluation of over 25 LLMs reveals substantial performance gaps and provides valuable insights into repository-level code understanding.", "arxiv_url": "https://arxiv.org/abs/2503.06689", "authors": ["Junjia Du", "Yadi Liu", "Hongcheng Guo", "Jiawei Wang", "Haojian Huang", "Yunyi Ni", "Zhoujun Li"], "first_author": "Junjia Du", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Repository dependency resolution", "Cross-file dependency parsing", "Repository structure generation from requirements", "Coordinated multi-file editing", "Hierarchical evaluation metrics", "Static import-based dependency extraction", "Call-chain extraction", "Multilingual repository benchmark"], "summary": "本文提出DependEval，一个覆盖8种编程语言、基于15,576个真实仓库的分层基准，用于评估LLM在依赖识别、仓库构建和多文件编辑等仓库级代码理解任务上的能力，并在25+模型上给出细粒度性能分析。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2503.06689v1", "published": "2025-03-09", "update_time": "2025-03-09", "download_time": "2025-12-11 16:45:03"}
{"id": "2503.07010", "title": "ProjectEval: A Benchmark for Programming Agents Automated Evaluation on Project-Level Code Generation", "abstract": "Recently, LLM agents have made rapid progress in improving their programming capabilities. However, existing benchmarks lack the ability to automatically evaluate from users' perspective, and also lack the explainability of the results of LLM agents' code generation capabilities. Thus, we introduce ProjectEval, a new benchmark for LLM agents project-level code generation's automated evaluation by simulating user interaction. ProjectEval is constructed by LLM with human reviewing. It has three different level inputs of natural languages or code skeletons. ProjectEval can evaluate the generated projects by user interaction simulation for execution, and by code similarity through existing objective indicators. Through ProjectEval, we find that systematic engineering project code, overall understanding of the project and comprehensive analysis capability are the keys for LLM agents to achieve practical projects. Our findings and benchmark provide valuable insights for developing more effective programming agents that can be deployed in future real-world production.", "arxiv_url": "https://arxiv.org/abs/2503.07010", "authors": ["Kaiyuan Liu", "Youcheng Pan", "Yang Xiang", "Daojing He", "Jing Li", "Yexing Du", "Tianrun Gao"], "first_author": "Kaiyuan Liu", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Project-level code generation benchmark", "User interaction simulation", "Execution-based automated testing", "Three-level input (NL / checklist / skeleton)", "Parameter-description alignment", "Semi-automated construction (LLM + human review)", "Web and console project evaluation", "Explainable evaluation metrics"], "summary": "本文提出了ProjectEval，一个通过模拟用户交互并结合三种输入级别与执行型测试套件，对编程代理项目级代码生成进行自动化且可解释评估的基准。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2503.07010v2", "published": "2025-03-10", "update_time": "2025-05-31", "download_time": "2025-12-11 16:45:49"}
{"id": "2503.07358", "title": "RepoST: Scalable Repository-Level Coding Environment Construction with Sandbox Testing", "abstract": "We present RepoST, a scalable method to construct environments that provide execution feedback for repository-level code generation for both training and evaluation. Unlike existing works that aim to build entire repositories for execution, which is challenging for both human and LLMs, we provide execution feedback with sandbox testing, which isolates a given target function and its dependencies to a separate script for testing. Sandbox testing reduces the complexity of external dependencies and enables constructing environments at a large scale. We use our method to construct RepoST-Train, a large-scale train set with 7,415 functions from 832 repositories. Training with the execution feedback provided by RepoST-Train leads to a performance gain of 5.5% Pass@1 on HumanEval and 3.5% Pass@1 on RepoEval. We also build an evaluation dataset, RepoST-Eval, and benchmark 12 code generation models.", "arxiv_url": "https://arxiv.org/abs/2503.07358", "authors": ["Yiqing Xie", "Alex Xie", "Divyanshu Sheth", "Pengfei Liu", "Daniel Fried", "Carolyn Rose"], "first_author": "Yiqing Xie", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Sandbox Testing", "Repository-level Environment Construction", "Function-level Sandboxing", "Automated Test Generation", "Equivalence Testing", "Mocking External APIs and Files", "AST-based Functionality Verification", "Iterative Debugging and Filtering", "Executable Evaluation Scripts", "Large-scale Train/Eval Dataset Construction"], "summary": "本文提出REPOST，通过将目标函数及其局部依赖沙箱化并由LLM生成测试与模拟资源，自动构建可执行的仓库级代码生成训练与评估环境，并发布大规模的训练集与评测集以提升模型性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2503.07358v1", "published": "2025-03-10", "update_time": "2025-03-10", "download_time": "2025-12-11 16:46:22"}
{"id": "2504.02605", "title": "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving", "abstract": "The task of issue resolving is to modify a codebase to generate a patch that addresses a given issue. However, existing benchmarks, such as SWE-bench, focus almost exclusively on Python, making them insufficient for evaluating Large Language Models (LLMs) across diverse software ecosystems. To address this, we introduce a multilingual issue-resolving benchmark, called Multi-SWE-bench, covering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a total of 1,632 high-quality instances, which were carefully annotated from 2,456 candidates by 68 expert annotators, ensuring that the benchmark can provide an accurate and reliable evaluation. Based on Multi-SWE-bench, we evaluate a series of state-of-the-art models using three representative methods (Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with key empirical insights. In addition, we launch a Multi-SWE-RL open-source community, aimed at building large-scale reinforcement learning (RL) training datasets for issue-resolving tasks. As an initial contribution, we release a set of 4,723 well-structured instances spanning seven programming languages, laying a solid foundation for RL research in this domain. More importantly, we open-source our entire data production pipeline, along with detailed tutorials, encouraging the open-source community to continuously contribute and expand the dataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL community as catalysts for advancing RL toward its full potential, bringing us one step closer to the dawn of AGI.", "arxiv_url": "https://arxiv.org/abs/2504.02605", "authors": ["Daoguang Zan", "Zhirong Huang", "Wei Liu", "Hanwu Chen", "Linhao Zhang", "Shulin Xin", "Lu Chen", "Qi Liu", "Xiaojian Zhong", "Aoyan Li", "Siyao Liu", "Yongsheng Xiao", "Liangqiang Chen", "Yuyu Zhang", "Jing Su", "Tianyu Liu", "Rui Long", "Kai Shen", "Liang Xiang"], "first_author": "Daoguang Zan", "category": ["Benchmark", "Empirical"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Multilingual issue-resolving benchmark", "Pull-request based instance curation", "Dockerized reproducible test environments", "Dual-stage human verification", "Cross-language generalization analysis", "Patch complexity & multi-file impact analysis", "Open RL training instance release", "Agentless vs agent-based evaluation"], "summary": "本文提出 Multi-SWE-bench，一个覆盖 Java、TypeScript、JavaScript、Go、Rust、C 与 C++ 的多语言 issue-resolving 基准（1,632 个人工验证实例）并发布 4,723 个用于 RL 的开放实例，提供可复现环境与严格的人工检验流程，同时对多种方法和模型进行了跨语言评测与细粒度表现分析。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2504.02605v1", "published": "2025-04-03", "update_time": "2025-04-03", "download_time": "2025-12-11 16:47:22"}
{"id": "2108.11590", "title": "AVATAR: A Parallel Corpus for Java-Python Program Translation", "abstract": "Program translation refers to migrating source code from one programming language to another. It has tremendous practical value in software development, as porting software across languages is time-consuming and costly. Automating program translation is of paramount importance in software migration, and recently researchers explored unsupervised approaches due to the unavailability of parallel corpora. However, the availability of pre-trained language models for programming languages enables supervised fine-tuning with a small number of labeled examples. Therefore, we present AVATAR, a collection of 9,515 programming problems and their solutions written in two popular languages, Java and Python. AVATAR is collected from competitive programming sites, online platforms, and open-source repositories. Furthermore, AVATAR includes unit tests for 250 examples to facilitate functional correctness evaluation. We benchmark several pre-trained language models fine-tuned on AVATAR. Experiment results show that the models lack in generating functionally accurate code.", "arxiv_url": "https://arxiv.org/abs/2108.11590", "authors": ["Wasi Uddin Ahmad", "Md Golam Rahman Tushar", "Saikat Chakraborty", "Kai-Wei Chang"], "first_author": "Wasi Uddin Ahmad", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Parallel Java-Python corpus", "Unit-test-based functional evaluation", "Competitive programming solutions", "Parallel function extraction", "AST and dataflow similarity metrics", "Solution diversity filtering", "Lexical vs. execution accuracy gap"], "summary": "本文构建了一个包含9,515道题及其Java/Python解法的并行语料并提供250个带单元测试的样例用于功能性评估，同时基准实验表明现有模型在生成功能正确代码方面仍有明显不足。", "quality": "High", "conference": "ACL 2023", "pdf_url": "https://arxiv.org/pdf/2108.11590v2", "published": "2021-08-26", "update_time": "2023-05-04", "download_time": "2025-12-11 16:50:32"}
{"id": "2206.08474", "title": "XLCoST: A Benchmark Dataset for Cross-lingual Code Intelligence", "abstract": "Recent advances in machine learning have significantly improved the understanding of source code data and achieved good performance on a number of downstream tasks. Open source repositories like GitHub enable this process with rich unlabeled code data. However, the lack of high quality labeled data has largely hindered the progress of several code related tasks, such as program translation, summarization, synthesis, and code search. This paper introduces XLCoST, Cross-Lingual Code SnippeT dataset, a new benchmark dataset for cross-lingual code intelligence. Our dataset contains fine-grained parallel data from 8 languages (7 commonly used programming languages and English), and supports 10 cross-lingual code tasks. To the best of our knowledge, it is the largest parallel dataset for source code both in terms of size and the number of languages. We also provide the performance of several state-of-the-art baseline models for each task. We believe this new dataset can be a valuable asset for the research community and facilitate the development and validation of new methods for cross-lingual code intelligence.", "arxiv_url": "https://arxiv.org/abs/2206.08474", "authors": ["Ming Zhu", "Aneesh Jain", "Karthik Suresh", "Roshan Ravindran", "Sindhu Tipirneni", "Chandan K. Reddy"], "first_author": "Ming Zhu", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Cross-lingual parallel snippets", "Snippet-level alignment", "Program-level alignment", "Multilingual code translation/summarization/synthesis", "Cross-lingual code search", "Fine-grained alignment from programming-problem solutions", "Baseline performance evaluation"], "summary": "本文提出并发布了 XLCoST，一个包含七种编程语言与英语的百万级细粒度并行代码片段数据集，支持十项跨语种代码任务并提供多种基线评测。 ", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2206.08474v1", "published": "2022-06-16", "update_time": "2022-06-16", "download_time": "2025-12-11 16:51:09"}
{"id": "2303.03004", "title": "xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval", "abstract": "Recently, pre-trained large language models (LLMs) have shown impressive abilities in generating codes from natural language descriptions, repairing buggy codes, translating codes between languages, and retrieving relevant code segments. However, the evaluation of these models has often been performed in a scattered way on only one or two specific tasks, in a few languages, at a partial granularity (e.g., function) level, and in many cases without proper training data. Even more concerning is that in most cases the evaluation of generated codes has been done in terms of mere lexical overlap with a reference code rather than actual execution. We introduce xCodeEval, the largest executable multilingual multitask benchmark to date consisting of $25$M document-level coding examples ($16.5$B tokens) from about $7.5$K unique problems covering up to $11$ programming languages with execution-level parallelism. It features a total of $7$ tasks involving code understanding, generation, translation and retrieval. xCodeEval adopts an execution-based evaluation and offers a multilingual code execution engine, ExecEval that supports unit test based execution in all the $11$ languages. To address the challenge of balancing the distributions of text-code samples over multiple attributes in validation/test sets, we propose a novel data splitting and a data selection schema based on the geometric mean and graph-theoretic principle. Our experiments with OpenAI's LLMs (zero-shot) and open-LLMs (zero-shot and fine-tuned) on the tasks and languages demonstrate **xCodeEval** to be quite challenging as per the current advancements in language models.", "arxiv_url": "https://arxiv.org/abs/2303.03004", "authors": ["Mohammad Abdullah Matin Khan", "M Saiful Bari", "Xuan Long Do", "Weishi Wang", "Md Rizwan Parvez", "Shafiq Joty"], "first_author": "Mohammad Abdullah Matin Khan", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Multilingual Program Synthesis & Evaluation Benchmark", "tags": ["Execution-based evaluation", "Unit-test harness", "Multilingual code parallelism", "Large-scale Codeforces corpus", "Program synthesis benchmark", "Code translation and retrieval tasks", "Geometric-mean split strategy", "Graph-theoretic data selection", "Difficulty-stratified sampling", "Distributed execution engine"], "summary": "本文提出XCODEEVAL——一个包含约2500万可执行多语言多任务代码样本的大规模基准及其ExecEval执行引擎，并通过基于单元测试的执行评估、几何平均的数据切分和图论数据选择策略为代码理解、生成、翻译和检索任务提供标准化评测。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2303.03004v4", "published": "2023-03-06", "update_time": "2023-11-06", "download_time": "2025-12-11 16:51:45"}
{"id": "2308.08961", "title": "On the Evaluation of Neural Code Translation: Taxonomy and Benchmark", "abstract": "In recent years, neural code translation has gained increasing attention. While most of the research focuses on improving model architectures and training processes, we notice that the evaluation process and benchmark for code translation models are severely limited: they primarily treat source code as natural languages and provide a holistic accuracy score while disregarding the full spectrum of model capabilities across different translation types and complexity. In this paper, we present a comprehensive investigation of four state-of-the-art models and analyze in-depth the advantages and limitations of three existing benchmarks. Based on the empirical results, we develop a taxonomy that categorizes code translation tasks into four primary types according to their complexity and knowledge dependence: token level (type 1), syntactic level (type 2), library level (type 3), and algorithm level (type 4). We then conduct a thorough analysis of how existing approaches perform across these four categories. Our findings indicate that while state-of-the-art code translation models excel in type-1 and type-2 translations, they struggle with knowledge-dependent ones such as type-3 and type-4. Existing benchmarks are biased towards trivial translations, such as keyword mapping. To overcome these limitations, we construct G-TransEval, a new benchmark by manually curating type-3 and type-4 translation pairs and unit test cases. Results on our new benchmark suggest that G-TransEval can exhibit more comprehensive and finer-grained capability of code translation models and thus provide a more rigorous evaluation. Our studies also provide more insightful findings and suggestions for future research, such as building type-3 and type-4 training data and ensembling multiple pretraining approaches.", "arxiv_url": "https://arxiv.org/abs/2308.08961", "authors": ["Mingsheng Jiao", "Tingrui Yu", "Xuan Li", "Guanjie Qiu", "Xiaodong Gu", "Beijun Shen"], "first_author": "Mingsheng Jiao", "category": ["Empirical", "Benchmark"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Translation Taxonomy", "Token/Syntax/Library/Algorithm Levels", "Unit-test Functional Evaluation", "Benchmark Bias Analysis", "Cross-language Difficulty (dynamic→static)", "Fine-grained Evaluation Metrics (BLEU/CodeBLEU/CA)", "Curated High-Complexity Translation Pairs"], "summary": "本文提出针对神经代码翻译的四层复杂度分类（词法、语法、库、算法），通过细粒度实证分析揭示现有模型在库级与算法级翻译上的不足，并构建了包含高难度样本与单元测试的G-TransEval基准以实现更严格的评估。", "quality": "High", "conference": "International Conference on Automated Software Engineering (ASE 2023)", "pdf_url": "https://arxiv.org/pdf/2308.08961v1", "published": "2023-08-17", "update_time": "2023-08-17", "download_time": "2025-12-11 16:52:12"}
{"id": "2310.04951", "title": "CodeTransOcean: A Comprehensive Multilingual Benchmark for Code Translation", "abstract": "Recent code translation techniques exploit neural machine translation models to translate source code from one programming language to another to satisfy production compatibility or to improve efficiency of codebase maintenance. Most existing code translation datasets only focus on a single pair of popular programming languages. To advance research on code translation and meet diverse requirements of real-world applications, we construct CodeTransOcean, a large-scale comprehensive benchmark that supports the largest variety of programming languages for code translation. CodeTransOcean consists of three novel multilingual datasets, namely, MultilingualTrans supporting translations between multiple popular programming languages, NicheTrans for translating between niche programming languages and popular ones, and LLMTrans for evaluating executability of translated code by large language models (LLMs). CodeTransOcean also includes a novel cross-framework dataset, DLTrans, for translating deep learning code across different frameworks. We develop multilingual modeling approaches for code translation and demonstrate their great potential in improving the translation quality of both low-resource and high-resource language pairs and boosting the training efficiency. We also propose a novel evaluation metric Debugging Success Rate@K for program-level code translation. Last but not least, we evaluate LLM ChatGPT on our datasets and investigate its potential for fuzzy execution predictions. We build baselines for CodeTransOcean and analyze challenges of code translation for guiding future research. The CodeTransOcean datasets and code are publicly available at https://github.com/WeixiangYAN/CodeTransOcean.", "arxiv_url": "https://arxiv.org/abs/2310.04951", "authors": ["Weixiang Yan", "Yuchen Tian", "Yunzhe Li", "Qian Chen", "Wen Wang"], "first_author": "Weixiang Yan", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Multilingual code translation", "Niche-to-popular language transfer", "Deep-learning-framework translation", "Execution-based evaluation", "Debugging Success Rate@K (DSR@K)", "AutoTransExecuter pipeline", "Fuzzy execution metric", "Multilingual modeling for low-resource pairs", "ChatGPT prompting and self-debugging study"], "summary": "本文提出了CodeTransOcean——一个覆盖多种主流与小众编程语言及深度学习框架的大规模代码翻译基准，包含多套数据集、自动化翻译-执行评估流水线与新的执行型评价指标，并展示了多语言建模与ChatGPT在代码翻译任务中的表现与挑战。", "quality": "High", "conference": "EMNLP 2023", "pdf_url": "https://arxiv.org/pdf/2310.04951v2", "published": "2023-10-08", "update_time": "2023-10-25", "download_time": "2025-12-11 16:52:47"}
{"id": "2411.06145", "title": "ClassEval-T: Evaluating Large Language Models in Class-Level Code Translation", "abstract": "In recent years, Large Language Models (LLMs) have dramatically advanced the performance of automated code translation, making their computational accuracy score reach up to over 80% on many previous benchmarks. However, most code samples in these benchmarks are short, standalone, statement/method-level, and algorithmic, which is not aligned with practical coding tasks. Therefore, it is still unknown the actual capability of LLMs in translating code samples written for daily development. To achieve this, we construct a class-level code translation benchmark, ClassEval-T, and make the first attempt to extensively assess recent LLMs' performance on class-level code translation. ClassEval-T is extended from ClassEval, a well-known class-level Python code generation benchmark consisting of multiple practical coding topics, such as database operation and game design, and diverse contextual dependencies (e.g., fields, methods, and libraries). It cost us 360 person-hours to accomplish the manual migration to Java and C++ with complete code samples and associated test suites. Subsequently, we design three translation strategies (i.e., holistic, min-dependency, and standalone) for class-level code translations and evaluate eight recent LLMs of commercial, general, and code kinds in diverse families and sizes on ClassEval-T. Experimental results demonstrate a remarkable performance drop compared with the most widely studied method-level code translation benchmark, and obvious discrepancies among LLMs appear, showing the effectiveness of ClassEval-T in measuring recent LLMs. Afterwards, we further discuss the usage scenarios for diverse translation strategies and LLMs' ability to dependency awareness when translating class samples. Finally, 1,243 failure cases made by the best-performing LLM under test are analyzed and categorized in this paper for practical guidance and future enlightenment.", "arxiv_url": "https://arxiv.org/abs/2411.06145", "authors": ["Pengyu Xue", "Linhao Wu", "Zhen Yang", "Chengyi Wang", "Xiang Li", "Yuxiang Zhang", "Jia Li", "Ruikai Jin", "Yifei Pei", "Zhaoyan Shen", "Xiran Lyu", "Jacky Wai Keung"], "first_author": "Pengyu Xue", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Class-level code translation", "Cross-language migration (Python, Java, C++)", "Holistic / min-dependency / standalone translation strategies", "Dependency-awareness evaluation", "High-coverage test suites for correctness & compilation", "Manual failure-case taxonomy"], "summary": "本文构建并公开了面向类级别的代码翻译基准ClassEval-T（含高覆盖测试用例），并用三种翻译策略评估多款LLM的跨语言翻译能力、依赖感知与失败模式，给出实践性分析与建议。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2411.06145v4", "published": "2024-11-09", "update_time": "2025-04-14", "download_time": "2025-12-11 16:53:16"}
{"id": "2411.13990", "title": "RustRepoTrans: Repository-level Code Translation Benchmark Targeting Rust", "abstract": "Recent advancements in large language models (LLMs) have demonstrated impressive capabilities in code translation, typically evaluated using benchmarks like CodeTransOcean and RepoTransBench. However, dependency-free benchmarks fail to capture real-world complexities by focusing primarily on simple function-level translations and overlooking repository-level context (e.g., dependencies). Full-repository translation benchmarks significantly exceed the current capabilities of existing models, resulting in performance bottlenecks that fail to provide actionable insights for guiding model development. Furthermore, existing benchmarks do not account for the scenario of incrementally translating new or modified modules from the source to the target language, which demands careful handling of repository-level contexts such as dependencies, cross-module references, and architectural divergence. Moreover, LLMs' effectiveness in translating to newer, low-resource languages like Rust remains largely underexplored. To address these gaps, we introduce RustRepoTrans, the first repository-level context code translation benchmark targeting incremental translation, comprising 375 tasks translating into Rust from C, Java, and Python. Using this benchmark, we evaluate seven representative LLMs, analyzing their errors to assess limitations in complex translation scenarios. Among them, DeepSeek-R1 performs best with 51.5% Pass@1, excelling in both basic functionality and additional translation abilities, such as noise robustness and syntactical difference identification. However, even DeepSeek-R1 experiences a 22.2% performance drop (Pass@1 from 73.7% to 51.5%) when handling repository-level context compared to previous benchmarks without such context.", "arxiv_url": "https://arxiv.org/abs/2411.13990", "authors": ["Guangsheng Ou", "Mingwei Liu", "Yuxuan Chen", "Yanlin Wang", "Xin Peng", "Zibin Zheng"], "first_author": "Guangsheng Ou", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Repository-level context", "Incremental code migration", "Rust migration challenges", "Dependency extraction", "Function-pair mining", "Test-case-driven verification", "Error taxonomy (dependency failures)", "Fine-grained translation metrics"], "summary": "本文提出RustRepoTrans——一个包含375个增量仓库级上下文代码翻译任务、面向从C/Java/Python到Rust迁移的基准，并评估七款LLM、归类错误并引入更细粒度的评估框架。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2411.13990v6", "published": "2024-11-21", "update_time": "2025-10-17", "download_time": "2025-12-11 16:53:49"}
{"id": "2412.17744", "title": "RepoTransBench: A Real-World Benchmark for Repository-Level Code Translation", "abstract": "Repository-level code translation refers to translating an entire code repository from one programming language to another while preserving the functionality of the source repository. Many benchmarks have been proposed to evaluate the performance of such code translators. However, previous benchmarks mostly provide fine-grained samples, focusing at either code snippet, function, or file-level code translation. Such benchmarks do not accurately reflect real-world demands, where entire repositories often need to be translated, involving longer code length and more complex functionalities. To address this gap, we propose a new benchmark, named RepoTransBench, which is a real-world repository-level code translation benchmark with an automatically executable test suite. We conduct experiments on RepoTransBench to evaluate the translation performance of 11 advanced LLMs. We find that the Success@1 score (test success in one attempt) of the best-performing LLM is only 7.33%. To further explore the potential of LLMs for repository-level code translation, we provide LLMs with error-related feedback to perform iterative debugging and observe an average 7.09% improvement on Success@1. However, even with this improvement, the Success@1 score of the best-performing LLM is only 21%, which may not meet the need for reliable automatic repository-level code translation. Finally, we conduct a detailed error analysis and highlight current LLMs' deficiencies in repository-level code translation, which could provide a reference for further improvements.", "arxiv_url": "https://arxiv.org/abs/2412.17744", "authors": ["Yanli Wang", "Yanlin Wang", "Suiquan Wang", "Daya Guo", "Jiachi Chen", "John Grundy", "Xilin Liu", "Yuchi Ma", "Mingzhi Mao", "Hongyu Zhang", "Zibin Zheng"], "first_author": "Yanli Wang", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Repository-level translation", "Execution-based test suites", "Iterative debugging with error feedback", "Resource and configuration file migration", "Compilability and functional correctness evaluation", "Real-world repository selection and filtering", "Error taxonomy for translation failures", "Success@k evaluation metric"], "summary": "本文提出了RepoTransBench——一个包含100个真实可执行仓库并附自动化测试套件的仓库级代码翻译基准，评估了多款大型模型并通过迭代调试与错误分析揭示了当前模型在仓库级翻译中的不足。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2412.17744v1", "published": "2024-12-23", "update_time": "2024-12-23", "download_time": "2025-12-11 16:54:25"}
{"id": "2501.16050", "title": "Skeleton-Guided-Translation: A Benchmarking Framework for Code Repository Translation with Fine-Grained Quality Evaluation", "abstract": "The advancement of large language models has intensified the need to modernize enterprise applications and migrate legacy systems to secure, versatile languages. However, existing code translation benchmarks primarily focus on individual functions, overlooking the complexities involved in translating entire repositories, such as maintaining inter-module coherence and managing dependencies. While some recent repository-level translation benchmarks attempt to address these challenges, they still face limitations, including poor maintainability and overly coarse evaluation granularity, which make them less developer-friendly. We introduce Skeleton-Guided-Translation, a framework for repository-level Java to C# code translation with fine-grained quality evaluation. It uses a two-step process: first translating the repository's structural \"skeletons\", then translating the full repository guided by these skeletons. Building on this, we present TRANSREPO-BENCH, a benchmark of high quality open-source Java repositories and their corresponding C# skeletons, including matching unit tests and build configurations. Our unit tests are fixed and can be applied across multiple or incremental translations without manual adjustments, enhancing automation and scalability in evaluations. Additionally, we develop fine-grained evaluation metrics that assess translation quality at the individual test case level, addressing traditional binary metrics' inability to distinguish when build failures cause all tests to fail. Evaluations using TRANSREPO-BENCH highlight key challenges and advance more accurate repository level code translation.", "arxiv_url": "https://arxiv.org/abs/2501.16050", "authors": ["Xing Zhang", "Jiaheng Wen", "Fangkai Yang", "Pu Zhao", "Yu Kang", "Junhao Wang", "Maoquan Wang", "Yufan Huang", "Elsie Nallipogu", "Qingwei Lin", "Yingnong Dang", "Saravan Rajmohan", "Dongmei Zhang", "Qi Zhang"], "first_author": "Xing Zhang", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Repository-level code translation", "Skeleton-guided translation", "Fine-grained unit-test evaluation", "Incremental/partial translation", "Cross-file dependency management", "Per-testcase translation scoring", "Build-config aware evaluation", "Java-to-C# migration"], "summary": "本文提出Skeleton-Guided-Translation框架并构建TRANSREPO-BENCH基准，通过先翻译仓库骨架再填充实现Java到C#的仓库级翻译，并基于单元测试引入细粒度的测试用例级别评价指标以提升可维护性与评估准确性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2501.16050v1", "published": "2025-01-27", "update_time": "2025-01-27", "download_time": "2025-12-11 16:54:54"}
{"id": "2504.15254", "title": "CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation", "abstract": "C-to-Rust transpilation is essential for modernizing legacy C code while enhancing safety and interoperability with modern Rust ecosystems. However, no dataset currently exists for evaluating whether a system can transpile C into safe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset of 100 C repositories, each paired with manually-written interfaces in safe Rust as well as test cases that can be used to validate correctness of the transpilation. By considering entire repositories rather than isolated functions, CRUST-Bench captures the challenges of translating complex projects with dependencies across multiple files. The provided Rust interfaces provide explicit specifications that ensure adherence to idiomatic, memory-safe Rust patterns, while the accompanying test cases enforce functional correctness. We evaluate state-of-the-art large language models (LLMs) on this task and find that safe and idiomatic Rust generation is still a challenging problem for various state-of-the-art methods and techniques. We also provide insights into the errors LLMs usually make in transpiling code from C to safe Rust. The best performing model, OpenAI o1, is able to solve only 15 tasks in a single-shot setting. Improvements on CRUST-Bench would lead to improved transpilation systems that can reason about complex scenarios and help in migrating legacy codebases from C into languages like Rust that ensure memory safety. You can find the dataset and code at https://github.com/anirudhkhatry/CRUST-bench.", "arxiv_url": "https://arxiv.org/abs/2504.15254", "authors": ["Anirudh Khatry", "Robert Zhang", "Jia Pan", "Ziteng Wang", "Qiaochu Chen", "Greg Durrett", "Isil Dillig"], "first_author": "Anirudh Khatry", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["C-to-safe-Rust transpilation", "Repository-scale multi-file translation", "Manually authored Rust interface specifications", "Test-driven correctness and compile-run validation", "Pointer-to-ownership abstraction mapping", "Memory-safety and borrowing challenges", "LLM error analysis for transpilation", "Generate-then-repair evaluation loop"], "summary": "本文提出CRUST-Bench——包含100个C仓库、手工编写的安全Rust接口与测试用例的基准，用于评估并分析LLM在将多文件C项目转译为内存安全、惯用Rust代码时的性能与常见错误模式。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2504.15254v3", "published": "2025-04-21", "update_time": "2025-10-01", "download_time": "2025-12-11 16:55:25"}
{"id": "2202.10868", "title": "Neural Program Repair: Systems, Challenges and Solutions", "abstract": "Automated Program Repair (APR) aims to automatically fix bugs in the source code. Recently, as advances in Deep Learning (DL) field, there is a rise of Neural Program Repair (NPR) studies, which formulate APR as a translation task from buggy code to correct code and adopt neural networks based on encoder-decoder architecture. Compared with other APR techniques, NPR approaches have a great advantage in applicability because they do not need any specification (i.e., a test suite). Although NPR has been a hot research direction, there isn't any overview on this field yet. In order to help interested readers understand architectures, challenges and corresponding solutions of existing NPR systems, we conduct a literature review on latest studies in this paper. We begin with introducing the background knowledge on this field. Next, to be understandable, we decompose the NPR procedure into a series of modules and explicate various design choices on each module. Furthermore, we identify several challenges and discuss the effect of existing solutions. Finally, we conclude and provide some promising directions for future research.", "arxiv_url": "https://arxiv.org/abs/2202.10868", "authors": ["Wenkang Zhong", "Chuanyi Li", "Jidong Ge", "Bin Luo"], "first_author": "Wenkang Zhong", "category": ["Survey"], "field": "Quality Management", "task": "Bug Repair", "tags": ["NPR pipeline", "Context extraction strategies", "AST and structural representations", "Code abstraction and anonymization", "Encoder-decoder seq2seq generation", "Edit-based vs token-based patching", "Patch ranking and validation", "Evaluation: plausible vs correct patches"], "summary": "本文系统回顾了神经程序修复（NPR）研究，按预处理、输入表示、输出搜索和补丁排序四个阶段详细分析各类设计选择、面临的挑战与现有解决方案并提出未来方向。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2202.10868v2", "published": "2022-02-22", "update_time": "2022-09-21", "download_time": "2025-12-11 16:55:53"}
{"id": "2301.03270", "title": "A Survey of Learning-based Automated Program Repair", "abstract": "Automated program repair (APR) aims to fix software bugs automatically and plays a crucial role in software development and maintenance. With the recent advances in deep learning (DL), an increasing number of APR techniques have been proposed to leverage neural networks to learn bug-fixing patterns from massive open-source code repositories. Such learning-based techniques usually treat APR as a neural machine translation (NMT) task, where buggy code snippets (i.e., source language) are translated into fixed code snippets (i.e., target language) automatically. Benefiting from the powerful capability of DL to learn hidden relationships from previous bug-fixing datasets, learning-based APR techniques have achieved remarkable performance. In this paper, we provide a systematic survey to summarize the current state-of-the-art research in the learning-based APR community. We illustrate the general workflow of learning-based APR techniques and detail the crucial components, including fault localization, patch generation, patch ranking, patch validation, and patch correctness phases. We then discuss the widely-adopted datasets and evaluation metrics and outline existing empirical studies. We discuss several critical aspects of learning-based APR techniques, such as repair domains, industrial deployment, and the open science issue. We highlight several practical guidelines on applying DL techniques for future APR studies, such as exploring explainable patch generation and utilizing code features. Overall, our paper can help researchers gain a comprehensive understanding about the achievements of the existing learning-based APR techniques and promote the practical application of these techniques. Our artifacts are publicly available at \\url{https://github.com/QuanjunZhang/AwesomeLearningAPR}.", "arxiv_url": "https://arxiv.org/abs/2301.03270", "authors": ["Quanjun Zhang", "Chunrong Fang", "Yuxiang Ma", "Weisong Sun", "Zhenyu Chen"], "first_author": "Quanjun Zhang", "category": ["Survey"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Neural Machine Translation formulation", "Fault Localization", "Patch Generation", "Patch Ranking & Validation", "Code Representation: Sequence/Tree/Graph", "Pre-trained Models for APR", "Evaluation: Plausible vs Correct Patches", "Multilingual and Multi-hunk Repair", "Explainable Patch Generation", "Open Science and Reproducibility"], "summary": "本文系统综述了基于深度学习的自动程序修复（APR）研究，梳理了常见工作流程与关键组件、数据集与评测指标、实证研究、挑战与未来方向。", "quality": "High", "conference": "ACM Transactions on Software Engineering and Methodology (TOSEM) 2023", "pdf_url": "https://arxiv.org/pdf/2301.03270v3", "published": "2023-01-09", "update_time": "2023-11-01", "download_time": "2025-12-11 16:56:32"}
{"id": "2303.18184", "title": "A Survey on Automated Program Repair Techniques", "abstract": "With the rapid development and large-scale popularity of program software, modern society increasingly relies on software systems. However, the problems exposed by software have also come to the fore. Software defect has become an important factor troubling developers. In this context, Automated Program Repair (APR) techniques have emerged, aiming to automatically fix software defect problems and reduce manual debugging work. In particular, benefiting from the advances in deep learning, numerous learning-based APR techniques have emerged in recent years, which also bring new opportunities for APR research. To give researchers a quick overview of APR techniques' complete development and future opportunities, we revisit the evolution of APR techniques and discuss in depth the latest advances in APR research. In this paper, the development of APR techniques is introduced in terms of four different patch generation schemes: search-based, constraint-based, template-based, and learning-based. Moreover, we propose a uniform set of criteria to review and compare each APR tool, summarize the advantages and disadvantages of APR techniques, and discuss the current state of APR development. Furthermore, we introduce the research on the related technical areas of APR that have also provided a strong motivation to advance APR development. Finally, we analyze current challenges and future directions, especially highlighting the critical opportunities that large language models bring to APR research.", "arxiv_url": "https://arxiv.org/abs/2303.18184", "authors": ["Kai Huang", "Zhengzi Xu", "Su Yang", "Hongyu Sun", "Xuejun Li", "Zheng Yan", "Yuqing Zhang"], "first_author": "Kai Huang", "category": ["Survey"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Search-based Repair", "Constraint-based Repair", "Template-based Repair", "Learning-based Repair", "Evaluation Criteria for APR", "Patch Assessment and Overfitting", "Fault Localization", "Dataset Quality and Overlap", "Industrial Deployment Challenges", "LLM-enabled Program Repair"], "summary": "本文为自动程序修复（APR）领域的系统综述，按照搜索式、约束式、模板式和学习式四类修复策略回顾进展，提出统一评估标准，分析现有问题并展望包括大语言模型在内的未来研究方向。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2303.18184v3", "published": "2023-03-31", "update_time": "2023-05-13", "download_time": "2025-12-11 16:57:05"}
{"id": "1812.08693", "title": "An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation", "abstract": "Millions of open-source projects with numerous bug fixes are available in code repositories. This proliferation of software development histories can be leveraged to learn how to fix common programming bugs. To explore such a potential, we perform an empirical study to assess the feasibility of using Neural Machine Translation techniques for learning bug-fixing patches for real defects. First, we mine millions of bug-fixes from the change histories of projects hosted on GitHub, in order to extract meaningful examples of such bug-fixes. Next, we abstract the buggy and corresponding fixed code, and use them to train an Encoder-Decoder model able to translate buggy code into its fixed version. In our empirical investigation we found that such a model is able to fix thousands of unique buggy methods in the wild. Overall, this model is capable of predicting fixed patches generated by developers in 9-50% of the cases, depending on the number of candidate patches we allow it to generate. Also, the model is able to emulate a variety of different Abstract Syntax Tree operations and generate candidate patches in a split second.", "arxiv_url": "https://arxiv.org/abs/1812.08693", "authors": ["Michele Tufano", "Cody Watson", "Gabriele Bavota", "Massimiliano Di Penta", "Martin White", "Denys Poshyvanyk"], "first_author": "Michele Tufano", "category": ["Empirical", "Technical"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Neural Machine Translation for Program Repair", "Method-level bug-fix pair mining from GitHub", "AST edit action emulation", "Code abstraction with idioms and identifier mapping", "GumTree-based fine-grained differencing", "RNN encoder-decoder with attention", "Candidate patch generation and top-k inference", "Syntactic correctness and edit-operation analysis"], "summary": "本文通过在大规模GitHub修复提交中挖掘方法级错误-修复对，使用编码器-解码器的神经机器翻译模型将有缺陷代码“翻译”成修复代码，并实证评估了该方法生成与开发者等效补丁的能力与效率。", "quality": "High", "conference": "ACM Transactions on Software Engineering and Methodology 2019", "pdf_url": "https://arxiv.org/pdf/1812.08693v2", "published": "2018-12-20", "update_time": "2019-05-21", "download_time": "2025-12-11 16:57:59"}
{"id": "1901.09102", "title": "On Learning Meaningful Code Changes via Neural Machine Translation", "abstract": "Recent years have seen the rise of Deep Learning (DL) techniques applied to source code. Researchers have exploited DL to automate several development and maintenance tasks, such as writing commit messages, generating comments and detecting vulnerabilities among others. One of the long lasting dreams of applying DL to source code is the possibility to automate non-trivial coding activities. While some steps in this direction have been taken (e.g., learning how to fix bugs), there is still a glaring lack of empirical evidence on the types of code changes that can be learned and automatically applied by DL. Our goal is to make this first important step by quantitatively and qualitatively investigating the ability of a Neural Machine Translation (NMT) model to learn how to automatically apply code changes implemented by developers during pull requests. We train and experiment with the NMT model on a set of 236k pairs of code components before and after the implementation of the changes provided in the pull requests. We show that, when applied in a narrow enough context (i.e., small/medium-sized pairs of methods before/after the pull request changes), NMT can automatically replicate the changes implemented by developers during pull requests in up to 36% of the cases. Moreover, our qualitative analysis shows that the model is capable of learning and replicating a wide variety of meaningful code changes, especially refactorings and bug-fixing activities. Our results pave the way for novel research in the area of DL on code, such as the automatic learning and applications of refactoring.", "arxiv_url": "https://arxiv.org/abs/1901.09102", "authors": ["Michele Tufano", "Jevgenija Pantiuchina", "Cody Watson", "Gabriele Bavota", "Denys Poshyvanyk"], "first_author": "Michele Tufano", "category": ["Technical", "Empirical", "Benchmark"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Method-level code transformations", "Neural Machine Translation (seq2seq)", "Identifier and literal abstraction", "Gerrit pull-request mining", "AST-based edit extraction", "Refactoring and bug-fix synthesis", "Concretization/mapping of abstract tokens", "Beam-search multi-hypothesis generation"], "summary": "该论文通过在从Gerrit挖掘的近24万对方法（改动前/后）上训练编码-解码神经机器翻译模型，定量与定性评估其学习并自动复现开发者在PR中实施的有意义代码变更（如重构与修复），在小/中等方法范围内最高可达36%的精确复现率，并公开了数据与工具。", "quality": "High", "conference": "ACM/IEEE International Conference on Software Engineering (ICSE 2019)", "pdf_url": "https://arxiv.org/pdf/1901.09102v1", "published": "2019-01-25", "update_time": "2019-01-25", "download_time": "2025-12-11 16:58:44"}
{"id": "2010.01544", "title": "Review4Repair: Code Review Aided Automatic Program Repairing", "abstract": "Context: Learning-based automatic program repair techniques are showing promise to provide quality fix suggestions for detected bugs in the source code of the software. These tools mostly exploit historical data of buggy and fixed code changes and are heavily dependent on bug localizers while applying to a new piece of code. With the increasing popularity of code review, dependency on bug localizers can be reduced. Besides, the code review-based bug localization is more trustworthy since reviewers' expertise and experience are reflected in these suggestions.   Objective: The natural language instructions scripted on the review comments are enormous sources of information about the bug's nature and expected solutions. However, none of the learning-based tools has utilized the review comments to fix programming bugs to the best of our knowledge. In this study, we investigate the performance improvement of repair techniques using code review comments.   Method: We train a sequence-to-sequence model on 55,060 code reviews and associated code changes. We also introduce new tokenization and preprocessing approaches that help to achieve significant improvement over state-of-the-art learning-based repair techniques.   Results: We boost the top-1 accuracy by 20.33% and top-10 accuracy by 34.82%. We could provide a suggestion for stylistics and non-code errors unaddressed by prior techniques.   Conclusion: We believe that the automatic fix suggestions along with code review generated by our approach would help developers address the review comment quickly and correctly and thus save their time and effort.", "arxiv_url": "https://arxiv.org/abs/2010.01544", "authors": ["Faria Huq", "Masum Hasan", "Mahim Anzum Haque Pantho", "Sazan Mahbub", "Anindya Iqbal", "Toufique Ahmed"], "first_author": "Faria Huq", "category": ["Technical", "Benchmark"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Review-to-Patch Generation", "Code Review Comment Utilization", "Pointer-Generator Seq2Seq", "Hard and Soft Tokenization", "Joint Natural Language and Code Modeling", "Stylistic and Non-functional Fixes", "Bug Localization via Reviews"], "summary": "本文提出Review4Repair，利用代码审查评论与代码变更联合训练指针生成序列到序列模型并采用新的硬/软分词预处理，在55,060条审查-修复对上显著提高了自动程序修复的top-1和top-10准确率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2010.01544v2", "published": "2020-10-04", "update_time": "2020-10-06", "download_time": "2025-12-11 16:59:33"}
{"id": "2108.04631", "title": "Megadiff: A Dataset of 600k Java Source Code Changes Categorized by Diff Size", "abstract": "This paper presents Megadiff, a dataset of source code diffs. It focuses on Java, with strict inclusion criteria based on commit message and diff size. Megadiff contains 663 029 Java diffs that can be used for research on commit comprehension, fault localization, automated program repair, and machine learning on code changes.", "arxiv_url": "https://arxiv.org/abs/2108.04631", "authors": ["Martin Monperrus", "Matias Martinez", "He Ye", "Fernanda Madeiral", "Thomas Durieux", "Zhongxing Yu"], "first_author": "Martin Monperrus", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Line-based unified diffs", "Java source-code change corpus", "Diff-size categorization (1–40 LOC)", "Full-file before-and-after context", "Fixing-commit heuristic (commit-message filtering)", "Raw unprocessed diffs for reproducibility", "Repository-scale collection and filtering"], "summary": "该论文发布了 Megadiﬀ，一个包含663,029条按改动行数（1–40行）分类、包含完整前后文件上下文且未加工的Java源代码差异数据集，旨在支持提交理解、程序修复和基于变更的机器学习研究。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2108.04631v1", "published": "2021-08-10", "update_time": "2021-08-10", "download_time": "2025-12-11 17:00:06"}
{"id": "2304.01102", "title": "RunBugRun -- An Executable Dataset for Automated Program Repair", "abstract": "Recently, we can notice a transition to data-driven techniques in Automated Program Repair (APR), in particular towards deep neural networks. This entails training on hundreds of thousands or even millions of non-executable code fragments. We would like to bring more attention to an aspect of code often neglected in Neural Program Repair (NPR), namely its execution. Code execution has several significant advantages. It allows for test-based evaluation of candidate fixes and can provide valuable information to aid repair. In this work we present a fully executable dataset of 450,000 small buggy/fixed program pairs originally submitted to programming competition websites written in eight different programming languages. Along with the dataset we provide infrastructure to compile, safely execute and test programs as well as fine-grained bug-type labels. To give a point of reference, we provide basic evaluation results for two baselines, one based on a generate-and-validate approach and one on deep learning. With this dataset we follow several goals: we want to lift Neural Program Repair beyond fully static code representations, foster the use of execution-based features and, by including several different languages, counterbalance the predominance of Java in the current landscape of APR datasets and benchmarks.", "arxiv_url": "https://arxiv.org/abs/2304.01102", "authors": ["Julian Aron Prenner", "Romain Robbes"], "first_author": "Julian Aron Prenner", "category": ["Benchmark"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Executable bug-fix pairs", "Polyglot (8 languages)", "Competitive programming submissions", "Test-case harness and sandboxed execution", "Fine-grained hierarchical bug labels", "Runtime errors and stack traces", "Curation and de-duplication pipeline", "Baseline evaluations for G&V and neural repair", "Cross-language knowledge transfer analysis"], "summary": "本文构建了一个包含45万条可执行错误/修复程序对的跨语言自动程序修复数据集，提供编译与沙箱执行基础设施、测试用例、细粒度错误标签并给出基线评估与分析。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2304.01102v1", "published": "2023-04-03", "update_time": "2023-04-03", "download_time": "2025-12-11 17:00:57"}
{"id": "2401.04621", "title": "DebugBench: Evaluating Debugging Capability of Large Language Models", "abstract": "Large Language Models (LLMs) have demonstrated exceptional coding capability. However, as another critical component of programming proficiency, the debugging capability of LLMs remains relatively unexplored. Previous evaluations of LLMs' debugging ability are significantly limited by the risk of data leakage, the scale of the dataset, and the variety of tested bugs. To overcome these deficiencies, we introduce `DebugBench', an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. To construct DebugBench, we collect code snippets from the LeetCode community, implant bugs into source data with GPT-4, and assure rigorous quality checks. We evaluate two commercial and four open-source models in a zero-shot scenario. We find that (1) while closed-source models exhibit inferior debugging performance compared to humans, open-source models relatively lower pass rate scores; (2) the complexity of debugging notably fluctuates depending on the bug category; (3) incorporating runtime feedback has a clear impact on debugging performance which is not always helpful. As an extension, we also compare LLM debugging and code generation, revealing a strong correlation between them for closed-source models. These findings will benefit the development of LLMs in debugging.", "arxiv_url": "https://arxiv.org/abs/2401.04621", "authors": ["Runchu Tian", "Yining Ye", "Yujia Qin", "Xin Cong", "Yankai Lin", "Yinxu Pan", "Yesai Wu", "Haotian Hui", "Weichuan Liu", "Zhiyuan Liu", "Maosong Sun"], "first_author": "Runchu Tian", "category": ["Benchmark", "Empirical"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Debugging benchmark", "Fine-grained bug taxonomy", "Automated bug implantation", "Data-leakage-aware curation", "Multi-language code snippets", "Runtime-feedback evaluation", "Zero-shot LLM assessment", "Error-type difficulty analysis", "Debugging vs. code-generation correlation", "Test-suite-based automatic evaluation"], "summary": "该论文提出DebugBench——一个包含4,253个实例、覆盖18种细分错误并在C++/Java/Python上构建的数据集，用以无泄露地评估LLM的调试能力，并通过零样本实验分析不同错误类别与运行时反馈对调试性能的影响。", "quality": "High", "conference": "ACL 2024", "pdf_url": "https://arxiv.org/pdf/2401.04621v3", "published": "2024-01-09", "update_time": "2024-06-06", "download_time": "2025-12-11 17:01:33"}
{"id": "2411.02310", "title": "MdEval: Massively Multilingual Code Debugging", "abstract": "Code large language models (LLMs) have made significant progress in code debugging by directly generating the correct code based on the buggy code snippet. Programming benchmarks, typically consisting of buggy code snippet and their associated test cases, are used to assess the debugging capabilities of LLMs. However, many existing benchmarks primarily focus on Python and are often limited in terms of language diversity (e.g., DebugBench and DebugEval). To advance the field of multilingual debugging with LLMs, we propose the first massively multilingual debugging benchmark, which includes 3.6K test samples of 18 programming languages and covers the automated program repair (APR) task, the code review (CR) task, and the bug identification (BI) task. Further, we introduce the debugging instruction corpora MDEVAL-INSTRUCT by injecting bugs into the correct multilingual queries and solutions (xDebugGen). Further, a multilingual debugger xDebugCoder trained on MDEVAL-INSTRUCT as a strong baseline specifically to handle the bugs of a wide range of programming languages (e.g. \"Missing Mut\" in language Rust and \"Misused Macro Definition\" in language C). Our extensive experiments on MDEVAL reveal a notable performance gap between open-source models and closed-source LLMs (e.g., GPT and Claude series), highlighting huge room for improvement in multilingual code debugging scenarios.", "arxiv_url": "https://arxiv.org/abs/2411.02310", "authors": ["Shukai Liu", "Linzheng Chai", "Jian Yang", "Jiajun Shi", "He Zhu", "Liran Wang", "Ke Jin", "Wei Zhang", "Hualei Zhu", "Shuyue Guo", "Tao Sun", "Jiaheng Liu", "Yunlong Duan", "Yu Hao", "Liqun Yang", "Guanglin Niu", "Ge Zhang", "Zhoujun Li"], "first_author": "Shukai Liu", "category": ["Benchmark", "Technical"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Multilingual debugging benchmark", "Automated program repair", "Bug localization", "Bug identification", "Bug-injection data augmentation", "Instruction-tuning for debugging", "Round-trip code translation for bug generation", "Language-specific error taxonomy", "Human-annotated multilingual dataset", "Multitask evaluation and leaderboard"], "summary": "本文提出了首个覆盖20种编程语言的多语言代码调试基准与指令语料，通过三种注入错误策略生成调试训练对并构建多语言调试基线模型，对40个模型进行大规模评测以揭示多语言调试能力差距。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2411.02310v2", "published": "2024-11-04", "update_time": "2025-02-24", "download_time": "2025-12-11 17:02:09"}
{"id": "2501.09745", "title": "Suggesting Code Edits in Interactive Machine Learning Notebooks Using Large Language Models", "abstract": "Machine learning developers frequently use interactive computational notebooks, such as Jupyter notebooks, to host code for data processing and model training. Jupyter notebooks provide a convenient tool for writing machine learning pipelines and interactively observing outputs, however, maintaining Jupyter notebooks, e.g., to add new features or fix bugs, can be challenging due to the length and complexity of the notebooks. Moreover, there is no existing benchmark related to developer edits on Jupyter notebooks. To address this, we present the first dataset of 48,398 Jupyter notebook edits derived from 20,095 revisions of 792 machine learning repositories on GitHub, and perform the first study of the using LLMs to predict code edits in Jupyter notebooks. Our dataset captures granular details of cell-level and line-level modifications, offering a foundation for understanding real-world maintenance patterns in machine learning workflows. We observed that the edits on Jupyter notebooks are highly localized, with changes averaging only 166 lines of code in repositories. While larger models outperform smaller counterparts in code editing, all models have low accuracy on our dataset even after finetuning, demonstrating the complexity of real-world machine learning maintenance tasks. Our findings emphasize the critical role of contextual information in improving model performance and point toward promising avenues for advancing large language models' capabilities in engineering machine learning code.", "arxiv_url": "https://arxiv.org/abs/2501.09745", "authors": ["Bihui Jin", "Jiayue Wang", "Pengyu Nie"], "first_author": "Bihui Jin", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Jupyter notebook edit dataset", "Cell-level and line-level diffs", "GitHub ML repository mining", "Commit-message conditioned edit prediction", "Localized/incremental notebook edits", "Few-shot prompting for code edits", "Supervised fine-tuning and parameter-efficient tuning", "Context-aware evaluation of edit predictions", "Edit-similarity and code-centric metrics"], "summary": "本文构建并开源了一个包含48,398次Jupyter笔记本代码编辑的基准数据集，分析笔记本维护的局部编辑模式并通过few-shot与微调评估LLM在单元格/行级代码修改预测上的性能，发现模型在真实维护任务上表现仍然较差且对上下文依赖显著。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2501.09745v1", "published": "2025-01-16", "update_time": "2025-01-16", "download_time": "2025-12-11 17:02:40"}
{"id": "2506.04418", "title": "Characterizing Multi-Hunk Patches: Divergence, Proximity, and LLM Repair Challenges", "abstract": "Multi-hunk bugs, where fixes span disjoint regions of code, are common in practice, yet remain underrepresented in automated repair. Existing techniques and benchmarks pre-dominantly target single-hunk scenarios, overlooking the added complexity of coordinating semantically related changes across the codebase. In this work, we characterize HUNK4J, a dataset of multi-hunk patches derived from 372 real-world defects. We propose hunk divergence, a metric that quantifies the variation among edits in a patch by capturing lexical, structural, and file-level differences, while incorporating the number of hunks involved. We further define spatial proximity, a classification that models how hunks are spatially distributed across the program hierarchy. Our empirical study spanning six LLMs reveals that model success rates decline with increased divergence and spatial dispersion. Notably, when using the LLM alone, no model succeeds in the most dispersed Fragment class. These findings highlight a critical gap in LLM capabilities and motivate divergence-aware repair strategies.", "arxiv_url": "https://arxiv.org/abs/2506.04418", "authors": ["Noor Nashid", "Daniel Ding", "Keheliya Gallaba", "Ahmed E. Hassan", "Ali Mesbah"], "first_author": "Noor Nashid", "category": ["Benchmark", "Empirical", "Technical"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Multi-hunk patch characterization", "Hunk divergence metric", "Spatial proximity classification", "Lexical/AST/file-level pairwise distance", "Divergence-aware repair analysis", "LLM-based program repair evaluation", "Context retrieval and scope effects"], "summary": "本文提出了衡量多块补丁内部差异的 hunk divergence 指标与表示补丁空间分布的 spatial proximity 分类，并基于372个真实多块缺陷构建基准和评估平台，对六种LLM的多块修复能力进行系统实证，揭示随着散度和分散程度增加模型修复成功率显著下降。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2506.04418v2", "published": "2025-06-04", "update_time": "2025-11-17", "download_time": "2025-12-11 17:03:20"}
{"id": "2509.21891", "title": "AgentPack: A Dataset of Code Changes, Co-Authored by Agents and Humans", "abstract": "Fine-tuning large language models for code editing has typically relied on mining commits and pull requests. The working hypothesis has been that commit messages describe human intent in natural language, and patches to code describe the changes that implement that intent. However, much of the previously collected data is noisy: commit messages are terse, human-written commits commingle several unrelated edits, and many commits come from simple, rule-based bots.   The recent adoption of software engineering agents changes this landscape. Code changes co-authored by humans and agents tend to be more narrowly scoped and focused on clearer goals. Their commit messages, generated by LLMs, articulate intent and rationale in much greater detail. Moreover, when these changes land in public repositories, they are implicitly filtered by humans: maintainers discard low-quality commits to their projects.   We present AgentPack, a corpus of 1.3M code edits co-authored by Claude Code, OpenAI Codex, and Cursor Agent across public GitHub projects up to mid-August 2025. We describe the identification and curation pipeline, quantify adoption trends of these agents, and analyze the structural properties of the edits. Finally, we show that models fine-tuned on AgentPack can outperform models trained on prior human-only commit corpora, highlighting the potential of using public data from software engineering agents to train future code-editing models.", "arxiv_url": "https://arxiv.org/abs/2509.21891", "authors": ["Yangtian Zi", "Zixuan Wu", "Aleksander Boruch-Gruszecki", "Jonathan Bell", "Arjun Guha"], "first_author": "Yangtian Zi", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Agent–human coauthored commits", "Commit-signature identification", "GitHub public-timeline mining & curation pipeline", "Multi-file code-change diffs", "Agent-written tests", "Detailed commit rationale / intent", "Adoption trends of coding agents", "Fine-tuning for code-editing performance"], "summary": "本文提出AGENTPACK——一个包含1.3M条由软件工程代理与人类共同撰写的GitHub代码更改数据集，说明了识别与清洗流水线、量化代理採用与编辑结构特性，并展示用该数据集微调模型可显著提升代码编辑能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2509.21891v1", "published": "2025-09-26", "update_time": "2025-09-26", "download_time": "2025-12-11 17:03:51"}
{"id": "2509.25203", "title": "Generating High-Quality Datasets for Code Editing via Open-Source Language Models", "abstract": "Code editing plays a vital role in software engineering, requiring developers to adjust existing code according to natural language instructions while keeping functionality intact and avoiding unnecessary modifications. However, commit-based datasets commonly used for this task are often noisy, lack diversity, and fail to reflect the style of real-world edit instructions. To address this, we introduce OpenCodeEdit, an open-source pipeline that leverages multiple LLMs to synthesize realistic code-edit triplets. The pipeline produces both concise \"lazy\" instructions and more detailed \"descriptive\" ones, and applies filtering based on diffs and topics to guarantee data quality and variety. Using this process, we construct OCEDataFT, a curated dataset of 20K samples. Fine-tuning three advanced base models on OCEDataFT leads to significant performance boosts on the CanItEdit benchmark, with relative pass@1 improvements ranging from 4.50% to 20.79%. Notably, the resulting models achieve performance close to closed-source systems, narrowing the gap to GPT-4 to just 3.54%, without relying on proprietary resources or manual annotation.", "arxiv_url": "https://arxiv.org/abs/2509.25203", "authors": ["Zekai Zhang", "Mingwei Liu", "Zhenxi Chen", "Linxi Liang", "Yuxuan Chen", "Guangsheng Ou", "Yanlin Wang", "Dan Li", "Xin Peng", "Zibin Zheng"], "first_author": "Zekai Zhang", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Synthetic pre-edit/instruction/post-edit triplets", "Lazy vs descriptive instruction styles", "Multi-LLM data synthesis", "Diff- and topic-based filtering", "Instruction-tuning for code editing", "Reproducible open-source data pipeline", "Data pruning / 'less-is-more' effect", "Fine-tuning small open-weight code models", "Quantitative pass@1 performance gains"], "summary": "本文提出OpenCodeEdit，一种利用开源大模型合成高质量（前代码-指令-后代码）编辑三元组并通过差异与主题过滤的可复现数据生成管道，发布了20K样本的OCEDataFT数据集并证明其能显著提升开源小模型在指令驱动代码编辑任务上的性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2509.25203v3", "published": "2025-09-19", "update_time": "2025-10-07", "download_time": "2025-12-11 17:04:30"}
{"id": "1707.02275", "title": "A parallel corpus of Python functions and documentation strings for automated code documentation and code generation", "abstract": "Automated documentation of programming source code and automated code generation from natural language are challenging tasks of both practical and scientific interest. Progress in these areas has been limited by the low availability of parallel corpora of code and natural language descriptions, which tend to be small and constrained to specific domains.   In this work we introduce a large and diverse parallel corpus of a hundred thousands Python functions with their documentation strings (\"docstrings\") generated by scraping open source repositories on GitHub. We describe baseline results for the code documentation and code generation tasks obtained by neural machine translation. We also experiment with data augmentation techniques to further increase the amount of training data.   We release our datasets and processing scripts in order to stimulate research in these areas.", "arxiv_url": "https://arxiv.org/abs/1707.02275", "authors": ["Antonio Valerio Miceli Barone", "Rico Sennrich"], "first_author": "Antonio Valerio Miceli Barone", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Summarization", "tags": ["Parallel code-docstring corpus", "Docstring extraction from GitHub", "Top-level Python functions", "Synthetic docstrings via back-translation", "Code normalization and parsing", "NMT baselines with BPE subtokenization"], "summary": "本文构建并发布了一个包含约15万条Python函数及其docstring的平行语料库，提供了数据预处理脚本、基于神经机器翻译的代码文档化和代码生成基线以及通过反向翻译生成的合成docstring以便后续研究使用。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/1707.02275v1", "published": "2017-07-07", "update_time": "2017-07-07", "download_time": "2025-12-11 17:05:13"}
{"id": "1811.07234", "title": "Improving Automatic Source Code Summarization via Deep Reinforcement Learning", "abstract": "Code summarization provides a high level natural language description of the function performed by code, as it can benefit the software maintenance, code categorization and retrieval. To the best of our knowledge, most state-of-the-art approaches follow an encoder-decoder framework which encodes the code into a hidden space and then decode it into natural language space, suffering from two major drawbacks: a) Their encoders only consider the sequential content of code, ignoring the tree structure which is also critical for the task of code summarization, b) Their decoders are typically trained to predict the next word by maximizing the likelihood of next ground-truth word with previous ground-truth word given. However, it is expected to generate the entire sequence from scratch at test time. This discrepancy can cause an \\textit{exposure bias} issue, making the learnt decoder suboptimal. In this paper, we incorporate an abstract syntax tree structure as well as sequential content of code snippets into a deep reinforcement learning framework (i.e., actor-critic network). The actor network provides the confidence of predicting the next word according to current state. On the other hand, the critic network evaluates the reward value of all possible extensions of the current state and can provide global guidance for explorations. We employ an advantage reward composed of BLEU metric to train both networks. Comprehensive experiments on a real-world dataset show the effectiveness of our proposed model when compared with some state-of-the-art methods.", "arxiv_url": "https://arxiv.org/abs/1811.07234", "authors": ["Yao Wan", "Zhou Zhao", "Min Yang", "Guandong Xu", "Haochao Ying", "Jian Wu", "Philip S. Yu"], "first_author": "Yao Wan", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Summarization", "tags": ["AST-based LSTM", "Sequence and tree structure fusion", "Hybrid attention", "Actor-Critic reinforcement learning", "Exposure-bias mitigation", "BLEU-based advantage reward", "Pretraining actor and critic", "Python code summarization"], "summary": "本文提出将抽象语法树（AST）和源代码序列通过混合注意力融合，并引入基于BLEU奖励的actor-critic深度强化学习框架以缓解暴露偏差，从而提升自动代码摘要的生成质量。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/1811.07234v1", "published": "2018-11-17", "update_time": "2018-11-17", "download_time": "2025-12-11 17:05:55"}
{"id": "1909.09436", "title": "CodeSearchNet Challenge: Evaluating the State of Semantic Code Search", "abstract": "Semantic code search is the task of retrieving relevant code given a natural language query. While related to other information retrieval tasks, it requires bridging the gap between the language used in code (often abbreviated and highly technical) and natural language more suitable to describe vague concepts and ideas.   To enable evaluation of progress on code search, we are releasing the CodeSearchNet Corpus and are presenting the CodeSearchNet Challenge, which consists of 99 natural language queries with about 4k expert relevance annotations of likely results from CodeSearchNet Corpus. The corpus contains about 6 million functions from open-source code spanning six programming languages (Go, Java, JavaScript, PHP, Python, and Ruby). The CodeSearchNet Corpus also contains automatically generated query-like natural language for 2 million functions, obtained from mechanically scraping and preprocessing associated function documentation. In this article, we describe the methodology used to obtain the corpus and expert labels, as well as a number of simple baseline solutions for the task.   We hope that CodeSearchNet Challenge encourages researchers and practitioners to study this interesting task further and will host a competition and leaderboard to track the progress on the challenge. We are also keen on extending CodeSearchNet Challenge to more queries and programming languages in the future.", "arxiv_url": "https://arxiv.org/abs/1909.09436", "authors": ["Hamel Husain", "Ho-Hsiang Wu", "Tiferet Gazit", "Miltiadis Allamanis", "Marc Brockschmidt"], "first_author": "Hamel Husain", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Retrieval", "tags": ["Function-documentation pairing", "Semantic code retrieval", "Expert relevance annotations", "Cross-language corpus (6 languages)", "Heuristic preprocessing and deduplication", "Annotation interface and protocol", "NDCG ranking evaluation", "Neural+Elasticsearch baseline ensemble"], "summary": "本文发布了包含约200万条函数-文档对的CodeSearchNet语料库并提出了包含99条查询与约4026条专家相关性标注的CodeSearchNet挑战，介绍了数据收集与过滤流程、注释方法及若干基线检索模型以评估语义代码搜索进展。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/1909.09436v3", "published": "2019-09-20", "update_time": "2020-06-08", "download_time": "2025-12-11 17:06:35"}
{"id": "1801.01681", "title": "VulDeePecker: A Deep Learning-Based System for Vulnerability Detection", "abstract": "The automatic detection of software vulnerabilities is an important research problem. However, existing solutions to this problem rely on human experts to define features and often miss many vulnerabilities (i.e., incurring high false negative rate). In this paper, we initiate the study of using deep learning-based vulnerability detection to relieve human experts from the tedious and subjective task of manually defining features. Since deep learning is motivated to deal with problems that are very different from the problem of vulnerability detection, we need some guiding principles for applying deep learning to vulnerability detection. In particular, we need to find representations of software programs that are suitable for deep learning. For this purpose, we propose using code gadgets to represent programs and then transform them into vectors, where a code gadget is a number of (not necessarily consecutive) lines of code that are semantically related to each other. This leads to the design and implementation of a deep learning-based vulnerability detection system, called Vulnerability Deep Pecker (VulDeePecker). In order to evaluate VulDeePecker, we present the first vulnerability dataset for deep learning approaches. Experimental results show that VulDeePecker can achieve much fewer false negatives (with reasonable false positives) than other approaches. We further apply VulDeePecker to 3 software products (namely Xen, Seamonkey, and Libav) and detect 4 vulnerabilities, which are not reported in the National Vulnerability Database but were \"silently\" patched by the vendors when releasing later versions of these products; in contrast, these vulnerabilities are almost entirely missed by the other vulnerability detection systems we experimented with.", "arxiv_url": "https://arxiv.org/abs/1801.01681", "authors": ["Zhen Li", "Deqing Zou", "Shouhuai Xu", "Xinyu Ou", "Hai Jin", "Sujuan Wang", "Zhijun Deng", "Yuyi Zhong"], "first_author": "Zhen Li", "category": ["Technical", "Benchmark"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Code Gadgets", "Code Vectorization", "Bidirectional LSTM", "Vulnerability Pattern Learning", "Fine-grained Vulnerability Localization", "Buffer Error (CWE-119)", "Resource Management Error (CWE-399)", "Large-scale labeled code-gadget dataset"], "summary": "该论文提出以“code gadget”作为中间表示并将其向量化，采用双向LSTM学习漏洞模式构建深度学习漏洞检测系统并发布首个面向该方法的大规模数据集，从而显著降低漏报率并发现若干未登记录但被静默修复的漏洞。", "quality": "High", "conference": "NDSS (Network and Distributed System Security Symposium) 2018", "pdf_url": "https://arxiv.org/pdf/1801.01681v1", "published": "2018-01-05", "update_time": "2018-01-05", "download_time": "2025-12-11 17:07:33"}
{"id": "1807.04320", "title": "Automated Vulnerability Detection in Source Code Using Deep Representation Learning", "abstract": "Increasing numbers of software vulnerabilities are discovered every year whether they are reported publicly or discovered internally in proprietary code. These vulnerabilities can pose serious risk of exploit and result in system compromise, information leaks, or denial of service. We leveraged the wealth of C and C++ open-source code available to develop a large-scale function-level vulnerability detection system using machine learning. To supplement existing labeled vulnerability datasets, we compiled a vast dataset of millions of open-source functions and labeled it with carefully-selected findings from three different static analyzers that indicate potential exploits. The labeled dataset is available at: https://osf.io/d45bw/. Using these datasets, we developed a fast and scalable vulnerability detection tool based on deep feature representation learning that directly interprets lexed source code. We evaluated our tool on code from both real software packages and the NIST SATE IV benchmark dataset. Our results demonstrate that deep feature representation learning on source code is a promising approach for automated software vulnerability detection.", "arxiv_url": "https://arxiv.org/abs/1807.04320", "authors": ["Rebecca L. Russell", "Louis Kim", "Lei H. Hamilton", "Tomo Lazovich", "Jacob A. Harer", "Onur Ozdemir", "Paul M. Ellingwood", "Marc W. McConley"], "first_author": "Rebecca L. Russell", "category": ["Technical", "Benchmark"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Function-level vulnerability detection", "Lexed token representation with placeholder tokens", "Static-analyzer-derived binary labels", "Strict duplicate and near-duplicate removal", "Neural embeddings with CNN/RNN feature extraction", "Random-forest ensemble on learned features", "Large-scale mined C/C++ corpus"], "summary": "本文构建了一个由静态分析器标注的大规模C/C++函数语料并提出一种基于词法化表示、神经表征学习（CNN/RNN）与随机森林集成的自动漏洞检测方法，实验证明在真实软件包与基准集上效果良好。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/1807.04320v2", "published": "2018-07-11", "update_time": "2018-11-28", "download_time": "2025-12-11 17:08:13"}
{"id": "1807.06756", "title": "SySeVR: A Framework for Using Deep Learning to Detect Software Vulnerabilities", "abstract": "The detection of software vulnerabilities (or vulnerabilities for short) is an important problem that has yet to be tackled, as manifested by the many vulnerabilities reported on a daily basis. This calls for machine learning methods for vulnerability detection. Deep learning is attractive for this purpose because it alleviates the requirement to manually define features. Despite the tremendous success of deep learning in other application domains, its applicability to vulnerability detection is not systematically understood. In order to fill this void, we propose the first systematic framework for using deep learning to detect vulnerabilities in C/C++ programs with source code. The framework, dubbed Syntax-based, Semantics-based, and Vector Representations (SySeVR), focuses on obtaining program representations that can accommodate syntax and semantic information pertinent to vulnerabilities. Our experiments with 4 software products demonstrate the usefulness of the framework: we detect 15 vulnerabilities that are not reported in the National Vulnerability Database. Among these 15 vulnerabilities, 7 are unknown and have been reported to the vendors, and the other 8 have been \"silently\" patched by the vendors when releasing newer versions of the pertinent software products.", "arxiv_url": "https://arxiv.org/abs/1807.06756", "authors": ["Zhen Li", "Deqing Zou", "Shouhuai Xu", "Hai Jin", "Yawei Zhu", "Zhaoxuan Chen"], "first_author": "Zhen Li", "category": ["Technical", "Benchmark"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Syntax-based Vulnerability Candidates (SyVC)", "Semantics-based Vulnerability Candidates (SeVC)", "Program vectorization for vulnerability detection", "AST-driven syntax extraction", "Data- and control-dependency slicing", "Bidirectional recurrent networks (BGRU) for code classification", "Explainability of false positives/false negatives", "Large-scale 126-type labeled vulnerability dataset"], "summary": "本文提出SySeVR框架，通过提取基于语法的候选（SyVC）并扩展为包含数据与控制依赖的语义候选（SeVC），将SeVC编码为向量并使用深度神经网络（尤其双向GRU）检测C/C++源代码中的漏洞，同时发布了包含126类漏洞的数据集并在实际软件中发现若干未登记的漏洞。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/1807.06756v3", "published": "2018-07-18", "update_time": "2021-01-12", "download_time": "2025-12-11 17:08:59"}
{"id": "1902.02595", "title": "A Manually-Curated Dataset of Fixes to Vulnerabilities of Open-Source Software", "abstract": "Advancing our understanding of software vulnerabilities, automating their identification, the analysis of their impact, and ultimately their mitigation is necessary to enable the development of software that is more secure. While operating a vulnerability assessment tool that we developed and that is currently used by hundreds of development units at SAP, we manually collected and curated a dataset of vulnerabilities of open-source software and the commits fixing them. The data was obtained both from the National Vulnerability Database (NVD) and from project-specific Web resources that we monitor on a continuous basis. From that data, we extracted a dataset that maps 624 publicly disclosed vulnerabilities affecting 205 distinct open-source Java projects, used in SAP products or internal tools, onto the 1282 commits that fix them. Out of 624 vulnerabilities, 29 do not have a CVE identifier at all and 46, which do have a CVE identifier assigned by a numbering authority, are not available in the NVD yet. The dataset is released under an open-source license, together with supporting scripts that allow researchers to automatically retrieve the actual content of the commits from the corresponding repositories and to augment the attributes available for each instance. Also, these scripts allow to complement the dataset with additional instances that are not security fixes (which is useful, for example, in machine learning applications). Our dataset has been successfully used to train classifiers that could automatically identify security-relevant commits in code repositories. The release of this dataset and the supporting code as open-source will allow future research to be based on data of industrial relevance; also, it represents a concrete step towards making the maintenance of this dataset a shared effort involving open-source communities, academia, and the industry.", "arxiv_url": "https://arxiv.org/abs/1902.02595", "authors": ["Serena E. Ponta", "Henrik Plate", "Antonino Sabetta", "Michele Bezzi", "Cédric Dangremont"], "first_author": "Serena E. Ponta", "category": ["Benchmark"], "field": "Quality Management", "task": "Vulnerability Repair", "tags": ["Fix-commit mapping", "Manual curation", "Security-relevant commit labels", "Java open-source projects", "NVD augmentation", "Industrial-grade vulnerability corpus", "Negative-sample generation scripts", "Commit-level vulnerability metadata"], "summary": "本文发布了一个手工整理的开源Java漏洞与其修复提交的数据集，包含624个漏洞与1282个修复提交并提供脚本用于检索、扩展和生成负样本。", "quality": "High", "conference": "Proceedings of The 16th International Conference on Mining Software Repositories (Data Showcase track) 2019", "pdf_url": "https://arxiv.org/pdf/1902.02595v3", "published": "2019-02-07", "update_time": "2019-03-19", "download_time": "2025-12-11 17:09:32"}
{"id": "2001.02334", "title": "$μ$VulDeePecker: A Deep Learning-Based System for Multiclass Vulnerability Detection", "abstract": "Fine-grained software vulnerability detection is an important and challenging problem. Ideally, a detection system (or detector) not only should be able to detect whether or not a program contains vulnerabilities, but also should be able to pinpoint the type of a vulnerability in question. Existing vulnerability detection methods based on deep learning can detect the presence of vulnerabilities (i.e., addressing the binary classification or detection problem), but cannot pinpoint types of vulnerabilities (i.e., incapable of addressing multiclass classification). In this paper, we propose the first deep learning-based system for multiclass vulnerability detection, dubbed $μ$VulDeePecker. The key insight underlying $μ$VulDeePecker is the concept of code attention, which can capture information that can help pinpoint types of vulnerabilities, even when the samples are small. For this purpose, we create a dataset from scratch and use it to evaluate the effectiveness of $μ$VulDeePecker. Experimental results show that $μ$VulDeePecker is effective for multiclass vulnerability detection and that accommodating control-dependence (other than data-dependence) can lead to higher detection capabilities.", "arxiv_url": "https://arxiv.org/abs/2001.02334", "authors": ["Deqing Zou", "Sujuan Wang", "Shouhuai Xu", "Zhen Li", "Hai Jin"], "first_author": "Deqing Zou", "category": ["Technical", "Benchmark"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Multiclass vulnerability classification", "Code attention", "Code gadget extraction", "Control-dependence analysis", "System Dependency Graphs", "Feature-fusion BLSTM architecture", "API/library call vulnerability detection", "Fine-grained CWE-based labeling"], "summary": "本文提出µVulDeePecker，通过引入“code attention”并在code gadget中融合数据依赖与控制依赖、采用特征融合的BLSTM架构，对C/C++中基于库/API调用的漏洞进行多类细粒度检测并发布了包含40类漏洞的代码片段数据集以验证其有效性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2001.02334v1", "published": "2020-01-08", "update_time": "2020-01-08", "download_time": "2025-12-11 17:10:27"}
{"id": "2009.07235", "title": "Deep Learning based Vulnerability Detection: Are We There Yet?", "abstract": "Automated detection of software vulnerabilities is a fundamental problem in software security. Existing program analysis techniques either suffer from high false positives or false negatives. Recent progress in Deep Learning (DL) has resulted in a surge of interest in applying DL for automated vulnerability detection. Several recent studies have demonstrated promising results achieving an accuracy of up to 95% at detecting vulnerabilities. In this paper, we ask, \"how well do the state-of-the-art DL-based techniques perform in a real-world vulnerability prediction scenario?\". To our surprise, we find that their performance drops by more than 50%. A systematic investigation of what causes such precipitous performance drop reveals that existing DL-based vulnerability prediction approaches suffer from challenges with the training data (e.g., data duplication, unrealistic distribution of vulnerable classes, etc.) and with the model choices (e.g., simple token-based models). As a result, these approaches often do not learn features related to the actual cause of the vulnerabilities. Instead, they learn unrelated artifacts from the dataset (e.g., specific variable/function names, etc.). Leveraging these empirical findings, we demonstrate how a more principled approach to data collection and model design, based on realistic settings of vulnerability prediction, can lead to better solutions. The resulting tools perform significantly better than the studied baseline: up to 33.57% boost in precision and 128.38% boost in recall compared to the best performing model in the literature. Overall, this paper elucidates existing DL-based vulnerability prediction systems' potential issues and draws a roadmap for future DL-based vulnerability prediction research. In that spirit, we make available all the artifacts supporting our results: https://git.io/Jf6IA.", "arxiv_url": "https://arxiv.org/abs/2009.07235", "authors": ["Saikat Chakraborty", "Rahul Krishna", "Yangruibo Ding", "Baishakhi Ray"], "first_author": "Saikat Chakraborty", "category": ["Empirical", "Benchmark", "Technical"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Data duplication and leakage", "Class imbalance in vulnerability datasets", "Graph-based program semantics", "Representation learning for class separation", "Learning dataset artifacts (identifier/name bias)", "Real-world vulnerability curation from issue/commit mining", "Feature-attribution explainability", "Evaluation generalizability gap"], "summary": "本文系统性评估了现有深度学习漏洞检测方法在真实世界场景中显著降级的原因（如数据重复、类不平衡和基于 token 的模型忽略语义），并通过公开从 Chromium 和 Debian 挖掘的真实漏洞数据集、去重与平衡策略以及基于图的语义表示和表征学习等改进显著提升了检测性能并给出未来研究建议。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2009.07235v1", "published": "2020-09-03", "update_time": "2020-09-03", "download_time": "2025-12-11 17:11:08"}
{"id": "2102.07995", "title": "D2A: A Dataset Built for AI-Based Vulnerability Detection Methods Using Differential Analysis", "abstract": "Static analysis tools are widely used for vulnerability detection as they understand programs with complex behavior and millions of lines of code. Despite their popularity, static analysis tools are known to generate an excess of false positives. The recent ability of Machine Learning models to understand programming languages opens new possibilities when applied to static analysis. However, existing datasets to train models for vulnerability identification suffer from multiple limitations such as limited bug context, limited size, and synthetic and unrealistic source code. We propose D2A, a differential analysis based approach to label issues reported by static analysis tools. The D2A dataset is built by analyzing version pairs from multiple open source projects. From each project, we select bug fixing commits and we run static analysis on the versions before and after such commits. If some issues detected in a before-commit version disappear in the corresponding after-commit version, they are very likely to be real bugs that got fixed by the commit. We use D2A to generate a large labeled dataset to train models for vulnerability identification. We show that the dataset can be used to build a classifier to identify possible false alarms among the issues reported by static analysis, hence helping developers prioritize and investigate potential true positives first.", "arxiv_url": "https://arxiv.org/abs/2102.07995", "authors": ["Yunhui Zheng", "Saurabh Pujar", "Burn Lewis", "Luca Buratti", "Edward Epstein", "Bo Yang", "Jim Laredo", "Alessandro Morari", "Zhong Su"], "first_author": "Yunhui Zheng", "category": ["Benchmark", "Technical"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Differential labeling from version pairs", "Auto-labeler pipeline", "Static-analyzer false-positive reduction", "Inter-procedural trace preservation", "Commit-pair based ground-truth heuristics", "Scalable parallel static analysis", "Static-analysis-derived features", "Label quality validation via manual review"], "summary": "本文提出D2A：一种基于版本差分静态分析和提交历史的自动标注方法，构建了大规模真实C/C++漏洞检测数据集并展示其在静态分析误报抑制任务中的有效性。", "quality": "High", "conference": "International Conference on Software Engineering (ICSE)", "pdf_url": "https://arxiv.org/pdf/2102.07995v1", "published": "2021-02-16", "update_time": "2021-02-16", "download_time": "2025-12-11 17:11:36"}
{"id": "2105.12787", "title": "Self-Supervised Bug Detection and Repair", "abstract": "Machine learning-based program analyses have recently shown the promise of integrating formal and probabilistic reasoning towards aiding software development. However, in the absence of large annotated corpora, training these analyses is challenging. Towards addressing this, we present BugLab, an approach for self-supervised learning of bug detection and repair. BugLab co-trains two models: (1) a detector model that learns to detect and repair bugs in code, (2) a selector model that learns to create buggy code for the detector to use as training data. A Python implementation of BugLab improves by up to 30% upon baseline methods on a test dataset of 2374 real-life bugs and finds 19 previously unknown bugs in open-source software.", "arxiv_url": "https://arxiv.org/abs/2105.12787", "authors": ["Miltiadis Allamanis", "Henry Jackson-Flux", "Marc Brockschmidt"], "first_author": "Miltiadis Allamanis", "category": ["Technical", "Benchmark"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Self-supervised adversarial co-training", "Bug selector-detector", "Syntax-tree rewrite operators", "Localization-and-rewrite prediction", "Heterogeneous code-graph representation", "Relational transformer encoding", "Curated real-world bug benchmark"], "summary": "本文提出BUGLAB——一种通过联合训练“选择器”（生成难以检测的语法重写错误）与“检测器”（定位并修复错误）的自监督方法，基于代码重写规则在未标注代码上学习缺陷定位与修复，并在Python实现上通过新整理的真实缺陷基准显著提升性能且在开源项目中发现若干新错误。", "quality": "High", "conference": "NeurIPS 2021", "pdf_url": "https://arxiv.org/pdf/2105.12787v3", "published": "2021-05-26", "update_time": "2021-11-16", "download_time": "2025-12-11 17:12:10"}
{"id": "2107.08760", "title": "CVEfixes: Automated Collection of Vulnerabilities and Their Fixes from Open-Source Software", "abstract": "Data-driven research on the automated discovery and repair of security vulnerabilities in source code requires comprehensive datasets of real-life vulnerable code and their fixes. To assist in such research, we propose a method to automatically collect and curate a comprehensive vulnerability dataset from Common Vulnerabilities and Exposures (CVE) records in the public National Vulnerability Database (NVD). We implement our approach in a fully automated dataset collection tool and share an initial release of the resulting vulnerability dataset named CVEfixes.   The CVEfixes collection tool automatically fetches all available CVE records from the NVD, gathers the vulnerable code and corresponding fixes from associated open-source repositories, and organizes the collected information in a relational database. Moreover, the dataset is enriched with meta-data such as programming language, and detailed code and security metrics at five levels of abstraction. The collection can easily be repeated to keep up-to-date with newly discovered or patched vulnerabilities. The initial release of CVEfixes spans all published CVEs up to 9 June 2021, covering 5365 CVE records for 1754 open-source projects that were addressed in a total of 5495 vulnerability fixing commits.   CVEfixes supports various types of data-driven software security research, such as vulnerability prediction, vulnerability classification, vulnerability severity prediction, analysis of vulnerability-related code changes, and automated vulnerability repair.", "arxiv_url": "https://arxiv.org/abs/2107.08760", "authors": ["Guru Prasad Bhandari", "Amara Naseer", "Leon Moonen"], "first_author": "Guru Prasad Bhandari", "category": ["Benchmark", "Survey"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["CVE-to-commit linking", "vulnerability-fix mining", "multi-granularity code artifacts", "CWE-based labeling", "CVSS severity enrichment", "code- and commit-level metrics", "language-agnostic collection", "automated repository mining tool"], "summary": "本文提出并开源了一种从NVD CVE记录自动收集开源项目中真实漏洞及其修复的工具与数据集，按多粒度（仓库/提交/文件/方法/CVE）组织并附加CWE分类、CVSS评分和多项代码度量，旨在支持漏洞检测、分类与自动修复等研究。", "quality": "High", "conference": "Proceedings of the 17th International Conference on Predictive Models and Data Analytics in Software Engineering (PROMISE '21) 2021", "pdf_url": "https://arxiv.org/pdf/2107.08760v1", "published": "2021-07-19", "update_time": "2021-07-19", "download_time": "2025-12-11 17:12:46"}
{"id": "2304.00409", "title": "DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection", "abstract": "We propose and release a new vulnerable source code dataset. We curate the dataset by crawling security issue websites, extracting vulnerability-fixing commits and source codes from the corresponding projects. Our new dataset contains 18,945 vulnerable functions spanning 150 CWEs and 330,492 non-vulnerable functions extracted from 7,514 commits. Our dataset covers 295 more projects than all previous datasets combined.   Combining our new dataset with previous datasets, we present an analysis of the challenges and promising research directions of using deep learning for detecting software vulnerabilities. We study 11 model architectures belonging to 4 families. Our results show that deep learning is still not ready for vulnerability detection, due to high false positive rate, low F1 score, and difficulty of detecting hard CWEs. In particular, we demonstrate an important generalization challenge for the deployment of deep learning-based models. We show that increasing the volume of training data may not further improve the performance of deep learning models for vulnerability detection, but might be useful to improve the generalization ability to unseen projects.   We also identify hopeful future research directions. We demonstrate that large language models (LLMs) are a promising research direction for ML-based vulnerability detection, outperforming Graph Neural Networks (GNNs) with code-structure features in our experiments. Moreover, developing source code specific pre-training objectives is a promising research direction to improve the vulnerability detection performance.", "arxiv_url": "https://arxiv.org/abs/2304.00409", "authors": ["Yizheng Chen", "Zhoujie Ding", "Lamya Alowain", "Xinyun Chen", "David Wagner"], "first_author": "Yizheng Chen", "category": ["Benchmark", "Empirical"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["C/C++ function-level vulnerability labels", "Vulnerability-fixing-commit labeling", "Label noise quantification", "Cross-project generalization", "Code-specific pretraining objectives", "Transformer vs GNN comparison", "False positive rate analysis", "CWE diversity coverage"], "summary": "本文发布了DiverseVul，一个大规模且多样化的C/C++漏洞函数数据集，并通过对多种模型的系统评估发现：在更大训练集下基于代码预训练的Transformer优于GNN但总体F1和跨项目泛化仍然很差且标签噪声显著，强调需改进代码专用预训练与泛化方法。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2304.00409v2", "published": "2023-04-01", "update_time": "2023-08-09", "download_time": "2025-12-11 17:13:24"}
{"id": "2306.17193", "title": "Uncovering the Limits of Machine Learning for Automatic Vulnerability Detection", "abstract": "Recent results of machine learning for automatic vulnerability detection (ML4VD) have been very promising. Given only the source code of a function $f$, ML4VD techniques can decide if $f$ contains a security flaw with up to 70% accuracy. However, as evident in our own experiments, the same top-performing models are unable to distinguish between functions that contain a vulnerability and functions where the vulnerability is patched. So, how can we explain this contradiction and how can we improve the way we evaluate ML4VD techniques to get a better picture of their actual capabilities?   In this paper, we identify overfitting to unrelated features and out-of-distribution generalization as two problems, which are not captured by the traditional approach of evaluating ML4VD techniques. As a remedy, we propose a novel benchmarking methodology to help researchers better evaluate the true capabilities and limits of ML4VD techniques. Specifically, we propose (i) to augment the training and validation dataset according to our cross-validation algorithm, where a semantic preserving transformation is applied during the augmentation of either the training set or the testing set, and (ii) to augment the testing set with code snippets where the vulnerabilities are patched.   Using six ML4VD techniques and two datasets, we find (a) that state-of-the-art models severely overfit to unrelated features for predicting the vulnerabilities in the testing data, (b) that the performance gained by data augmentation does not generalize beyond the specific augmentations applied during training, and (c) that state-of-the-art ML4VD techniques are unable to distinguish vulnerable functions from their patches.", "arxiv_url": "https://arxiv.org/abs/2306.17193", "authors": ["Niklas Risse", "Marcel Böhme"], "first_author": "Niklas Risse", "category": ["Empirical", "Benchmark", "Technical"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Semantic-preserving code transformations", "Overfitting to superficial features", "Cross-transformation generalization", "Patch-pair evaluation", "Data augmentation robustness", "Vuln–patch paired dataset", "Evaluation algorithms for ML4VD", "Out-of-distribution generalization"], "summary": "本文提出两种用于评估自动化漏洞检测（ML4VD）的算法并发布包含漏洞与对应补丁对的新数据集，实验证明现有基于令牌的模型严重依赖与漏洞无关的特征且无法区分漏洞与其补丁。", "quality": "High", "conference": "Proceedings of the 33rd USENIX Security Symposium (USENIX Security 2024) 2024", "pdf_url": "https://arxiv.org/pdf/2306.17193v2", "published": "2023-06-28", "update_time": "2024-06-06", "download_time": "2025-12-11 17:14:01"}
{"id": "2311.12420", "title": "How Far Have We Gone in Vulnerability Detection Using Large Language Models", "abstract": "As software becomes increasingly complex and prone to vulnerabilities, automated vulnerability detection is critically important, yet challenging. Given the significant successes of large language models (LLMs) in various tasks, there is growing anticipation of their efficacy in vulnerability detection. However, a quantitative understanding of their potential in vulnerability detection is still missing. To bridge this gap, we introduce a comprehensive vulnerability benchmark VulBench. This benchmark aggregates high-quality data from a wide range of CTF (Capture-the-Flag) challenges and real-world applications, with annotations for each vulnerable function detailing the vulnerability type and its root cause. Through our experiments encompassing 16 LLMs and 6 state-of-the-art (SOTA) deep learning-based models and static analyzers, we find that several LLMs outperform traditional deep learning approaches in vulnerability detection, revealing an untapped potential in LLMs. This work contributes to the understanding and utilization of LLMs for enhanced software security.", "arxiv_url": "https://arxiv.org/abs/2311.12420", "authors": ["Zeyu Gao", "Hao Wang", "Yuchen Zhou", "Wenyu Zhu", "Chao Zhang"], "first_author": "Zeyu Gao", "category": ["Benchmark", "Empirical"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Function-level vulnerability labels", "CTF and real-world CVE aggregation", "Human expert annotation", "Binary and multi-class classification", "Root-cause vulnerability annotation", "Natural-language vulnerability descriptions", "LLM vs. static analyzer benchmarking", "Cross-model evaluation protocol", "Dataset quality curation"], "summary": "本文提出了VulBench——一个从CTF与真实漏洞数据源汇聚并经人工校验的高质量函数级漏洞基准，并在该基准上系统评估了16个LLM与多种深度学习模型和静态分析器，揭示了LLM在漏洞检测中的潜力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2311.12420v3", "published": "2023-11-21", "update_time": "2023-12-22", "download_time": "2025-12-11 17:14:38"}
{"id": "2403.18624", "title": "Vulnerability Detection with Code Language Models: How Far Are We?", "abstract": "In the context of the rising interest in code language models (code LMs) and vulnerability detection, we study the effectiveness of code LMs for detecting vulnerabilities. Our analysis reveals significant shortcomings in existing vulnerability datasets, including poor data quality, low label accuracy, and high duplication rates, leading to unreliable model performance in realistic vulnerability detection scenarios. Additionally, the evaluation methods used with these datasets are not representative of real-world vulnerability detection.   To address these challenges, we introduce PrimeVul, a new dataset for training and evaluating code LMs for vulnerability detection. PrimeVul incorporates a novel set of data labeling techniques that achieve comparable label accuracy to human-verified benchmarks while significantly expanding the dataset. It also implements a rigorous data de-duplication and chronological data splitting strategy to mitigate data leakage issues, alongside introducing more realistic evaluation metrics and settings. This comprehensive approach aims to provide a more accurate assessment of code LMs' performance in real-world conditions.   Evaluating code LMs on PrimeVul reveals that existing benchmarks significantly overestimate the performance of these models. For instance, a state-of-the-art 7B model scored 68.26% F1 on BigVul but only 3.09% F1 on PrimeVul. Attempts to improve performance through advanced training techniques and larger models like GPT-3.5 and GPT-4 were unsuccessful, with results akin to random guessing in the most stringent settings. These findings underscore the considerable gap between current capabilities and the practical requirements for deploying code LMs in security roles, highlighting the need for more innovative research in this domain.", "arxiv_url": "https://arxiv.org/abs/2403.18624", "authors": ["Yangruibo Ding", "Yanjun Fu", "Omniyyah Ibrahim", "Chawin Sitawarin", "Xinyun Chen", "Basel Alomair", "David Wagner", "Baishakhi Ray", "Yizheng Chen"], "first_author": "Yangruibo Ding", "category": ["Benchmark", "Empirical"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Label Noise Analysis", "Data De-duplication", "Chronological Train-Test Split", "False-Positive-Constrained Metric", "Pairwise Vulnerable-vs-Fixed Evaluation", "Commit-based Labeling Pitfalls", "CWE Coverage Expansion", "Real-world Evaluation Protocols"], "summary": "本文分析了现有漏洞数据集的标签噪声与数据泄露问题，提出高质量去重的大规模漏洞数据集PRIMEVUL并引入VD‑S与成对评估等更真实的评估指南，实验证明现有代码语言模型在现实漏洞检测场景下表现严重不足。", "quality": "High", "conference": "IEEE/ACM International Conference on Software Engineering (ICSE 2025)", "pdf_url": "https://arxiv.org/pdf/2403.18624v2", "published": "2024-03-27", "update_time": "2024-07-10", "download_time": "2025-12-11 17:15:08"}
{"id": "2406.07595", "title": "VulDetectBench: Evaluating the Deep Capability of Vulnerability Detection with Large Language Models", "abstract": "Large Language Models (LLMs) have training corpora containing large amounts of program code, greatly improving the model's code comprehension and generation capabilities. However, sound comprehensive research on detecting program vulnerabilities, a more specific task related to code, and evaluating the performance of LLMs in this more specialized scenario is still lacking. To address common challenges in vulnerability analysis, our study introduces a new benchmark, VulDetectBench, specifically designed to assess the vulnerability detection capabilities of LLMs. The benchmark comprehensively evaluates LLM's ability to identify, classify, and locate vulnerabilities through five tasks of increasing difficulty. We evaluate the performance of 17 models (both open- and closed-source) and find that while existing models can achieve over 80% accuracy on tasks related to vulnerability identification and classification, they still fall short on specific, more detailed vulnerability analysis tasks, with less than 30% accuracy, making it difficult to provide valuable auxiliary information for professional vulnerability mining. Our benchmark effectively evaluates the capabilities of various LLMs at different levels in the specific task of vulnerability detection, providing a foundation for future research and improvements in this critical area of code security. VulDetectBench is publicly available at https://github.com/Sweetaroo/VulDetectBench.", "arxiv_url": "https://arxiv.org/abs/2406.07595", "authors": ["Yu Liu", "Lang Gao", "Mingxin Yang", "Yu Xie", "Ping Chen", "Xiaojin Zhang", "Wei Chen"], "first_author": "Yu Liu", "category": ["Benchmark", "Empirical"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Vulnerability existence detection", "CWE type classification", "Root cause localization", "Trigger point identification", "Key objects/functions extraction", "C/C++ memory-safety vulnerabilities", "Mixed real-world and synthetic corpus", "Multi-task benchmark", "Cross-model comparative evaluation"], "summary": "本文提出VulDetectBench——一个面向C/C++漏洞检测的多任务基准（包含五个难度递增任务并结合真实与合成代码），对17个开闭源模型进行了比较评估，发现模型在漏洞存在检测与类型分类上表现良好但在根因与触发点等精细定位任务上表现不佳。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2406.07595v4", "published": "2024-06-11", "update_time": "2024-08-21", "download_time": "2025-12-11 17:15:44"}
{"id": "2411.17274", "title": "CleanVul: Automatic Function-Level Vulnerability Detection in Code Commits Using LLM Heuristics", "abstract": "Accurate identification of software vulnerabilities is crucial for system integrity. Vulnerability datasets, often derived from the National Vulnerability Database (NVD) or directly from GitHub, are essential for training machine learning models to detect these security flaws. However, these datasets frequently suffer from significant noise, typically 40% to 75%, due primarily to the automatic and indiscriminate labeling of all changes in vulnerability-fixing commits (VFCs) as vulnerability-related. This misclassification occurs because not all changes in a commit aimed at fixing vulnerabilities pertain to security threats; many are routine updates like bug fixes or test improvements.   This paper introduces the first methodology that uses the Large Language Model (LLM) with a heuristic enhancement to automatically identify vulnerability-fixing changes from VFCs, achieving an F1-score of 0.82. VulSifter was applied to a large-scale study, where we conducted a crawl of 127,063 repositories on GitHub, resulting in the acquisition of 5,352,105 commits. VulSifter involves utilizing an LLM to comprehend code semantics and contextual information, while applying heuristics to filter out unrelated changes. We then developed CleanVul, a high-quality dataset comprising 8,198 functions using our LLM heuristic enhancement approach, demonstrating Correctness (90.6%) comparable to established datasets such as SVEN and PrimeVul.   To evaluate the CleanVul dataset, we conducted experiments focusing on fine-tuning various LLMs on CleanVul and other high-quality datasets. Evaluation results reveal that LLMs fine-tuned on CleanVul not only exhibit enhanced accuracy but also superior generalization capabilities compared to those trained on uncleaned datasets. Specifically, models trained on CleanVul and tested on PrimeVul achieve accuracy higher than those trained and tested exclusively on PrimeVul.", "arxiv_url": "https://arxiv.org/abs/2411.17274", "authors": ["Yikun Li", "Ting Zhang", "Ratnadira Widyasari", "Yan Naing Tun", "Huu Hung Nguyen", "Tan Bui", "Ivana Clairine Irsan", "Yiran Cheng", "Xiang Lan", "Han Wei Ang", "Frank Liauw", "Martin Weyssow", "Hong Jin Kang", "Eng Lieh Ouh", "Lwin Khin Shar", "David Lo"], "first_author": "Yikun Li", "category": ["Technical", "Benchmark"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["LLM-based noise reduction", "Vulnerability-fixing commit analysis", "Function-level vulnerability labeling", "Heuristic filtering of code changes", "Commit diff and message contextualization", "Automated dataset curation pipeline", "Cross-dataset generalization evaluation"], "summary": "本文提出将大模型与启发式规则结合的VulSifter方法，自动识别并过滤漏洞修复提交中与漏洞无关的改动，构建高质量的函数级漏洞数据集CleanVul并验证其能提升漏洞检测模型的泛化能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2411.17274v7", "published": "2024-11-26", "update_time": "2025-09-11", "download_time": "2025-12-11 17:19:29"}
{"id": "2503.09433", "title": "CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards CWE Detection", "abstract": "Identifying vulnerabilities in source code is crucial, especially in critical software components. Existing methods such as static analysis, dynamic analysis, formal verification, and recently Large Language Models are widely used to detect security flaws. This paper introduces CASTLE (CWE Automated Security Testing and Low-Level Evaluation), a benchmarking framework for evaluating the vulnerability detection capabilities of different methods. We assess 13 static analysis tools, 10 LLMs, and 2 formal verification tools using a hand-crafted dataset of 250 micro-benchmark programs covering 25 common CWEs. We propose the CASTLE Score, a novel evaluation metric to ensure fair comparison. Our results reveal key differences: ESBMC (a formal verification tool) minimizes false positives but struggles with vulnerabilities beyond model checking, such as weak cryptography or SQL injection. Static analyzers suffer from high false positives, increasing manual validation efforts for developers. LLMs perform exceptionally well in the CASTLE dataset when identifying vulnerabilities in small code snippets. However, their accuracy declines, and hallucinations increase as the code size grows. These results suggest that LLMs could play a pivotal role in future security solutions, particularly within code completion frameworks, where they can provide real-time guidance to prevent vulnerabilities. The dataset is accessible at https://github.com/CASTLE-Benchmark.", "arxiv_url": "https://arxiv.org/abs/2503.09433", "authors": ["Richard A. Dubniczky", "Krisztofer Zoltán Horvát", "Tamás Bisztray", "Mohamed Amine Ferrag", "Lucas C. Cordeiro", "Norbert Tihanyi"], "first_author": "Richard A. Dubniczky", "category": ["Benchmark", "Empirical"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["CWE micro-benchmarks", "compilable C snippets", "line-level vulnerability labeling", "benchmark scoring metric", "static analyzer false-positive analysis", "LLM detection scalability", "formal verification coverage"], "summary": "本文构建了一个包含250个可编译C微基准、覆盖25类CWE的漏洞检测基准并提出评估度量，通过比较静态分析器、形式验证工具与多款LLM揭示各方法在误报率、覆盖范围与随代码规模扩展时性能衰减的差异。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2503.09433v2", "published": "2025-03-12", "update_time": "2025-03-31", "download_time": "2025-12-11 17:19:57"}
{"id": "2503.22388", "title": "Why Stop at One Error? Benchmarking LLMs as Data Science Code Debuggers for Multi-Hop and Multi-Bug Errors", "abstract": "LLMs are transforming software development, yet current code generation and code repair benchmarks mainly assess syntactic and functional correctness in simple, single-error cases. LLMs' capabilities to autonomously find and fix runtime logical errors in complex data science code remain largely unexplored. To address this gap, we introduce DSDBench: the Data Science Debugging Benchmark, the first benchmark for systematic evaluation of LLMs on multi-hop error tracing and multi-bug detection in data science code debugging. DSDBench adapts datasets from existing data science task benchmarks, such as DABench and MatPlotBench, featuring realistic data science debugging tasks with automatically synthesized multi-hop, multi-bug code snippets. DSDBench includes 1,117 annotated samples with 741 cause-effect error pairs and runtime error messages. Evaluations of state-of-the-art LLMs on DSDBench show significant performance gaps, highlighting challenges in debugging logical runtime errors in data science code. DSDBench offers a crucial resource to evaluate and improve LLMs' debugging and reasoning capabilities, enabling more reliable AI-assisted data science in the future. DSDBench is publicly available at github.com/KevinCL16/DSDBench.", "arxiv_url": "https://arxiv.org/abs/2503.22388", "authors": ["Zhiyu Yang", "Shuo Wang", "Yukun Yan", "Yang Deng"], "first_author": "Zhiyu Yang", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Multi-hop Error Tracing", "Multi-Bug Detection", "Runtime Error Injection", "Cause-Effect Line Annotation", "Automated Error Injection Pipeline", "Library API Misuse", "Interactive Notebook Debugging", "Data Science Workflow Failures"], "summary": "本文提出并发布了DSDBench——一个针对数据科学脚本的基准数据集与自动化注入/对齐流水线，用以评估LLM在多跳与多错误运行时调试（定位根因并关联错误触发行）方面的能力，并通过大规模实证揭示了现有模型在此任务上的显著不足。", "quality": "High", "conference": "EMNLP 2025", "pdf_url": "https://arxiv.org/pdf/2503.22388v3", "published": "2025-03-28", "update_time": "2025-09-16", "download_time": "2025-12-11 17:20:27"}
{"id": "2505.19828", "title": "SecVulEval: Benchmarking LLMs for Real-World C/C++ Vulnerability Detection", "abstract": "Large Language Models (LLMs) have shown promise in software engineering tasks, but evaluating their effectiveness in vulnerability detection is challenging due to the lack of high-quality datasets. Most existing datasets are limited to function-level labels, ignoring finer-grained vulnerability patterns and crucial contextual information. Also, poor data quality such as mislabeling, inconsistent annotations, and duplicates can lead to inflated performance and weak generalization. Moreover, by including only the functions, these datasets miss broader program context, like data/control dependencies and interprocedural interactions, that are essential for accurately understanding real-world security flaws. Without this context, detection models are evaluated under unrealistic assumptions.   To address these limitations, this paper introduces SecVulEval, a benchmark designed to support fine-grained evaluation of LLMs and other detection methods with rich contextual information. SecVulEval focuses on real-world C/C++ vulnerabilities at the statement level. This granularity enables more precise evaluation of a model's ability to localize vulnerabilities, beyond simple binary classification at the function level. By incorporating rich contextual information, SecVulEval sets a new standard for vulnerability detection benchmarks in realistic scenarios. This benchmark includes 25,440 function samples covering 5,867 unique CVEs in C/C++ projects from 1999 to 2024. We evaluated the SOTA LLMs with a multi-agent-based approach. The evaluation on our dataset shows that the models are still far from accurately predicting vulnerable statements in a given function. The best-performing Claude-3.7-Sonnet model achieves 23.83% F1-score for detecting vulnerable statements with correct reasoning. Finally, we analyze the LLM outputs and provide insights into their behavior in vulnerability detection for C/C++.", "arxiv_url": "https://arxiv.org/abs/2505.19828", "authors": ["Md Basim Uddin Ahmed", "Nima Shiri Harzevili", "Jiho Shin", "Hung Viet Pham", "Song Wang"], "first_author": "Md Basim Uddin Ahmed", "category": ["Benchmark", "Empirical"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Statement-level vulnerability localization", "Patch-derived labels and commit metadata", "CWE/CVE annotation", "Context extraction across five context levels", "Interprocedural and data/control dependency context", "Multi-agent LLM evaluation pipeline", "De-duplication and data quality curation", "Empirical F1 evaluation of LLM vulnerability localization"], "summary": "本文提出SECVULEVAL——一个面向真实C/C++项目、具备语句级漏洞标注与丰富上下文（函数参数、外部函数、类型定义、全局量、执行环境）和CVE/CWE元数据的基准，并通过多代理LLM评估显示当前模型在语句级漏洞定位上表现仍然很差。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2505.19828v1", "published": "2025-05-26", "update_time": "2025-05-26", "download_time": "2025-12-11 17:21:02"}
{"id": "2505.20630", "title": "SV-TrustEval-C: Evaluating Structure and Semantic Reasoning in Large Language Models for Source Code Vulnerability Analysis", "abstract": "As Large Language Models (LLMs) evolve in understanding and generating code, accurately evaluating their reliability in analyzing source code vulnerabilities becomes increasingly vital. While studies have examined LLM capabilities in tasks like vulnerability detection and repair, they often overlook the importance of both structure and semantic reasoning crucial for trustworthy vulnerability analysis. To address this gap, we introduce SV-TrustEval-C, a benchmark designed to evaluate LLMs' abilities for vulnerability analysis of code written in the C programming language through two key dimensions: structure reasoning - assessing how models identify relationships between code elements under varying data and control flow complexities; and semantic reasoning - examining their logical consistency in scenarios where code is structurally and semantically perturbed. Our results show that current LLMs are far from satisfactory in understanding complex code relationships and that their vulnerability analyses rely more on pattern matching than on robust logical reasoning. These findings underscore the effectiveness of the SV-TrustEval-C benchmark and highlight critical areas for enhancing the reasoning capabilities and trustworthiness of LLMs in real-world vulnerability analysis tasks. Our initial benchmark dataset is publicly available.", "arxiv_url": "https://arxiv.org/abs/2505.20630", "authors": ["Yansong Li", "Paula Branco", "Alexander M. Hoole", "Manish Marwah", "Hari Manassery Koduvely", "Guy-Vincent Jourdan", "Stephan Jou"], "first_author": "Yansong Li", "category": ["Benchmark", "Empirical"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Structure reasoning", "Semantic reasoning", "Counterfactual code perturbations", "Goal-driven completion scenarios", "Predictive vulnerability scenarios", "Structure-oriented variants generator", "Data-flow perturbation", "Control-flow perturbation", "C language vulnerabilities", "Reasoning-based QA evaluation", "Pattern-matching vs logical reasoning"], "summary": "本文提出了 SV-TRUSTEVAL-C 基准，通过结构导向变体生成器对 C 语言代码进行语义和结构扰动，以评估 LLM 在漏洞分析中对结构与语义推理的能力，并通过多模型实验发现现有模型更依赖模式匹配而非稳健逻辑推理。", "quality": "High", "conference": "IEEE Symposium on Security and Privacy (SP) 2025", "pdf_url": "https://arxiv.org/pdf/2505.20630v1", "published": "2025-05-27", "update_time": "2025-05-27", "download_time": "2025-12-11 17:23:17"}
{"id": "2509.25242", "title": "A Benchmark for Localizing Code and Non-Code Issues in Software Projects", "abstract": "Accurate project localization (e.g., files and functions) for issue resolution is a critical first step in software maintenance. However, existing benchmarks for issue localization, such as SWE-Bench and LocBench, are limited. They focus predominantly on pull-request issues and code locations, ignoring other evidence and non-code files such as commits, comments, configurations, and documentation. To address this gap, we introduce MULocBench, a comprehensive dataset of 1,100 issues from 46 popular GitHub Python projects. Comparing with existing benchmarks, MULocBench offers greater diversity in issue types, root causes, location scopes, and file types, providing a more realistic testbed for evaluation. Using this benchmark, we assess the performance of state-of-the-art localization methods and five LLM-based prompting strategies. Our results reveal significant limitations in current techniques: even at the file level, performance metrics (Acc@5, F1) remain below 40%. This underscores the challenge of generalizing to realistic, multi-faceted issue resolution. To enable future research on project localization for issue resolution, we publicly release MULocBench at https://huggingface.co/datasets/somethingone/MULocBench.", "arxiv_url": "https://arxiv.org/abs/2509.25242", "authors": ["Zejun Zhang", "Jian Wang", "Qingyun Yang", "Yifan Pan", "Yi Tang", "Yi Li", "Zhenchang Xing", "Tian Zhang", "Xuandong Li", "Guoan Zhang"], "first_author": "Zejun Zhang", "category": ["Benchmark", "Empirical"], "field": "Quality Management", "task": "Bug Localization", "tags": ["Multi-granularity localization (file/class/function)", "Cross-artifact localization (code, config, docs, tests, assets)", "Resolution-evidence extraction from PRs/commits/comments", "Manual annotation & patch parsing", "LLM prompting strategies for localization", "Issue type and root-cause taxonomy", "Comparison of retrieval-/procedure-/agent-based localizers", "Real-world GitHub Python corpus"], "summary": "本文提出并公开了 MULocBench——一个涵盖46个流行 Python 项目、1100 个问题的定位基准，包含代码与非代码文件及详细位置标注，并通过实证比较现有定位方法与五种基于提示的 LLM 策略，揭示当前方法在真实场景下（文件/类/函数级）定位效果仍然较差（文件级 Acc@5 < 40%）。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2509.25242v1", "published": "2025-09-26", "update_time": "2025-09-26", "download_time": "2025-12-11 17:42:48"}
{"id": "2305.05959", "title": "Survey of Code Search Based on Deep Learning", "abstract": "Code writing is repetitive and predictable, inspiring us to develop various code intelligence techniques. This survey focuses on code search, that is, to retrieve code that matches a given query by effectively capturing the semantic similarity between the query and code. Deep learning, being able to extract complex semantics information, has achieved great success in this field. Recently, various deep learning methods, such as graph neural networks and pretraining models, have been applied to code search with significant progress. Deep learning is now the leading paradigm for code search. In this survey, we provide a comprehensive overview of deep learning-based code search. We review the existing deep learning-based code search framework which maps query/code to vectors and measures their similarity. Furthermore, we propose a new taxonomy to illustrate the state-of-the-art deep learning-based code search in a three-steps process: query semantics modeling, code semantics modeling, and matching modeling which involves the deep learning model training. Finally, we suggest potential avenues for future research in this promising field.", "arxiv_url": "https://arxiv.org/abs/2305.05959", "authors": ["Yutao Xie", "Jiayi Lin", "Hande Dong", "Lei Zhang", "Zhonghai Wu"], "first_author": "Yutao Xie", "category": ["Survey"], "field": "Coding Assistant", "task": "Code Retrieval", "tags": ["NL-to-code retrieval", "Query intent modeling", "Code semantic representation", "Graph-based code representations", "Transformer-based code encoders", "Contrastive and ranking training objectives", "Evaluation metrics and benchmarking", "Training / pretraining strategies for retrieval"], "summary": "本文综述了基于深度学习的代码检索研究，提出了查询语义建模、代码语义建模与匹配建模的三步分类法，并总结了现有方法、评估指标与未来研究方向。", "quality": "High", "conference": "ACM Transactions on Software Engineering and Methodology 2023", "pdf_url": "https://arxiv.org/pdf/2305.05959v2", "published": "2023-05-10", "update_time": "2023-12-13", "download_time": "2025-12-11 17:43:30"}
{"id": "1803.09371", "title": "StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow", "abstract": "Stack Overflow (SO) has been a great source of natural language questions and their code solutions (i.e., question-code pairs), which are critical for many tasks including code retrieval and annotation. In most existing research, question-code pairs were collected heuristically and tend to have low quality. In this paper, we investigate a new problem of systematically mining question-code pairs from Stack Overflow (in contrast to heuristically collecting them). It is formulated as predicting whether or not a code snippet is a standalone solution to a question. We propose a novel Bi-View Hierarchical Neural Network which can capture both the programming content and the textual context of a code snippet (i.e., two views) to make a prediction. On two manually annotated datasets in Python and SQL domain, our framework substantially outperforms heuristic methods with at least 15% higher F1 and accuracy. Furthermore, we present StaQC (Stack Overflow Question-Code pairs), the largest dataset to date of ~148K Python and ~120K SQL question-code pairs, automatically mined from SO using our framework. Under various case studies, we demonstrate that StaQC can greatly help develop data-hungry models for associating natural language with programming language.", "arxiv_url": "https://arxiv.org/abs/1803.09371", "authors": ["Ziyu Yao", "Daniel S. Weld", "Wei-Peng Chen", "Huan Sun"], "first_author": "Ziyu Yao", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Alignment", "tags": ["question-code pairing", "standalone-solution identification", "text-code joint representation", "block-level hierarchical encoding", "Stack Overflow mining", "how-to-do-it question filtering", "code snippet role classification", "Python and SQL domains"], "summary": "本文提出一种同时利用文本上下文与代码内容的双视角分层神经网络来识别 Stack Overflow 回答中作为独立解法的代码片段，并基于此系统化地挖掘出大规模高质量的问答-代码配对数据集（Python 与 SQL）。", "quality": "High", "conference": "The Web Conference (WWW 2018)", "pdf_url": "https://arxiv.org/pdf/1803.09371v1", "published": "2018-03-26", "update_time": "2018-03-26", "download_time": "2025-12-11 17:44:02"}
{"id": "1805.08949", "title": "Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow", "abstract": "For tasks like code synthesis from natural language, code retrieval, and code summarization, data-driven models have shown great promise. However, creating these models require parallel data between natural language (NL) and code with fine-grained alignments. Stack Overflow (SO) is a promising source to create such a data set: the questions are diverse and most of them have corresponding answers with high-quality code snippets. However, existing heuristic methods (e.g., pairing the title of a post with the code in the accepted answer) are limited both in their coverage and the correctness of the NL-code pairs obtained. In this paper, we propose a novel method to mine high-quality aligned data from SO using two sets of features: hand-crafted features considering the structure of the extracted snippets, and correspondence features obtained by training a probabilistic model to capture the correlation between NL and code using neural networks. These features are fed into a classifier that determines the quality of mined NL-code pairs. Experiments using Python and Java as test beds show that the proposed method greatly expands coverage and accuracy over existing mining methods, even when using only a small number of labeled examples. Further, we find that reasonable results are achieved even when training the classifier on one language and testing on another, showing promise for scaling NL-code mining to a wide variety of programming languages beyond those for which we are able to annotate data.", "arxiv_url": "https://arxiv.org/abs/1805.08949", "authors": ["Pengcheng Yin", "Bowen Deng", "Edgar Chen", "Bogdan Vasilescu", "Graham Neubig"], "first_author": "Pengcheng Yin", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Alignment", "tags": ["Neural correspondence features", "Bidirectional conditional probabilities", "Structural snippet features", "Intent–snippet–context segmentation", "Line-contiguous fragment enumeration", "Stack Overflow mining", "Cross-language transfer", "Annotation protocol and labeling tool"], "summary": "本文提出一种将手工结构特征与基于神经翻译模型的双向条件概率相结合的分类方法，用以从 Stack Overflow 自动挖掘高质量对齐的自然语言–代码片段对，并发布了标注数据与工具。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/1805.08949v1", "published": "2018-05-23", "update_time": "2018-05-23", "download_time": "2025-12-11 17:44:34"}
{"id": "1908.09804", "title": "Neural Code Search Evaluation Dataset", "abstract": "There has been an increase of interest in code search using natural language. Assessing the performance of such code search models can be difficult without a readily available evaluation suite. In this paper, we present an evaluation dataset consisting of natural language query and code snippet pairs, with the hope that future work in this area can use this dataset as a common benchmark. We also provide the results of two code search models ([1] and [6]) from recent work.   The evaluation dataset is available at https://github.com/facebookresearch/Neural-Code-Search-Evaluation-Dataset", "arxiv_url": "https://arxiv.org/abs/1908.09804", "authors": ["Hongyu Li", "Seohyun Kim", "Satish Chandra"], "first_author": "Hongyu Li", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Retrieval", "tags": ["Stack Overflow Q&A curation", "Method-level code extraction", "Android-focused search corpus", "GitHub commit-aware indexing", "Automatic code-to-code relevance labeling", "Evaluation metrics (MRR, Answered@k)"], "summary": "该论文发布了一个面向自然语言到代码检索的评估数据集，包含约4.7M个方法级搜索语料、287个经人工筛选的Stack Overflow问答对以及用于自动判定相关性的相似度标注和基准模型评测结果。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/1908.09804v6", "published": "2019-08-26", "update_time": "2019-10-02", "download_time": "2025-12-11 17:45:08"}
{"id": "2008.12193", "title": "Neural Code Search Revisited: Enhancing Code Snippet Retrieval through Natural Language Intent", "abstract": "In this work, we propose and study annotated code search: the retrieval of code snippets paired with brief descriptions of their intent using natural language queries. On three benchmark datasets, we investigate how code retrieval systems can be improved by leveraging descriptions to better capture the intents of code snippets. Building on recent progress in transfer learning and natural language processing, we create a domain-specific retrieval model for code annotated with a natural language description. We find that our model yields significantly more relevant search results (with absolute gains up to 20.6% in mean reciprocal rank) compared to state-of-the-art code retrieval methods that do not use descriptions but attempt to compute the intent of snippets solely from unannotated code.", "arxiv_url": "https://arxiv.org/abs/2008.12193", "authors": ["Geert Heyman", "Tom Van Cutsem"], "first_author": "Geert Heyman", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Retrieval", "tags": ["Annotated code snippets", "Natural language intent descriptions", "Description-aware ranking", "Semantic-similarity fine-tuning", "Transfer learning for retrieval", "Benchmark dataset creation", "Snippet-description pairing heuristics"], "summary": "本文提出并研究“注释代码搜索”任务（带有自然语言意图描述的代码片段检索），构建三个基准数据集并通过对预训练模型的细化微调利用描述显著提升检索性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2008.12193v1", "published": "2020-08-27", "update_time": "2020-08-27", "download_time": "2025-12-11 17:45:44"}
{"id": "2102.04664", "title": "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation", "abstract": "Benchmark datasets have a significant impact on accelerating research in programming language tasks. In this paper, we introduce CodeXGLUE, a benchmark dataset to foster machine learning research for program understanding and generation. CodeXGLUE includes a collection of 10 tasks across 14 datasets and a platform for model evaluation and comparison. CodeXGLUE also features three baseline systems, including the BERT-style, GPT-style, and Encoder-Decoder models, to make it easy for researchers to use the platform. The availability of such data and baselines can help the development and validation of new methods that can be applied to various program understanding and generation problems.", "arxiv_url": "https://arxiv.org/abs/2102.04664", "authors": ["Shuai Lu", "Daya Guo", "Shuo Ren", "Junjie Huang", "Alexey Svyatkovskiy", "Ambrosio Blanco", "Colin Clement", "Dawn Drain", "Daxin Jiang", "Duyu Tang", "Ge Li", "Lidong Zhou", "Linjun Shou", "Long Zhou", "Michele Tufano", "Ming Gong", "Ming Zhou", "Nan Duan", "Neel Sundaresan", "Shao Kun Deng", "Shengyu Fu", "Shujie Liu"], "first_author": "Shuai Lu", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Understanding", "tags": ["Multi-task code benchmark", "Program understanding and generation", "Cloze-style semantic probing", "Line-level code completion", "Code-to-code translation (Java↔C#)", "Natural-language code search (web queries, normalized identifiers)", "Documentation translation", "Clone detection", "Defect / vulnerability detection", "Baseline model suite and evaluation platform"], "summary": "本文提出并发布了 CodeXGLUE，一个包含14个数据集、覆盖10类代码理解与生成任务并提供评测平台与基线实现的多任务代码基准套件。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2102.04664v2", "published": "2021-02-09", "update_time": "2021-03-16", "download_time": "2025-12-11 17:46:31"}
{"id": "2105.13239", "title": "CoSQA: 20,000+ Web Queries for Code Search and Question Answering", "abstract": "Finding codes given natural language query isb eneficial to the productivity of software developers. Future progress towards better semantic matching between query and code requires richer supervised training resources. To remedy this, we introduce the CoSQA dataset.It includes 20,604 labels for pairs of natural language queries and codes, each annotated by at least 3 human annotators. We further introduce a contrastive learning method dubbed CoCLR to enhance query-code matching, which works as a data augmenter to bring more artificially generated training instances. We show that evaluated on CodeXGLUE with the same CodeBERT model, training on CoSQA improves the accuracy of code question answering by 5.1%, and incorporating CoCLR brings a further improvement of 10.5%.", "arxiv_url": "https://arxiv.org/abs/2105.13239", "authors": ["Junjie Huang", "Duyu Tang", "Linjun Shou", "Ming Gong", "Ke Xu", "Daxin Jiang", "Ming Zhou", "Nan Duan"], "first_author": "Junjie Huang", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Alignment", "tags": ["Web-search-query curation from search logs", "Query intent filtering via heuristic keywords", "Crowdsourced binary relevance annotations", "Function-with-documentation as code answer unit", "Contrastive learning for query-code matching", "Data augmentation via synthetic query-code pairs", "Query-code semantic retrieval evaluation", "Large-scale human-labeled code search benchmark"], "summary": "该论文发布了一个包含20,604对经过多人标注的真实网页查询与Python函数的查询-代码匹配数据集，并提出一种基于对比学习的数据增强方法以显著提升查询到代码的检索与问答性能。", "quality": "High", "conference": "ACL 2021", "pdf_url": "https://arxiv.org/pdf/2105.13239v1", "published": "2021-05-27", "update_time": "2021-05-27", "download_time": "2025-12-11 17:47:07"}
{"id": "2403.16702", "title": "ProCQA: A Large-scale Community-based Programming Question Answering Dataset for Code Search", "abstract": "Retrieval-based code question answering seeks to match user queries in natural language to relevant code snippets. Previous approaches typically rely on pretraining models using crafted bi-modal and uni-modal datasets to align text and code representations. In this paper, we introduce ProCQA, a large-scale programming question answering dataset extracted from the StackOverflow community, offering naturally structured mixed-modal QA pairs. To validate its effectiveness, we propose a modality-agnostic contrastive pre-training approach to improve the alignment of text and code representations of current code language models. Compared to previous models that primarily employ bimodal and unimodal pairs extracted from CodeSearchNet for pre-training, our model exhibits significant performance improvements across a wide range of code retrieval benchmarks.", "arxiv_url": "https://arxiv.org/abs/2403.16702", "authors": ["Zehan Li", "Jianfei Zhang", "Chuantao Yin", "Yuanxin Ouyang", "Wenge Rong"], "first_author": "Zehan Li", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Alignment", "tags": ["Mixed-modal (interleaved code and text) QA pairs", "Community Q&A sourcing (StackOverflow)", "Modality-agnostic contrastive pretraining", "Dual-encoder representation learning", "Retrieval-based code search", "Rule-based filtering and decontamination", "Large-scale multi-language corpus"], "summary": "本文提出ProCQA——一个从StackOverflow挖掘的约500万条跨11种编程语言的混合模态（代码与文本交织）问答数据集，并基于此提出模态不可知的对比预训练方法以提升代码与文本表示对齐与检索性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2403.16702v1", "published": "2024-03-25", "update_time": "2024-03-25", "download_time": "2025-12-11 17:47:46"}
{"id": "2406.11589", "title": "CoSQA+: Pioneering the Multi-Choice Code Search Benchmark with Test-Driven Agents", "abstract": "Semantic code search, retrieving code that matches a given natural language query, is an important task to improve productivity in software engineering. Existing code search datasets face limitations: they rely on human annotators who assess code primarily through semantic understanding rather than functional verification, leading to potential inaccuracies and scalability issues. Additionally, current evaluation metrics often overlook the multi-choice nature of code search. This paper introduces CoSQA+, pairing high-quality queries from CoSQA with multiple suitable codes. We develop an automated pipeline featuring multiple model-based candidate selections and the novel test-driven agent annotation system. Among a single Large Language Model (LLM) annotator and Python expert annotators (without test-based verification), agents leverage test-based verification and achieve the highest accuracy of 93.9%. Through extensive experiments, CoSQA+ has demonstrated superior quality over CoSQA. Models trained on CoSQA+ exhibit improved performance. We publicly release both CoSQA+_all, which contains 412,080 agent-annotated pairs, and CoSQA+_verified, which contains 1,000 human-verified pairs, at https://github.com/DeepSoftwareAnalytics/CoSQA_Plus.", "arxiv_url": "https://arxiv.org/abs/2406.11589", "authors": ["Jing Gong", "Yanghui Wu", "Linxi Liang", "Yanlin Wang", "Jiachi Chen", "Mingwei Liu", "Zibin Zheng"], "first_author": "Jing Gong", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Retrieval", "tags": ["Multi-choice code search", "Test-driven agent annotation", "Automatic test generation and execution", "Multi-model candidate selection", "Executable verification", "Human-verified gold subset", "MAP@10 evaluation", "Cross-language generalization"], "summary": "本文提出CoSQA+，通过多模型候选筛选与测试驱动的多智能体自动标注流水线构建面向多选场景的代码检索基准，并公开了大规模自动标注集与1000条人工验证子集以提升检索评估与模型性能。", "quality": "High", "conference": "IEEE Transactions on Software Engineering (TSE) 2025", "pdf_url": "https://arxiv.org/pdf/2406.11589v6", "published": "2024-06-17", "update_time": "2025-11-10", "download_time": "2025-12-11 17:48:25"}
{"id": "2407.02883", "title": "CoIR: A Comprehensive Benchmark for Code Information Retrieval Models", "abstract": "Despite the substantial success of Information Retrieval (IR) in various NLP tasks, most IR systems predominantly handle queries and corpora in natural language, neglecting the domain of code retrieval. Code retrieval is critically important yet remains under-explored, with existing methods and benchmarks inadequately representing the diversity of code in various domains and tasks. Addressing this gap, we present COIR (Code Information Retrieval Benchmark), a robust and comprehensive benchmark specifically designed to assess code retrieval capabilities. COIR comprises ten meticulously curated code datasets, spanning eight distinctive retrieval tasks across seven diverse domains. We first discuss the construction of COIR and its diverse dataset composition. Further, we evaluate nine widely used retrieval models using COIR, uncovering significant difficulties in performing code retrieval tasks even with state-of-the-art systems. To facilitate easy adoption and integration within existing research workflows, COIR has been developed as a user-friendly Python framework, readily installable via pip. It shares same data schema as other popular benchmarks like MTEB and BEIR, enabling seamless cross-benchmark evaluations. Through COIR, we aim to invigorate research in the code retrieval domain, providing a versatile benchmarking tool that encourages further development and exploration of code retrieval systems. https://github.com/CoIR-team/coir.", "arxiv_url": "https://arxiv.org/abs/2407.02883", "authors": ["Xiangyang Li", "Kuicai Dong", "Yi Quan Lee", "Wei Xia", "Hao Zhang", "Xinyi Dai", "Yasheng Wang", "Ruiming Tang"], "first_author": "Xiangyang Li", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Retrieval", "tags": ["Cross-domain code retrieval", "Text-to-code and code-to-text retrieval", "Code-to-code similarity and context retrieval", "Hybrid text-and-code queries", "Manual dataset curation and filtering", "Standardized evaluation pipeline (BEIR/MTEB-compatible)", "Overfitting analysis to existing leaderboards", "Zero-shot retrieval evaluation with nDCG/MAP"], "summary": "本文提出COIR，一个包含10个经人工审校的数据集、覆盖多语言与多种检索任务并兼容BEIR/MTEB的统一评测框架，用于全面评估代码信息检索模型的泛化能力。", "quality": "High", "conference": "ACL 2025", "pdf_url": "https://arxiv.org/pdf/2407.02883v3", "published": "2024-07-03", "update_time": "2025-06-06", "download_time": "2025-12-11 17:49:00"}
{"id": "2408.11081", "title": "What can Large Language Models Capture about Code Functional Equivalence?", "abstract": "Code-LLMs, LLMs pre-trained on large code corpora, have shown great progress in learning rich representations of the structure and syntax of code, successfully using it to generate or classify code fragments. At the same time, understanding if they are able to do so because they capture code semantics, and how well, is still an open question. In this paper, we tackle this problem by introducing SeqCoBench, a benchmark for systematically assessing how Code-LLMs can capture code functional equivalence. SeqCoBench contains over 20 code transformations that either preserve or alter the semantics of Python programs. We conduct extensive evaluations in different settings, including zero-shot and parameter-efficient finetuning methods on state-of-the-art (Code)-LLMs to see if they can discern semantically equivalent or different pairs of programs in SeqCoBench. We find that the performance gap between these LLMs and classical match-based retrieval scores is minimal, with both approaches showing a concerning lack of depth in understanding code semantics.", "arxiv_url": "https://arxiv.org/abs/2408.11081", "authors": ["Nickil Maveli", "Antonio Vergari", "Shay B. Cohen"], "first_author": "Nickil Maveli", "category": ["Benchmark", "Empirical"], "field": "Maintenance", "task": "Clone Detection", "tags": ["Semantic code clones", "Functional equivalence classification", "Semantic-preserving transformations", "Semantic-altering transformations", "Pairwise program comparison", "Parameter-efficient fine-tuning evaluation", "Comparison to match-based similarity metrics", "Python program transformation suite"], "summary": "本文构建了SeqCoBench基准，包含20余种对Python程序进行语义保持或改变的变换，用以系统评估代码LLM在判别函数功能等价性方面的能力，并发现这些模型在该任务上与传统匹配度量表现相近且整体理解深度不足。", "quality": "High", "conference": "NAACL 2025", "pdf_url": "https://arxiv.org/pdf/2408.11081v2", "published": "2024-08-20", "update_time": "2025-02-12", "download_time": "2025-12-11 17:49:40"}
{"id": "2506.11066", "title": "CoQuIR: A Comprehensive Benchmark for Code Quality-Aware Information Retrieval", "abstract": "Code retrieval is essential in modern software development, as it boosts code reuse and accelerates debugging. However, current benchmarks primarily emphasize functional relevance while neglecting critical dimensions of software quality. Motivated by this gap, we introduce CoQuIR, the first large-scale, multilingual benchmark specifically designed to evaluate quality-aware code retrieval across four key dimensions: correctness, efficiency, security, and maintainability. CoQuIR provides fine-grained quality annotations for 42,725 queries and 134,907 code snippets in 11 programming languages, and is accompanied by two quality-centric evaluation metrics: Pairwise Preference Accuracy and Margin-based Ranking Score. Using CoQuIR, we benchmark 23 retrieval models, covering both open-source and proprietary systems, and find that even top-performing models frequently fail to distinguish buggy or insecure code from their more robust counterparts. Furthermore, we conduct preliminary investigations into training methods that explicitly encourage retrievers to recognize code quality. Using synthetic datasets, we demonstrate promising improvements in quality-aware metrics across various models, without sacrificing semantic relevance. Downstream code generation experiments further validate the effectiveness of our approach. Overall, our work highlights the importance of integrating quality signals into code retrieval systems, laying the groundwork for more trustworthy and robust software development tools.", "arxiv_url": "https://arxiv.org/abs/2506.11066", "authors": ["Jiahui Geng", "Fengyu Cai", "Shaobo Cui", "Qing Li", "Liangwei Chen", "Chenyang Lyu", "Haonan Li", "Derui Zhu", "Walter Pretschner", "Heinz Koeppl", "Fakhri Karray"], "first_author": "Jiahui Geng", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Retrieval", "tags": ["Quality-aware retrieval", "Contrastive training for code quality", "Quality annotations: correctness, efficiency, security, maintainability", "Pairwise Preference Accuracy", "Margin-based Ranking Score", "Multilingual code corpus", "Contrastive code pairs and hard negatives", "Downstream retrieval-augmented generation evaluation"], "summary": "本文提出CoQuIR，一个面向正确性、效率、安全性和可维护性四个代码质量维度的大规模多语言代码检索基准，包含质量标注、质量感知评估指标，并通过对比训练提高检索器的质量识别能力，从而改善检索和下游生成的安全与可靠性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2506.11066v2", "published": "2025-05-31", "update_time": "2025-08-27", "download_time": "2025-12-11 17:50:17"}
{"id": "1912.03768", "title": "TypeWriter: Neural Type Prediction with Search-based Validation", "abstract": "Maintaining large code bases written in dynamically typed languages, such as JavaScript or Python, can be challenging due to the absence of type annotations: simple data compatibility errors proliferate, IDE support is limited, and APIs are hard to comprehend. Recent work attempts to address those issues through either static type inference or probabilistic type prediction. Unfortunately, static type inference for dynamic languages is inherently limited, while probabilistic approaches suffer from imprecision. This paper presents TypeWriter, the first combination of probabilistic type prediction with search-based refinement of predicted types. TypeWriter's predictor learns to infer the return and argument types for functions from partially annotated code bases by combining the natural language properties of code with programming language-level information. To validate predicted types, TypeWriter invokes a gradual type checker with different combinations of the predicted types, while navigating the space of possible type combinations in a feedback-directed manner. We implement the TypeWriter approach for Python and evaluate it on two code corpora: a multi-million line code base at Facebook and a collection of 1,137 popular open-source projects. We show that TypeWriter's type predictor achieves an F1 score of 0.64 (0.79) in the top-1 (top-5) predictions for return types, and 0.57 (0.80) for argument types, which clearly outperforms prior type prediction models. By combining predictions with search-based validation, TypeWriter can fully annotate between 14% to 44% of the files in a randomly selected corpus, while ensuring type correctness. A comparison with a static type inference tool shows that TypeWriter adds many more non-trivial types. TypeWriter currently suggests types to developers at Facebook and several thousands of types have already been accepted with minimal changes.", "arxiv_url": "https://arxiv.org/abs/1912.03768", "authors": ["Michael Pradel", "Georgios Gousios", "Jason Liu", "Satish Chandra"], "first_author": "Michael Pradel", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Neural type prediction", "Feedback-directed search", "Gradual type checker validation", "Function-level argument and return typing", "Identifier and docstring context", "Usage-token sequence embeddings", "Combinatorial search for consistent annotations", "Automated type annotation for Python"], "summary": "本文提出 TypeWriter：通过结合基于神经网络的函数参数/返回类型预测与基于反馈的搜索验证（使用渐进式类型检查器）自动为 Python 代码添加类型注解，并在大规模代码库上验证了其实用性与效果。", "quality": "High", "conference": "ICSE 2019", "pdf_url": "https://arxiv.org/pdf/1912.03768v2", "published": "2019-12-08", "update_time": "2020-03-06", "download_time": "2025-12-11 17:51:07"}
{"id": "2104.04706", "title": "ManyTypes4Py: A Benchmark Python Dataset for Machine Learning-based Type Inference", "abstract": "In this paper, we present ManyTypes4Py, a large Python dataset for machine learning (ML)-based type inference. The dataset contains a total of 5,382 Python projects with more than 869K type annotations. Duplicate source code files were removed to eliminate the negative effect of the duplication bias. To facilitate training and evaluation of ML models, the dataset was split into training, validation and test sets by files. To extract type information from abstract syntax trees (ASTs), a lightweight static analyzer pipeline is developed and accompanied with the dataset. Using this pipeline, the collected Python projects were analyzed and the results of the AST analysis were stored in JSON-formatted files. The ManyTypes4Py dataset is shared on zenodo and its tools are publicly available on GitHub.", "arxiv_url": "https://arxiv.org/abs/2104.04706", "authors": ["Amir M. Mir", "Evaldas Latoskinas", "Georgios Gousios"], "first_author": "Amir M. Mir", "category": ["Benchmark"], "field": "Type Inference", "task": "ML-based Type Inference", "tags": ["AST-based feature extraction", "Lightweight static analysis pipeline", "File-level deduplication (CD4Py)", "Seq2seq code representation with token-type alignment", "Natural and contextual type hints (ret_exprs, params_occur)", "Train/validation/test split by files", "Long-tail type distribution analysis", "mypy-based project selection"], "summary": "本文提出ManyTypes4Py——一个包含5,382个Python项目、约869,825条类型注解并附带轻量静态分析工具（LibSA4Py）与去重处理的用于机器学习类型推断的大型基准数据集。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2104.04706v1", "published": "2021-04-10", "update_time": "2021-04-10", "download_time": "2025-12-11 17:52:01"}
{"id": "2302.12163", "title": "Do Machine Learning Models Produce TypeScript Types That Type Check?", "abstract": "Type migration is the process of adding types to untyped code to gain assurance at compile time. TypeScript and other gradual type systems facilitate type migration by allowing programmers to start with imprecise types and gradually strengthen them. However, adding types is a manual effort and several migrations on large, industry codebases have been reported to have taken several years. In the research community, there has been significant interest in using machine learning to automate TypeScript type migration. Existing machine learning models report a high degree of accuracy in predicting individual TypeScript type annotations. However, in this paper we argue that accuracy can be misleading, and we should address a different question: can an automatic type migration tool produce code that passes the TypeScript type checker?   We present TypeWeaver, a TypeScript type migration tool that can be used with an arbitrary type prediction model. We evaluate TypeWeaver with three models from the literature: DeepTyper, a recurrent neural network; LambdaNet, a graph neural network; and InCoder, a general-purpose, multi-language transformer that supports fill-in-the-middle tasks. Our tool automates several steps that are necessary for using a type prediction model, (1) importing types for a project's dependencies; (2) migrating JavaScript modules to TypeScript notation; (3) inserting predicted type annotations into the program to produce TypeScript when needed; and (4) rejecting non-type predictions when needed.   We evaluate TypeWeaver on a dataset of 513 JavaScript packages, including packages that have never been typed before. With the best type prediction model, we find that only 21% of packages type check, but more encouragingly, 69% of files type check successfully.", "arxiv_url": "https://arxiv.org/abs/2302.12163", "authors": ["Ming-Ho Yee", "Arjun Guha"], "first_author": "Ming-Ho Yee", "category": ["Technical", "Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Type migration automation", "Type annotation weaving", "Type checking validation", "CommonJS to ECMAScript module conversion", "Dependency .d.ts importing", "Filtering non-type model outputs", "Empirical evaluation on npm packages", "Analysis of trivial vs. meaningful annotations"], "summary": "本文提出 TypeWeaver 工具并在 513 个 npm 包上评估多种机器学习类型预测模型，展示了在实际迁移中需要处理依赖类型导入、模块系统转换和非类型预测过滤等工程步骤，且尽管单注解准确率高，只有 21% 的包（69% 的文件）最终通过 TypeScript 类型检查。", "quality": "High", "conference": "European Conference on Object-Oriented Programming (ECOOP) 2023", "pdf_url": "https://arxiv.org/pdf/2302.12163v2", "published": "2023-02-23", "update_time": "2023-07-11", "download_time": "2025-12-11 17:52:33"}
{"id": "2303.09564", "title": "TypeT5: Seq2seq Type Inference using Static Analysis", "abstract": "There has been growing interest in automatically predicting missing type annotations in programs written in Python and JavaScript. While prior methods have achieved impressive accuracy when predicting the most common types, they often perform poorly on rare or complex types. In this paper, we present a new type inference method that treats type prediction as a code infilling task by leveraging CodeT5, a state-of-the-art seq2seq pre-trained language model for code. Our method uses static analysis to construct dynamic contexts for each code element whose type signature is to be predicted by the model. We also propose an iterative decoding scheme that incorporates previous type predictions in the model's input context, allowing information exchange between related code elements. Our evaluation shows that the proposed approach, TypeT5, not only achieves a higher overall accuracy (particularly on rare and complex types) but also produces more coherent results with fewer type errors -- while enabling easy user intervention.", "arxiv_url": "https://arxiv.org/abs/2303.09564", "authors": ["Jiayi Wei", "Greg Durrett", "Isil Dillig"], "first_author": "Jiayi Wei", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Type Inference for Python", "Static usage graph", "Context augmentation via static analysis", "Seq2seq code infilling", "Iterative two-pass decoding", "Coherence via type-checking constraints", "Interactive human-in-the-loop correction", "Handling parametric and user-defined types"], "summary": "本文提出TypeT5，将类型推断视为seq2seq代码填空任务，结合静态分析构造跨文件使用关系上下文并采用迭代解码以提高对罕见与复杂类型的一致性和准确率。", "quality": "High", "conference": "ICLR 2023", "pdf_url": "https://arxiv.org/pdf/2303.09564v1", "published": "2023-03-16", "update_time": "2023-03-16", "download_time": "2025-12-11 17:53:05"}
{"id": "2305.17145", "title": "Type Prediction With Program Decomposition and Fill-in-the-Type Training", "abstract": "TypeScript and Python are two programming languages that support optional type annotations, which are useful but tedious to introduce and maintain. This has motivated automated type prediction: given an untyped program, produce a well-typed output program. Large language models (LLMs) are promising for type prediction, but there are challenges: fill-in-the-middle performs poorly, programs may not fit into the context window, generated types may not type check, and it is difficult to measure how well-typed the output program is. We address these challenges by building OpenTau, a search-based approach for type prediction that leverages large language models. We propose a new metric for type prediction quality, give a tree-based program decomposition that searches a space of generated types, and present fill-in-the-type fine-tuning for LLMs. We evaluate our work with a new dataset for TypeScript type prediction, and show that 47.4% of files type check (14.5% absolute improvement) with an overall rate of 3.3 type errors per file. All code, data, and models are available at: https://github.com/GammaTauAI/opentau.", "arxiv_url": "https://arxiv.org/abs/2305.17145", "authors": ["Federico Cassano", "Ming-Ho Yee", "Noah Shinn", "Arjun Guha", "Steven Holtzen"], "first_author": "Federico Cassano", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Tree-based program decomposition", "Fill-in-the-type fine-tuning", "Typedness evaluation metric", "Search over type candidates", "Type-checking guided ranking", "Local type inference", "TypeScript gradual migration"], "summary": "本文提出OPENTAU，通过将程序递归分解为树状代码块、基于搜索的候选类型生成与一种名为fill-in-the-type的微调方法，并辅以新的typedness评估和TypeScript数据集，有效提升了自动类型注入的可检类型率与类型精确度。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2305.17145v1", "published": "2023-05-25", "update_time": "2023-05-25", "download_time": "2025-12-11 17:53:38"}
{"id": "2408.10718", "title": "CodeJudge-Eval: Can Large Language Models be Good Judges in Code Understanding?", "abstract": "Recent advancements in large language models (LLMs) have showcased impressive code generation capabilities, primarily evaluated through language-to-code benchmarks. However, these benchmarks may not fully capture a model's code understanding abilities. We introduce CodeJudge-Eval (CJ-Eval), a novel benchmark designed to assess LLMs' code understanding abilities from the perspective of code judging rather than code generation. CJ-Eval challenges models to determine the correctness of provided code solutions, encompassing various error types and compilation issues. By leveraging a diverse set of problems and a fine-grained judging system, CJ-Eval addresses the limitations of traditional benchmarks, including the potential memorization of solutions. Evaluation of 12 well-known LLMs on CJ-Eval reveals that even state-of-the-art models struggle, highlighting the benchmark's ability to probe deeper into models' code understanding abilities. Our codes and benchmark are available at \\url{https://github.com/CodeLLM-Research/CodeJudge-Eval}.", "arxiv_url": "https://arxiv.org/abs/2408.10718", "authors": ["Yuwei Zhao", "Ziyang Luo", "Yuchen Tian", "Hongzhan Lin", "Weixiang Yan", "Annan Li", "Jing Ma"], "first_author": "Yuwei Zhao", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["LLM-as-a-judge", "Code understanding evaluation", "Fine-grained verdict taxonomy", "Execution-grounded annotations", "Programming-contest problems", "Cross-model candidate solutions", "Memorization mitigation", "Multiple-choice judging"], "summary": "本文提出了CodeJudge-Eval基准，通过让模型对来自不同模型的候选代码进行细粒度（如AC/WA/TLE等）判定以评估代码理解能力，并在竞赛级题目上对多种模型的判题性能进行了实证分析，结果表明当前模型在判题任务上仍存在显著不足。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2408.10718v2", "published": "2024-08-20", "update_time": "2024-09-13", "download_time": "2025-12-11 17:54:13"}
{"id": "2512.09679", "title": "Understanding Chain-of-Thought Effectiveness in Code Generation: An Empirical and Information-Theoretic Analysis", "abstract": "Large language models (LLMs) achieve strong performance on code generation, but the mechanisms by which Chain-of-Thought (CoT) prompting helps remain unclear. We present a systematic empirical and information-theoretic study of CoT effectiveness in neural code generation, evaluating five paradigms (Zero-Shot, Zero-Shot CoT, Self-Planning, Structured CoT, Reasoning-CoT) across six Python benchmarks, a multilingual benchmark with 12 programming languages, and six models from 7B to 480B parameters, using conditional mutual information $I(Y;C|X)$ as a conceptual lens. Our results show that externally guided CoT consistently outperforms direct generation, with structured methods improving Pass@1 by 5--12\\% on average while using substantially fewer tokens than reflective reasoning, and that CoT benefits depend on language type systems and model capacity. We further find that reasoning \\emph{quality} is critical: high-quality structured CoT from strong generators yields significantly higher accuracy than lightweight alternatives with the same template, whereas naive Zero-Shot CoT can even degrade performance. These findings provide practical guidance for choosing CoT strategies based on model capacity, language characteristics, and task complexity.", "arxiv_url": "https://arxiv.org/abs/2512.09679", "authors": ["Naizhu Jin", "Zhong Li", "Guang Yang", "Tian Zhang", "Qingkai Zeng"], "first_author": "Naizhu Jin", "category": ["Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Chain-of-Thought prompting", "Structured CoT (SCoT)", "Self-planning prompts", "Reflective reasoning", "Conditional mutual information", "Information density metric", "Cross-language generalization", "Model scale / capacity effects", "Token-efficiency vs accuracy", "Reasoning quality evaluation"], "summary": "本文提出基于条件互信息的信息论框架，并通过跨语言、跨模型的大规模实证实验系统地评估多种 Chain-of-Thought 提示范式在代码生成中对准确性与效率的影响，发现结构化 CoT 在令牌效率和稳定性上优于反思式方法且推理质量与模型容量显著影响效果。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.09679v1", "published": "2025-12-10", "update_time": "2025-12-10", "download_time": "2025-12-12 01:51:32"}
{"id": "2512.09627", "title": "LogICL: Distilling LLM Reasoning to Bridge the Semantic Gap in Cross-Domain Log Anomaly Detection", "abstract": "Effective log anomaly detection is critical to sustaining reliability in large-scale IT infrastructures. Transformer-based models require substantial resources and labeled data, exacerbating the cold-start problem in target domains where logs are scarce. Existing cross-domain methods leverage source logs but struggle with generalization due to reliance on surface lexical similarity, failing to capture latent semantic equivalence amid structural divergences. To address this, we propose LogICL, a framework distilling Large Language Model (LLM) reasoning into a lightweight encoder for cross-domain anomaly detection. During training, LogICL constructs a delta matrix measuring the utility of demonstrations selected via Maximal Marginal Relevance relative to zero-shot inference. The encoder is optimized via a multi-objective loss comprising an ICL-Guided term that aligns representations based on reasoning assistance utility, maximum mean discrepancy for domain alignment, and supervised contrastive loss. At inference, the optimized encoder retrieves reasoning-aware demonstrations using semantic similarity and delta scores, enabling frozen-LLM in-context learning with Chain-of-Thought for accurate and interpretable detection. Experiments on few-shot and zero-shot cross-domain benchmarks confirm LogICL achieves state-of-the-art performance across heterogeneous systems. Further analysis via visualizations and case studies confirms LogICL bridges the semantic gap beyond surface lexical similarity, effectively capturing latent semantic equivalence for rapid deployment.", "arxiv_url": "https://arxiv.org/abs/2512.09627", "authors": ["Jingwei Ye", "Zhi Wang", "Chenbin Su", "Jieshuai Yang", "Jiayi Ding", "Chunbo Liu", "Ge Chu"], "first_author": "Jingwei Ye", "category": ["Technical"], "field": "AIOps", "task": "Log Anomaly Detection", "tags": ["In-Context Learning Distillation", "Delta Matrix for Demonstration Utility", "Maximal Marginal Relevance Demonstration Selection", "ICL-guided Contrastive Representation Learning", "Maximum Mean Discrepancy Domain Alignment", "Supervised Contrastive Loss for Anomaly Discrimination", "Reasoning-aware Demonstration Retrieval", "Frozen-LLM Chain-of-Thought Inference", "Lightweight Log Sequence Encoder", "Cross-domain Few-shot/Zero-shot Adaptation"], "summary": "本文提出LogICL，通过将大模型的推理能力蒸馏到轻量级日志编码器并结合基于ICL的示例选择、delta矩阵衡量、MMD域对齐和对比损失，实现对异构系统的跨域少样本/零样本日志异常检测与可解释推理。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.09627v1", "published": "2025-12-10", "update_time": "2025-12-10", "download_time": "2025-12-12 01:52:03"}
{"id": "2512.09543", "title": "SWEnergy: An Empirical Study on Energy Efficiency in Agentic Issue Resolution Frameworks with SLMs", "abstract": "Context. LLM-based autonomous agents in software engineering rely on large, proprietary models, limiting local deployment. This has spurred interest in Small Language Models (SLMs), but their practical effectiveness and efficiency within complex agentic frameworks for automated issue resolution remain poorly understood.   Goal. We investigate the performance, energy efficiency, and resource consumption of four leading agentic issue resolution frameworks when deliberately constrained to using SLMs. We aim to assess the viability of these systems for this task in resource-limited settings and characterize the resulting trade-offs.   Method. We conduct a controlled evaluation of four leading agentic frameworks (SWE-Agent, OpenHands, Mini SWE Agent, AutoCodeRover) using two SLMs (Gemma-3 4B, Qwen-3 1.7B) on the SWE-bench Verified Mini benchmark. On fixed hardware, we measure energy, duration, token usage, and memory over 150 runs per configuration.   Results. We find that framework architecture is the primary driver of energy consumption. The most energy-intensive framework, AutoCodeRover (Gemma), consumed 9.4x more energy on average than the least energy-intensive, OpenHands (Gemma). However, this energy is largely wasted. Task resolution rates were near-zero, demonstrating that current frameworks, when paired with SLMs, consume significant energy on unproductive reasoning loops. The SLM's limited reasoning was the bottleneck for success, but the framework's design was the bottleneck for efficiency.   Conclusions. Current agentic frameworks, designed for powerful LLMs, fail to operate efficiently with SLMs. We find that framework architecture is the primary driver of energy consumption, but this energy is largely wasted due to the SLMs' limited reasoning. Viable low-energy solutions require shifting from passive orchestration to architectures that actively manage SLM weaknesses.", "arxiv_url": "https://arxiv.org/abs/2512.09543", "authors": ["Arihant Tripathy", "Ch Pavan Harshit", "Karthik Vaidhyanathan"], "first_author": "Arihant Tripathy", "category": ["Empirical"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Energy-aware agentic frameworks", "Hardware-level CPU/GPU energy profiling", "Small-model multi-turn reasoning limitations", "Framework architectural trade-offs", "Resource-constrained autonomous bug repair", "Wasted inference energy from reasoning loops", "Reproducible energy measurement methodology", "Failure-mode analysis for agentic workflows"], "summary": "本文在固定硬件上用两种小型语言模型对四种代理式问题修复框架进行大规模能耗与性能评估，发现框架架构决定能耗且大部分能耗因SLM推理能力受限导致无效循环被浪费，表明需设计主动管理SLM弱点的新型架构以实现低能耗可行性。", "quality": "High", "conference": "AGENT (ICSE 2026 workshop) 2026", "pdf_url": "https://arxiv.org/pdf/2512.09543v1", "published": "2025-12-10", "update_time": "2025-12-10", "download_time": "2025-12-12 01:58:17"}
{"id": "2512.09108", "title": "Evolving Excellence: Automated Optimization of LLM-based Agents", "abstract": "Agentic AI systems built on large language models (LLMs) offer significant potential for automating complex workflows, from software development to customer support. However, LLM agents often underperform due to suboptimal configurations; poorly tuned prompts, tool descriptions, and parameters that typically require weeks of manual refinement. Existing optimization methods either are too complex for general use or treat components in isolation, missing critical interdependencies.   We present ARTEMIS, a no-code evolutionary optimization platform that jointly optimizes agent configurations through semantically-aware genetic operators. Given only a benchmark script and natural language goals, ARTEMIS automatically discovers configurable components, extracts performance signals from execution logs, and evolves configurations without requiring architectural modifications.   We evaluate ARTEMIS on four representative agent systems: the \\emph{ALE Agent} for competitive programming on AtCoder Heuristic Contest, achieving a \\textbf{$13.6\\%$ improvement} in acceptance rate; the \\emph{Mini-SWE Agent} for code optimization on SWE-Perf, with a statistically significant \\textbf{10.1\\% performance gain}; and the \\emph{CrewAI Agent} for cost and mathematical reasoning on Math Odyssey, achieving a statistically significant \\textbf{$36.9\\%$ reduction} in the number of tokens required for evaluation. We also evaluate the \\emph{MathTales-Teacher Agent} powered by a smaller open-source model (Qwen2.5-7B) on GSM8K primary-level mathematics problems, achieving a \\textbf{22\\% accuracy improvement} and demonstrating that ARTEMIS can optimize agents based on both commercial and local models.", "arxiv_url": "https://arxiv.org/abs/2512.09108", "authors": ["Paul Brookes", "Vardan Voskanyan", "Rafail Giavrimis", "Matthew Truscott", "Mina Ilieva", "Chrystalla Pavlou", "Alexandru Staicu", "Manal Adham", "Will Evers- Hood", "Jingzhi Gong", "Kejia Zhang", "Matvey Fedoseev", "Vishal Sharma", "Roman Bauer", "Zheng Wang", "Hema Nair", "Wei Jie", "Tianhua Xu", "Aurora Constantin", "Leslie Kanthan", "Michail Basios"], "first_author": "Paul Brookes", "category": ["Technical"], "field": "LLM Agents & Deployment", "task": "Agent Configuration Optimization", "tags": ["Black-box agent tuning", "Semantically-aware mutation and crossover", "No-code evolutionary platform", "Execution-log based fitness", "LLM-ensemble guided edits", "Joint textual and parametric optimization", "Prompt + tool description co-optimization", "Cost vs. performance trade-off analysis"], "summary": "本文提出Artemis——一个无代码的基于进化算法的代理配置自动优化平台，通过语义感知的变异与交叉操作并利用执行日志与基准反馈，自动联合调优提示、工具描述和参数，从而显著提升不同LLM代理在多种任务上的准确性与效率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.09108v1", "published": "2025-12-09", "update_time": "2025-12-09", "download_time": "2025-12-12 02:05:43"}
{"id": "2501.03447", "title": "CoReQA: Uncovering Potentials of Language Models in Code Repository Question Answering", "abstract": "Large language models that enhance software development tasks, such as code generation, code completion, and code question answering (QA), have been extensively studied in both academia and the industry. The models are integrated into popular intelligent IDEs like JetBrains and Cursor. Current benchmarks for evaluating models' code comprehension capabilities primarily focus on code generation or completion, often neglecting QA, which is a crucial aspect of understanding code. Existing code QA benchmarks are derived from code comments with predefined patterns (e.g., CodeQA) or focus on specific domains, such as education (e.g., CS1QA). These benchmarks fail to capture the real-world complexity of software engineering and user requirements for understanding code repositories. To address this gap, we introduce CoReQA, a benchmark for Code Repository-level question answering, constructed from GitHub issues and comments from 176 popular repositories across four programming languages. Since questions and answers may include both natural language and code snippets, traditional evaluation metrics such as BLEU are inadequate for assessing repository-level QA performance. Thus, we provide an LLM-as-a-judge framework to evaluate QA performance from five aspects. Based on CoReQA, we evaluate the performance of three baselines, including two short-context models using generic retrieval strategies and one long-context model that utilizes the entire repository context. Evaluation results show that state-of-the-art proprietary and long-context models struggle to address repository-level questions effectively. Our analysis highlights the limitations of language models in assisting developers in understanding repositories and suggests future directions for improving repository comprehension systems through effective context retrieval methodologies.", "arxiv_url": "https://arxiv.org/abs/2501.03447", "authors": ["Jialiang Chen", "Kaifa Zhao", "Jie Liu", "Chao Peng", "Jierui Liu", "Hang Zhu", "Pengfei Gao", "Ping Yang", "Shuiguang Deng"], "first_author": "Jialiang Chen", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Retrieval", "tags": ["Repository-level question answering", "GitHub issue-based QA pairs", "Automated QA generation from issue threads", "LLM-as-a-judge evaluation", "Retrieval-augmented generation (RAG)", "Long-context model evaluation", "Cross-file code comprehension", "Context retrieval strategies"], "summary": "本文构建了CoReQA——一个基于 GitHub issue 的仓库级代码问答基准，并提出自动化构建与 LLM-as-a-judge 评估流程，用以评测短/长上下文模型在跨文件检索与仓库理解上的能力与局限。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2501.03447v1", "published": "2025-01-07", "update_time": "2025-01-07", "download_time": "2025-12-12 21:32:08"}
{"id": "2502.12466", "title": "EquiBench: Benchmarking Large Language Models' Reasoning about Program Semantics via Equivalence Checking", "abstract": "As large language models (LLMs) become integral to code-related tasks, a central question emerges: Do LLMs truly understand program semantics? We introduce EquiBench, a new benchmark for evaluating LLMs through equivalence checking, i.e., determining whether two programs produce identical outputs for all possible inputs. Unlike prior code generation benchmarks, this task directly tests a model's ability to reason about program semantics. EquiBench consists of 2400 program pairs across four languages and six categories. These pairs are generated through program analysis, compiler scheduling, and superoptimization, ensuring high-confidence labels, nontrivial difficulty, and full automation. We evaluate 19 state-of-the-art LLMs and find that in the most challenging categories, the best accuracies are 63.8% and 76.2%, only modestly above the 50% random baseline. Further analysis reveals that models often rely on syntactic similarity rather than exhibiting robust reasoning about program semantics, highlighting current limitations. Our code and dataset are publicly available at https://github.com/Anjiang-Wei/equibench", "arxiv_url": "https://arxiv.org/abs/2502.12466", "authors": ["Anjiang Wei", "Jiannan Cao", "Ran Li", "Hongyu Chen", "Yuhui Zhang", "Ziheng Wang", "Yuan Liu", "Thiago S. F. X. Teixeira", "Diyi Yang", "Ke Wang", "Alex Aiken"], "first_author": "Anjiang Wei", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Program Equivalence", "Equivalence-checking Benchmark", "Dead Code Elimination", "Superoptimization-generated Assembly Pairs", "Compiler Scheduling Transformations", "CUDA tensor scheduling with FP tolerance", "Algorithmic/variable-renaming transformations (OJ)", "Automated transformation and labeling pipeline", "Syntactic-similarity bias analysis", "Evaluation of prompting (few-shot, CoT) on semantic reasoning"], "summary": "本文提出 EquiBench：一个包含2400对跨四种语言与六类等价/不等价程序对的自动构造基准，用以衡量大模型对程序语义（等价性判定）的推理能力，并通过对19个模型的评估揭示模型常依赖句法相似性而非稳健语义推理，且在最难类别上性能接近随机基线。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2502.12466v3", "published": "2025-02-18", "update_time": "2025-09-19", "download_time": "2025-12-12 21:32:49"}
{"id": "2503.04359", "title": "LONGCODEU: Benchmarking Long-Context Language Models on Long Code Understanding", "abstract": "Current advanced long-context language models offer great potential for real-world software engineering applications. However, progress in this critical domain remains hampered by a fundamental limitation: the absence of a rigorous evaluation framework for long code understanding. To gap this obstacle, we propose a long code understanding benchmark LONGCODEU from four aspects (8 tasks) to evaluate LCLMs' long code understanding ability required for practical applications, including code unit perception, intra-code unit understanding, inter-code unit relation understanding, and long code documentation understanding. We evaluate 9 popular LCLMs on LONGCODEU (i.e., 6 general models and 3 code models). Our experimental results reveal key limitations in current LCLMs' capabilities for long code understanding. Particularly, the performance of LCLMs drops dramatically when the long code length is greater than 32K, falling far short of their claimed 128K-1M context windows. In the four aspects, inter-code unit relation understanding is the most challenging for LCLMs. Our study provides valuable insights for optimizing LCLMs and driving advancements in software engineering.", "arxiv_url": "https://arxiv.org/abs/2503.04359", "authors": ["Jia Li", "Xuyuan Guo", "Lei Li", "Kechi Zhang", "Ge Li", "Jia Li", "Zhengwei Tao", "Fang Liu", "Chongyang Tao", "Yuqi Zhu", "Zhi Jin"], "first_author": "Jia Li", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Understanding", "tags": ["Long-context evaluation", "Repository-level code understanding", "Inter-code-unit relation reasoning", "Function (code unit) identification", "Long documentation comprehension", "Temporal filtering to reduce data contamination", "Context-length stress testing (up to 128K tokens)"], "summary": "该论文提出了LONGCODEU基准，收集真实仓库中的超长代码并从代码单元感知、单位内理解、单位间关系理解和长文档理解四个方面设计8项任务，对多款长上下文模型进行评测以揭示其在超32K上下文长度下的性能瓶颈。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2503.04359v1", "published": "2025-03-06", "update_time": "2025-03-06", "download_time": "2025-12-12 21:33:30"}
{"id": "2506.00750", "title": "CodeSense: a Real-World Benchmark and Dataset for Code Semantic Reasoning", "abstract": "Understanding and reasoning about code semantics is essential for enhancing code LLMs' abilities to solve real-world software engineering (SE) tasks. Although several code reasoning benchmarks exist, most rely on synthetic datasets or educational coding problems and focus on coarse-grained reasoning tasks such as input/output prediction, limiting their effectiveness in evaluating LLMs in practical SE contexts. To bridge this gap, we propose CodeSense, the first benchmark that makes available a spectrum of fine-grained code reasoning tasks concerned with the software engineering of real-world code. We collected Python, C and Java software projects from real-world repositories. We executed tests from these repositories, collected their execution traces, and constructed a ground truth dataset for fine-grained semantic reasoning tasks. We then performed comprehensive evaluations on state-of-the-art LLMs. Our results show a clear performance gap for the models to handle fine-grained reasoning tasks. Although prompting techniques such as chain-of-thought and in-context learning helped, the lack of code semantics in LLMs fundamentally limit models' capabilities of code reasoning. Besides dataset, benchmark and evaluation, our work produced an execution tracing framework and tool set that make it easy to collect ground truth for fine-grained SE reasoning tasks, offering a strong basis for future benchmark construction and model post training. Our code and data are located at https://codesense-bench.github.io/.", "arxiv_url": "https://arxiv.org/abs/2506.00750", "authors": ["Monoshi Kumar Roy", "Simin Chen", "Benjamin Steenhoek", "Jinjun Peng", "Gail Kaiser", "Baishakhi Ray", "Wei Le"], "first_author": "Monoshi Kumar Roy", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Fine-grained statement-level semantics", "Execution trace instrumentation", "Dynamic value collection", "Branch condition prediction", "Loop iteration inference", "Pointer and aliasing reasoning", "Real-world multi-language projects (Python/C/Java)", "Semantic annotation toolchain", "Benchmark construction and public leaderboard", "Evaluation of prompting strategies (CoT, few-shot, ICL)"], "summary": "CodeSense 提出并公开了基于真实 Python、C 和 Java 项目的细粒度代码语义推理基准、执行跟踪工具与带标注的数据集，并在 14 个主流大模型上进行系统评估以揭示其语义推理能力的局限性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2506.00750v2", "published": "2025-05-31", "update_time": "2025-10-02", "download_time": "2025-12-12 21:34:37"}
{"id": "2507.05269", "title": "CoRe: Benchmarking LLMs Code Reasoning Capabilities through Static Analysis Tasks", "abstract": "Large language models (LLMs) have been widely adopted across diverse domains of software engineering, such as code generation, program repair, and vulnerability detection. These applications require understanding beyond surface-level code patterns: value propagation, control flow, and interdependence between program elements. However, existing benchmarks primarily evaluate end-to-end outcomes, such as whether code is correctly repaired or generated, leaving the models' ability for program semantic reasoning underexplored. This work presents CORE, a high-quality, human-verified benchmark designed to evaluate LLMs on fundamental static analysis tasks. CORE includes 12,553 task instances spanning data dependency, control dependency, and information flow across programs written in C/C++, Java, and Python. To ensure semantic diversity and reasoning complexity, we propose a semantics-aware diverse sampling strategy that selects targets and task instances based on structural coverage and dependency depth. We evaluate 10 mainstream LLMs and show that, while they perform well at identifying dependencies, models still struggle with tasks that require deeper semantic understanding and multi-step reasoning. We further conduct qualitative analyses to uncover key challenges, such as complex control structures and backward dependency patterns, offering insights into improving LLMs' code reasoning capabilities.", "arxiv_url": "https://arxiv.org/abs/2507.05269", "authors": ["Danning Xie", "Mingwei Zheng", "Xuwei Liu", "Jiannan Wang", "Chengpeng Wang", "Lin Tan", "Xiangyu Zhang"], "first_author": "Danning Xie", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Static analysis benchmarking", "Data-dependency tasks", "Control-dependency tasks", "Information-flow tasks", "Semantics-aware diverse sampling", "Human-verified semi-automated annotation", "Multilingual (C/C++, Java, Python)", "Trace generation and source enumeration", "Dependency depth and structural coverage", "Failure modes: complex control and backward dependencies"], "summary": "本文提出CORE基准——包含12,553条经人工验证的多语言静态分析任务（数据依赖、控制依赖、信息流），并通过语义感知抽样与半自动注释管线评测10个主流LLM，发现模型在深度多步语义推理上仍存在明显不足。", "quality": "High", "conference": "NeurIPS", "pdf_url": "https://arxiv.org/pdf/2507.05269v2", "published": "2025-07-03", "update_time": "2025-11-07", "download_time": "2025-12-12 21:35:32"}
{"id": "2509.14635", "title": "SWE-QA: Can Language Models Answer Repository-level Code Questions?", "abstract": "Understanding and reasoning about entire software repositories is an essential capability for intelligent software engineering tools. While existing benchmarks such as CoSQA and CodeQA have advanced the field, they predominantly focus on small, self-contained code snippets. These setups fail to capture the complexity of real-world repositories, where effective understanding and reasoning often require navigating multiple files, understanding software architecture, and grounding answers in long-range code dependencies. In this paper, we present SWE-QA, a repository-level code question answering (QA) benchmark designed to facilitate research on automated QA systems in realistic code environments. SWE-QA involves 576 high-quality question-answer pairs spanning diverse categories, including intention understanding, cross-file reasoning, and multi-hop dependency analysis. To construct SWE-QA, we first crawled 77,100 GitHub issues from 11 popular repositories. Based on an analysis of naturally occurring developer questions extracted from these issues, we developed a two-level taxonomy of repository-level questions and constructed a set of seed questions for each category. For each category, we manually curated and validated questions and collected their corresponding answers. As a prototype application, we further develop SWE-QA-Agent, an agentic framework in which LLM agents reason and act to find answers automatically. We evaluate six advanced LLMs on SWE-QA under various context augmentation strategies. Experimental results highlight the promise of LLMs, particularly our SWE-QA-Agent framework, in addressing repository-level QA, while also revealing open challenges and pointing to future research directions.", "arxiv_url": "https://arxiv.org/abs/2509.14635", "authors": ["Weihan Peng", "Yuling Shi", "Yuhang Wang", "Xinyun Zhang", "Beijun Shen", "Xiaodong Gu"], "first_author": "Weihan Peng", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Understanding", "tags": ["Repository-level QA benchmark", "Cross-file reasoning", "Multi-hop dependency analysis", "Issue-derived question taxonomy", "Seed-based question instantiation pipeline", "ReAct-style agent with tool usage", "Context augmentation (RAG) evaluation", "Rubric-guided human evaluation"], "summary": "本文提出SWE-QA——一个包含12个开源仓库共576个高质量仓库级问答对的基准，并构建SWE-QA-Agent代理与多种上下文增强策略对LLM在跨文件和多跳代码推理任务上的性能进行评估与分析。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2509.14635v1", "published": "2025-09-18", "update_time": "2025-09-18", "download_time": "2025-12-12 21:36:22"}
{"id": "2511.02778", "title": "VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation", "abstract": "Code has emerged as a precise and executable medium for reasoning and action in the agent era. Yet, progress has largely focused on language-centric tasks such as program synthesis and debugging, leaving visual-centric coding underexplored. Inspired by how humans reason over sketches, we advocate SVG code as a compact, interpretable, and executable visual representation. We introduce VCode, a benchmark that reframes multimodal understanding as code generation: given an image, a model must produce SVG that preserves symbolic meaning for downstream reasoning. VCode covers three domains - general commonsense (MM-Vet), professional disciplines (MMMU), and visual-centric perception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel evaluation protocol in which a policy model answers questions over rendered SVGs; correct answers indicate faithful symbolic preservation. Empirically, frontier VLMs struggle to generate faithful SVGs, revealing a persistent gap between language-centric and visual-centric coding. To close this gap, we introduce VCoder, an agentic framework that augments VLMs along two axes: (i) Thinking with Revision, which iteratively analyzes discrepancies and refines SVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply structured cues such as objects, shapes, and text beyond the model's intrinsic capacity. Across benchmarks, frontier VLMs with strong reasoning capabilities score well overall yet remain limited in professional knowledge and 3D reasoning. VCoder delivers a 12.3-point overall gain over the top-performing Claude-4-Opus. Human studies show that both humans and VLMs perform worse on rendered SVGs, their consistency reveals the promise of symbolic visual representation. The benchmark and code are available at https://github.com/CSU-JPG/VCode.", "arxiv_url": "https://arxiv.org/abs/2511.02778", "authors": ["Kevin Qinghong Lin", "Yuhao Zheng", "Hangyu Ran", "Dantong Zhu", "Dongxing Mao", "Linjie Li", "Philip Torr", "Alex Jinpeng Wang"], "first_author": "Kevin Qinghong Lin", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["SVG as symbolic visual representation", "Image→SVG code generation", "Render→VQA evaluation (CodeVQA)", "Iterative discrepancy-driven refinement", "Integration of perception tools (detectors/segmenters)", "Symbolic abstraction for visual reasoning", "3D/spatial relation preservation", "Human vs. model consistency study"], "summary": "本文提出VCode，一个将自然图像转换为可执行且可解释的SVG符号化视觉表示的多模态编码基准，并通过CodeVQA评估协议验证符号保真性，同时引入VCoder（迭代修订+视觉工具）以显著提升图像到SVG的生成质量。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.02778v1", "published": "2025-11-04", "update_time": "2025-11-04", "download_time": "2025-12-12 21:43:43"}
{"id": "2512.04355", "title": "Counting Without Running: Evaluating LLMs' Reasoning About Code Complexity", "abstract": "Modern GPU software stacks demand developers who can anticipate performance bottlenecks before ever launching a kernel; misjudging floating-point workloads upstream can derail tuning, scheduling, and even hardware procurement. Yet despite rapid progress in code generation, today's Large Language Models (LLMs) are rarely tested on this kind of forward-looking reasoning. We close that gap with gpuFLOPBench, a benchmark that asks models to \"count without running\" by predicting single and double-precision FLOP counts for 577 CUDA kernels drawn from HeCBench, annotated with ground-truth profiles and eight execution attributes that distinguish trivially analyzable code from kernels whose FLOPs depend on hidden compiler or runtime behavior. Evaluating current closed-source reasoning models shows clear but uneven progress: the newest LLMs achieve perfect classification on straightforward kernels but still incur multiple order-of-magnitude errors whenever implicit FLOPs arise from division, intrinsic math functions, or common subexpressions. These results surface a core limitation of existing code assistants -- the inability to internalize hardware-specific microcode effects -- and position gpuFLOPBench as a focused testbed for developing LLM tooling that can reason about performance with the same rigor as experienced GPU developers. Sources are available at our repository: https://github.com/Scientific-Computing-Lab/gpuFLOPBench", "arxiv_url": "https://arxiv.org/abs/2512.04355", "authors": ["Gregory Bolet", "Giorgis Georgakoudis", "Konstantinos Parasyris", "Harshitha Menon", "Niranjan Hasabnis", "Kirk W. Cameron", "Gal Oren"], "first_author": "Gregory Bolet", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Performance Optimization", "tags": ["Static FLOP counting", "CUDA kernel FLOP estimation", "Execution-attribute annotations", "Implicit/hidden FLOPs (division, intrinsics, templates)", "Single vs double-precision FLOP labels", "Prompting for structured performance predictions", "Hardware/microcode execution effects", "Benchmark for static performance reasoning"], "summary": "本文提出GPUFLOPBENCH数据集与评测框架，包含577个CUDA内核的单/双精度FLOP计数与执行属性标注，并评估现有封闭式推理模型在静态预测代码FLOP时的表现，揭示其在处理隐式FLOP来源（如除法、内建函数、编译器/运行时行为）时的显著失败模式。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.04355v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-12 21:46:22"}
{"id": "2208.04415", "title": "Deep Learning Driven Natural Languages Text to SQL Query Conversion: A Survey", "abstract": "With the future striving toward data-centric decision-making, seamless access to databases is of utmost importance. There is extensive research on creating an efficient text-to-sql (TEXT2SQL) model to access data from the database. Using a Natural language is one of the best interfaces that can bridge the gap between the data and results by accessing the database efficiently, especially for non-technical users. It will open the doors and create tremendous interest among users who are well versed in technical skills or not very skilled in query languages. Even if numerous deep learning-based algorithms are proposed or studied, there still is very challenging to have a generic model to solve the data query issues using natural language in a real-work scenario. The reason is the use of different datasets in different studies, which comes with its limitations and assumptions. At the same time, we do lack a thorough understanding of these proposed models and their limitations with the specific dataset it is trained on. In this paper, we try to present a holistic overview of 24 recent neural network models studied in the last couple of years, including their architectures involving convolutional neural networks, recurrent neural networks, pointer networks, reinforcement learning, generative models, etc. We also give an overview of the 11 datasets that are widely used to train the models for TEXT2SQL technologies. We also discuss the future application possibilities of TEXT2SQL technologies for seamless data queries.", "arxiv_url": "https://arxiv.org/abs/2208.04415", "authors": ["Ayush Kumar", "Parth Nagarkar", "Prabhav Nalhe", "Sanjeev Vijayakumar"], "first_author": "Ayush Kumar", "category": ["Survey"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Text-to-SQL", "Cross-domain semantic parsing", "Schema linking and grounding", "Complex SQL composition (joins, nested/nested queries)", "Conversational/contextual Text-to-SQL", "Evaluation metrics (exact match, component F1)"], "summary": "本文系统综述了近年基于深度学习的文本到SQL转换研究，比较了24种模型架构与11个常用数据集，并分析了数据集特性、评估指标及未来挑战。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2208.04415v1", "published": "2022-08-08", "update_time": "2022-08-08", "download_time": "2025-12-12 21:47:09"}
{"id": "2208.13629", "title": "A Survey on Text-to-SQL Parsing: Concepts, Methods, and Future Directions", "abstract": "Text-to-SQL parsing is an essential and challenging task. The goal of text-to-SQL parsing is to convert a natural language (NL) question to its corresponding structured query language (SQL) based on the evidences provided by relational databases. Early text-to-SQL parsing systems from the database community achieved a noticeable progress with the cost of heavy human engineering and user interactions with the systems. In recent years, deep neural networks have significantly advanced this task by neural generation models, which automatically learn a mapping function from an input NL question to an output SQL query. Subsequently, the large pre-trained language models have taken the state-of-the-art of the text-to-SQL parsing task to a new level. In this survey, we present a comprehensive review on deep learning approaches for text-to-SQL parsing. First, we introduce the text-to-SQL parsing corpora which can be categorized as single-turn and multi-turn. Second, we provide a systematical overview of pre-trained language models and existing methods for text-to-SQL parsing. Third, we present readers with the challenges faced by text-to-SQL parsing and explore some potential future directions in this field.", "arxiv_url": "https://arxiv.org/abs/2208.13629", "authors": ["Bowen Qin", "Binyuan Hui", "Lihan Wang", "Min Yang", "Jinyang Li", "Binhua Li", "Ruiying Geng", "Rongyu Cao", "Jian Sun", "Luo Si", "Fei Huang", "Yongbin Li"], "first_author": "Bowen Qin", "category": ["Survey"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Single-turn vs Multi-turn Parsing", "Schema Linking and Table-grounding", "Table-aware Pretraining Objectives", "Graph-based Schema Encoding", "Sketch-based and Grammar-guided Decoding", "AST/Tree-based SQL Generation", "Execution- and Exact-match Evaluation Metrics", "Cross-domain and Contextual Generalization"], "summary": "本文为文本到SQL解析的综合性综述，系统介绍了相关数据集、表格预训练模型、编码与解码方法，并讨论了评估指标、挑战与未来研究方向。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2208.13629v1", "published": "2022-08-29", "update_time": "2022-08-29", "download_time": "2025-12-12 21:48:20"}
{"id": "1709.00103", "title": "Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning", "abstract": "A significant amount of the world's knowledge is stored in relational databases. However, the ability for users to retrieve facts from a database is limited due to a lack of understanding of query languages such as SQL. We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries. Our model leverages the structure of SQL queries to significantly reduce the output space of generated queries. Moreover, we use rewards from in-the-loop query execution over the database to learn a policy to generate unordered parts of the query, which we show are less suitable for optimization via cross entropy loss. In addition, we will publish WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia. This dataset is required to train our model and is an order of magnitude larger than comparable datasets. By applying policy-based reinforcement learning with a query execution environment to WikiSQL, our model Seq2SQL outperforms attentional sequence to sequence models, improving execution accuracy from 35.9% to 59.4% and logical form accuracy from 23.4% to 48.3%.", "arxiv_url": "https://arxiv.org/abs/1709.00103", "authors": ["Victor Zhong", "Caiming Xiong", "Richard Socher"], "first_author": "Victor Zhong", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["NL-to-SQL generation", "Execution-based policy gradient", "Schema-aware pointer decoding", "Modeling unordered WHERE conditions", "Structured SQL decomposition (aggregation/select/where)"], "summary": "本文提出Seq2SQL，一种将自然语言问题翻译为结构化SQL查询的模型，通过将SQL解码分解为聚合、SELECT和WHERE三部分并使用基于执行结果的强化学习训练，同时发布大规模手工标注的SQL问答数据集以提升评估与训练效果。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/1709.00103v7", "published": "2017-08-31", "update_time": "2017-11-09", "download_time": "2025-12-12 21:49:12"}
{"id": "1809.08887", "title": "Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task", "abstract": "We present Spider, a large-scale, complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 college students. It consists of 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables, covering 138 different domains. We define a new complex and cross-domain semantic parsing and text-to-SQL task where different complex SQL queries and databases appear in train and test sets. In this way, the task requires the model to generalize well to both new SQL queries and new database schemas. Spider is distinct from most of the previous semantic parsing tasks because they all use a single database and the exact same programs in the train set and the test set. We experiment with various state-of-the-art models and the best model achieves only 12.4% exact matching accuracy on a database split setting. This shows that Spider presents a strong challenge for future research. Our dataset and task are publicly available at https://yale-lily.github.io/spider", "arxiv_url": "https://arxiv.org/abs/1809.08887", "authors": ["Tao Yu", "Rui Zhang", "Kai Yang", "Michihiro Yasunaga", "Dongxu Wang", "Zifan Li", "James Ma", "Irene Li", "Qingning Yao", "Shanelle Roman", "Zilin Zhang", "Dragomir Radev"], "first_author": "Tao Yu", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Cross-domain text-to-SQL", "Complex multi-table SQL (JOIN/GROUP BY/NESTED)", "Database-schema-aware semantic parsing", "Database-split generalization evaluation", "Human expert annotation and review pipeline", "Large-scale relational schema collection"], "summary": "该论文提出并公开了Spider，一个包含200个多表数据库、10,181个自然语言问题和5,693个复杂SQL的大规模人工标注跨领域文本到SQL数据集，并定义了数据库分割的泛化任务与基线评估。", "quality": "High", "conference": "EMNLP 2018", "pdf_url": "https://arxiv.org/pdf/1809.08887v5", "published": "2018-09-24", "update_time": "2019-02-02", "download_time": "2025-12-12 21:50:00"}
{"id": "2010.12773", "title": "Structure-Grounded Pretraining for Text-to-SQL", "abstract": "Learning to capture text-table alignment is essential for tasks like text-to-SQL. A model needs to correctly recognize natural language references to columns and values and to ground them in the given database schema. In this paper, we present a novel weakly supervised Structure-Grounded pretraining framework (StruG) for text-to-SQL that can effectively learn to capture text-table alignment based on a parallel text-table corpus. We identify a set of novel prediction tasks: column grounding, value grounding and column-value mapping, and leverage them to pretrain a text-table encoder. Additionally, to evaluate different methods under more realistic text-table alignment settings, we create a new evaluation set Spider-Realistic based on Spider dev set with explicit mentions of column names removed, and adopt eight existing text-to-SQL datasets for cross-database evaluation. STRUG brings significant improvement over BERT-LARGE in all settings. Compared with existing pretraining methods such as GRAPPA, STRUG achieves similar performance on Spider, and outperforms all baselines on more realistic sets. The Spider-Realistic dataset is available at https://doi.org/10.5281/zenodo.5205322.", "arxiv_url": "https://arxiv.org/abs/2010.12773", "authors": ["Xiang Deng", "Ahmed Hassan Awadallah", "Christopher Meek", "Oleksandr Polozov", "Huan Sun", "Matthew Richardson"], "first_author": "Xiang Deng", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Structure-grounded pretraining", "Text-table alignment", "Column grounding", "Value grounding", "Column-value mapping", "Weakly-supervised alignment labeling", "Schema linking for SQL generation", "Cross-database generalization"], "summary": "本文提出STRUG，一种基于并行文本-表格语料通过列对齐、值对齐和列-值映射等弱监督预训练任务来增强文本到SQL的结构对齐能力，并构建更现实的评估设置以验证其跨库泛化效果。", "quality": "High", "conference": "NAACL 2021", "pdf_url": "https://arxiv.org/pdf/2010.12773v3", "published": "2020-10-24", "update_time": "2022-08-31", "download_time": "2025-12-12 21:52:10"}
{"id": "2106.01065", "title": "Towards Robustness of Text-to-SQL Models against Synonym Substitution", "abstract": "Recently, there has been significant progress in studying neural networks to translate text descriptions into SQL queries. Despite achieving good performance on some public benchmarks, existing text-to-SQL models typically rely on the lexical matching between words in natural language (NL) questions and tokens in table schemas, which may render the models vulnerable to attacks that break the schema linking mechanism. In this work, we investigate the robustness of text-to-SQL models to synonym substitution. In particular, we introduce Spider-Syn, a human-curated dataset based on the Spider benchmark for text-to-SQL translation. NL questions in Spider-Syn are modified from Spider, by replacing their schema-related words with manually selected synonyms that reflect real-world question paraphrases. We observe that the accuracy dramatically drops by eliminating such explicit correspondence between NL questions and table schemas, even if the synonyms are not adversarially selected to conduct worst-case adversarial attacks. Finally, we present two categories of approaches to improve the model robustness. The first category of approaches utilizes additional synonym annotations for table schemas by modifying the model input, while the second category is based on adversarial training. We demonstrate that both categories of approaches significantly outperform their counterparts without the defense, and the first category of approaches are more effective.", "arxiv_url": "https://arxiv.org/abs/2106.01065", "authors": ["Yujian Gan", "Xinyun Chen", "Qiuping Huang", "Matthew Purver", "John R. Woodward", "Jinxia Xie", "Pengsheng Huang"], "first_author": "Yujian Gan", "category": ["Benchmark", "Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Synonym substitution robustness", "Schema linking vulnerability", "Human-curated paraphrase benchmark", "Schema annotation augmentation", "Adversarial training for robustness", "Cell-value paraphrasing", "Cross-domain text-to-SQL evaluation", "Input-level defense (multiple schema annotations)"], "summary": "本文构建了一个人工整理的同义替换问句基准以评估并改进 text-to-SQL 模型对模式词同义替换的鲁棒性，提出通过多重模式注释的输入修改和对抗训练两类防御方法并验证其有效性，且输入修改方法在资源消耗更低的情况下表现更好。", "quality": "High", "conference": "ACL 2021", "pdf_url": "https://arxiv.org/pdf/2106.01065v2", "published": "2021-06-02", "update_time": "2021-06-19", "download_time": "2025-12-12 21:52:49"}
{"id": "2106.05006", "title": "Text-to-SQL in the Wild: A Naturally-Occurring Dataset Based on Stack Exchange Data", "abstract": "Most available semantic parsing datasets, comprising of pairs of natural utterances and logical forms, were collected solely for the purpose of training and evaluation of natural language understanding systems. As a result, they do not contain any of the richness and variety of natural-occurring utterances, where humans ask about data they need or are curious about. In this work, we release SEDE, a dataset with 12,023 pairs of utterances and SQL queries collected from real usage on the Stack Exchange website. We show that these pairs contain a variety of real-world challenges which were rarely reflected so far in any other semantic parsing dataset, propose an evaluation metric based on comparison of partial query clauses that is more suitable for real-world queries, and conduct experiments with strong baselines, showing a large gap between the performance on SEDE compared to other common datasets.", "arxiv_url": "https://arxiv.org/abs/2106.05006", "authors": ["Moshe Hazoom", "Vibhor Malik", "Ben Bogin"], "first_author": "Moshe Hazoom", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["naturally-occurring SQL queries", "web-forum query logs", "under-specified questions", "parameterized queries", "nested subqueries", "high template diversity", "data cleaning & validation", "partial-clause evaluation (PCM-F1)", "query canonization & anonymization"], "summary": "本文发布了SEDE，一个由Stack Exchange Data Explorer真实用户生成的12,023条自然语言与对应SQL对的数据集，分析了其中的真实世界挑战（如欠定问题与参数化查询）、提出了基于部分子句匹配的评估指标PCM-F1，并展示了在该数据集上现有模型性能明显下降。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2106.05006v1", "published": "2021-06-09", "update_time": "2021-06-09", "download_time": "2025-12-12 21:53:32"}
{"id": "1806.09029", "title": "Improving Text-to-SQL Evaluation Methodology", "abstract": "To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.", "arxiv_url": "https://arxiv.org/abs/1806.09029", "authors": ["Catherine Finegan-Dollak", "Jonathan K. Kummerfeld", "Li Zhang", "Karthik Ramanathan", "Sesh Sadasivam", "Rui Zhang", "Dragomir Radev"], "first_author": "Catherine Finegan-Dollak", "category": ["Benchmark", "Empirical", "Technical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Query-based dataset split", "SQL canonicalization", "Variable anonymization analysis", "Template-based slot-filling baseline", "Dataset standardization and error fixing", "Human vs. automatically generated question complexity", "Paraphrase plus entity replacement augmentation", "Duplicate query deduplication"], "summary": "本文通过标准化并修复多个现有 text-to-SQL 数据集、引入更具挑战性的问答样本、提出基于 SQL 查询的训练/测试切分以及分析变量匿名化的影响，从而改进和规范了 text-to-SQL 的评估方法。", "quality": "High", "conference": "ACL 2018", "pdf_url": "https://arxiv.org/pdf/1806.09029v1", "published": "2018-06-23", "update_time": "2018-06-23", "download_time": "2025-12-12 21:55:04"}
{"id": "1909.05378", "title": "CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases", "abstract": "We present CoSQL, a corpus for building cross-domain, general-purpose database (DB) querying dialogue systems. It consists of 30k+ turns plus 10k+ annotated SQL queries, obtained from a Wizard-of-Oz (WOZ) collection of 3k dialogues querying 200 complex DBs spanning 138 domains. Each dialogue simulates a real-world DB query scenario with a crowd worker as a user exploring the DB and a SQL expert retrieving answers with SQL, clarifying ambiguous questions, or otherwise informing of unanswerable questions. When user questions are answerable by SQL, the expert describes the SQL and execution results to the user, hence maintaining a natural interaction flow. CoSQL introduces new challenges compared to existing task-oriented dialogue datasets:(1) the dialogue states are grounded in SQL, a domain-independent executable representation, instead of domain-specific slot-value pairs, and (2) because testing is done on unseen databases, success requires generalizing to new domains. CoSQL includes three tasks: SQL-grounded dialogue state tracking, response generation from query results, and user dialogue act prediction. We evaluate a set of strong baselines for each task and show that CoSQL presents significant challenges for future research. The dataset, baselines, and leaderboard will be released at https://yale-lily.github.io/cosql.", "arxiv_url": "https://arxiv.org/abs/1909.05378", "authors": ["Tao Yu", "Rui Zhang", "He Yang Er", "Suyi Li", "Eric Xue", "Bo Pang", "Xi Victoria Lin", "Yi Chern Tan", "Tianze Shi", "Zihan Li", "Youxuan Jiang", "Michihiro Yasunaga", "Sungrok Shim", "Tao Chen", "Alexander Fabbri", "Zifan Li", "Luyao Chen", "Yuwen Zhang", "Shreya Dixit", "Vincent Zhang", "Caiming Xiong", "Richard Socher", "Walter S Lasecki", "Dragomir Radev"], "first_author": "Tao Yu", "category": ["Benchmark"], "field": "Natural Language Interfaces to Databases", "task": "Conversational Text-to-SQL (SQL-grounded dialogue state tracking, response generation from executed SQL/results, user dialogue act prediction)", "tags": ["Wizard-of-Oz data collection", "Cross-domain complex databases", "SQL-grounded dialogue state tracking", "Natural language response generation from SQL and results", "User dialogue act prediction (clarify/unanswerable)", "Execution-grounded annotations", "Unseen-database split for generalization", "Multi-turn conversational query interactions", "Baselines and public leaderboard"], "summary": "CoSQL 提出并发布了一个大规模跨域会话式文本到 SQL 语料库（含对话、SQL 标注、系统回应与对话行为标签），并为 SQL-驱动的对话状态跟踪、基于查询结果的自然语言回复生成和用户对话行为预测等任务提供基线与挑战性评估。", "quality": "High", "conference": "EMNLP 2019", "pdf_url": "https://arxiv.org/pdf/1909.05378v1", "published": "2019-09-11", "update_time": "2019-09-11", "download_time": "2025-12-12 21:58:40"}
{"id": "2109.05157", "title": "Exploring Underexplored Limitations of Cross-Domain Text-to-SQL Generalization", "abstract": "Recently, there has been significant progress in studying neural networks for translating text descriptions into SQL queries under the zero-shot cross-domain setting. Despite achieving good performance on some public benchmarks, we observe that existing text-to-SQL models do not generalize when facing domain knowledge that does not frequently appear in the training data, which may render the worse prediction performance for unseen domains. In this work, we investigate the robustness of text-to-SQL models when the questions require rarely observed domain knowledge. In particular, we define five types of domain knowledge and introduce Spider-DK (DK is the abbreviation of domain knowledge), a human-curated dataset based on the Spider benchmark for text-to-SQL translation. NL questions in Spider-DK are selected from Spider, and we modify some samples by adding domain knowledge that reflects real-world question paraphrases. We demonstrate that the prediction accuracy dramatically drops on samples that require such domain knowledge, even if the domain knowledge appears in the training set, and the model provides the correct predictions for related training samples.", "arxiv_url": "https://arxiv.org/abs/2109.05157", "authors": ["Yujian Gan", "Xinyun Chen", "Matthew Purver"], "first_author": "Yujian Gan", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Cross-domain text-to-SQL", "Domain knowledge categories", "Human-curated challenge set", "Robustness and generalization evaluation", "Omitted-column and multi-column mention handling", "Cell-value synonym substitution", "Boolean-like condition inference", "Schema-item vs SQL-structure conflict"], "summary": "本文提出了面向跨域 Text-to-SQL 的挑战性数据集，通过将领域知识归为五类（如省略列、多列推理、同义词替换、布尔类条件等）并对535条示例进行人工构造与评估，展示了现有模型在需要特定领域知识的样本上准确率显著下降，表明模型未能有效泛化训练集中出现的领域知识。", "quality": "High", "conference": "EMNLP 2021", "pdf_url": "https://arxiv.org/pdf/2109.05157v1", "published": "2021-09-11", "update_time": "2021-09-11", "download_time": "2025-12-12 21:59:48"}
{"id": "2305.03111", "title": "Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs", "abstract": "Text-to-SQL parsing, which aims at converting natural language instructions into executable SQLs, has gained increasing attention in recent years. In particular, Codex and ChatGPT have shown impressive results in this task. However, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on database schema with few rows of database contents leaving the gap between academic study and real-world applications. To mitigate this gap, we present Bird, a big benchmark for large-scale database grounded in text-to-SQL tasks, containing 12,751 pairs of text-to-SQL data and 95 databases with a total size of 33.4 GB, spanning 37 professional domains. Our emphasis on database values highlights the new challenges of dirty database contents, external knowledge between NL questions and database contents, and SQL efficiency, particularly in the context of massive databases. To solve these problems, text-to-SQL models must feature database value comprehension in addition to semantic parsing. The experimental results demonstrate the significance of database values in generating accurate text-to-SQLs for big databases. Furthermore, even the most effective text-to-SQL models, i.e. ChatGPT, only achieves 40.08% in execution accuracy, which is still far from the human result of 92.96%, proving that challenges still stand. Besides, we also provide an efficiency analysis to offer insights into generating text-to-efficient-SQLs that are beneficial to industries. We believe that BIRD will contribute to advancing real-world applications of text-to-SQL research. The leaderboard and source code are available: https://bird-bench.github.io/.", "arxiv_url": "https://arxiv.org/abs/2305.03111", "authors": ["Jinyang Li", "Binyuan Hui", "Ge Qu", "Jiaxi Yang", "Binhua Li", "Bowen Li", "Bailin Wang", "Bowen Qin", "Rongyu Cao", "Ruiying Geng", "Nan Huo", "Xuanhe Zhou", "Chenhao Ma", "Guoliang Li", "Kevin C. C. Chang", "Fei Huang", "Reynold Cheng", "Yongbin Li"], "first_author": "Jinyang Li", "category": ["Benchmark", "Empirical"], "field": "Text-to-SQL & Databases", "task": "Text-to-SQL Benchmark (Large-scale, value-grounded, efficiency-aware)", "tags": ["Large-scale database values", "Noisy / dirty value handling", "External knowledge grounding", "SQL execution efficiency", "Valid Efficiency Score (VES)", "Crowdsourced NL–SQL annotation", "Double-blind annotation workflow", "Domain-diverse databases", "Hidden test set for leakage avoidance", "FT vs ICL evaluation (T5 vs LLMs)", "Execution-based accuracy evaluation"], "summary": "该论文提出了BIRD基准，包含95个真实大规模数据库（33.4GB）与12,751对文本到SQL样本，强调数据库值的噪声处理、外部知识推理与查询效率并提出VES指标及多模型基线评测。", "quality": "High", "conference": "NeurIPS 2023", "pdf_url": "https://arxiv.org/pdf/2305.03111v3", "published": "2023-05-04", "update_time": "2023-11-15", "download_time": "2025-12-12 22:21:49"}
{"id": "2406.07860", "title": "BookSQL: A Large Scale Text-to-SQL Dataset for Accounting Domain", "abstract": "Several large-scale datasets (e.g., WikiSQL, Spider) for developing natural language interfaces to databases have recently been proposed. These datasets cover a wide breadth of domains but fall short on some essential domains, such as finance and accounting. Given that accounting databases are used worldwide, particularly by non-technical people, there is an imminent need to develop models that could help extract information from accounting databases via natural language queries. In this resource paper, we aim to fill this gap by proposing a new large-scale Text-to-SQL dataset for the accounting and financial domain: BookSQL. The dataset consists of 100k natural language queries-SQL pairs, and accounting databases of 1 million records. We experiment with and analyze existing state-of-the-art models (including GPT-4) for the Text-to-SQL task on BookSQL. We find significant performance gaps, thus pointing towards developing more focused models for this domain.", "arxiv_url": "https://arxiv.org/abs/2406.07860", "authors": ["Rahul Kumar", "Amar Raja Dibbu", "Shrutendra Harsola", "Vignesh Subrahmaniam", "Ashutosh Modi"], "first_author": "Rahul Kumar", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Accounting-domain Text-to-SQL", "Expert-curated accounting queries", "Seven-table double-entry transaction schema", "Large-scale NL→SQL pairs (100k) over 1M records", "Complex SQL constructs: nested queries, aggregations, GROUP BY, ORDER BY, DISTINCT", "Cross-business schema variability (27 businesses)", "Evaluation of domain generalization gaps for SOTA models", "Benchmark statistics and dataset release"], "summary": "本文构建并公开了面向会计与财务领域的大规模Text-to-SQL基准数据集（100k问-SQL对、1M记录、七表会计模式），并验证现有SOTA模型在该领域存在显著泛化性能缺陷。", "quality": "High", "conference": "NAACL 2024", "pdf_url": "https://arxiv.org/pdf/2406.07860v1", "published": "2024-06-12", "update_time": "2024-06-12", "download_time": "2025-12-12 22:22:52"}
{"id": "2409.02038", "title": "BEAVER: An Enterprise Benchmark for Text-to-SQL", "abstract": "Existing text-to-SQL benchmarks have largely been constructed from web tables with human-generated question-SQL pairs. LLMs typically show strong results on these benchmarks, leading to a belief that LLMs are effective at text-to-SQL tasks. However, how these results transfer to enterprise settings is unclear because tables in enterprise databases might differ substantially from web tables in structure and content. To contend with this problem, we introduce a new dataset BEAVER, the first enterprise text-to-SQL benchmark sourced from real private enterprise data warehouses. This dataset includes natural language queries and their correct SQL statements, which we collected from actual query logs. We then benchmark off-the-shelf LLMs on this dataset. LLMs perform poorly, even when augmented with standard prompt engineering and RAG techniques. We identify three main reasons for the poor performance: (1) schemas of enterprise tables are more complex than the schemas in public data, resulting in SQL-generation tasks intrinsically harder; (2) business-oriented questions are often more complex, requiring joins over multiple tables, aggregations, and nested queries; (3) public LLMs cannot train on private enterprise data warehouses that are not publicly accessible, and therefore it is difficult for the model to learn to solve (1) and (2). We believe BEAVER will facilitate future research in building text-to-SQL systems that perform better in enterprise settings.", "arxiv_url": "https://arxiv.org/abs/2409.02038", "authors": ["Peter Baile Chen", "Fabian Wenz", "Yi Zhang", "Devin Yang", "Justin Choi", "Nesime Tatbul", "Michael Cafarella", "Çağatay Demiralp", "Michael Stonebraker"], "first_author": "Peter Baile Chen", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Enterprise data warehouse schemas", "Anonymized real-user SQL query logs", "Column-to-question mapping annotation", "Table retrieval / selection for SQL generation", "Query complexity analysis (joins, aggregations, nesting)", "Retrieval-augmented generation (RAG) evaluation", "Error analysis of SQL generation (incorrect columns/values)", "Challenges from private-data domain shift"], "summary": "本文提出BEAVER——来自真实私有企业数据仓库的首个企业级Text-to-SQL基准数据集，并通过对现成模型的大规模评测与误差分析揭示了企业场景中架构复杂性、复杂业务查询与私有数据不可访问性导致的性能崩溃问题。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2409.02038v2", "published": "2024-09-03", "update_time": "2025-01-20", "download_time": "2025-12-12 22:23:35"}
{"id": "2410.11076", "title": "PRACTIQ: A Practical Conversational Text-to-SQL dataset with Ambiguous and Unanswerable Queries", "abstract": "Previous text-to-SQL datasets and systems have primarily focused on user questions with clear intentions that can be answered. However, real user questions can often be ambiguous with multiple interpretations or unanswerable due to a lack of relevant data. In this work, we construct a practical conversational text-to-SQL dataset called PRACTIQ, consisting of ambiguous and unanswerable questions inspired by real-world user questions. We first identified four categories of ambiguous questions and four categories of unanswerable questions by studying existing text-to-SQL datasets. Then, we generate conversations with four turns: the initial user question, an assistant response seeking clarification, the user's clarification, and the assistant's clarified SQL response with the natural language explanation of the execution results. For some ambiguous queries, we also directly generate helpful SQL responses, that consider multiple aspects of ambiguity, instead of requesting user clarification. To benchmark the performance on ambiguous, unanswerable, and answerable questions, we implemented large language model (LLM)-based baselines using various LLMs. Our approach involves two steps: question category classification and clarification SQL prediction. Our experiments reveal that state-of-the-art systems struggle to handle ambiguous and unanswerable questions effectively. We will release our code for data generation and experiments on GitHub.", "arxiv_url": "https://arxiv.org/abs/2410.11076", "authors": ["Mingwen Dong", "Nischal Ashok Kumar", "Yiqun Hu", "Anuj Chauhan", "Chung-Wei Hang", "Shuaichen Chang", "Lin Pan", "Wuwei Lan", "Henghui Zhu", "Jiarong Jiang", "Patrick Ng", "Zhiguo Wang"], "first_author": "Mingwen Dong", "category": ["Benchmark", "Empirical"], "field": "Natural Language Interfaces to Databases (Text-to-SQL)", "task": "Conversational Text-to-SQL with Ambiguity and Unanswerability", "tags": ["Conversational text-to-SQL", "Ambiguity taxonomy for NL queries", "Unanswerable query categorization", "Clarification question generation", "Helpful multi-SQL responses", "Database/schema modification for data generation", "Human annotation for conversation quality", "Prompt-based LLM baselines", "Question category classification", "Clarification SQL prediction"], "summary": "本文提出 PRACTIQ，一个面向对话式 text-to-SQL 的数据集，包含细化的歧义与不可回答查询类别、基于模式的生成与人工注释流程，并用提示式 LLM 基线评测显示现有模型在处理实务性歧义/不可回答问题上仍有显著不足。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2410.11076v1", "published": "2024-10-14", "update_time": "2024-10-14", "download_time": "2025-12-12 22:24:35"}
{"id": "2410.22925", "title": "BIS: NL2SQL Service Evaluation Benchmark for Business Intelligence Scenarios", "abstract": "NL2SQL (Natural Language to Structured Query Language) transformation has seen wide adoption in Business Intelligence (BI) applications in recent years. However, existing NL2SQL benchmarks are not suitable for production BI scenarios, as they are not designed for common business intelligence questions. To address this gap, we have developed a new benchmark focused on typical NL questions in industrial BI scenarios. We discuss the challenges of constructing a BI-focused benchmark and the shortcomings of existing benchmarks. Additionally, we introduce question categories in our benchmark that reflect common BI inquiries. Lastly, we propose two novel semantic similarity evaluation metrics for assessing NL2SQL capabilities in BI applications and services.", "arxiv_url": "https://arxiv.org/abs/2410.22925", "authors": ["Bora Caglayan", "Mingxue Wang", "John D. Kelleher", "Shen Fei", "Gui Tong", "Jiandong Ding", "Puchao Zhang"], "first_author": "Bora Caglayan", "category": ["Benchmark"], "field": "Natural Language Interfaces to Databases", "task": "BI-focused NL2SQL Benchmarking and Evaluation", "tags": ["Business Intelligence query patterns", "Temporal / time-series queries", "Schema irregularities and synonym mapping", "Multi-table joins and join resolution", "SQL semantic similarity metric", "Partial SQL result similarity metric", "No-code BI query interfaces"], "summary": "该论文提出了面向商业智能场景的NL2SQL基准并引入了两种用于评估查询语义相似性和结果部分相似性的评估指标，以更真实地衡量NL2SQL在BI服务中的表现。", "quality": "High", "conference": "ICSOC (International Conference on Service-Oriented Computing) 2024", "pdf_url": "https://arxiv.org/pdf/2410.22925v1", "published": "2024-10-30", "update_time": "2024-10-30", "download_time": "2025-12-12 22:25:38"}
{"id": "2411.07763", "title": "Spider 2.0: Evaluating Language Models on Real-World Enterprise Text-to-SQL Workflows", "abstract": "Real-world enterprise text-to-SQL workflows often involve complex cloud or local data across various database systems, multiple SQL queries in various dialects, and diverse operations from data transformation to analytics. We introduce Spider 2.0, an evaluation framework comprising 632 real-world text-to-SQL workflow problems derived from enterprise-level database use cases. The databases in Spider 2.0 are sourced from real data applications, often containing over 1,000 columns and stored in local or cloud database systems such as BigQuery and Snowflake. We show that solving problems in Spider 2.0 frequently requires understanding and searching through database metadata, dialect documentation, and even project-level codebases. This challenge calls for models to interact with complex SQL workflow environments, process extremely long contexts, perform intricate reasoning, and generate multiple SQL queries with diverse operations, often exceeding 100 lines, which goes far beyond traditional text-to-SQL challenges. Our evaluations indicate that based on o1-preview, our code agent framework successfully solves only 21.3% of the tasks, compared with 91.2% on Spider 1.0 and 73.0% on BIRD. Our results on Spider 2.0 show that while language models have demonstrated remarkable performance in code generation -- especially in prior text-to-SQL benchmarks -- they require significant improvement in order to achieve adequate performance for real-world enterprise usage. Progress on Spider 2.0 represents crucial steps towards developing intelligent, autonomous, code agents for real-world enterprise settings. Our code, baseline models, and data are available at https://spider2-sql.github.io", "arxiv_url": "https://arxiv.org/abs/2411.07763", "authors": ["Fangyu Lei", "Jixuan Chen", "Yuxiao Ye", "Ruisheng Cao", "Dongchan Shin", "Hongjin Su", "Zhaoqing Suo", "Hongcheng Gao", "Wenjing Hu", "Pengcheng Yin", "Victor Zhong", "Caiming Xiong", "Ruoxi Sun", "Qian Liu", "Sida Wang", "Tao Yu"], "first_author": "Fangyu Lei", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Enterprise text-to-SQL benchmark", "Agentic code execution", "Multi-step SQL workflows", "Large-scale schemas and schema linking", "SQL dialect handling (BigQuery/Snowflake/etc.)", "Execution-feedback and environment interaction", "Data transformation / DBT pipelines", "Project codebase grounding"], "summary": "Spider 2.0 提出一个面向真实企业场景的文本到 SQL 基准（含632个复杂工作流），要求模型在大规模模式、多方言和多步代理交互下生成并执行复杂 SQL 与数据转换，评估显示当前 LLMs 在此现实任务上性能仍显不足。", "quality": "High", "conference": "ICLR 2025", "pdf_url": "https://arxiv.org/pdf/2411.07763v2", "published": "2024-11-12", "update_time": "2025-03-17", "download_time": "2025-12-12 22:27:52"}
{"id": "2505.18744", "title": "LogicCat: A Chain-of-Thought Text-to-SQL Benchmark for Complex Reasoning", "abstract": "Text-to-SQL is a critical task in natural language processing that aims to transform natural language questions into accurate and executable SQL queries. In real-world scenarios, these reasoning tasks are often accompanied by complex mathematical computations, domain knowledge, and hypothetical reasoning scenarios. However, existing large-scale Text-to-SQL datasets typically focus on business logic and task logic, neglecting critical factors such as vertical domain knowledge, complex mathematical reasoning, and hypothetical reasoning, which are essential for realistically reflecting the reasoning demands in practical applications and completing data querying and analysis. To bridge this gap, we introduce LogicCat, the first Text-to-SQL benchmark dataset specifically designed for complex reasoning and chain-of-thought parsing, encompassing physics, arithmetic, commonsense, and hypothetical reasoning scenarios. LogicCat comprises 4,038 English questions paired 12,114 detailed chain-of-thought reasoning steps, spanning 45 databases across diverse domains, significantly surpassing existing datasets in complexity. Experimental results demonstrate that LogicCat substantially increases the task difficulty for current state-of-the-art models to at most 33.20% execution accuracy, indicating that this task remains exceptionally challenging. The advancement of LogicCat represents a crucial step toward developing systems suitable for real-world enterprise data analysis and autonomous query generation. We have released our dataset code at https://github.com/Ffunkytao/LogicCat.", "arxiv_url": "https://arxiv.org/abs/2505.18744", "authors": ["Tao Liu", "Xutao Mao", "Hongying Zan", "Dixuan Zhang", "Yifan Li", "Haixin Liu", "Lulu Kong", "Jiaming Hou", "Rui Li", "YunLong Li", "aoze zheng", "Zhiqiang Zhang", "Luo Zhewei", "Kunli Zhang", "Min Peng"], "first_author": "Tao Liu", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Text-to-SQL benchmark", "Chain-of-Thought annotation", "Complex mathematical reasoning", "Physical domain knowledge", "Hypothetical scenario reasoning", "Cross-domain database schemas", "Execution-based SQL validation", "Multi-step reasoning decomposition", "Formula grounding in queries"], "summary": "本文提出LogicCat，一个包含4,038个问题、12,114步链式推理注释并覆盖45个领域的Text-to-SQL基准，专注于数学、物理与假设性复杂推理，实验证明现有模型在该基准上的执行准确率显著下降（最高约33%）。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2505.18744v3", "published": "2025-05-24", "update_time": "2025-09-09", "download_time": "2025-12-12 22:30:01"}
{"id": "2505.20321", "title": "BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge Bases", "abstract": "Biomedical researchers increasingly rely on large-scale structured databases for complex analytical tasks. However, current text-to-SQL systems often struggle to map qualitative scientific questions into executable SQL, particularly when implicit domain reasoning is required. We introduce BiomedSQL, the first benchmark explicitly designed to evaluate scientific reasoning in text-to-SQL generation over a real-world biomedical knowledge base. BiomedSQL comprises 68,000 question/SQL query/answer triples generated from templates and grounded in a harmonized BigQuery knowledge base that integrates gene-disease associations, causal inference from omics data, and drug approval records. Each question requires models to infer domain-specific criteria, such as genome-wide significance thresholds, effect directionality, or trial phase filtering, rather than rely on syntactic translation alone. We evaluate a range of open- and closed-source LLMs across prompting strategies and interaction paradigms. Our results reveal a substantial performance gap: GPT-o3-mini achieves 59.0% execution accuracy, while our custom multi-step agent, BMSQL, reaches 62.6%, both well below the expert baseline of 90.0%. BiomedSQL provides a new foundation for advancing text-to-SQL systems capable of supporting scientific discovery through robust reasoning over structured biomedical knowledge bases. Our dataset is publicly available at https://huggingface.co/datasets/NIH-CARD/BiomedSQL, and our code is open-source at https://github.com/NIH-CARD/biomedsql.", "arxiv_url": "https://arxiv.org/abs/2505.20321", "authors": ["Mathew J. Koretsky", "Maya Willey", "Adi Asija", "Owen Bianchi", "Chelsea X. Alvarado", "Tanay Nayak", "Nicole Kuznetsov", "Sungwon Kim", "Mike A. Nalls", "Daniel Khashabi", "Faraz Faghri"], "first_author": "Mathew J. Koretsky", "category": ["Benchmark", "Empirical", "Technical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Text-to-SQL Benchmark", "Scientific Reasoning Evaluation", "Harmonized BigQuery Biomedical Schema", "Genome-wide significance thresholds", "Multi-omic causal inference queries", "Drug approval and trial-phase filtering", "Template-based question augmentation", "Expert-authored gold SQL annotations", "Multi-step agent orchestration", "Execution-based accuracy evaluation"], "summary": "本文提出了面向生物医学科学推理的BiomedSQL基准（68,000条问/SQL/答案三元组）并发布了对应的BigQuery知识库与评测工具，评估多种LLM与自定义多步代理在生成可执行SQL并据此回答科研问题时的表现，揭示了显著的性能差距。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2505.20321v3", "published": "2025-05-23", "update_time": "2025-10-09", "download_time": "2025-12-12 22:31:39"}
{"id": "2509.23338", "title": "PARROT: A Benchmark for Evaluating LLMs in Cross-System SQL Translation", "abstract": "Large language models (LLMS) have shown increasing effectiveness in Text-to-SQL tasks. However, another closely related problem, Cross-System SQL Translation (a.k.a., SQL-to-SQL), which adapts a query written for one database system (e.g., MySQL) into its equivalent one for another system (e.g., ClickHouse), is of great practical importance but remains underexplored. Existing SQL benchmarks are not well-suited for SQL-to-SQL evaluation, which (1) focus on a limited set of database systems (often just SQLite) and (2) cannot capture many system-specific SQL dialects (e.g., customized functions, data types, and syntax rules). Thus, in this paper, we introduce PARROT, a Practical And Realistic BenchmaRk for CrOss-System SQL Translation. PARROT comprises 598 translation pairs from 38 open-source benchmarks and real-world business services, specifically prepared to challenge system-specific SQL understanding (e.g., LLMS achieve lower than 38.53% accuracy on average). We also provide multiple benchmark variants, including PARROT-Diverse with 28,003 translations (for extensive syntax testing) and PARROT-Simple with 5,306 representative samples (for focused stress testing), covering 22 production-grade database systems. To promote future research, we release a public leaderboard and source code at: https://code4db.github.io/parrot-bench/.", "arxiv_url": "https://arxiv.org/abs/2509.23338", "authors": ["Wei Zhou", "Guoliang Li", "Haoyu Wang", "Yuxing Han", "Xufei Wu", "Fan Wu", "Xuanhe Zhou"], "first_author": "Wei Zhou", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Cross-system SQL translation", "SQL dialect diversity", "Execution-first evaluation", "Reference executors", "Schema normalization", "Unit-style challenge cases", "Manual curation from production workloads", "System-specific functions and types"], "summary": "该论文提出并公开了面向跨系统 SQL 翻译的实用基准与评测套件（含人工验证的翻译对、多种变体、执行优先度量、参照执行器和挑战集），并用其揭示现有大模型在方言适应性上的显著不足。", "quality": "High", "conference": "NeurIPS 2025", "pdf_url": "https://arxiv.org/pdf/2509.23338v1", "published": "2025-09-27", "update_time": "2025-09-27", "download_time": "2025-12-12 22:32:49"}
{"id": "2509.24405", "title": "Multilingual Text-to-SQL: Benchmarking the Limits of Language Models with Collaborative Language Agents", "abstract": "Text-to-SQL enables natural access to databases, yet most benchmarks are English-only, limiting multilingual progress. We introduce MultiSpider 2.0, extending Spider 2.0 to eight languages (English, German, French, Spanish, Portuguese, Japanese, Chinese, Vietnamese). It preserves Spider 2.0's structural difficulty while adding linguistic and dialectal variability, demanding deeper reasoning for complex SQL. On this benchmark, state-of-the-art LLMs (such as DeepSeek-R1 and OpenAI o1) reach only 4\\% execution accuracy when relying on intrinsic reasoning, versus 60\\% on MultiSpider 1.0. Therefore, we provide a collaboration-driven language agents baseline that iteratively refines queries, improving accuracy to 15\\%. These results reveal a substantial multilingual gap and motivate methods that are robust across languages and ready for real-world enterprise deployment. Our benchmark is available at https://github.com/phkhanhtrinh23/Multilingual_Text_to_SQL.", "arxiv_url": "https://arxiv.org/abs/2509.24405", "authors": ["Khanh Trinh Pham", "Thu Huong Nguyen", "Jun Jo", "Quoc Viet Hung Nguyen", "Thanh Tam Nguyen"], "first_author": "Khanh Trinh Pham", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Multilingual Text-to-SQL", "Enterprise-scale schemas", "SQL dialect diversity", "Schema localization and bilingual alignment", "Compositional and nested SQL", "Collaborative language agents", "Iterative query decomposition and refinement", "Schema linking and lexical ambiguity"], "summary": "本文提出了MultiSpider 2.0——一个覆盖八种语言、面向企业级复杂模式与多SQL方言的多语言Text-to-SQL基准，并引入基于协作语言智能体（COLA）的迭代分解与校正基线以提升多语言查询的执行准确率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2509.24405v1", "published": "2025-09-29", "update_time": "2025-09-29", "download_time": "2025-12-12 22:34:21"}
{"id": "2510.05318", "title": "BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation for Large Language Models via Lens of Dynamic Interactions", "abstract": "Large language models (LLMs) have demonstrated remarkable performance on single-turn text-to-SQL tasks, but real-world database applications predominantly require multi-turn interactions to handle ambiguous queries, execution errors, and evolving user requirements. Existing multi-turn benchmarks fall short by treating conversation histories as static context or limiting evaluation to read-only operations, failing to reflect production-grade database assistant challenges. We introduce BIRD-INTERACT, a benchmark that restores this realism through: (1) a comprehensive interaction environment coupling each database with a hierarchical knowledge base, metadata files, and a function-driven user simulator, enabling models to solicit clarifications, retrieve knowledge, and recover from errors without human supervision; (2) two evaluation settings consisting of a pre-defined conversational protocol (c-Interact) and an open-ended agentic setting (a-Interact) where models autonomously decide when to query the user simulator or explore the environment; (3) a challenging task suite covering the full CRUD spectrum for business-intelligence and operational use cases, guarded by executable test cases. Each task features ambiguous and follow-up sub-tasks requiring dynamic interaction. The suite comprises BIRD-INTERACT-FULL (600 tasks, up to 11,796 interactions) for comprehensive performance assessment, and BIRD-INTERACT-LITE (300 tasks with simplified databases) for detailed behavioral analysis and rapid method development. Our empirical results highlight BIRD-INTERACT's difficulty: GPT-5 completes only 8.67% of tasks in c-Interact and 17.00% in a-Interact. Analysis via memory grafting and Interaction Test-time Scaling validates the importance of effective interaction for complex, dynamic text-to-SQL tasks.", "arxiv_url": "https://arxiv.org/abs/2510.05318", "authors": ["Nan Huo", "Xiaohan Xu", "Jinyang Li", "Per Jacobsson", "Shipei Lin", "Bowen Qin", "Binyuan Hui", "Xiaolong Li", "Ge Qu", "Shuzheng Si", "Linheng Han", "Edward Alexander", "Xintong Zhu", "Rui Qin", "Ruihan Yu", "Yiyao Jin", "Feige Zhou", "Weihao Zhong", "Yun Chen", "Hongyu Liu", "Chenhao Ma", "Fatma Ozcan", "Yannis Papakonstantinou", "Reynold Cheng"], "first_author": "Nan Huo", "category": ["Benchmark", "Empirical"], "field": "Natural Language Interfaces to Databases (NLIDB)", "task": "Interactive Text-to-SQL Evaluation", "tags": ["Function-driven user simulator", "Hierarchical knowledge base and metadata", "Protocol-guided (c-Interact) vs agentic (a-Interact) settings", "Executable test-case based correctness", "Full CRUD and schema/modification operations", "Controlled simulator to prevent ground-truth leakage", "Interaction Test-time Scaling (ITS)", "Memory grafting behavioral analysis"], "summary": "BIRD-INTERACT 提出一个面向动态多轮交互的文本到 SQL 基准，包含函数驱动的用户模拟器、层级知识库与可执行测试，并通过协议式与自治式两种评估设置衡量 LLM 在覆盖 CRUD 的交互式数据库任务中的表现与挑战。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2510.05318v2", "published": "2025-10-06", "update_time": "2025-10-08", "download_time": "2025-12-12 22:49:08"}
{"id": "2510.24762", "title": "Falcon: A Comprehensive Chinese Text-to-SQL Benchmark for Enterprise-Grade Evaluation", "abstract": "We introduce Falcon, a cross-domain Chinese text-to-SQL benchmark grounded in an enterprise-compatible dialect (MaxCompute/Hive). It contains 600 Chinese questions over 28 databases; 77% require multi-table reasoning and over half touch more than four tables. Each example is annotated along SQL-computation features and Chinese semantics. For evaluation, we release a robust execution comparator and an automated evaluation pipeline, under which all current state-of-the-art large-scale models (including Deepseek) achieve accuracies of at most 50%. Major errors originate from two sources: (1) schema linking in large enterprise landscapes - hundreds of tables, denormalized fields, ambiguous column names, implicit foreign-key relations and domain-specific synonyms that make correct join/column selection difficult; and (2) mapping concise, colloquial Chinese into the exact operators and predicates required for analytics - e.g., choosing the correct aggregation and group-by keys, expressing time windows and granularities, applying unit conversions, handling NULLs and data-quality rules, and formulating nested or windowed subqueries. Falcon therefore targets Chinese-specific semantics and enterprise dialects (abbreviations, business jargon, fuzzy entity references) and provides a reproducible middle ground before full production deployment by using realistic enterprise schemas, query templates, an execution comparator, and an automated evaluation pipeline for end-to-end validation.", "arxiv_url": "https://arxiv.org/abs/2510.24762", "authors": ["Wenzhen Luo", "Wei Guan", "Yifan Yao", "Yimin Pan", "Feng Wang", "Zhipeng Yu", "Zhe Wen", "Liang Chen", "Yihong Zhuang"], "first_author": "Wenzhen Luo", "category": ["Benchmark"], "field": "Natural Language Interfaces to Databases", "task": "Text-to-SQL (Chinese, enterprise dialects)", "tags": ["MaxCompute/Hive dialect support", "Enterprise-scale wide/denormalized schemas", "Multi-table join reasoning", "Schema linking and ambiguous column resolution", "Chinese ellipsis and business-jargon mapping", "Execution-based schema-aware SQL comparator", "Automated dialect-aware evaluation pipeline", "Synthetic enterprise case generation and annotation"], "summary": "该论文构建并公开了 Falcon——一个针对企业级中文文本到 SQL 的基准与评测工具，覆盖 MaxCompute/Hive 方言、复杂多表推理与中文商业术语并提供精细注释与可执行性比较器以衡量模型在真实企业场景下的性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2510.24762v1", "published": "2025-10-23", "update_time": "2025-10-23", "download_time": "2025-12-12 22:49:40"}
{"id": "2006.03511", "title": "Unsupervised Translation of Programming Languages", "abstract": "A transcompiler, also known as source-to-source translator, is a system that converts source code from a high-level programming language (such as C++ or Python) to another. Transcompilers are primarily used for interoperability, and to port codebases written in an obsolete or deprecated language (e.g. COBOL, Python 2) to a modern one. They typically rely on handcrafted rewrite rules, applied to the source code abstract syntax tree. Unfortunately, the resulting translations often lack readability, fail to respect the target language conventions, and require manual modifications in order to work properly. The overall translation process is timeconsuming and requires expertise in both the source and target languages, making code-translation projects expensive. Although neural models significantly outperform their rule-based counterparts in the context of natural language translation, their applications to transcompilation have been limited due to the scarcity of parallel data in this domain. In this paper, we propose to leverage recent approaches in unsupervised machine translation to train a fully unsupervised neural transcompiler. We train our model on source code from open source GitHub projects, and show that it can translate functions between C++, Java, and Python with high accuracy. Our method relies exclusively on monolingual source code, requires no expertise in the source or target languages, and can easily be generalized to other programming languages. We also build and release a test set composed of 852 parallel functions, along with unit tests to check the correctness of translations. We show that our model outperforms rule-based commercial baselines by a significant margin.", "arxiv_url": "https://arxiv.org/abs/2006.03511", "authors": ["Marie-Anne Lachaux", "Baptiste Roziere", "Lowik Chanussot", "Guillaume Lample"], "first_author": "Marie-Anne Lachaux", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Unsupervised code-to-code translation", "Back-translation for code", "Cross-language masked pretraining", "Shared multilingual seq2seq transformer", "Monolingual GitHub mining", "API/standard-library alignment", "Unit-test-based functional evaluation", "Type inference challenges"], "summary": "本文提出一种基于无监督机器翻译技术的神经源代码转译方法，仅使用单语代码（跨语言预训练+回译）在 C++/Java/Python 之间高精度翻译，并发布包含852个并行函数及单元测试的评测集，显著优于规则化基线。", "quality": "High", "conference": "NeurIPS 2020", "pdf_url": "https://arxiv.org/pdf/2006.03511v3", "published": "2020-06-05", "update_time": "2020-09-22", "download_time": "2025-12-12 22:50:22"}
{"id": "2507.04952", "title": "ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code Generation Evaluation", "abstract": "The generative capabilities of Large Language Models (LLMs) are rapidly expanding from static code to dynamic, interactive visual artifacts. This progress is bottlenecked by a critical evaluation gap: established benchmarks focus on algorithmic correctness and are blind to the visual fidelity and interactive integrity that define modern user experiences. To bridge this gap, we introduce ArtifactsBench, a new benchmark and paradigm for the automated, multimodal evaluation of visual code generation. Our framework programmatically renders each generated artifact and captures its dynamic behavior through temporal screenshots. This visual evidence, alongside the source code, is then assessed by a Multimodal LLM (MLLM)-as-Judge, which is rigorously guided by a fine-grained, per-task checklist to ensure holistic and reproducible scoring. We construct a new benchmark of 1,825 diverse tasks and evaluate over 30 leading LLMs. Our automated evaluation achieves a striking 94.4% ranking consistency with WebDev Arena, the gold-standard for human preference in web development, and over 90% pairwise agreement with human experts. This establishes ArtifactsBench as the first framework to reliably automate the assessment of human-perceived quality at scale. Our analysis provides a high-resolution map of the current SOTA, revealing that generalist models often outperform domain-specific ones. We open-source ArtifactsBench, including the benchmark, evaluation harness, and baseline results at https://artifactsbenchmark.github.io/, to provide the community with a scalable and accurate tool to accelerate the development of user-centric generative models.", "arxiv_url": "https://arxiv.org/abs/2507.04952", "authors": ["Chenchen Zhang", "Yuhang Li", "Can Xu", "Jiaheng Liu", "Ao Liu", "Changzhi Zhou", "Ken Deng", "Dengpeng Wu", "Guanhua Huang", "Kejiao Li", "Qi Yi", "Ruibin Xiong", "Shihui Hu", "Yue Zhang", "Yuhao Jiang", "Zenan Xu", "Yuanxing Zhang", "Wiggin Zhou", "Chayse Zhou", "Fengzong Lian"], "first_author": "Chenchen Zhang", "category": ["Benchmark", "Technical"], "field": "Interactive Visual Artifacts", "task": "Visual and Interactive Code Generation Evaluation", "tags": ["Multimodal automated evaluation", "MLLM-as-judge with checklist guidance", "Sandboxed execution with staged screenshots", "Interactive visual artifact benchmark", "Per-task 10-dimension checklists", "Perceptual-hash de-duplication", "Difficulty stratification (Easy/Medium/Hard)", "Human-alignment validation and pairwise agreement", "Dual-referee cross-judge calibration", "Diagnostic failure-mode analysis"], "summary": "ArtifactsBench 提出一个包含1825个交互式可视制品任务的大规模基准和自动化多模态评估流水线：在受控沙箱中执行代码、截取三阶段截图，并由清单引导的多模态LLM裁判打分，从而实现对视觉保真度、交互正确性和代码质量的可扩展、可复现评估并与人工偏好高度对齐。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2507.04952v2", "published": "2025-07-07", "update_time": "2025-09-29", "download_time": "2025-12-12 23:22:36"}
{"id": "2508.09945", "title": "VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models", "abstract": "Multimodal large language models (MLLMs) have significantly advanced the integration of visual and textual understanding. However, their ability to generate code from multimodal inputs remains limited. In this work, we introduce VisCodex, a unified framework that seamlessly merges vision and coding language models to empower MLLMs with strong multimodal code generation abilities. Leveraging a task vector-based model merging technique, we integrate a state-of-the-art coding LLM into a strong vision-language backbone, while preserving both visual comprehension and advanced coding skills. To support training and evaluation, we introduce the Multimodal Coding Dataset (MCD), a large-scale and diverse collection of 598k samples, including high-quality HTML code, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic problems. Furthermore, we propose InfiBench-V, a novel and challenging benchmark specifically designed to assess models on visually-rich, real-world programming questions that demand a nuanced understanding of both textual and visual contexts. Extensive experiments show that VisCodex achieves state-of-the-art performance among open-source MLLMs and approaches proprietary models like GPT-4o, highlighting the effectiveness of our model merging strategy and new datasets.", "arxiv_url": "https://arxiv.org/abs/2508.09945", "authors": ["Lingjie Jiang", "Shaohan Huang", "Xun Wu", "Yixia Li", "Dongdong Zhang", "Furu Wei"], "first_author": "Lingjie Jiang", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Instruction-Tuning", "tags": ["Multimodal code generation", "Task-vector model merging", "Parameter arithmetic fusion of LMs", "Preserve visual encoder while merging LM backbone", "Large-scale multimodal instruction tuning data", "HTML-from-screenshot synthesis", "Chart-image-to-code synthesis", "Visually-rich code QA benchmark"], "summary": "VisCodex通过基于任务向量的参数合并，将视觉-语言模型与专用代码语言模型融合为统一的多模态代码生成器，并同时发布了一个598k样本的多模态指令微调数据集和一个用于视觉密集型编程问答的挑战性基准以提升与评估多模态代码生成能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2508.09945v1", "published": "2025-08-13", "update_time": "2025-08-13", "download_time": "2025-12-12 23:23:19"}
{"id": "2109.08365", "title": "CodeQA: A Question Answering Dataset for Source Code Comprehension", "abstract": "We propose CodeQA, a free-form question answering dataset for the purpose of source code comprehension: given a code snippet and a question, a textual answer is required to be generated. CodeQA contains a Java dataset with 119,778 question-answer pairs and a Python dataset with 70,085 question-answer pairs. To obtain natural and faithful questions and answers, we implement syntactic rules and semantic analysis to transform code comments into question-answer pairs. We present the construction process and conduct systematic analysis of our dataset. Experiment results achieved by several neural baselines on our dataset are shown and discussed. While research on question-answering and machine reading comprehension develops rapidly, few prior work has drawn attention to code question answering. This new dataset can serve as a useful research benchmark for source code comprehension.", "arxiv_url": "https://arxiv.org/abs/2109.08365", "authors": ["Chenxiao Liu", "Xiaojun Wan"], "first_author": "Chenxiao Liu", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Understanding", "tags": ["Free-form code question answering", "Comment-to-QA transformation", "Dependency-parsing of comments", "Semantic role labeling for question generation", "Yes/No question generation and balancing", "Method-level Java and Python QA", "QA taxonomy: functionality/purpose/property/workflow", "Baseline sequence-to-sequence evaluation"], "summary": "本文提出了CodeQA，一个基于代码注释自动生成的用于源代码理解的大规模自由形式问答基准（包含Java和Python问答对），并描述了构建流程、问题生成规则与基线实验。", "quality": "High", "conference": "EMNLP 2021", "pdf_url": "https://arxiv.org/pdf/2109.08365v1", "published": "2021-09-17", "update_time": "2021-09-17", "download_time": "2025-12-12 23:23:59"}
{"id": "2210.14494", "title": "CS1QA: A Dataset for Assisting Code-based Question Answering in an Introductory Programming Course", "abstract": "We introduce CS1QA, a dataset for code-based question answering in the programming education domain. CS1QA consists of 9,237 question-answer pairs gathered from chat logs in an introductory programming class using Python, and 17,698 unannotated chat data with code. Each question is accompanied with the student's code, and the portion of the code relevant to answering the question. We carefully design the annotation process to construct CS1QA, and analyze the collected dataset in detail. The tasks for CS1QA are to predict the question type, the relevant code snippet given the question and the code and retrieving an answer from the annotated corpus. Results for the experiments on several baseline models are reported and thoroughly analyzed. The tasks for CS1QA challenge models to understand both the code and natural language. This unique dataset can be used as a benchmark for source code comprehension and question answering in the educational setting.", "arxiv_url": "https://arxiv.org/abs/2210.14494", "authors": ["Changyoon Lee", "Yeon Seonwoo", "Alice Oh"], "first_author": "Changyoon Lee", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Understanding", "tags": ["Student-TA chat logs", "Keystroke-level code snapshots", "Question type annotation", "Relevant-code-line labeling", "Educational code QA", "Answer retrieval task"], "summary": "本文提出CS1QA——一个来自编程入门课的包含9,237个问答对并标注问题类型与相关代码行的数据集，设计了问题类型分类、代码行选择与答案检索三项任务并给出基线实验结果。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2210.14494v1", "published": "2022-10-26", "update_time": "2022-10-26", "download_time": "2025-12-12 23:27:23"}
{"id": "2309.01940", "title": "CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models", "abstract": "With the emergence of Large Language Models (LLMs), there has been a significant improvement in the programming capabilities of models, attracting growing attention from researchers. Evaluating the programming capabilities of LLMs is crucial as it reflects the multifaceted abilities of LLMs, and it has numerous downstream applications. In this paper, we propose CodeApex, a bilingual benchmark dataset focusing on the programming comprehension, code generation, and code correction abilities of LLMs. Programming comprehension task tests LLMs on multiple-choice exam questions covering conceptual understanding, commonsense reasoning, and multi-hop reasoning. The code generation task evaluates LLMs through completing C++ functions based on provided descriptions and prototypes. The code correction task asks LLMs to fix real-world erroneous code segments with different error messages. We evaluate 12 widely used LLMs, including both general-purpose and specialized models. GPT-4 exhibits the best programming capabilities, achieving approximate accuracy of 69%, 54%, and 66% on the three tasks, respectively. Compared to human performance, there is still significant room for improvement in LLM programming. We hope that CodeApex can serve as a reference for evaluating the coding capabilities of LLMs, further promoting their development and growth.", "arxiv_url": "https://arxiv.org/abs/2309.01940", "authors": ["Lingyue Fu", "Huacan Chai", "Shuang Luo", "Kounianhua Du", "Weiming Zhang", "Longteng Fan", "Jiayi Lei", "Renting Rui", "Jianghao Lin", "Yuchen Fang", "Yifan Liu", "Jingkuan Wang", "Siyuan Qi", "Kangning Zhang", "Weinan Zhang", "Yong Yu"], "first_author": "Lingyue Fu", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Understanding", "tags": ["Bilingual evaluation (English and Chinese)", "Programming comprehension multiple-choice", "C++ function completion", "Real-world code correction (error-message driven)", "Fine-grained category annotations", "Human-expert validation and human-vs-model comparison", "Prompt strategy and bilingual performance analysis", "Leader-board style large-scale model evaluation"], "summary": "本文提出了CodeApex——一个面向大模型的双语（中英）编程能力评测基准，包含编程理解、C++代码生成与真实代码修复三项任务并在大规模细粒度标注与人工验证下评估多款主流LLM以比较其与人类的差距。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2309.01940v4", "published": "2023-09-05", "update_time": "2024-03-11", "download_time": "2025-12-12 23:28:17"}
{"id": "2401.03065", "title": "CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution", "abstract": "We present CRUXEval (Code Reasoning, Understanding, and eXecution Evaluation), a benchmark consisting of 800 Python functions (3-13 lines). Each function comes with an input-output pair, leading to two natural tasks: input prediction and output prediction. First, we propose a generic recipe for generating our execution benchmark which can be used to create future variation of the benchmark. Second, we evaluate twenty code models on our benchmark and discover that many recent high-scoring models on HumanEval do not show the same improvements on our benchmark. Third, we show that simple CoT and fine-tuning schemes can improve performance on our benchmark but remain far from solving it. The best setup, GPT-4 with chain of thought (CoT), achieves a pass@1 of 75% and 81% on input and output prediction, respectively. In contrast, Code Llama 34B achieves a pass@1 of 50% and 46% on input and output prediction, highlighting the gap between open and closed source models. As no model is close to acing CRUXEval, we provide examples of consistent GPT-4 failures on simple programs as a lens into its code reasoning capabilities and areas for improvement.", "arxiv_url": "https://arxiv.org/abs/2401.03065", "authors": ["Alex Gu", "Baptiste Rozière", "Hugh Leather", "Armando Solar-Lezama", "Gabriel Synnaeve", "Sida I. Wang"], "first_author": "Alex Gu", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Output prediction (execution)", "Input prediction (reverse execution)", "Execution tracing of short Python programs", "Generate-and-filter benchmark construction", "Chain-of-thought prompting evaluation", "Fine-tuning on I/O assertion data", "Open-source vs closed-source model performance gap", "Analysis of consistent model failure cases"], "summary": "本文提出CRUXEval，一个包含800个短Python函数的基准（包含输出预测与输入预测两项任务），并在20个代码模型上评估，展示了模型在代码执行推理上的短板以及链式思维与微调的改进效果。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2401.03065v1", "published": "2024-01-05", "update_time": "2024-01-05", "download_time": "2025-12-12 23:29:31"}
{"id": "2405.11966", "title": "Multiple-Choice Questions are Efficient and Robust LLM Evaluators", "abstract": "We present GSM-MC, a multiple-choice (MC) dataset constructed by collecting answers and incorrect predictions on GSM8K from 60 open-source models. Through extensive experiments, we show that LLMs' performance on the MC version of this popular benchmark is strongly correlated with their performance on the original version and is quite robust to distractor choices and option orders, while the evaluation time is reduced by a factor of up to 30. Following similar procedures, we introduce MATH-MC, constructed from MATH, and PythonIO, a new program reasoning MC dataset constructed from HumanEval and MBPP. Experimental results indicate that LLMs' performance on these MC benchmarks leaves much room for improvement. Our data and code are available at https://github.com/Geralt-Targaryen/MC-Evaluation.", "arxiv_url": "https://arxiv.org/abs/2405.11966", "authors": ["Ziyin Zhang", "Zhaokun Jiang", "Lizhen Xu", "Hongkun Hao", "Rui Wang"], "first_author": "Ziyin Zhang", "category": ["Benchmark", "Empirical"], "field": "Evaluation & Benchmarks", "task": "Conversion of generation benchmarks to multiple-choice evaluation", "tags": ["Multiple-choice conversion of generation benchmarks", "Distractor pool from model incorrect outputs", "Program-output-as-MC evaluation", "Robustness to distractor choice and option order", "Logit-based scoring for efficiency", "Correlation analysis with open-ended evaluation", "Analysis of invalid generation formats"], "summary": "本文通过从大量开源模型收集错误预测作为干扰项，将短答案数学与编程基准转换为多项选择格式并构建MC数据集，实验表明MC评估在计算效率、对干扰项和选项顺序的鲁棒性以及与原始生成式评测的相关性方面表现良好。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2405.11966v4", "published": "2024-05-20", "update_time": "2024-06-26", "download_time": "2025-12-12 23:30:24"}
{"id": "2406.00037", "title": "Aligning LLMs through Multi-perspective User Preference Ranking-based Feedback for Programming Question Answering", "abstract": "Code Community Question Answering (CCQA) seeks to tackle programming-related issues, thereby boosting productivity in both software engineering and academic research. Recent advancements in Reinforcement Learning from Human Feedback (RLHF) have transformed the fine-tuning process of Large Language Models (LLMs) to produce responses that closely mimic human behavior. Leveraging LLMs with RLHF for practical CCQA applications has thus emerged as a promising area of study. Unlike standard code question-answering tasks, CCQA involves multiple possible answers, with varying user preferences for each response. Additionally, code communities often show a preference for new APIs. These challenges prevent LLMs from generating responses that cater to the diverse preferences of users in CCQA tasks. To address these issues, we propose a novel framework called Aligning LLMs through Multi-perspective User Preference Ranking-based Feedback for Programming Question Answering (ALMupQA) to create user-focused responses. Our approach starts with Multi-perspective Preference Ranking Alignment (MPRA), which synthesizes varied user preferences based on the characteristics of answers from code communities. We then introduce a Retrieval-augmented In-context Learning (RIL) module to mitigate the problem of outdated answers by retrieving responses to similar questions from a question bank. Due to the limited availability of high-quality, multi-answer CCQA datasets, we also developed a dataset named StaCCQA from real code communities. Extensive experiments demonstrated the effectiveness of the ALMupQA framework in terms of accuracy and user preference. Compared to the base model, ALMupQA showed nearly an 11% improvement in BLEU, with increases of 20% and 17.5% in BERTScore and CodeBERTScore, respectively.", "arxiv_url": "https://arxiv.org/abs/2406.00037", "authors": ["Hongyu Yang", "Liyang He", "Min Hou", "Shuanghong Shen", "Rui Li", "Jiahui Hou", "Jianhui Ma", "Junda Zhao"], "first_author": "Hongyu Yang", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Alignment", "tags": ["Multi-perspective preference scoring", "Listwise contrastive ranking loss", "Retrieval-augmented in-context learning", "Questioner vs community vote modeling", "Mitigating outdated API answers", "Preference-aligned fine-tuning for CCQA", "Multi-answer ranking in code communities"], "summary": "本文提出ALMupQA框架，通过多视角偏好评分（问答者偏差、用户投票与LLM内容评分）结合基于检索的上下文学习，对大型模型进行排序式偏好对齐，并构建了来自真实社区的多答案CCQA数据以提升编程问答质量。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2406.00037v1", "published": "2024-05-27", "update_time": "2024-05-27", "download_time": "2025-12-12 23:31:56"}
{"id": "2406.06025", "title": "RepoQA: Evaluating Long Context Code Understanding", "abstract": "Recent advances have been improving the context windows of Large Language Models (LLMs). To quantify the real long-context capabilities of LLMs, evaluators such as the popular Needle in a Haystack have been developed to test LLMs over a large chunk of raw texts. While effective, current evaluations overlook the insight of how LLMs work with long-context code, i.e., repositories. To this end, we initiate the RepoQA benchmark to evaluate LLMs on long-context code understanding. Traditional needle testers ask LLMs to directly retrieve the answer from the context without necessary deep understanding. In RepoQA, we built our initial task, namely Searching Needle Function (SNF), which exercises LLMs to search functions given their natural-language description, i.e., LLMs cannot find the desired function if they cannot understand the description and code. RepoQA is multilingual and comprehensive: it includes 500 code search tasks gathered from 50 popular repositories across 5 modern programming languages. By evaluating 26 general and code-specific LLMs on RepoQA, we show (i) there is still a small gap between the best open and proprietary models; (ii) different models are good at different languages; and (iii) models may understand code better without comments.", "arxiv_url": "https://arxiv.org/abs/2406.06025", "authors": ["Jiawei Liu", "Jia Le Tian", "Vijay Daita", "Yuxiang Wei", "Yifeng Ding", "Yuhan Katherine Wang", "Jun Yang", "Lingming Zhang"], "first_author": "Jiawei Liu", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Retrieval", "tags": ["Repo-level function search", "Long-context code retrieval", "Function description decomposition", "Automatic dataset curation pipeline", "Multilingual repository evaluation", "Context-depth placement (needle depth)", "BLEU-based retrieval scoring"], "summary": "RepoQA 提出并发布了一个用于评估模型在仓库级长上下文中理解与检索函数能力的多语言基准（Searching Needle Function），包含 50 个仓库共 500 个任务，并通过自动化构建流水线和对 33 个模型的评测展示当前模型的长上下文代码理解现状。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2406.06025v1", "published": "2024-06-10", "update_time": "2024-06-10", "download_time": "2025-12-12 23:32:31"}
{"id": "2408.13001", "title": "CRUXEval-X: A Benchmark for Multilingual Code Reasoning, Understanding and Execution", "abstract": "Code benchmarks such as HumanEval are widely adopted to evaluate Large Language Models' (LLMs) coding capabilities. However, there is an unignorable programming language bias in existing code benchmarks -- over 95% code generation benchmarks are dominated by Python, leaving the LLMs' capabilities in other programming languages such as Java and C/C++ unknown. Moreover, coding task bias is also crucial. Most benchmarks focus on code generation capability, while benchmarks for code reasoning (given input, reasoning output; and given output, reasoning input), an essential coding capability, are insufficient. Yet, constructing multi-lingual benchmarks can be expensive and labor-intensive, and codes in contest websites such as Leetcode suffer from data contamination during training. To fill this gap, we propose CRUXEVAL-X, a multi-lingual code reasoning benchmark that contains 19 programming languages. It comprises at least 600 subjects for each language, along with 19K content-consistent tests in total. In particular, the construction pipeline of CRUXEVAL-X works in a fully automated and test-guided manner, which iteratively generates and repairs based on execution feedback. Also, to cross language barriers (e.g., dynamic/static type systems in Python/C++), we formulated various transition rules between language pairs to facilitate translation. Our intensive evaluation of 24 representative LLMs reveals the correlation between language pairs. For example, TypeScript and JavaScript show a significant positive correlation, while Racket has less correlation with other languages. More interestingly, even a model trained solely on Python can achieve at most 34.4% Pass@1 in other languages, revealing the cross-language generalization of LLMs.", "arxiv_url": "https://arxiv.org/abs/2408.13001", "authors": ["Ruiyang Xu", "Jialun Cao", "Yaojie Lu", "Ming Wen", "Hongyu Lin", "Xianpei Han", "Ben He", "Shing-Chi Cheung", "Le Sun"], "first_author": "Ruiyang Xu", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Multilingual code reasoning benchmark", "Test-guided automated code translation", "Iterative generation-and-repair pipeline", "Type and function-signature mapping rules", "Test-suite templating and translation", "Execution-feedback-driven compilation/runtime repair", "Cross-language correlation and generalization analysis", "Contamination-aware benchmark construction"], "summary": "本文提出了CRUXEVAL-X——一个覆盖19种编程语言、通过测试驱动的自动化生成与迭代修复流水线构建的多语言代码推理基准，并在24个主流LLM上评估以揭示跨语言泛化与语言间相关性。", "quality": "High", "conference": "ACL 2025", "pdf_url": "https://arxiv.org/pdf/2408.13001v2", "published": "2024-08-23", "update_time": "2025-05-17", "download_time": "2025-12-12 23:33:53"}
{"id": "2409.12866", "title": "SpecEval: Evaluating Code Comprehension in Large Language Models via Program Specifications", "abstract": "Large Language models have achieved impressive performance in automated software engineering. Extensive efforts have been made to evaluate the abilities of code LLMs in various aspects, with an increasing number of benchmarks and evaluation frameworks proposed. Apart from the most sought-after capability of code generation, the capability of code comprehension is being granted growing attention. Nevertheless, existing works assessing the code comprehension capability of LLMs exhibit varied limitations. Evaluation frameworks like CRUXEval and REval usually focus on code reasoning tasks over a certain input case, leading to a limited range of execution traces covered, resulting in a loss in code semantics examined and the inability to assess the comprehensive understanding of LLMs concerning the target program. To tackle these challenges, we propose SpecEval, a novel black-box evaluation framework to evaluate code comprehension in LLMs via program specifications. Inspired by the idea that specifications can act as a comprehensive articulation of program behaviors concerning all possible execution traces, we employ formalized program specifications to represent program semantics and perform comprehensive evaluations. In particular, four specification-related tasks are designed meticulously to assess the capability of LLMs from basic to advanced levels. Counterfactual analysis is further conducted to study the performance variance of LLMs under semantics-preserving perturbations. Systematic experiments are conducted on six state-of-the-art LLMs. Extensive experimental results present a below-satisfactory performance of LLMs on specification-related tasks, revealing the limitations of existing LLMs in terms of articulating program semantics with formal specifications. Counterfactual analysis also reveals the sensitivity of LLMs towards semantic-preserving perturbations.", "arxiv_url": "https://arxiv.org/abs/2409.12866", "authors": ["Lezhi Ma", "Shangqing Liu", "Lei Bu", "Shangru Li", "Yida Wang", "Yang Liu"], "first_author": "Lezhi Ma", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Understanding", "tags": ["Formal program specifications", "JML specifications", "Specification correctness judgment", "Specification selection", "Specification infilling", "Specification generation", "Black-box evaluation", "Counterfactual semantic-preserving perturbations", "Mutation operators", "Semantics-equivalent program variants"], "summary": "本文提出SpecEval，一个基于形式化程序规格（JML）的黑盒评估框架，设计四类规格相关任务并构建含204个带可验证规格的Java程序基准，通过对六种主流LLM的系统性实验及反事实扰动分析，揭示模型在用形式化规格表述程序语义方面的不足及对语义保持扰动的敏感性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2409.12866v2", "published": "2024-09-19", "update_time": "2025-03-22", "download_time": "2025-12-12 23:34:48"}
{"id": "2410.01999", "title": "CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding & Reasoning Capabilities of CodeLLMs", "abstract": "Recent advances in Code Large Language Models (CodeLLMs) have primarily focused on open-ended code generation, often overlooking the crucial aspect of code understanding and reasoning. To bridge this gap, we introduce CodeMMLU, a comprehensive multiple-choice benchmark designed to evaluate the depth of software and code comprehension in LLMs. CodeMMLU includes nearly 20,000 questions spanning diverse domains, including code analysis, defect detection, and software engineering principles across multiple programming languages. Unlike traditional benchmarks that emphasize code generation, CodeMMLU assesses a model's ability to reason about programs across a wide-range of tasks such as code repair, execution reasoning, and fill-in-the-blank challenges. Our extensive evaluation reveals that even state-of-the-art models struggle with CodeMMLU, highlighting significant gaps in comprehension beyond generation. By emphasizing the essential connection between code understanding and effective AI-assisted development, CodeMMLU provides a critical resource for advancing more reliable and capable coding assistants.", "arxiv_url": "https://arxiv.org/abs/2410.01999", "authors": ["Dung Nguyen Manh", "Thang Phan Chau", "Nam Le Hai", "Thong T. Doan", "Nam V. Nguyen", "Quang Pham", "Nghi D. Q. Bui"], "first_author": "Dung Nguyen Manh", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Understanding", "tags": ["MCQ format for code evaluation", "Code comprehension and reasoning", "Execution/behavior prediction", "Defect detection / code repair questions", "Answer-choice permutation and debiasing", "Cross-language / multi-domain coverage", "Prompting sensitivity (CoT harms)", "Model scaling and family-specific effects"], "summary": "CodeMMLU 提出一个近两万题的多项选择基准用于评估 CodeLLMs 的代码理解与推理能力，实验证明现有模型在理解方面存在显著不足并揭示了诸多偏差与模型族/提示策略的影响。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2410.01999v4", "published": "2024-10-02", "update_time": "2025-04-09", "download_time": "2025-12-12 23:41:10"}
{"id": "2411.03012", "title": "Leveraging Large Language Models in Code Question Answering: Baselines and Issues", "abstract": "Question answering over source code provides software engineers and project managers with helpful information about the implemented features of a software product. This paper presents a work devoted to using large language models for question answering over source code in Python. The proposed method for implementing a source code question answering system involves fine-tuning a large language model on a unified dataset of questions and answers for Python code. To achieve the highest quality answers, we tested various models trained on datasets preprocessed in different ways: a dataset without grammar correction, a dataset with grammar correction, and a dataset augmented with the generated summaries. The model answers were also analyzed for errors manually. We report BLEU-4, BERTScore F1, BLEURT, and Exact Match metric values, along with the conclusions from the manual error analysis. The obtained experimental results highlight the current problems of the research area, such as poor quality of the public genuine question-answering datasets. In addition, the findings include the positive effect of the grammar correction of the training data on the testing metric values. The addressed findings and issues could be important for other researchers who attempt to improve the quality of source code question answering solutions. The training and evaluation code is publicly available at https://github.com/IU-AES-AI4Code/CodeQuestionAnswering.", "arxiv_url": "https://arxiv.org/abs/2411.03012", "authors": ["Georgy Andryushchenko", "Vladimir Ivanov", "Vladimir Makharev", "Elizaveta Tukhtina", "Aidar Valeev"], "first_author": "Georgy Andryushchenko", "category": ["Empirical"], "field": "Coding Assistant", "task": "Code Understanding", "tags": ["Code question answering baselines", "Fine-tuning for code QA", "Grammar correction of QA data", "Summary-based data augmentation", "Manual error analysis of answers", "Evaluation with BLEU/BERTScore/BLEURT/ExactMatch", "Quality issues in public QA datasets", "File-level vs function-level context"], "summary": "本文通过对不同预处理（语法修正、摘要增强）下的开源代码大模型进行微调并以自动指标与人工错误分析评估，构建了Python代码问答的基线并揭示了公共问答数据质量等问题。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2411.03012v1", "published": "2024-11-05", "update_time": "2024-11-05", "download_time": "2025-12-12 23:41:50"}
{"id": "2411.18932", "title": "ScratchEval: Are GPT-4o Smarter than My Child? Evaluating Large Multimodal Models with Visual Programming Challenges", "abstract": "Recent advancements in large multimodal models (LMMs) have showcased impressive code generation capabilities, primarily evaluated through image-to-code benchmarks. However, these benchmarks are limited to specific visual programming scenarios where the logic reasoning and the multimodal understanding capacities are split apart. To fill this gap, we propose ScratchEval, a novel benchmark designed to evaluate the visual programming reasoning ability of LMMs. ScratchEval is based on Scratch, a block-based visual programming language widely used in children's programming education. By integrating visual elements and embedded programming logic, ScratchEval requires the model to process both visual information and code structure, thereby comprehensively evaluating its programming intent understanding ability. Our evaluation approach goes beyond the traditional image-to-code mapping and focuses on unified logical thinking and problem-solving abilities, providing a more comprehensive and challenging framework for evaluating the visual programming ability of LMMs. ScratchEval not only fills the gap in existing evaluation methods, but also provides new insights for the future development of LMMs in the field of visual programming. Our benchmark can be accessed at https://github.com/HKBUNLP/ScratchEval .", "arxiv_url": "https://arxiv.org/abs/2411.18932", "authors": ["Rao Fu", "Ziyang Luo", "Hongzhan Lin", "Zhen Ye", "Jing Ma"], "first_author": "Rao Fu", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Scratch (block-based programming)", "Visual program reasoning", "Multimodal multiple-choice QA", "Cross-lingual evaluation (English/Chinese)", "Chain-of-Thought and eCoT prompting analysis", "Spatial and graphic perception tasks", "Mathematical and logical reasoning in visual code"], "summary": "本文提出了基于儿童图形化编程语言Scratch的ScratchEval基准，通过305道中英双语的可视化编程多项选择题评估大规模多模态模型在图像与嵌入式程序逻辑理解与推理上的能力，并分析了不同提示策略的影响。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2411.18932v1", "published": "2024-11-28", "update_time": "2024-11-28", "download_time": "2025-12-12 23:43:36"}
{"id": "2412.14764", "title": "CodeRepoQA: A Large-scale Benchmark for Software Engineering Question Answering", "abstract": "In this work, we introduce CodeRepoQA, a large-scale benchmark specifically designed for evaluating repository-level question-answering capabilities in the field of software engineering. CodeRepoQA encompasses five programming languages and covers a wide range of scenarios, enabling comprehensive evaluation of language models. To construct this dataset, we crawl data from 30 well-known repositories in GitHub, the largest platform for hosting and collaborating on code, and carefully filter raw data. In total, CodeRepoQA is a multi-turn question-answering benchmark with 585,687 entries, covering a diverse array of software engineering scenarios, with an average of 6.62 dialogue turns per entry.   We evaluate ten popular large language models on our dataset and provide in-depth analysis. We find that LLMs still have limitations in question-answering capabilities in the field of software engineering, and medium-length contexts are more conducive to LLMs' performance. The entire benchmark is publicly available at https://github.com/kinesiatricssxilm14/CodeRepoQA.", "arxiv_url": "https://arxiv.org/abs/2412.14764", "authors": ["Ruida Hu", "Chao Peng", "Jingyi Ren", "Bo Jiang", "Xiangxin Meng", "Qinyun Wu", "Pengfei Gao", "Xinchen Wang", "Cuiyun Gao"], "first_author": "Ruida Hu", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Understanding", "tags": ["Repository-level question answering", "Multi-turn developer dialogues", "GitHub issue mining", "Cross-language coverage (Python/JavaScript/TypeScript/Java/Go)", "Automatic data filtering heuristics", "Context-length impact analysis", "Evaluation with text-similarity metrics"], "summary": "本文提出CodeRepoQA——一个从30个热门GitHub仓库爬取并过滤得到的包含585,687条多轮（平均6.62轮）、跨五种语言的仓库级软件工程问答大规模基准，并用BLEU/ROUGE/编辑相似度评估多款大语言模型，发现模型在真实仓库QA场景仍存在明显不足且中等长度上下文更有利于性能。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2412.14764v1", "published": "2024-12-19", "update_time": "2024-12-19", "download_time": "2025-12-12 23:44:07"}
{"id": "2508.09101", "title": "AutoCodeBench: Large Language Models are Automatic Code Benchmark Generators", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, with code generation emerging as a key area of focus. While numerous benchmarks have been proposed to evaluate their code generation abilities, these benchmarks face several critical limitations. First, they often rely on manual annotations, which are time-consuming and difficult to scale across different programming languages and problem complexities. Second, most existing benchmarks focus primarily on Python, while the few multilingual benchmarks suffer from limited difficulty and uneven language distribution. To address these challenges, we propose AutoCodeGen, an automated method for generating high-difficulty multilingual code generation datasets without manual annotations. AutoCodeGen ensures the correctness and completeness of test cases by generating test inputs with LLMs and obtaining test outputs through a multilingual sandbox, while achieving high data quality through reverse-order problem generation and multiple filtering steps. Using this novel method, we introduce AutoCodeBench, a large-scale code generation benchmark comprising 3,920 problems evenly distributed across 20 programming languages. It is specifically designed to evaluate LLMs on challenging, diverse, and practical multilingual tasks. We evaluate over 30 leading open-source and proprietary LLMs on AutoCodeBench and its simplified version AutoCodeBench-Lite. The results show that even the most advanced LLMs struggle with the complexity, diversity, and multilingual nature of these tasks. Besides, we introduce AutoCodeBench-Complete, specifically designed for base models to assess their few-shot code generation capabilities. We hope the AutoCodeBench series will serve as a valuable resource and inspire the community to focus on more challenging and practical multilingual code generation scenarios.", "arxiv_url": "https://arxiv.org/abs/2508.09101", "authors": ["Jason Chou", "Ao Liu", "Yuchi Deng", "Zhiying Zeng", "Tao Zhang", "Haotian Zhu", "Jianwei Cai", "Yue Mao", "Chenchen Zhang", "Lingyun Tan", "Ziyan Xu", "Bohui Zhai", "Hengyi Liu", "Speed Zhu", "Wiggin Zhou", "Fengzong Lian"], "first_author": "Jason Chou", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Automated dataset synthesis", "LLM-sandbox interaction", "Reverse-order problem generation", "Multilingual sandbox execution", "LLM-as-critic filtering", "High-difficulty multilingual problems", "Multi-logical problem design", "Balanced language distribution"], "summary": "本文提出AutoCodeGen自动化工作流并发布AutoCodeBench，一个包含3920个高难度、多语言且无人工注释的代码生成基准数据集，通过LLM与多语言沙箱交互生成并验证测试用例、采用反向问题生成与多重过滤以保证高质量，并评估了30+模型的性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2508.09101v1", "published": "2025-08-12", "update_time": "2025-08-12", "download_time": "2025-12-12 23:46:00"}
{"id": "2508.16402", "title": "AetherCode: Evaluating LLMs' Ability to Win In Premier Programming Competitions", "abstract": "Competitive programming has emerged as a critical benchmark for evaluating the reasoning and coding capabilities of Large Language Models (LLMs). Despite impressive progress on existing benchmarks, we argue that current evaluations overstate model proficiency, masking a substantial gap between LLMs and elite human programmers. This gap arises from two key limitations: insufficient difficulty and scope of benchmark problems, and evaluation bias from low-quality test cases. To address these shortcomings, we present AetherCode, a new benchmark that draws problems from premier programming competitions such as IOI and ICPC, offering broader coverage and higher difficulty. AetherCode further incorporates comprehensive, expert-validated test suites built through a hybrid of automated generation and human curation, ensuring rigorous and reliable assessment. By combining challenging problem design with robust evaluation, AetherCode provides a more faithful measure of LLM capabilities and sets a new standard for future research in code reasoning.", "arxiv_url": "https://arxiv.org/abs/2508.16402", "authors": ["Zihan Wang", "Jiaze Chen", "Zhicheng Liu", "Markus Mak", "Yidi Du", "Geonsik Moon", "Luoqi Xu", "Aaron Tua", "Kunshuo Peng", "Jiayi Lu", "Mingfei Xia", "Boqian Zou", "Chenyang Ran", "Guang Tian", "Shoutai Zhu", "Yeheng Duan", "Zhenghui Kang", "Zhenxing Lin", "Shangshu Li", "Qiang Luo", "Qingshen Long", "Zhiyong Chen", "Yihan Xiao", "Yurong Wu", "Daoguang Zan", "Yuyi Fu", "Mingxuan Wang", "Ming Ding"], "first_author": "Zihan Wang", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Competitive programming problems", "Olympiad and ICPC curation", "Expert-validated test suites", "Hybrid automated-and-human test generation", "Zero false-positive/negative validation", "Solution-corpus-based test validation", "PDF-to-Markdown/LaTeX problem conversion"], "summary": "AetherCode 提出一个来自 IOI、ICPC 等顶级竞赛、并以自动化生成结合专家审核的高质量测试用例构建的新基准，以更严格地评估大模型在竞赛级编程题上的能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2508.16402v1", "published": "2025-08-22", "update_time": "2025-08-22", "download_time": "2025-12-12 23:46:31"}
{"id": "2510.09595", "title": "LiveOIBench: Can Large Language Models Outperform Human Contestants in Informatics Olympiads?", "abstract": "Competitive programming problems increasingly serve as valuable benchmarks to evaluate the coding capabilities of large language models (LLMs) due to their complexity and ease of verification. Yet, current coding benchmarks face limitations such as lack of exceptionally challenging problems, insufficient test case coverage, reliance on online platform APIs that limit accessibility. To address these issues, we introduce LiveOIBench, a comprehensive benchmark featuring 403 expert-curated Olympiad-level competitive programming problems, each with an average of 60 expert-designed test cases. The problems are sourced directly from 72 official Informatics Olympiads in different regions conducted between 2023 and 2025. LiveOIBench distinguishes itself through four key features: (1) meticulously curated high-quality tasks with detailed subtask rubrics and extensive private test cases; (2) direct integration of elite contestant performance data to enable informative comparison against top-performing humans; (3) planned continuous, contamination-free updates from newly released Olympiad problems; and (4) a self-contained evaluation system facilitating offline and easy-to-reproduce assessments. Benchmarking 32 popular general-purpose and reasoning LLMs, we find that GPT-5 achieves a notable 81.76th percentile, a strong result that nonetheless falls short of top human contestant performance, who usually place above 90th. In contrast, among open-weight reasoning models, GPT-OSS-120B achieves only a 60th percentile, underscoring significant capability disparities from frontier closed models. Detailed analyses indicate that robust reasoning models prioritize precise problem analysis over excessive exploration, suggesting future models should emphasize structured analysis and minimize unnecessary exploration. All data, code, and leaderboard results will be made publicly available on our website.", "arxiv_url": "https://arxiv.org/abs/2510.09595", "authors": ["Kaijian Zou", "Aaron Xiong", "Yunxiang Zhang", "Frederick Zhang", "Yueqi Ren", "Jirong Yang", "Ayoung Lee", "Shitanshu Bhushan", "Lu Wang"], "first_author": "Kaijian Zou", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Olympiad-level competitive programming", "Expert-curated private test cases", "Fine-grained subtask rubrics", "Offline reproducible judge", "Human-contestant percentile comparison", "Continuous contamination-free updates", "Reasoning-trace / token-efficiency analysis", "Algorithm-specific failure analysis (e.g., dynamic programming)"], "summary": "本文提出LiveOIBench——一个包含403道信息学奥赛题、平均60个专家私有测试用例、细粒度子任务评分与离线评测系统的持续更新竞赛编程基准，并用该基准对32个主流模型进行了评测与深入分析，揭示模型在动态规划等算法与推理策略上的不足。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2510.09595v1", "published": "2025-10-10", "update_time": "2025-10-10", "download_time": "2025-12-12 23:47:36"}
{"id": "2510.12803", "title": "AutoCode: LLMs as Problem Setters for Competitive Programming", "abstract": "Writing competitive programming problems is exacting. Authors must: set constraints, input distributions, and edge cases that rule out shortcuts; target specific algorithms (e.g., max-flow, dynamic programming, data structures); and calibrate complexity beyond the reach of most competitors. We argue that this makes for an ideal test of general large language model capabilities and study whether they can do this reliably. We introduce AutoCode, which uses multiple rounds of validation to yield competition-grade problem statements and test cases. On held-out problems, AutoCode test suites approach 99% consistency with official judgments, a significant improvement over current state-of-the-art methods like HardTests, which achieve less than 81%. Furthermore, starting with a random seed problem, AutoCode can create novel variants with reference and brute-force solutions. By cross-verifying these generated solutions against test cases, we can further filter out malformed problems. Our system ensures high correctness, as verified by human experts. AutoCode successfully produces novel problems judged by Grandmaster-level (top 0.3%) competitive programmers to be of contest quality.", "arxiv_url": "https://arxiv.org/abs/2510.12803", "authors": ["Shang Zhou", "Zihan Zheng", "Kaiyuan Liu", "Zeyu Shen", "Zerui Cheng", "Zexing Chen", "Hansen He", "Jianzhu Yao", "Huanzhi Mao", "Qiuyang Mang", "Tianfu Fu", "Beichen Li", "Dongruixuan Li", "Wenhao Chai", "Zhuang Liu", "Aleksandra Korolova", "Peter Henderson", "Natasha Jaques", "Pramod Viswanath", "Saining Xie", "Jingbo Shang"], "first_author": "Shang Zhou", "category": ["Technical", "Benchmark"], "field": "Software Testing", "task": "Test Generation", "tags": ["Validator-Generator-Checker framework", "Dual verification (reference & brute-force)", "Adversarial and stress test synthesis", "Seed-driven problem generation", "TLE/time-complexity case generation", "Cross-verification of solutions", "Grandmaster human vetting"], "summary": "该论文提出AutoCode，一个基于闭环多角色（Validator-Generator-Checker）和双重验证（参考解与暴力解）机制的系统，利用LLM自动生成并验证竞赛级编程题目与高质量测试用例，达到接近官方判定的高一致性并产出经顶级选手评审的可赛用新题目。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2510.12803v1", "published": "2025-09-29", "update_time": "2025-09-29", "download_time": "2025-12-12 23:48:48"}
{"id": "2510.17868", "title": "UniCode: A Framework for Generating High Quality Competitive Coding Problems", "abstract": "The reliance of competitive coding benchmarks on static, human-authored problems creates significant challenges, including data contamination and limited scalability. To address these issues, we introduce UniCode, a novel framework that automatically generates high-quality algorithmic problems alongside robust, contamination-resistant test cases. Inspired by biological evolution that creates better and diverse offspring, our framework leverages Large Language Models (LLMs) to systematically diversify problems through three strategies: single problem extension, same-type fusion, and cross-type fusion. A key innovation is our stress-driven test case synthesis pipeline, which generates reliable test suites without requiring a canonical ground-truth solution. This pipeline combines brute-force grounding for small-scale inputs with a consensus-based validation mechanism for large-scale inputs to ensure high correctness and coverage. We demonstrate effectiveness of our framework by curating a benchmark of 492 problems and evaluating 19 state-of-the-art LLMs. The results reveal that UniCode is highly challenging and discriminative, with the top-performing model, o4-mini, achieving a pass rate of only 70.3%. Our framework provides a scalable and reliable solution for generating dynamic evaluation datasets in coding domain.", "arxiv_url": "https://arxiv.org/abs/2510.17868", "authors": ["Xinyue Zheng", "Haowei Lin", "Shaofei Cai", "Zilong Zheng", "Yitao Liang"], "first_author": "Xinyue Zheng", "category": ["Technical", "Benchmark"], "field": "Benchmarking & Dataset Generation", "task": "Problem Generation and Stress-driven Testcase Synthesis", "tags": ["Generative evaluation", "Problem evolution strategies", "Single-problem extension", "Same-type fusion", "Cross-type fusion", "Stress-driven test-case synthesis", "Brute-force grounding for small inputs", "Consensus-based validation with LLM adjudication", "Hybrid input sampling (random/adversarial/LLM)", "Contamination-resistant competitive coding benchmark"], "summary": "本文提出UniCode框架，通过单题扩展、同类/跨类融合等进化策略自动生成竞赛级编程题，并结合穷举验证与多数共识（由LLM仲裁）合成高质量抗污染测试用例，构建了492道题的可扩展评测基准并评估了19个模型。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2510.17868v1", "published": "2025-10-16", "update_time": "2025-10-16", "download_time": "2025-12-12 23:49:41"}
{"id": "2510.26130", "title": "Beyond Synthetic Benchmarks: Evaluating LLM Performance on Real-World Class-Level Code Generation", "abstract": "Large language models (LLMs) have demonstrated strong performance on function-level code generation benchmarks, yet real-world software development increasingly demands class-level implementations that integrate multiple methods, attributes, and dependencies within authentic project contexts. This gap between benchmark performance and practical utility raises critical questions about LLMs' readiness for production code assistance, particularly regarding their ability to generalize across familiar and novel codebases.   We introduce a benchmark derived from real-world open-source repositories, comprising classes divided into seen and unseen partitions to evaluate generalization under practical conditions. We systematically examine how input specification completeness and retrieval-augmented generation affect class-level correctness across multiple state-of-the-art LLMs.   Our evaluation reveals a substantial performance gap: while LLMs achieve 84 to 89% correctness on synthetic benchmarks, they attain only 25 to 34% on real-world class tasks, with minimal distinction between familiar and novel codebases. Comprehensive documentation provides marginal improvements (1 to 3%), whereas retrieval augmentation yields greater gains (4 to 7%) by supplying concrete implementation patterns. Error analysis identifies AttributeError, TypeError, and AssertionError as dominant failure modes, with distinct patterns between synthetic and real-world scenarios.   These findings provide actionable insights for enhancing context modelling, documentation strategies, and retrieval integration in production code assistance tools.", "arxiv_url": "https://arxiv.org/abs/2510.26130", "authors": ["Musfiqur Rahman", "SayedHassan Khatoonabadi", "Emad Shihab"], "first_author": "Musfiqur Rahman", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Real-world class-level benchmark", "Seen-vs-unseen repository split", "Retrieval-augmented generation evaluation", "Docstring completeness analysis", "Error taxonomy (AttributeError/TypeError/AssertionError)", "Functional correctness measurement", "Repository-level dependency modeling"], "summary": "本文构建并公开了来自真实开源项目的类级代码基准（包含seen/unseen划分），系统评估LLM在真实类实现上的生成表现，分析文档完整性与检索增强对函数正确性与错误类型的影响，并发现真实场景下的性能显著低于合成基准且RAG能带来小幅提升。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2510.26130v2", "published": "2025-10-30", "update_time": "2025-11-04", "download_time": "2025-12-12 23:50:47"}
{"id": "2404.09486", "title": "MMCode: Benchmarking Multimodal Large Language Models for Code Generation with Visually Rich Programming Problems", "abstract": "Programming often involves converting detailed and complex specifications into code, a process during which developers typically utilize visual aids to more effectively convey concepts. While recent developments in Large Multimodal Models have demonstrated remarkable abilities in visual reasoning and mathematical tasks, there is little work on investigating whether these models can effectively interpret visual elements for code generation. To this end, we present MMCode, the first multi-modal coding dataset for evaluating algorithmic problem-solving skills in visually rich contexts. MMCode contains 3,548 questions and 6,620 images collected from real-world programming challenges harvested from 10 code competition websites, presenting significant challenges due to the extreme demand for reasoning abilities. Our experiment results show that current state-of-the-art models struggle to solve these problems. The results highlight the lack of powerful vision-code models, and we hope MMCode can serve as an inspiration for future works in this domain. The data and code are publicly available at https://github.com/likaixin2000/MMCode.", "arxiv_url": "https://arxiv.org/abs/2404.09486", "authors": ["Kaixin Li", "Yuchen Tian", "Qisheng Hu", "Ziyang Luo", "Zhiyong Huang", "Jing Ma"], "first_author": "Kaixin Li", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Multimodal code generation", "Visually-rich programming problems", "Contest-sourced problem collection", "Automatic test-case judging", "Image-augmented algorithmic challenges", "Data curation pipeline (crawler + human filtering)", "Long-horizon symbolic reasoning"], "summary": "本文提出MMCode——一个来自编程竞赛网站的带图像的多模态代码生成基准，并通过自动判题评估现有多模态大模型，发现其在复杂视觉增强算法题上表现显著不足。", "quality": "High", "conference": "EMNLP 2024", "pdf_url": "https://arxiv.org/pdf/2404.09486v2", "published": "2024-04-15", "update_time": "2024-09-26", "download_time": "2025-12-12 23:52:47"}
{"id": "2405.07990", "title": "Plot2Code: A Comprehensive Benchmark for Evaluating Multi-modal Large Language Models in Code Generation from Scientific Plots", "abstract": "The remarkable progress of Multi-modal Large Language Models (MLLMs) has attracted significant attention due to their superior performance in visual contexts. However, their capabilities in turning visual figure to executable code, have not been evaluated thoroughly. To address this, we introduce Plot2Code, a comprehensive visual coding benchmark designed for a fair and in-depth assessment of MLLMs. We carefully collect 132 manually selected high-quality matplotlib plots across six plot types from publicly available matplotlib galleries. For each plot, we carefully offer its source code, and an descriptive instruction summarized by GPT-4. This approach enables Plot2Code to extensively evaluate MLLMs' code capabilities across various input modalities. Furthermore, we propose three automatic evaluation metrics, including code pass rate, text-match ratio, and GPT-4V overall rating, for a fine-grained assessment of the output code and rendered images. Instead of simply judging pass or fail, we employ GPT-4V to make an overall judgement between the generated and reference images, which has been shown to be consistent with human evaluation. The evaluation results, which include analyses of 14 MLLMs such as the proprietary GPT-4V, Gemini-Pro, and the open-sourced Mini-Gemini, highlight the substantial challenges presented by Plot2Code. With Plot2Code, we reveal that most existing MLLMs struggle with visual coding for text-dense plots, heavily relying on textual instruction. We hope that the evaluation results from Plot2Code on visual coding will guide the future development of MLLMs. All data involved with Plot2Code are available at https://huggingface.co/datasets/TencentARC/Plot2Code.", "arxiv_url": "https://arxiv.org/abs/2405.07990", "authors": ["Chengyue Wu", "Yixiao Ge", "Qiushan Guo", "Jiahao Wang", "Zhixuan Liang", "Zeyu Lu", "Ying Shan", "Ping Luo"], "first_author": "Chengyue Wu", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Prompting", "tags": ["Matplotlib code generation", "Visual-to-code synthesis", "Multi-modal evaluation settings", "LLM-mediated image comparison", "Code pass rate metric", "Text-match ratio metric", "Text-dense plot parsing", "Plot reconstruction fidelity"], "summary": "该论文提出了Plot2Code，一个包含132个高质量matplotlib图及其源码与描述的多模态可执行代码基准，设计了多种输入/输出评估设置和自动化指标（如代码通过率、文本匹配率与基于视觉LLM的图像评分），并在14个多模态大模型上评测以揭示当前模型在从图像生成绘图代码方面的挑战与不足。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2405.07990v1", "published": "2024-05-13", "update_time": "2024-05-13", "download_time": "2025-12-12 23:54:03"}
{"id": "2406.09961", "title": "ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation", "abstract": "We introduce a new benchmark, ChartMimic, aimed at assessing the visually-grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual charts and textual instructions as inputs, requiring LMMs to generate the corresponding code for chart rendering. ChartMimic includes 4,800 human-curated (figure, instruction, code) triplets, which represent the authentic chart use cases found in scientific papers across various domains (e.g., Physics, Computer Science, Economics, etc). These charts span 18 regular types and 4 advanced types, diversifying into 201 subcategories. Furthermore, we propose multi-level evaluation metrics to provide an automatic and thorough assessment of the output code and the rendered charts. Unlike existing code generation benchmarks, ChartMimic places emphasis on evaluating LMMs' capacity to harmonize a blend of cognitive capabilities, encompassing visual understanding, code generation, and cross-modal reasoning. The evaluation of $3$ proprietary models and 14 open-weight models highlights the substantial challenges posed by ChartMimic. Even the advanced GPT-4o, InternVL2-Llama3-76B only achieved an average score across Direct Mimic and Customized Mimic tasks of 82.2 and 61.6, respectively, indicating significant room for improvement. We anticipate that ChartMimic will inspire the development of LMMs, advancing the pursuit of artificial general intelligence.", "arxiv_url": "https://arxiv.org/abs/2406.09961", "authors": ["Cheng Yang", "Chufan Shi", "Yaxin Liu", "Bo Shui", "Junjie Wang", "Mohan Jing", "Linran Xu", "Xinyu Zhu", "Siheng Li", "Yuxiang Zhang", "Gongye Liu", "Xiaomei Nie", "Deng Cai", "Yujiu Yang"], "first_author": "Cheng Yang", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Chart-to-Code Generation", "tags": ["Chart-to-code generation", "Multimodal visual understanding", "Chart rendering code synthesis", "Direct Mimic and Customized Mimic tasks", "Human-curated (figure,instruction,code) triplets", "Multi-level execution and visual-similarity metrics", "Chart taxonomy (types and subcategories)", "Prompting analysis and System 2 self-reflection", "Hallucination and error analysis"], "summary": "本文提出ChartMimic基准，收集4800个人工标注的图表-指令-代码三元组并定义Direct Mimic与Customized Mimic两项任务与多层次自动评估指标，用以评测大规模多模态模型在图表到代码生成中的视觉理解、跨模态推理与代码合成能力并进行广泛实验与分析。", "quality": "High", "conference": "ICLR (International Conference on Learning Representations) 2025", "pdf_url": "https://arxiv.org/pdf/2406.09961v2", "published": "2024-06-14", "update_time": "2025-02-28", "download_time": "2025-12-12 23:57:00"}
{"id": "2410.12381", "title": "HumanEval-V: Benchmarking High-Level Visual Reasoning with Complex Diagrams in Coding Tasks", "abstract": "Understanding and reasoning over diagrams is a fundamental aspect of human intelligence. While Large Multimodal Models (LMMs) have demonstrated impressive capabilities across various tasks, existing benchmarks lack comprehensive evaluation of their diagram interpretation and reasoning abilities, particularly in coding contexts. We present HumanEval-V, a rigorous benchmark of human-annotated coding tasks that spans six task types and evaluates diverse visual reasoning capabilities. Each task features carefully crafted diagrams paired with function signatures and test cases, employing novel code generation tasks to thoroughly assess models' diagram comprehension. Through extensive experiments with 22 LMMs, we find that even top-performing models achieve modest success rates, with Claude 3.5 Sonnet reaching only 36.8% pass@1, highlighting substantial room for improvement. Our analysis reveals that current LMMs struggle with spatial transformations, topological relationships, and dynamic patterns that humans find intuitive. These findings provide valuable insights for advancing LMMs' visual reasoning abilities. We have open-sourced our code and benchmark at https://github.com/HumanEval-V/HumanEval-V-Benchmark.", "arxiv_url": "https://arxiv.org/abs/2410.12381", "authors": ["Fengji Zhang", "Linquan Wu", "Huiyu Bai", "Guancheng Lin", "Xiao Li", "Xiao Yu", "Yue Wang", "Bei Chen", "Jacky Keung"], "first_author": "Fengji Zhang", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Diagram-to-code evaluation", "High-level visual reasoning", "Spatial transformation reasoning", "Topological relationship reasoning", "Dynamic pattern reasoning", "Two-stage evaluation (diagram description then coding)", "Human-annotated coding tasks", "Test-case-based correctness verification", "LLM-assisted data diversification", "Execution-guided self-refinement"], "summary": "本文提出HumanEval-V——一个包含253个人工注释的图示编程任务基准，用以评估模型对复杂图表的高层视觉推理（如空间变换、拓扑关系和动态模式）的理解能力，并通过对22个多模态模型的大规模实验揭示当前模型在该类任务上存在显著不足。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2410.12381v3", "published": "2024-10-16", "update_time": "2025-02-18", "download_time": "2025-12-13 00:03:46"}
{"id": "2411.00264", "title": "TurtleBench: A Visual Programming Benchmark in Turtle Geometry", "abstract": "Humans have the ability to reason about geometric patterns in images and scenes from a young age. However, developing large multimodal models (LMMs) capable of similar reasoning remains a challenge, highlighting the need for robust evaluation methods to assess these capabilities. We introduce \\Turtle, a benchmark designed to evaluate LMMs' capacity to interpret geometric patterns -- given visual examples, textual instructions, or both -- and generate precise code outputs. Inspired by turtle geometry, a notion used to teach children foundational coding and geometric concepts, TurtleBench features tasks with patterned shapes that have underlying algorithmic logic. Our evaluation reveals that leading LMMs struggle significantly with these tasks, with GPT-4o achieving only 19\\% accuracy on the simplest tasks and few-shot prompting only marginally improves their performance ($<2\\%$). \\Turtle highlights the gap between human and AI performance in intuitive and visual geometrical understanding, setting the stage for future research in this area. \\Turtle stands as one of the few benchmarks to evaluate the integration of visual understanding and code generation capabilities in LMMs, setting the stage for future research. Code and Dataset for this paper is provided here: \\href{https://github.com/sinaris76/TurtleBench}{https://github.com/sinaris76/TurtleBench}", "arxiv_url": "https://arxiv.org/abs/2411.00264", "authors": ["Sina Rismanchian", "Yasaman Razeghi", "Sameer Singh", "Shayan Doroudi"], "first_author": "Sina Rismanchian", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Turtle geometry", "Visual-to-code synthesis", "Image-guided code editing", "Multimodal benchmark", "Automated render-based evaluation", "Bitwise image similarity metric", "Scratch vs Tweak task taxonomy", "Sandboxed code execution"], "summary": "本文提出TurtleBench——一个包含260个基于海龟几何的图像/文本到代码的多模态视觉编程基准，提供自动化执行与像素相似性评估并实证显示当前多模态模型在此类视觉几何到代码任务上性能较差。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2411.00264v2", "published": "2024-10-31", "update_time": "2025-04-11", "download_time": "2025-12-13 00:04:46"}
{"id": "2412.04626", "title": "BigDocs: An Open Dataset for Training Multimodal Models on Document and Code Tasks", "abstract": "Multimodal AI has the potential to significantly enhance document-understanding tasks, such as processing receipts, understanding workflows, extracting data from documents, and summarizing reports. Code generation tasks that require long-structured outputs can also be enhanced by multimodality. Despite this, their use in commercial applications is often limited due to limited access to training data and restrictive licensing, which hinders open access. To address these limitations, we introduce BigDocs-7.5M, a high-quality, open-access dataset comprising 7.5 million multimodal documents across 30 tasks. We use an efficient data curation process to ensure our data is high-quality and license-permissive. Our process emphasizes accountability, responsibility, and transparency through filtering rules, traceable metadata, and careful content analysis. Additionally, we introduce BigDocs-Bench, a benchmark suite with 10 novel tasks where we create datasets that reflect real-world use cases involving reasoning over Graphical User Interfaces (GUI) and code generation from images. Our experiments show that training with BigDocs-Bench improves average performance up to 25.8% over closed-source GPT-4o in document reasoning and structured output tasks such as Screenshot2HTML or Image2Latex generation. Finally, human evaluations showed a preference for outputs from models trained on BigDocs over GPT-4o. This suggests that BigDocs can help both academics and the open-source community utilize and improve AI tools to enhance multimodal capabilities and document reasoning. The project is hosted at https://bigdocs.github.io .", "arxiv_url": "https://arxiv.org/abs/2412.04626", "authors": ["Juan Rodriguez", "Xiangru Jian", "Siba Smarak Panigrahi", "Tianyu Zhang", "Aarash Feizi", "Abhay Puri", "Akshay Kalkunte", "François Savard", "Ahmed Masry", "Shravan Nayak", "Rabiul Awal", "Mahsa Massoud", "Amirhossein Abaskohi", "Zichao Li", "Suyuchen Wang", "Pierre-André Noël", "Mats Leon Richter", "Saverio Vadacchino", "Shubham Agarwal", "Sanket Biswas", "Sara Shanian", "Ying Zhang", "Noah Bolger", "Kurt MacDonald", "Simon Fauvel", "Sathwik Tejaswi", "Srinivas Sunkara", "Joao Monteiro", "Krishnamurthy DJ Dvijotham", "Torsten Scholak", "Nicolas Chapados", "Sepideh Kharagani", "Sean Hughes", "M. Özsu", "Siva Reddy", "Marco Pedersoli", "Yoshua Bengio", "Christopher Pal", "Issam Laradji", "Spandana Gella", "Perouz Taslakian", "David Vazquez", "Sai Rajeswar"], "first_author": "Juan Rodriguez", "category": ["Benchmark", "Technical"], "field": "Multimodal Document Understanding", "task": "Image-to-Structured-Code Generation (HTML/LaTeX/JSON/SVG)", "tags": ["License-permissive dataset curation", "Continual pretraining dataset for multimodal models", "Image-to-markup/code generation", "GUI / Screenshot-to-HTML tasks", "Unified metadata and dataset traceability", "Data contamination analysis", "Benchmark suite for long-structured outputs", "Data preprocessing & curation toolkit", "Human preference evaluation against closed-source models"], "summary": "该论文提出并开源BigDocs-7.5M大规模、许可友好的多模态文档数据集及BigDocs-Bench基准、工具包和模型，专注于图像到长结构化输出（如HTML/LaTeX/JSON/SVG）的生成与文档理解，并通过自动与人工评估展示了显著性能提升。", "quality": "High", "conference": "ICLR 2025", "pdf_url": "https://arxiv.org/pdf/2412.04626v2", "published": "2024-12-05", "update_time": "2025-03-17", "download_time": "2025-12-13 00:09:54"}
{"id": "2412.17315", "title": "CodeV: Issue Resolving with Visual Data", "abstract": "Large Language Models (LLMs) have advanced rapidly in recent years, with their applications in software engineering expanding to more complex repository-level tasks. GitHub issue resolving is a key challenge among these tasks. While recent approaches have made progress on this task, they focus on textual data within issues, neglecting visual data. However, this visual data is crucial for resolving issues as it conveys additional knowledge that text alone cannot. We propose CodeV, the first approach to leveraging visual data to enhance the issue-resolving capabilities of LLMs. CodeV resolves each issue by following a two-phase process: data processing and patch generation. To evaluate CodeV, we construct a benchmark for visual issue resolving, namely Visual SWE-bench. Through extensive experiments, we demonstrate the effectiveness of CodeV, as well as provide valuable insights into leveraging visual data to resolve GitHub issues.", "arxiv_url": "https://arxiv.org/abs/2412.17315", "authors": ["Linhao Zhang", "Daoguang Zan", "Quanshun Yang", "Zhirong Huang", "Dong Chen", "Bo Shen", "Tianyu Liu", "Yongshun Gong", "Pengjie Huang", "Xudong Lu", "Guangtai Liang", "Lizhen Cui", "Qianxiang Wang"], "first_author": "Linhao Zhang", "category": ["Technical", "Benchmark"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Visual GitHub issue resolving", "Vision–language fine-grained descriptions", "Structured issue summarization", "Multimodal prompt enrichment for patch generation", "Repository-level patch generation", "Low-cost multimodal pipeline", "Integration with agent and agentless resolvers"], "summary": "本文提出CODEV，通过视觉-语言模型对Issue中的截图/视频生成细粒度描述与结构化摘要，并将这些信息拼接到Issue中以辅助大模型生成修复补丁，同时构建了用于评估视觉问题修复的基准并验证了显著性能提升。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2412.17315v1", "published": "2024-12-23", "update_time": "2024-12-23", "download_time": "2025-12-13 00:12:22"}
{"id": "2502.11829", "title": "Code-Vision: Evaluating Multimodal LLMs Logic Understanding and Code Generation Capabilities", "abstract": "This paper introduces Code-Vision, a benchmark designed to evaluate the logical understanding and code generation capabilities of Multimodal Large Language Models (MLLMs). It challenges MLLMs to generate a correct program that fulfills specific functionality requirements based on a given flowchart, which visually represents the desired algorithm or process. Code-Vision comprises three subsets: HumanEval-V, Algorithm, and MATH, which evaluate MLLMs' coding abilities across basic programming, algorithmic, and mathematical problem-solving domains. Our experiments evaluate 12 MLLMs on Code-Vision. Experimental results demonstrate that there is a large performance difference between proprietary and open-source models. On Hard problems, GPT-4o can achieve 79.3% pass@1, but the best open-source model only achieves 15%. Further experiments reveal that Code-Vision can pose unique challenges compared to other multimodal reasoning benchmarks MMCode and MathVista. We also explore the reason for the poor performance of the open-source models. All data and codes are available at https://github.com/wanghanbinpanda/CodeVision.", "arxiv_url": "https://arxiv.org/abs/2502.11829", "authors": ["Hanbin Wang", "Xiaoxuan Zhou", "Zhipeng Xu", "Keyuan Cheng", "Yuxin Zuo", "Kai Tian", "Jingwei Song", "Junting Lu", "Wenhui Hu", "Xueyang Liu"], "first_author": "Hanbin Wang", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Flowchart-to-code translation", "Visual-centric code generation", "Multimodal benchmark construction", "Algorithmic and mathematical problems", "Test-case driven correctness evaluation", "Proprietary vs. open-source performance gap", "Failure-mode analysis (logical vs. syntactic)"], "summary": "提出 CODE-VISION 基准，通过以流程图为主的视觉输入评估多模态大模型的逻辑理解与代码生成能力，并对 12 款模型进行实证比较与错误分析。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2502.11829v1", "published": "2025-02-17", "update_time": "2025-02-17", "download_time": "2025-12-13 00:13:02"}
{"id": "2506.02073", "title": "Flow2Code: Evaluating Large Language Models for Flowchart-based Code Generation Capability", "abstract": "While large language models (LLMs) show promise in code generation, existing benchmarks neglect the flowchart-based code generation. To promote further research on flowchart-based code generation, this work presents Flow2Code, a novel benchmark for flowchart-based code generation evaluation. The evaluation dataset spans 15 programming languages and includes 5,622 code segments paired with 16,866 flowcharts of three types: code, UML, and pseudocode. Extensive experiments with 13 multimodal LLMs reveal that current LLMs can not generate code based on flowcharts perfectly. Besides, experiment results show that the supervised fine-tuning technique contributes greatly to the models' performance. We publicly release our code and datasets at https://github.com/hml-github/Flow2Code.", "arxiv_url": "https://arxiv.org/abs/2506.02073", "authors": ["Mengliang He", "Jiayi Zeng", "Yankai Jiang", "Wei Zhang", "Zeming Liu", "Xiaoming Shi", "Aimin Zhou"], "first_author": "Mengliang He", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Flowchart-to-code benchmark", "Multimodal code generation", "Code/UML/Pseudocode flowcharts", "Multilingual coverage (15 languages)", "DOT flowchart representation", "Human-in-the-loop verification", "Zero-shot vs supervised fine-tuning comparison", "Evaluation of 13 multimodal models", "Flowchart construction and transformation pipeline"], "summary": "本文提出了 Flow2Code，一个包含代码、UML 与伪代码三类流程图并覆盖15种编程语言的大规模多模态基准，并在13个多模态模型上评测，发现当前模型在流程图驱动的代码生成上仍存在明显不足且有监督微调能显著提升性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2506.02073v1", "published": "2025-06-02", "update_time": "2025-06-02", "download_time": "2025-12-13 00:13:38"}
{"id": "2512.10713", "title": "PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code", "abstract": "Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks.", "arxiv_url": "https://arxiv.org/abs/2512.10713", "authors": ["Itay Dreyfuss", "Antonio Abu Nassar", "Samuel Ackerman", "Axel Ben David", "Rami Katan", "Orna Raz", "Marcel Zalmanovici"], "first_author": "Itay Dreyfuss", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Automated benchmark generation", "Deterministic expected-result checking", "Code dry-running simulation", "Instruction concatenation pipelines", "Contamination-resistant variant generation", "Difficulty control via instruction count and output length", "Prompt vs chat mode evaluation"], "summary": "本文提出了PACIFIC框架，用可控的指令拼接与引用实现自动生成可确定性评估样本，以检测大型模型的代码逐步推理（干运行）与指令遵循能力，并通过可变难度和去污染的基准区分模型表现。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.10713v1", "published": "2025-12-11", "update_time": "2025-12-11", "download_time": "2025-12-13 01:45:41"}
{"id": "2512.10493", "title": "Decoding Human-LLM Collaboration in Coding: An Empirical Study of Multi-Turn Conversations in the Wild", "abstract": "Large language models (LLMs) are increasingly acting as dynamic conversational interfaces, supporting multi-turn interactions that mimic human-like conversation and facilitate complex tasks like coding. While datasets such as LMSYS-Chat-1M and WildChat capture real-world user-LLM conversations, few studies systematically explore the mechanisms of human-LLM collaboration in coding scenarios. What tortuous paths do users experience during the interaction process? How well do the LLMs follow instructions? Are users satisfied? In this paper, we conduct an empirical analysis on human-LLM coding collaboration using LMSYS-Chat-1M and WildChat datasets to explore the human-LLM collaboration mechanism, LLMs' instruction following ability, and human satisfaction. This study yields interesting findings: 1) Task types shape interaction patterns(linear, star and tree), with code quality optimization favoring linear patterns, design-driven tasks leaning toward tree structures, and queries preferring star patterns; 2) Bug fixing and code refactoring pose greater challenges to LLMs' instruction following, with non-compliance rates notably higher than in information querying; 3) Code quality optimization and requirements-driven development tasks show lower user satisfaction, whereas structured knowledge queries and algorithm designs yield higher levels. These insights offer recommendations for improving LLM interfaces and user satisfaction in coding collaborations, while highlighting avenues for future research on adaptive dialogue systems. We believe this work broadens understanding of human-LLM synergies and supports more effective AI-assisted development.", "arxiv_url": "https://arxiv.org/abs/2512.10493", "authors": ["Binquan Zhang", "Li Zhang", "Haoyuan Zhang", "Fang Liu", "Song Wang", "Bo Shen", "An Fu", "Lin Shi"], "first_author": "Binquan Zhang", "category": ["Empirical"], "field": "Coding Assistant", "task": "Code Prompting", "tags": ["Multi-turn dialogue patterns", "Interaction topology (linear/star/tree)", "Instruction-following compliance", "User satisfaction trajectory", "Task taxonomy for coding interactions", "LLM-assisted data disentanglement", "Open card sorting annotation", "Statistical significance testing of interaction effects"], "summary": "本文基于真实多轮对话数据实证分析了编码场景下的人-LLM协作，归纳出五类任务与线性/星状/树状三种交互模式，评估模型指令遵循能力与用户满意度并提出改进建议。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.10493v1", "published": "2025-12-11", "update_time": "2025-12-11", "download_time": "2025-12-13 01:46:15"}
{"id": "2512.10789", "title": "Natural Language Interface for Firewall Configuration", "abstract": "This paper presents the design and prototype implementation of a natural language interface for configuring enterprise firewalls. The framework allows administrators to express access control policies in plain language, which are then translated into vendor specific configurations. A compact schema bound intermediate representation separates human intent from device syntax and in the current prototype compiles to Palo Alto PAN OS command line configuration while remaining extensible to other platforms. Large language models are used only as assistive parsers that generate typed intermediate representation objects, while compilation and enforcement remain deterministic. The prototype integrates three validation layers, namely a static linter that checks structural and vendor specific constraints, a safety gate that blocks overly permissive rules such as any to any allows, and a Batfish based simulator that validates configuration syntax and referential integrity against a synthetic device model. The paper describes the architecture, implementation, and test methodology on synthetic network context datasets and discusses how this approach can evolve into a scalable auditable and human centered workflow for firewall policy management.", "arxiv_url": "https://arxiv.org/abs/2512.10789", "authors": ["F. Taghiyev", "A. Aslanbayli"], "first_author": "F. Taghiyev", "category": ["Technical"], "field": "Network Security & Configuration", "task": "Natural Language to Firewall Configuration", "tags": ["Schema-bound intermediate representation", "Schema-constrained LLM parsing", "Resolver agent for entity grounding", "Vendor-agnostic compiler (Palo Alto backend prototype)", "IR linter (general and vendor-specific)", "Safety Gate preventing any-to-any and missing-fields", "Batfish-based configuration simulation", "Role-conditioned prompting and constrained decoding", "Least-privilege policy synthesis", "Audit logging and stage-only (side-effect free) compilation"], "summary": "本文提出并实现了一个原型系统，利用受约束的LLM将管理员的自然语言访问策略解析为类型化中间表示，并通过静态lint、安全门控和Batfish仿真将其无副作用地编译为供应商特定的防火墙配置以确保安全与可审计性。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.10789v1", "published": "2025-12-11", "update_time": "2025-12-11", "download_time": "2025-12-13 01:50:04"}
{"id": "2512.10563", "title": "NormCode: A Semi-Formal Language for Context-Isolated AI Planning", "abstract": "Multistep workflows that chain large language model (LLM) calls suffer from context pollution: as information accumulates across steps, models hallucinate, confuse intermediate outputs, and lose track of task constraints. We present NormCode, a semiformal language for constructing plans of inferences, structured decompositions where each step operates in data isolation and receives only explicitly passed inputs, which eliminates crossstep contamination by design. NormCode enforces a strict separation between semantic operations (LLMdriven reasoning, nondeterministic) and syntactic operations (deterministic data restructuring), enabling precise cost and reliability tracing. The language exists in three isomorphic formats: .ncds for human authoring, .ncd for machine execution, and .ncn for human verification, supporting progressive formalization from sketch to production. We validate NormCode through two demonstrations: (1) a base X addition algorithm achieving 100 percent accuracy on arbitrary length inputs, and (2) self hosted execution of NormCode's own five phase compiler pipeline. The working orchestrator provides dependency driven scheduling, SQLite backed checkpointing, and loop management, making AI workflows auditable by design and addressing a critical need for transparency in high stakes domains such as legal reasoning, medical decision making, and financial analysis.", "arxiv_url": "https://arxiv.org/abs/2512.10563", "authors": ["Xin Guan"], "first_author": "Xin Guan", "category": ["Technical"], "field": "Requirements & Design", "task": "Specification & Validation", "tags": ["Context Isolation", "Inference Decomposition", "Semi-Formal Intermediate Representation", "Semantic vs Syntactic Separation", "Progressive Formalization (.ncds/.ncd/.ncn)", "Auditable AI Workflows", "Dependency-Driven Orchestration", "Checkpointed Execution and Loop Management"], "summary": "NormCode提出一种半形式化语言与执行框架，通过在多步LLM推理中强制显式数据隔离、将语义（非确定性LLM推理）与句法（确定性数据重构）分离，并提供三种同构格式与可检查的编译/运行时支持，从而实现可审计、可靠的AI工作流编排与执行。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.10563v1", "published": "2025-12-11", "update_time": "2025-12-11", "download_time": "2025-12-13 01:53:27"}
{"id": "2512.10485", "title": "From Lab to Reality: A Practical Evaluation of Deep Learning Models and LLMs for Vulnerability Detection", "abstract": "Vulnerability detection methods based on deep learning (DL) have shown strong performance on benchmark datasets, yet their real-world effectiveness remains underexplored. Recent work suggests that both graph neural network (GNN)-based and transformer-based models, including large language models (LLMs), yield promising results when evaluated on curated benchmark datasets. These datasets are typically characterized by consistent data distributions and heuristic or partially noisy labels. In this study, we systematically evaluate two representative DL models-ReVeal and LineVul-across four representative datasets: Juliet, Devign, BigVul, and ICVul. Each model is trained independently on each respective dataset, and their code representations are analyzed using t-SNE to uncover vulnerability related patterns. To assess realistic applicability, we deploy these models along with four pretrained LLMs, Claude 3.5 Sonnet, GPT-o3-mini, GPT-4o, and GPT-5 on a curated dataset, VentiVul, comprising 20 recently (May 2025) fixed vulnerabilities from the Linux kernel. Our experiments reveal that current models struggle to distinguish vulnerable from non-vulnerable code in representation space and generalize poorly across datasets with differing distributions. When evaluated on VentiVul, our newly constructed time-wise out-of-distribution dataset, performance drops sharply, with most models failing to detect vulnerabilities reliably. These results expose a persistent gap between academic benchmarks and real-world deployment, emphasizing the value of our deployment-oriented evaluation framework and the need for more robust code representations and higher-quality datasets.", "arxiv_url": "https://arxiv.org/abs/2512.10485", "authors": ["Chaomeng Lu", "Bert Lagaisse"], "first_author": "Chaomeng Lu", "category": ["Empirical", "Benchmark"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Time-wise out-of-distribution evaluation", "Whole-file evaluation mode", "Function-pair evaluation mode", "Cross-dataset generalization", "Code representation clustering (t-SNE & centroid distance)", "Graph-based vs token-based code representations", "Zero-shot LLM vulnerability probing", "Linux CVE fix-based testbed", "Label quality and dataset bias analysis"], "summary": "本文构建了一个时间分离的真实漏洞测试集并提出部署导向的评估框架，系统性比较图/序列深度模型与LLM在漏洞检测上的表示能力与跨数据集泛化性，结果表明现有模型在真实场景中表现显著下降。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.10485v1", "published": "2025-12-11", "update_time": "2025-12-11", "download_time": "2025-12-14 01:58:04"}
{"id": "2512.10452", "title": "UniCoR: Modality Collaboration for Robust Cross-Language Hybrid Code Retrieval", "abstract": "Effective code retrieval is indispensable and it has become an important paradigm to search code in hybrid mode using both natural language and code snippets. Nevertheless, it remains unclear whether existing approaches can effectively leverage such hybrid queries, particularly in cross-language contexts. We conduct a comprehensive empirical study of representative code models and reveal three challenges: (1) insufficient semantic understanding; (2) inefficient fusion in hybrid code retrieval; and (3) weak generalization in cross-language scenarios. To address these challenges, we propose UniCoR, a novel self-supervised framework that learns Unified Code Representations framework designed to learn unified and robust code representations. Firstly, we design a multi-perspective supervised contrastive learning module to enhance semantic understanding and modality fusion. It aligns representations from multiple perspectives, including code-to-code, natural language-to-code, and natural language-to-natural language, enforcing the model to capture a semantic essence among modalities. Secondly, we introduce a representation distribution consistency learning module to improve cross-language generalization, which explicitly aligns the feature distributions of different programming languages, enabling language-agnostic representation learning. Extensive experiments on both empirical benchmark and large-scale benchmark show that UniCoR outperforms all baseline models, achieving an average improvement of 8.64% in MRR and 11.54% in MAP over the best-performing baseline. Furthermore, UniCoR exhibits stability in hybrid code retrieval and generalization capability in cross-language scenarios.", "arxiv_url": "https://arxiv.org/abs/2512.10452", "authors": ["Yang Yang", "Li Kuang", "Jiakun Liu", "Zhongxin Liu", "Yingjie Xia", "David Lo"], "first_author": "Yang Yang", "category": ["Technical", "Empirical", "Benchmark"], "field": "Coding Assistant", "task": "Code Retrieval", "tags": ["Hybrid query fusion (code + natural language)", "Cross-language alignment", "Multi-perspective supervised contrastive learning", "Representation distribution consistency", "Maximum Mean Discrepancy (MMD) for language-agnostic alignment", "Modality collaboration and fusion", "Functionally equivalent code pairing", "Automated natural-language query generation", "Self-supervised code representation learning", "Fusion strategies: input remix / vector concat / score weighting"], "summary": "本文提出UniCoR，一种结合多视角监督对比学习与表示分布一致性约束的自监督框架，通过强化模态协同与跨语言对齐，显著提升混合（代码+自然语言）跨语言代码检索的语义理解、融合效果与泛化能力。", "quality": "High", "conference": "IEEE/ACM International Conference on Software Engineering (ICSE 2026)", "pdf_url": "https://arxiv.org/pdf/2512.10452v1", "published": "2025-12-11", "update_time": "2025-12-11", "download_time": "2025-12-14 01:58:31"}
{"id": "2512.10415", "title": "How to Trick Your AI TA: A Systematic Study of Academic Jailbreaking in LLM Code Evaluation", "abstract": "The use of Large Language Models (LLMs) as automatic judges for code evaluation is becoming increasingly prevalent in academic environments. But their reliability can be compromised by students who may employ adversarial prompting strategies in order to induce misgrading and secure undeserved academic advantages. In this paper, we present the first large-scale study of jailbreaking LLM-based automated code evaluators in academic context. Our contributions are: (i) We systematically adapt 20+ jailbreaking strategies for jailbreaking AI code evaluators in the academic context, defining a new class of attacks termed academic jailbreaking. (ii) We release a poisoned dataset of 25K adversarial student submissions, specifically designed for the academic code-evaluation setting, sourced from diverse real-world coursework and paired with rubrics and human-graded references, and (iii) In order to capture the multidimensional impact of academic jailbreaking, we systematically adapt and define three jailbreaking metrics (Jailbreak Success Rate, Score Inflation, and Harmfulness). (iv) We comprehensively evalulate the academic jailbreaking attacks using six LLMs. We find that these models exhibit significant vulnerability, particularly to persuasive and role-play-based attacks (up to 97% JSR). Our adversarial dataset and benchmark suite lay the groundwork for next-generation robust LLM-based evaluators in academic code assessment.", "arxiv_url": "https://arxiv.org/abs/2512.10415", "authors": ["Devanshu Sahoo", "Vasudev Majhi", "Arjun Neekhra", "Yash Sinha", "Murari Mandal", "Dhruv Kumar"], "first_author": "Devanshu Sahoo", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Understanding", "tags": ["Academic Jailbreaking", "Adversarial Prompt Injection", "Poisoned Student Submissions", "Rubric-aware Attack Design", "Jailbreak Metrics (JSR/SIR/Harmfulness)", "Persona/Role-play Attacks", "Token/Emoji-level Manipulation", "Cross-model Robustness Benchmark"], "summary": "本文首次系统研究了针对LLM自动代码评分器的“学术越狱”攻击，构建了约25K条对抗性学生提交数据集、提出专门的评价指标并在六种模型上进行基准评估，揭示了角色扮演与说服类攻击下的严重脆弱性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.10415v1", "published": "2025-12-11", "update_time": "2025-12-11", "download_time": "2025-12-14 02:06:36"}
{"id": "2512.10398", "title": "Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale", "abstract": "Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale.", "arxiv_url": "https://arxiv.org/abs/2512.10398", "authors": ["Zhaodong Wang", "Zhenting Qi", "Sherman Wong", "Nathan Hu", "Samuel Lin", "Jun Ge", "Erwin Gao", "Yining Yang", "Ben Maurer", "Wenlin Chen", "David Recordon", "Yilun Du", "Minlan Yu", "Ying Zhang"], "first_author": "Zhaodong Wang", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Hierarchical working memory", "Adaptive context compression", "Persistent note-taking", "Cross-session continual learning", "Meta-agent build–test–improve loop", "Modular tool extensions with typed callbacks", "Agent orchestration for long-horizon tasks", "Developer observability and DX-focused tooling", "Industrial-scale repository navigation", "Ablation-driven agent evaluation"], "summary": "本文提出并开源了Confucius SDK及其实例化的Confucius Code Agent，通过层次化工作内存、自适应上下文压缩、持久化笔记和模块化扩展，并借助元代理的构建-测试-改进循环，实现面向工业规模代码库的长期推理与跨会话记忆，在多种软件工程任务上表现优异。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.10398v1", "published": "2025-12-11", "update_time": "2025-12-11", "download_time": "2025-12-14 02:07:20"}
{"id": "2512.10393", "title": "Cross-modal Retrieval Models for Stripped Binary Analysis", "abstract": "LLM-agent based binary code analysis has demonstrated significant potential across a wide range of software security scenarios, including vulnerability detection, malware analysis, etc. In agent workflow, however, retrieving the positive from thousands of stripped binary functions based on user query remains under-studied and challenging, as the absence of symbolic information distinguishes it from source code retrieval. In this paper, we introduce, BinSeek, the first two-stage cross-modal retrieval framework for stripped binary code analysis. It consists of two models: BinSeekEmbedding is trained on large-scale dataset to learn the semantic relevance of the binary code and the natural language description, furthermore, BinSeek-Reranker learns to carefully judge the relevance of the candidate code to the description with context augmentation. To this end, we built an LLM-based data synthesis pipeline to automate training construction, also deriving a domain benchmark for future research. Our evaluation results show that BinSeek achieved the state-of-the-art performance, surpassing the the same scale models by 31.42% in Rec@3 and 27.17% in MRR@3, as well as leading the advanced general-purpose models that have 16 times larger parameters.", "arxiv_url": "https://arxiv.org/abs/2512.10393", "authors": ["Guoqiang Chen", "Lingyun Ying", "Ziyang Song", "Daguang Liu", "Qiang Wang", "Zhiqi Wang", "Li Hu", "Shaoyin Cheng", "Weiming Zhang", "Nenghai Yu"], "first_author": "Guoqiang Chen", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Retrieval", "tags": ["stripped-binary retrieval", "decompiled pseudocode embeddings", "two-stage retrieval (embedding + reranker)", "context-augmented reranking with calling-context", "LLM-driven synthetic label generation", "contrastive embedding training (InfoNCE)", "domain-specific binary retrieval benchmark", "retrieval-augmented binary analysis for security"], "summary": "本文提出了BinSeek，一种针对无符号（stripped）二进制函数的两阶段跨模态检索框架（嵌入召回 + 上下文增强重排序），并通过LLM驱动的数据合成构建训练集与首个领域基准，显著提升二进制代码检索性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.10393v1", "published": "2025-12-11", "update_time": "2025-12-11", "download_time": "2025-12-15 01:56:55"}
{"id": "2512.10238", "title": "Studying and Automating Issue Resolution for Software Quality", "abstract": "Effective issue resolution is crucial for maintaining software quality. Yet developers frequently encounter challenges such as low-quality issue reports, limited understanding of real-world workflows, and a lack of automated support. This research aims to address these challenges through three complementary directions. First, we enhance issue report quality by proposing techniques that leverage LLM reasoning and application-specific information. Second, we empirically characterize developer workflows in both traditional and AI-augmented systems. Third, we automate cognitively demanding resolution tasks, including buggy UI localization and solution identification, through ML, DL, and LLM-based approaches. Together, our work delivers empirical insights, practical tools, and automated methods to advance AI-driven issue resolution, supporting more maintainable and high-quality software systems.", "arxiv_url": "https://arxiv.org/abs/2512.10238", "authors": ["Antu Saha"], "first_author": "Antu Saha", "category": ["Technical", "Empirical"], "field": "Quality Management", "task": "Bug Localization", "tags": ["Issue report quality assessment", "Steps-to-Reproduce (S2R) enhancement", "LLM reasoning aligned with dynamic app execution", "Execution model of screens and interactions", "Context-aware issue report generation", "Buggy UI localization", "Multi-modal text+vision retrieval for UI mapping", "Solution identification in issue discussions", "Empirical analysis of issue resolution workflows", "Cross-project generalizability of classifiers"], "summary": "本文结合基于应用执行模型的多模态信息与LLM推理，提升缺陷报告（OB/EB/S2R）质量，实证分析传统与AI/ML集成系统的问题解决流程，并自动化实现Buggy UI定位与解决方案识别以加速问题定位与修复。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.10238v1", "published": "2025-12-11", "update_time": "2025-12-11", "download_time": "2025-12-15 01:57:20"}
{"id": "2512.10501", "title": "Zero-shot 3D Map Generation with LLM Agents: A Dual-Agent Architecture for Procedural Content Generation", "abstract": "Procedural Content Generation (PCG) offers scalable methods for algorithmically creating complex, customizable worlds. However, controlling these pipelines requires the precise configuration of opaque technical parameters. We propose a training-free architecture that utilizes LLM agents for zero-shot PCG parameter configuration. While Large Language Models (LLMs) promise a natural language interface for PCG tools, off-the-shelf models often fail to bridge the semantic gap between abstract user instructions and strict parameter specifications. Our system pairs an Actor agent with a Critic agent, enabling an iterative workflow where the system autonomously reasons over tool parameters and refines configurations to progressively align with human design preferences. We validate this approach on the generation of various 3D maps, establishing a new benchmark for instruction-following in PCG. Experiments demonstrate that our approach outperforms single-agent baselines, producing diverse and structurally valid environments from natural language descriptions. These results demonstrate that off-the-shelf LLMs can be effectively repurposed as generalized agents for arbitrary PCG tools. By shifting the burden from model training to architectural reasoning, our method offers a scalable framework for mastering complex software without task-specific fine-tuning.", "arxiv_url": "https://arxiv.org/abs/2512.10501", "authors": ["Lim Chien Her", "Ming Yan", "Yunshu Bai", "Ruihao Li", "Hao Zhang"], "first_author": "Lim Chien Her", "category": ["Technical", "Benchmark"], "field": "Procedural Content Generation", "task": "Zero-shot PCG parameter configuration for 3D map generation (dual-agent Actor–Critic)", "tags": ["Dual-agent Actor–Critic architecture", "Zero-shot parameter synthesis", "Documentation-grounded verification", "Iterative agent dialogic refinement", "Training-free LLM tool control", "3D map / terrain synthesis", "PCG parameter hallucination correction", "Instruction-following benchmark for PCG"], "summary": "本文提出一种训练免费、双代理（Actor–Critic）的架构，利用离线大模型在零样本情形下通过迭代对话将自然语言提示映射为可执行的 PCG 参数，从而生成多样且结构有效的 3D 地图，并在新建的指令跟随基准上显著优于单代理基线。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.10501v1", "published": "2025-12-11", "update_time": "2025-12-11", "download_time": "2025-12-15 02:05:01"}
{"id": "2512.10317", "title": "Translating Informal Proofs into Formal Proofs Using a Chain of States", "abstract": "We address the problem of translating informal mathematical proofs expressed in natural language into formal proofs in Lean4 under a constrained computational budget. Our approach is grounded in two key insights. First, informal proofs tend to proceed via a sequence of logical transitions - often implications or equivalences - without explicitly specifying intermediate results or auxiliary lemmas. In contrast, formal systems like Lean require an explicit representation of each proof state and the tactics that connect them. Second, each informal reasoning step can be viewed as an abstract transformation between proof states, but identifying the corresponding formal tactics often requires nontrivial domain knowledge and precise control over proof context. To bridge this gap, we propose a two stage framework. Rather than generating formal tactics directly, we first extract a Chain of States (CoS), a sequence of intermediate formal proof states aligned with the logical structure of the informal argument. We then generate tactics to transition between adjacent states in the CoS, thereby constructing the full formal proof. This intermediate representation significantly reduces the complexity of tactic generation and improves alignment with informal reasoning patterns. We build dedicated datasets and benchmarks for training and evaluation, and introduce an interactive framework to support tactic generation from formal states. Empirical results show that our method substantially outperforms existing baselines, achieving higher proof success rates.", "arxiv_url": "https://arxiv.org/abs/2512.10317", "authors": ["Ziyu Wang", "Bowen Yang", "Shihao Zhou", "Chenyi Li", "Yuan Zhang", "Bin Dong", "Zaiwen Wen"], "first_author": "Ziyu Wang", "category": ["Technical", "Benchmark"], "field": "Formal Methods & Theorem Proving", "task": "Informal-to-Formal Proof Translation", "tags": ["Chain of States (CoS) intermediate representation", "proof-state extraction from informal text", "state-aligned tactic synthesis", "error-aware tactic regeneration", "Lean4-specific proof rewriting heuristics", "metaprogramming-based dataset construction", "interactive tactic generation framework", "reduction of prover invocations / compute-efficient proving"], "summary": "本文提出了CoS两阶段框架：先从非正式数学证明抽取对齐的形式化证明状态链，再在相邻状态之间生成tactic以在Lean4中还原完整形式化证明，并通过元编程构建数据集与基准，显著提高了证明成功率与效率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.10317v1", "published": "2025-12-11", "update_time": "2025-12-11", "download_time": "2025-12-15 02:09:36"}
{"id": "2512.11589", "title": "A Study of Library Usage in Agent-Authored Pull Requests", "abstract": "Coding agents are becoming increasingly capable of completing end-to-end software engineering workflows that previously required a human developer, including raising pull requests (PRs) to propose their changes. However, we still know little about how these agents use libraries when generating code, a core part of real-world software development. To fill this gap, we study 26,760 agent-authored PRs from the AIDev dataset to examine three questions: how often do agents import libraries, how often do they introduce new dependencies (and with what versioning), and which specific libraries do they choose? We find that agents often import libraries (29.5% of PRs) but rarely add new dependencies (1.3% of PRs); and when they do, they follow strong versioning practices (75.0% specify a version), an improvement on direct LLM usage where versions are rarely mentioned. Generally, agents draw from a surprisingly diverse set of external libraries, contrasting with the limited \"library preferences\" seen in prior non-agentic LLM studies. Our results offer an early empirical view into how AI coding agents interact with today's software ecosystems.", "arxiv_url": "https://arxiv.org/abs/2512.11589", "authors": ["Lukas Twist"], "first_author": "Lukas Twist", "category": ["Empirical"], "field": "Version Control & Collaboration", "task": "Git VCS", "tags": ["Agent-authored pull requests", "Library import frequency", "New dependency introduction", "Dependency version specification (version hygiene)", "Library diversity across ecosystems", "Multi-language empirical analysis (TypeScript, Python, Go, C#)", "Import and manifest regex extraction", "Agentic workflow behavior"], "summary": "本文通过分析26,760个由代码代理提交的拉取请求，实证研究代理在导入库、引入新依赖及选择库上的行为，发现代理常导入外部库但很少新增依赖且通常会指定版本，同时展现出较高的库多样性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.11589v1", "published": "2025-12-12", "update_time": "2025-12-12", "download_time": "2025-12-16 01:52:27"}
{"id": "2512.11482", "title": "Towards Privacy-Preserving Code Generation: Differentially Private Code Language Models", "abstract": "Large language models specialized for code (CodeLLMs) have demonstrated remarkable capabilities in generating code snippets, documentation, and test cases. However, despite their promising capabilities, CodeLLMs can inadvertently memorize and reproduce snippets from their training data, which poses risks of privacy breaches and intellectual property violations. These risks restrict the deployment of CodeLLMs in sensitive domains and limit their training datasets to publicly available sources. To mitigate the memorization risk without compromising their task performance, we apply Differential Privacy (DP) to CodeLLMs. To the best of our knowledge, this is the first comprehensive study that systematically evaluates the effectiveness of DP in CodeLLMs. DP adds calibrated noise to the training process to protect individual data points while still allowing the model to learn useful patterns. To this end, we first identify and understand the driving reasons of the memorization behaviour of the CodeLLMs during their fine-tuning. Then, to address this issue, we empirically evaluate the effect of DP on mitigating memorization while preserving code generation capabilities. Our findings show that DP substantially reduces memorization in CodeLLMs across all the tested snippet types. The snippet types most prone to memorization are also the most effectively mitigated by DP. Furthermore, we observe that DP slightly increases perplexity but preserves, and can even enhance, the code generation capabilities of CodeLLMs, which makes it feasible to apply DP in practice without significantly compromising model utility. Finally, we analyze the impact of DP on training efficiency and energy consumption, finding that DP does not significantly affect training time or energy usage, making it a practical choice for privacy-preserving CodeLLMs training.", "arxiv_url": "https://arxiv.org/abs/2512.11482", "authors": ["Melih Catal", "Pooja Rani", "Harald C. Gall"], "first_author": "Melih Catal", "category": ["Empirical", "Benchmark"], "field": "Coding Assistant", "task": "Code Pre-Training", "tags": ["DP-SGD training", "Memorization taxonomy", "Data‑extraction and membership‑inference attacks", "Privacy‑utility trade-off", "Snippet entropy and frequency analysis", "Functional correctness evaluation", "Training efficiency & energy analysis", "Privacy evaluation benchmarks"], "summary": "本文系统评估在代码生成模型中应用差分隐私（DP）的可行性，发现DP显著降低对训练片段的记忆风险且在大多数情况下不会显著损害甚至可提升代码生成功能性，并发布了两个用于隐私与效用评估的新基准数据集。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.11482v1", "published": "2025-12-12", "update_time": "2025-12-12", "download_time": "2025-12-16 01:53:12"}
{"id": "2512.11783", "title": "Super Suffixes: Bypassing Text Generation Alignment and Guard Models Simultaneously", "abstract": "The rapid deployment of Large Language Models (LLMs) has created an urgent need for enhanced security and privacy measures in Machine Learning (ML). LLMs are increasingly being used to process untrusted text inputs and even generate executable code, often while having access to sensitive system controls. To address these security concerns, several companies have introduced guard models, which are smaller, specialized models designed to protect text generation models from adversarial or malicious inputs. In this work, we advance the study of adversarial inputs by introducing Super Suffixes, suffixes capable of overriding multiple alignment objectives across various models with different tokenization schemes. We demonstrate their effectiveness, along with our joint optimization technique, by successfully bypassing the protection mechanisms of Llama Prompt Guard 2 on five different text generation models for malicious text and code generation. To the best of our knowledge, this is the first work to reveal that Llama Prompt Guard 2 can be compromised through joint optimization.   Additionally, by analyzing the changing similarity of a model's internal state to specific concept directions during token sequence processing, we propose an effective and lightweight method to detect Super Suffix attacks. We show that the cosine similarity between the residual stream and certain concept directions serves as a distinctive fingerprint of model intent. Our proposed countermeasure, DeltaGuard, significantly improves the detection of malicious prompts generated through Super Suffixes. It increases the non-benign classification rate to nearly 100%, making DeltaGuard a valuable addition to the guard model stack and enhancing robustness against adversarial prompt attacks.", "arxiv_url": "https://arxiv.org/abs/2512.11783", "authors": ["Andrew Adiletta", "Kathryn Adiletta", "Kemal Derya", "Berk Sunar"], "first_author": "Andrew Adiletta", "category": ["Technical", "Benchmark"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Super Suffixes", "Joint optimization across tokenization schemes", "Adversarial suffix attacks", "Guard model bypass", "Refusal concept direction", "Dynamic residual-similarity detection", "DeltaGuard", "Malicious code-generation dataset"], "summary": "本文提出 Super Suffixes —— 通过在不同分词方案下联合优化以同时绕过文本生成模型的对齐与守卫模型，并基于残差流与拒绝概念方向的动态余弦相似度提出 DeltaGuard 检测方法，同时构建恶意代码生成数据集进行评估。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.11783v1", "published": "2025-12-12", "update_time": "2025-12-12", "download_time": "2025-12-16 01:53:54"}
{"id": "2512.11402", "title": "REMODEL-LLM: Transforming C code to Java using LLMs", "abstract": "The automated translation of C code to Java code is a notoriously difficult task, fraught with challenges stemming from fundamental paradigm shifts (procedural vs. Object Oriented), memory models (manual pointers vs. Garbage Collection), and incompatible data types. This paper investigates the efficacy of 19 small, quantized LLMs (under 20 billion parameters) for the C to Java translation task. We use a novel, hybrid pipeline that leverages Abstract Syntax Trees (ASTs) for semantic decomposition and employs a highly constrained, rule based prompting strategy. The results are stark: a clear multi tiered performance divide emerged. The vast majority of models (Tier 3, e.g., llama3.1, gemma3, starcoder2) failed 100\\% of the tests, proving incapable of generating even basic, runnable Java boilerplate. A small middle tier (Tier 2, e.g., mistral-nemo and mistral) produced runnable code but was plagued by dangerous semantic failures and wrong translations. Only three models (Tier 1: phi4, deepseek-coder-v2, codeqwen) proved viable, passing over 50\\% of the test suite. Even these top models failed on the most complex C concepts, such as function pointers, sizeof, and enum logic, revealing a hard ceiling for the reasoning capabilities of current quantized models.", "arxiv_url": "https://arxiv.org/abs/2512.11402", "authors": ["Aryan Gupta", "Y. Raghu Reddy"], "first_author": "Aryan Gupta", "category": ["Empirical", "Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["C-to-Java migration", "AST-guided decomposition", "Constrained rule-based prompting", "Quantized edge LLM evaluation", "Semantic vs. syntactic failure analysis", "Translation edge cases (function pointers, sizeof, enums)", "20-case C→Java benchmark"], "summary": "本文提出基于AST的混合翻译流水线和受限提示策略，评估了19个量化小型LLM在C到Java迁移任务上的表现并构建了20个边界用例基准，结果显示大多数模型在关键语义转换（如函数指针、sizeof、枚举逻辑）上失败，仅三款模型在测试中通过率超过50%。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.11402v1", "published": "2025-12-12", "update_time": "2025-12-12", "download_time": "2025-12-16 01:59:31"}
{"id": "2512.12719", "title": "Towards AI Agents Supported Research Problem Formulation", "abstract": "Poorly formulated research problems can compromise the practical relevance of Software Engineering studies by not reflecting the complexities of industrial practice. This vision paper explores the use of artificial intelligence agents to support SE researchers during the early stage of a research project, the formulation of the research problem. Based on the Lean Research Inception framework and using a published study on code maintainability in machine learning as a reference, we developed a descriptive evaluation of a scenario illustrating how AI agents, integrated into LRI, can support SE researchers by pre filling problem attributes, aligning stakeholder perspectives, refining research questions, simulating multiperspective assessments, and supporting decision making. The descriptive evaluation of the scenario suggests that AI agent support can enrich collaborative discussions and enhance critical reflection on the value, feasibility, and applicability of the research problem. Although the vision of integrating AI agents into LRI was perceived as promising to support the context aware and practice oriented formulation of research problems, empirical validation is needed to confirm and refine the integration of AI agents into problem formulation.", "arxiv_url": "https://arxiv.org/abs/2512.12719", "authors": ["Anrafel Fernandes Pereira", "Maria Teresa Baldassarre", "Daniel Mendez", "Marcos Kalinowski"], "first_author": "Anrafel Fernandes Pereira", "category": ["Technical"], "field": "Research Process & Methodology", "task": "Research Problem Formulation", "tags": ["AI-assisted problem framing", "Problem Vision board pre-filling", "Multi-agent reasoning for research design", "Stakeholder-perspective simulation", "Semantic-differential assessment augmentation", "Literature-and-industry evidence synthesis", "Decision-support for go/pivot/abort"], "summary": "该愿景论文提出将 AI 多代理体整合入 Lean Research Inception 框架，通过预填问题属性、促进利益相关者对齐、生成多视角评估并支持决策流程以改善软件工程研究问题的表述，但仍需实证验证。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.12719v1", "published": "2025-12-14", "update_time": "2025-12-14", "download_time": "2025-12-16 02:40:02"}
{"id": "2512.12706", "title": "Synergizing Code Coverage and Gameplay Intent: Coverage-Aware Game Playtesting with LLM-Guided Reinforcement Learning", "abstract": "The widespread adoption of the \"Games as a Service\" model necessitates frequent content updates, placing immense pressure on quality assurance. In response, automated game testing has been viewed as a promising solution to cope with this demanding release cadence. However, existing automated testing approaches typically create a dichotomy: code-centric methods focus on structural coverage without understanding gameplay context, while player-centric agents validate high-level intent but often fail to cover specific underlying code changes. To bridge this gap, we propose SMART (Structural Mapping for Augmented Reinforcement Testing), a novel framework that synergizes structural verification and functional validation for game update testing. SMART leverages large language models (LLMs) to interpret abstract syntax tree (AST) differences and extract functional intent, constructing a context-aware hybrid reward mechanism. This mechanism guides reinforcement learning agents to sequentially fulfill gameplay goals while adaptively exploring modified code branches. We evaluate SMART on two environments, Overcooked and Minecraft. The results demonstrate that SMART significantly outperforms state-of-the-art baselines; it achieves over 94% branch coverage of modified code, nearly double that of traditional reinforcement learning methods, while maintaining a 98% task completion rate, effectively balancing structural comprehensiveness with functional correctness.", "arxiv_url": "https://arxiv.org/abs/2512.12706", "authors": ["Enhong Mu", "Minami Yoda", "Yan Zhang", "Mingyue Zhang", "Yutaka Matsuno", "Jialong Li"], "first_author": "Enhong Mu", "category": ["Technical"], "field": "Software Testing", "task": "Testing automation", "tags": ["AST-diff semantic interpretation", "LLM-driven subgoal decomposition", "Hybrid structural-functional reward", "Coverage-aware reinforcement learning", "Context-aware mapping of subgoals to code anchors", "Game-update regression testing"], "summary": "本文提出SMART框架，利用LLM解析AST差异将更新意图分解为语义子目标，并通过将这些子目标与结构性代码锚点动态映射并构建混合奖励来引导强化学习代理在游戏更新测试中同时实现高分支覆盖率与高任务完成率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.12706v1", "published": "2025-12-14", "update_time": "2025-12-14", "download_time": "2025-12-16 02:40:31"}
{"id": "2512.11296", "title": "Few-Shot VLM-Based G-Code and HMI Verification in CNC Machining", "abstract": "Manual generation of G-code is important for learning the operation of CNC machines. Prior work in G-code verification uses Large-Language Models (LLMs), which primarily examine errors in the written programming. However, CNC machining requires extensive use and knowledge of the Human-Machine Interface (HMI), which displays machine status and errors. LLMs currently lack the capability to leverage knowledge of HMIs due to their inability to access the vision modality. This paper proposes a few-shot VLM-based verification approach that simultaneously evaluates the G-code and the HMI display for errors and safety status. The input dataset includes paired G-code text and associated HMI screenshots from a 15-slant-PRO lathe, including both correct and error-prone cases. To enable few-shot learning, the VLM is provided with a structured JSON schema based on prior heuristic knowledge. After determining the prompts, instances of G-code and HMI that either contain errors or are error free are used as few-shot examples to guide the VLM. The model was then evaluated in comparison to a zero-shot VLM through multiple scenarios of incorrect G-code and HMI errors with respect to per-slot accuracy. The VLM showed that few-shot prompting led to overall enhancement of detecting HMI errors and discrepancies with the G-code for more comprehensive debugging. Therefore, the proposed framework was demonstrated to be suitable for verification of manually generated G-code that is typically developed in CNC training.", "arxiv_url": "https://arxiv.org/abs/2512.11296", "authors": ["Yasaman Hashem Pour", "Nazanin Mahjourian", "Vinh Nguyen"], "first_author": "Yasaman Hashem Pour", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Understanding", "tags": ["G-code verification", "HMI indicator recognition", "Multimodal VLM few-shot prompting", "Structured JSON slot-filling", "G-code–machine-state alignment", "CNC safety/status checking"], "summary": "本文提出一种基于视觉-语言模型的少样本联合校验方法，通过将G-code文本与机床HMI截图配对并使用结构化JSON schema进行few-shot提示，从而同时检测G-code语法错误与HMI显示与代码的不一致以提升人工编写G-code的验证与安全性。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.11296v1", "published": "2025-12-12", "update_time": "2025-12-12", "download_time": "2025-12-16 02:48:32"}
{"id": "2512.11270", "title": "A-LAMP: Agentic LLM-Based Framework for Automated MDP Modeling and Policy Generation", "abstract": "Applying reinforcement learning (RL) to real-world tasks requires converting informal descriptions into a formal Markov decision process (MDP), implementing an executable environment, and training a policy agent. Automating this process is challenging due to modeling errors, fragile code, and misaligned objectives, which often impede policy training. We introduce an agentic large language model (LLM)-based framework for automated MDP modeling and policy generation (A-LAMP), that automatically translates free-form natural language task descriptions into an MDP formulation and trained policy. The framework decomposes modeling, coding, and training into verifiable stages, ensuring semantic alignment throughout the pipeline. Across both classic control and custom RL domains, A-LAMP consistently achieves higher policy generation capability than a single state-of-the-art LLM model. Notably, even its lightweight variant, which is built on smaller language models, approaches the performance of much larger models. Failure analysis reveals why these improvements occur. In addition, a case study also demonstrates that A-LAMP generates environments and policies that preserve the task's optimality, confirming its correctness and reliability.", "arxiv_url": "https://arxiv.org/abs/2512.11270", "authors": ["Hong Je-Gal", "Chan-Bin Yi", "Hyun-Suk Lee"], "first_author": "Hong Je-Gal", "category": ["Technical"], "field": "Requirements & Design", "task": "Specification & Validation", "tags": ["Agentic multi-agent orchestration", "Automated MDP synthesis", "Natural-language to environment code generation", "Reward function specification", "Policy training automation", "Modular verifiable pipeline", "Failure-mode analysis"], "summary": "该论文提出A-LAMP，一个基于多智能体LLM的框架，能够将自由文本任务描述自动转为形式化MDP、生成可执行环境与训练代码并产出可验证的策略，且在多域实验中优于单一大模型。", "quality": "High", "conference": "NeurIPS 2025 Workshop: Multi-Turn Interactions in Large Language Models 2025", "pdf_url": "https://arxiv.org/pdf/2512.11270v1", "published": "2025-12-12", "update_time": "2025-12-12", "download_time": "2025-12-16 02:49:40"}
{"id": "2512.13515", "title": "Fine-tuned LLM-based Code Migration Framework", "abstract": "The study presents the outcomes of research and experimental validation in the domain of automated codebase migration, with a focus on addressing challenges in transitioning SQL-based systems. The proposed method for migration essentially appears as a framework that leverages the best aspects of traditional software engineering techniques and provides an iterative, scalable, precise and efficient solution for modern database transformations. The central piece of the approach is the integration of a fine-tuned Large Language Model to address critical issues in SQL code conversion, such as syntax mapping, resolving discrepancies between Oracle PL/SQL and PostgreSQL, and optimising database elements such as stored procedures, triggers, views, and overall database logic. Thus, the method involves a trade-off between fine-tuning and prompt engineering. Special attention is given to a fine-tuning approach, which enhances the adaptability and compatibility with migration requirements across the entire database. According to the achieved results, fine-tuning plays a very important role. The study employs targeted evaluation methodologies along with computational metrics to measure the success of iterative conversion cycles. Core innovations include automated SQL feature detection, semi-supervised error analysis and integration of Subject Matter Experts feedback within a systematic migration workflow. The methodology achieves significant reductions in Syntax Error Rates, enhances feature alignment throughout migration iterations, and leverages dataset sampling to ensure continual improvement. By embedding GAI into the migration process, the framework facilitates precise feature mapping, semi-automated error resolution, and data-driven optimisation loops, improving workflow efficiency.", "arxiv_url": "https://arxiv.org/abs/2512.13515", "authors": ["Oleg Grynets", "Vasyl Lyashkevych", "Dmytro Baran", "Maksym Orliansky", "Taras Zelenyy", "Markiian Leshchyshyn"], "first_author": "Oleg Grynets", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Two-stage fine-tuning", "Hybrid Code Feature Profiling Engine (HCFPE)", "Retrieval-Augmented Generation for migration", "Closed-loop human-in-the-loop error correction", "SQL feature detection and annotation", "Multi-repository vs unified FAISS knowledge base", "Syntactic-description and direct transformation dataset configurator", "Semi-supervised error analysis and failure taxonomy"], "summary": "本文提出了一个面向Oracle到PostgreSQL的自动化代码迁移框架，结合两阶段微调、特征分析引擎、RAG知识库与人机闭环错误分析以提升迁移的语法与语义准确性。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.13515v1", "published": "2025-12-15", "update_time": "2025-12-15", "download_time": "2025-12-17 01:48:11"}
{"id": "2512.13438", "title": "From User Interface to Agent Interface: Efficiency Optimization of UI Representations for LLM Agents", "abstract": "While Large Language Model (LLM) agents show great potential for automated UI navigation such as automated UI testing and AI assistants, their efficiency has been largely overlooked. Our motivating study reveals that inefficient UI representation creates a critical performance bottleneck. However, UI representation optimization, formulated as the task of automatically generating programs that transform UI representations, faces two unique challenges. First, the lack of Boolean oracles, which traditional program synthesis uses to decisively validate semantic correctness, poses a fundamental challenge to co-optimization of token efficiency and completeness. Second, the need to process large, complex UI trees as input while generating long, compositional transformation programs, making the search space vast and error-prone. Toward addressing the preceding limitations, we present UIFormer, the first automated optimization framework that synthesizes UI transformation programs by conducting constraint-based optimization with structured decomposition of the complex synthesis task. First, UIFormer restricts the program space using a domain-specific language (DSL) that captures UI-specific operations. Second, UIFormer conducts LLM-based iterative refinement with correctness and efficiency rewards, providing guidance for achieving the efficiency-completeness co-optimization. UIFormer operates as a lightweight plugin that applies transformation programs for seamless integration with existing LLM agents, requiring minimal modifications to their core logic. Evaluations across three UI navigation benchmarks spanning Android and Web platforms with five LLMs demonstrate that UIFormer achieves 48.7% to 55.8% token reduction with minimal runtime overhead while maintaining or improving agent performance. Real-world industry deployment at WeChat further validates the practical impact of UIFormer.", "arxiv_url": "https://arxiv.org/abs/2512.13438", "authors": ["Dezhi Ran", "Zhi Gong", "Yuzhe Guo", "Mengzhou Wu", "Yuan Cao", "Haochuan Lu", "Hengyu Zhang", "Xia Zeng", "Gang Cao", "Liangchao Yao", "Yuetang Deng", "Wei Yang", "Tao Xie"], "first_author": "Dezhi Ran", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Performance Optimization", "tags": ["UI representation optimization", "DSL for UI transformations", "Program synthesis for UI trees", "Iterative LLM-based refinement", "Efficiency–completeness co-optimization", "Local decomposition and tree merging", "Lightweight plugin integration for agents", "Constraint-based evaluation rewards"], "summary": "本文提出UIFORMER，通过设计面向UI的DSL并结合基于LLM的迭代约束优化与结构化分解，自动合成将复杂UI树转换为低令牌开销且保持语义完备的表示的转换程序，从而显著降低LLM代理的token成本并在多平台基准与实际微信部署中保持或提升代理性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.13438v1", "published": "2025-12-15", "update_time": "2025-12-15", "download_time": "2025-12-17 01:49:07"}
{"id": "2512.13607", "title": "Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models", "abstract": "Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes.", "arxiv_url": "https://arxiv.org/abs/2512.13607", "authors": ["Boxin Wang", "Chankyu Lee", "Nayeon Lee", "Sheng-Chieh Lin", "Wenliang Dai", "Yang Chen", "Yangyi Chen", "Zhuolin Yang", "Zihan Liu", "Mohammad Shoeybi", "Bryan Catanzaro", "Wei Ping"], "first_author": "Boxin Wang", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Cascaded domain-wise reinforcement learning", "RLHF as a pre-step to boost reasoning", "Instruction-following RL (IF-RL)", "Math-specific RL curriculum", "Code RL for competitive programming", "Software-engineering RL (SWE RL)", "Execution-free reward modeling for SWE", "Generation–retrieval for code localization", "Test-time scaling / thinking modes", "Sequential-domain curriculum to mitigate forgetting"], "summary": "本文提出级联领域强化学习（Cascade RL）来训练通用推理模型Nemotron‑Cascade，通过序贯的领域级RL（包含RLHF、IF‑RL、Math RL、Code RL、SWE RL）与工程化技巧提升代码推理与竞赛编程等任务的表现，并公开训练与数据配方。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.13607v1", "published": "2025-12-15", "update_time": "2025-12-15", "download_time": "2025-12-17 01:50:27"}
{"id": "2512.13102", "title": "Socratic Students: Teaching Language Models to Learn by Asking Questions", "abstract": "Large Language Models (LLMs) excel at static interactions, where they answer user queries by retrieving knowledge encoded in their parameters. However, in many real-world settings, such as educational tutoring or medical assistance, relevant information is not directly available and must be actively acquired through dynamic interactions. An interactive agent would recognize its own uncertainty, ask targeted questions, and retain new knowledge efficiently. Prior work has primarily explored effective ways for a teacher to instruct the student, where the teacher identifies student gaps and provides guidance. In this work, we shift the focus to the student and investigate effective strategies to actively query the teacher in seeking useful information. Across math and coding benchmarks, where baseline student models begin with near-zero performance, we show that student-led approaches consistently yield absolute Pass@k improvements of at least 0.5 over static baselines. To improve question quality, we train students using Direct Preference Optimization (DPO) with guidance from either self or stronger students. We find that this guided training enables smaller models to learn how to ask better questions, further enhancing learning efficiency.", "arxiv_url": "https://arxiv.org/abs/2512.13102", "authors": ["Rajeev Bhatt Ambati", "Tianyi Niu", "Aashu Singh", "Shlok Mishra", "Shashank Srivastava", "Snigdha Chaturvedi"], "first_author": "Rajeev Bhatt Ambati", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Student-led interaction", "Active question generation", "Direct Preference Optimization for question selection", "Self- and peer-guidance", "Chain-of-Thought guided questioning", "Pre- and mid-assessment grounding", "Teacher-student dialogue protocol", "Pass@k-based preference collection"], "summary": "本文提出一种学生主导的交互学习框架，训练小型模型通过主动提问（包含CoT指导、评估插入和基于DPO的自我/同辈指导）与强教师交互，在数学与编程任务上显著提升了Pass@k表现。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.13102v1", "published": "2025-12-15", "update_time": "2025-12-15", "download_time": "2025-12-17 01:57:25"}
{"id": "2506.11928", "title": "LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?", "abstract": "Recent reports claim that large language models (LLMs) now outperform elite humans in competitive programming. Drawing on knowledge from a group of medalists in international algorithmic contests, we revisit this claim, examining how LLMs differ from human experts and where limitations still remain. We introduce LiveCodeBench Pro, a benchmark composed of problems from Codeforces, ICPC, and IOI that are continuously updated to reduce the likelihood of data contamination. A team of Olympiad medalists annotates every problem for algorithmic categories and conducts a line-by-line analysis of failed model-generated submissions. Using this new data and benchmark, we find that frontier models still have significant limitations: without external tools, the best model achieves only 53% pass@1 on medium-difficulty problems and 0% on hard problems, domains where expert humans still excel. We also find that LLMs succeed at implementation-heavy problems but struggle with nuanced algorithmic reasoning and complex case analysis, often generating confidently incorrect justifications. High performance appears largely driven by implementation precision and tool augmentation, not superior reasoning. LiveCodeBench Pro thus highlights the significant gap to human grandmaster levels, while offering fine-grained diagnostics to steer future improvements in code-centric LLM reasoning.", "arxiv_url": "https://arxiv.org/abs/2506.11928", "authors": ["Zihan Zheng", "Zerui Cheng", "Zeyu Shen", "Shang Zhou", "Kaiyuan Liu", "Hansen He", "Dongruixuan Li", "Stanley Wei", "Hangyi Hao", "Jianzhu Yao", "Peiyao Sheng", "Zixuan Wang", "Wenhao Chai", "Aleksandra Korolova", "Peter Henderson", "Sanjeev Arora", "Pramod Viswanath", "Jingbo Shang", "Saining Xie"], "first_author": "Zihan Zheng", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Competitive Programming Benchmark", "Contamination-Resistant Evaluation", "Olympiad-Expert Annotations", "Cognitive-Focus Taxonomy", "Elo-Based Model Rating", "Implementation vs Algorithmic Reasoning", "Failure Mode Analysis", "Tool-Augmentation Effects", "Test-Suite Robustness", "Pass@1 Evaluation"], "summary": "LiveCodeBench Pro 提出一个由奥赛奖牌得主持续更新并注释的竞赛编程基准，通过细粒度技能标签、抗污染策略和逐行失败分析，揭示当代 LLM 在算法推理与复杂案例分析上的显著局限性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2506.11928v1", "published": "2025-06-13", "update_time": "2025-06-13", "download_time": "2025-12-17 18:37:43"}
{"id": "2506.12713", "title": "Humanity's Last Code Exam: Can Advanced LLMs Conquer Human's Hardest Code Competition?", "abstract": "Code generation is a core capability of large language models (LLMs), yet mainstream benchmarks (e.g., APPs and LiveCodeBench) contain questions with medium-level difficulty and pose no challenge to advanced LLMs. To better reflected the advanced reasoning and code generation ability, We introduce Humanity's Last Code Exam (HLCE), comprising 235 most challenging problems from the International Collegiate Programming Contest (ICPC World Finals) and the International Olympiad in Informatics (IOI) spanning 2010 - 2024. As part of HLCE, we design a harmonized online-offline sandbox that guarantees fully reproducible evaluation. Through our comprehensive evaluation, we observe that even the strongest reasoning LLMs: o4-mini(high) and Gemini-2.5 Pro, achieve pass@1 rates of only 15.9% and 11.4%, respectively. Meanwhile, we propose a novel \"self-recognition\" task to measure LLMs' awareness of their own capabilities. Results indicate that LLMs' self-recognition abilities are not proportionally correlated with their code generation performance. Finally, our empirical validation of test-time scaling laws reveals that current advanced LLMs have substantial room for improvement on complex programming tasks. We expect HLCE to become a milestone challenge for code generation and to catalyze advances in high-performance reasoning and human-AI collaborative programming. Our code and dataset are also public available(https://github.com/Humanity-s-Last-Code-Exam/HLCE).", "arxiv_url": "https://arxiv.org/abs/2506.12713", "authors": ["Xiangyang Li", "Xiaopeng Li", "Kuicai Dong", "Quanhu Zhang", "Rongju Ruan", "Xinyi Dai", "Xiaoshuang Liu", "Shengchun Xu", "Yasheng Wang", "Ruiming Tang"], "first_author": "Xiangyang Li", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Competitive Programming Benchmark", "Interactive Problem Evaluation", "ICPC/IOI Problem Curation", "Model Self-Assessment", "Test-time Scaling Laws", "Reproducible Sandbox Evaluation", "Pass@1 Performance Analysis", "Human-vs-Model Comparison", "Algorithmic Reasoning Challenges"], "summary": "本文提出HLCE——一个包含2010–2024年IOI与ICPC世界决赛中235道高难度题目的代码生成基准（含交互式题型）并提供可复现的线上-线下评测平台，基于此对12个先进模型进行全面评估、引入模型自我识别任务并验证测试时扩展律，结果显示当前最强模型在该基准上的表现仍然有限。", "quality": "High", "conference": "EMNLP 2025", "pdf_url": "https://arxiv.org/pdf/2506.12713v2", "published": "2025-06-15", "update_time": "2025-10-18", "download_time": "2025-12-17 18:38:19"}
{"id": "2506.16395", "title": "OJBench: A Competition Level Code Benchmark For Large Language Models", "abstract": "Recent advancements in large language models (LLMs) have demonstrated significant progress in math and code reasoning capabilities. However, existing code benchmark are limited in their ability to evaluate the full spectrum of these capabilities, particularly at the competitive level. To bridge this gap, we introduce OJBench, a novel and challenging benchmark designed to assess the competitive-level code reasoning abilities of LLMs. OJBench comprises 232 programming competition problems from NOI and ICPC, providing a more rigorous test of models' reasoning skills. We conducted a comprehensive evaluation using OJBench on 37 models, including both closed-source and open-source models, reasoning-oriented and non-reasoning-oriented models. Our results indicate that even state-of-the-art reasoning-oriented models, such as o4-mini and Gemini-2.5-pro-exp, struggle with highly challenging competition-level problems. This highlights the significant challenges that models face in competitive-level code reasoning.", "arxiv_url": "https://arxiv.org/abs/2506.16395", "authors": ["Zhexu Wang", "Yiping Liu", "Yejie Wang", "Wenyang He", "Bofei Gao", "Muxi Diao", "Yanxu Chen", "Kelin Fu", "Flood Sung", "Zhilin Yang", "Tianyu Liu", "Weiran Xu"], "first_author": "Zhexu Wang", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Competition-level algorithmic problems", "Contest-sourced problem collection", "Testcase-based judging harness", "Difficulty calibration from contest metrics", "Dual-language evaluation (CPP vs Python)", "Execution-feedback iterative correction", "Cross-model comparative analysis", "Open-vs-closed-source performance gap", "Long-chain-of-thought reasoning assessment"], "summary": "本文提出OJBench——一个包含232道NOI与ICPC竞赛题的竞赛级代码基准，并在37个模型上评测，揭示当前即使是顶尖推理型模型在高难度竞赛题上仍存在显著不足、CPP通常优于Python且执行反馈能提升模型表现的事实。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2506.16395v1", "published": "2025-06-19", "update_time": "2025-06-19", "download_time": "2025-12-17 18:38:51"}
{"id": "2507.22462", "title": "IFEvalCode: Controlled Code Generation", "abstract": "Code large language models (Code LLMs) have made significant progress in code generation by translating natural language descriptions into functional code; however, real-world applications often demand stricter adherence to detailed requirements such as coding style, line count, and structural constraints, beyond mere correctness. To address this, the paper introduces forward and backward constraints generation to improve the instruction-following capabilities of Code LLMs in controlled code generation, ensuring outputs align more closely with human-defined guidelines. The authors further present IFEvalCode, a multilingual benchmark comprising 1.6K test samples across seven programming languages (Python, Java, JavaScript, TypeScript, Shell, C++, and C#), with each sample featuring both Chinese and English queries. Unlike existing benchmarks, IFEvalCode decouples evaluation into two metrics: correctness (Corr.) and instruction-following (Instr.), enabling a more nuanced assessment. Experiments on over 40 LLMs reveal that closed-source models outperform open-source ones in controllable code generation and highlight a significant gap between the models' ability to generate correct code versus code that precisely follows instructions.", "arxiv_url": "https://arxiv.org/abs/2507.22462", "authors": ["Jian Yang", "Wei Zhang", "Shukai Liu", "Linzheng Chai", "Yingshui Tan", "Jiaheng Liu", "Ge Zhang", "Wangchunshu Zhou", "Guanglin Niu", "Zhoujun Li", "Binyuan Hui", "Junyang Lin"], "first_author": "Jian Yang", "category": ["Benchmark", "Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Instruction-Tuning", "tags": ["Controlled Code Generation", "Forward/Backward Constraint Generation", "Instruction Corpus Generation", "Runtime Instruction Verification", "Execution-based Controllability Checks", "Multilingual (Chinese-English) Code Benchmark", "Function Signature and Format Constraints", "Constraint-aware Prompting", "Closed-source vs Open-source Model Gap"], "summary": "本文提出了前向/后向约束生成以增强模型的指令遵循能力，构建了多语言可控代码生成基准 IFEvalCode（1.6K 样本，中英双语）用于分别评估代码正确性与指令遵循性，并在40+模型上进行详尽对比分析。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2507.22462v2", "published": "2025-07-30", "update_time": "2025-08-01", "download_time": "2025-12-17 18:39:29"}
{"id": "2508.03622", "title": "Refining Critical Thinking in LLM Code Generation: A Faulty Premise-based Evaluation Framework", "abstract": "With the advancement of code generation capabilities in large language models (LLMs), their reliance on input premises has intensified. When users provide inputs containing faulty premises, the probability of code generation hallucinations rises significantly, exposing deficiencies in their self-scrutiny capabilities. This paper proposes Faulty Premises Bench (FPBench), the first code generation evaluation framework targeting faulty premises. By systematically constructing three categories of faulty premises and integrating multi-dimensional evaluation metrics, it conducts in-depth assessments of 15 representative LLMs. The key findings are as follows: (1) Most models exhibit poor reasoning abilities and suboptimal code generation performance under faulty premises, heavily relying on explicit prompts for error detection, with limited self-scrutiny capabilities; (2) Faulty premises trigger a point of diminishing returns in resource investment, leading to blindly increasing length fails to enhance quality; (3) The three types of faulty premises respectively activate distinct defect patterns in models, revealing a triple dissociation in the cognitive mechanisms of code generation models. This study not only highlights the urgent need for LLMs to proactively verify premises in code generation but also, through the proposed FPBench framework and multi-dimensional evaluation system, provides a theoretical foundation and practical pathway for developing reliable, human-centric code generation models.", "arxiv_url": "https://arxiv.org/abs/2508.03622", "authors": ["Jialin Li", "Jinzhe Li", "Gengxu Li", "Yi Chang", "Yuan Wu"], "first_author": "Jialin Li", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Faulty-premise robustness", "Premise verification and critique", "Proactive vs. passive error detection", "Self-scrutiny overhead metrics", "Prompt perturbation / error injection", "Code-generation hallucination analysis", "Defect-pattern (triple dissociation) analysis", "Benchmark construction for adversarial prompts"], "summary": "本文提出了FPBench——一个专门评估LLM在含有错误前提的代码生成任务中自我审查能力的基准框架，构造三类错误前提与多维评价指标并在15个模型上实证揭示模型在识别错误、资源开销与缺陷模式方面的系统性不足。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2508.03622v1", "published": "2025-08-05", "update_time": "2025-08-05", "download_time": "2025-12-17 18:40:02"}
{"id": "2502.13897", "title": "DataSciBench: An LLM Agent Benchmark for Data Science", "abstract": "This paper presents DataSciBench, a comprehensive benchmark for evaluating Large Language Model (LLM) capabilities in data science. Recent related benchmarks have primarily focused on single tasks, easily obtainable ground truth, and straightforward evaluation metrics, which limits the scope of tasks that can be evaluated. In contrast, DataSciBench is constructed based on a more comprehensive and curated collection of natural and challenging prompts for uncertain ground truth and evaluation metrics. We develop a semi-automated pipeline for generating ground truth (GT) and validating evaluation metrics. This pipeline utilizes and implements an LLM-based self-consistency and human verification strategy to produce accurate GT by leveraging collected prompts, predefined task types, and aggregate functions (metrics). Furthermore, we propose an innovative Task - Function - Code (TFC) framework to assess each code execution outcome based on precisely defined metrics and programmatic rules. Our experimental framework involves testing 6 API-based models, 8 open-source general models, and 9 open-source code generation models using the diverse set of prompts we have gathered. This approach aims to provide a more comprehensive and rigorous evaluation of LLMs in data science, revealing their strengths and weaknesses. Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models. We release all code and data at https://github.com/THUDM/DataSciBench.", "arxiv_url": "https://arxiv.org/abs/2502.13897", "authors": ["Dan Zhang", "Sining Zhoubian", "Min Cai", "Fengzu Li", "Lekang Yang", "Wei Wang", "Tianjiao Dong", "Ziniu Hu", "Jie Tang", "Yisong Yue"], "first_author": "Dan Zhang", "category": ["Benchmark", "Technical"], "field": "Data Science", "task": "Data Science Benchmarking", "tags": ["TFC evaluation", "Ground-truth synthesis", "Semi-automated GT validation", "Programmatic metrics", "Aggregate evaluation functions", "Prompt collection and curation", "Data visualization evaluation", "Predictive modeling evaluation", "Code execution verification", "Human-in-the-loop review"], "summary": "本文提出DataSciBench，一个面向数据科学任务的综合基准，包含222条挑战性提示、519个半自动生成并人工验证的GT以及Task‑Function‑Code（TFC）评估框架，用以通过程序化指标评估LLM在数据清洗、统计、可视化、预测建模等方面的能力并提供详尽比较分析。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2502.13897v1", "published": "2025-02-19", "update_time": "2025-02-19", "download_time": "2025-12-17 18:46:06"}
{"id": "2502.14752", "title": "TritonBench: Benchmarking Large Language Model Capabilities for Generating Triton Operators", "abstract": "Triton, a high-level Python-like language designed for building efficient GPU kernels, is widely adopted in deep learning frameworks due to its portability, flexibility, and accessibility. However, programming and parallel optimization still require considerable trial and error from Triton developers. Despite advances in large language models (LLMs) for conventional code generation, these models struggle to generate accurate, performance-optimized Triton code, as they lack awareness of its specifications and the complexities of GPU programming. More critically, there is an urgent need for systematic evaluations tailored to Triton. In this work, we introduce TritonBench, the first comprehensive benchmark for Triton operator generation. TritonBench features two evaluation channels: a curated set of 184 real-world operators from GitHub and a collection of operators aligned with PyTorch interfaces. Unlike conventional code benchmarks prioritizing functional correctness, TritonBench also profiles efficiency performance on widely deployed GPUs aligned with industry applications. Our study reveals that current state-of-the-art code LLMs struggle to generate efficient Triton operators, highlighting a significant gap in high-performance code generation. TritonBench will be available at https://github.com/thunlp/TritonBench.", "arxiv_url": "https://arxiv.org/abs/2502.14752", "authors": ["Jianling Li", "Shangzhan Li", "Zhenye Gao", "Qi Shi", "Yuxuan Li", "Zefan Wang", "Jiacheng Huang", "Haojie Wang", "Jianrong Wang", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "first_author": "Jianling Li", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Performance Optimization", "tags": ["Triton Operators", "GPU Kernel Generation", "DSL Code Generation", "Performance-aware Benchmarking", "GPU Efficiency Profiling", "Execution Accuracy", "Speedup Evaluation", "Operator Corpus Curation", "Testcase Synthesis", "HPC Optimization"], "summary": "本文提出TRITONBENCH——首个面向Triton算子生成的性能感知基准，收集真实GitHub算子与PyTorch对齐任务，并在功能正确性和GPU运行效率上评估多种LLM以揭示其在高性能Triton代码生成方面的不足。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2502.14752v1", "published": "2025-02-20", "update_time": "2025-02-20", "download_time": "2025-12-17 18:46:36"}
{"id": "2502.18726", "title": "Deep-Bench: Deep Learning Benchmark Dataset for Code Generation", "abstract": "Deep learning (DL) has revolutionized areas such as computer vision, natural language processing, and more. However, developing DL systems is challenging due to the complexity of DL workflows. Large Language Models (LLMs), such as GPT, Claude, Llama, Mistral, etc., have emerged as promising tools to assist in DL code generation, offering potential solutions to these challenges. Despite this, existing benchmarks such as DS-1000 are limited, as they primarily focus on small DL code snippets related to pre/post-processing tasks and lack a comprehensive coverage of the full DL pipeline, including different DL phases and input data types.   To address this, we introduce DeepBench, a novel benchmark dataset designed for function-level DL code generation. DeepBench categorizes DL problems based on three key aspects: phases such as pre-processing, model construction, and training; tasks, including classification, regression, and recommendation; and input data types such as tabular, image, and text.   GPT-4o -- the state-of-the-art LLM -- achieved 31% accuracy on DeepBench, significantly lower than its 60% on DS-1000. We observed similar difficulty for other LLMs (e.g., 28% vs. 54% for Claude, 21% vs. 41% for LLaMA, and 15% vs. 20% for Mistral). This result underscores DeepBench's greater complexity. We also construct a taxonomy of issues and bugs found in LLM-generated DL code, which highlights the distinct challenges that LLMs face when generating DL code compared to general code.   Furthermore, our analysis also reveals substantial performance variations across categories, with differences of up to 7% among phases and 37% among tasks. These disparities suggest that DeepBench offers valuable insights into the LLMs' performance and areas for potential improvement in the DL domain.", "arxiv_url": "https://arxiv.org/abs/2502.18726", "authors": ["Alireza Daghighfarsoodeh", "Chung-Yu Wang", "Hamed Taherkhani", "Melika Sepidband", "Mohammad Abdollahi", "Hadi Hemmati", "Hung Viet Pham"], "first_author": "Alireza Daghighfarsoodeh", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Function-level DL code", "Unit-test evaluation", "DL pipeline taxonomy", "ML task categorization", "Input data type diversity", "GitHub curation", "Prompt synthesis", "Bug taxonomy", "Arithmetic and logical errors", "Phase-specific difficulty"], "summary": "本文提出Deep-Bench，一个从高质量GitHub仓库构建的、包含520个函数级深度学习代码生成样例并配套单元测试与按DL阶段/任务/输入类型三维标注的基准数据集，同时评估多款LLM性能并构建了LLM生成DL代码的缺陷分类法。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2502.18726v1", "published": "2025-02-26", "update_time": "2025-02-26", "download_time": "2025-12-17 18:47:09"}
{"id": "2502.19067", "title": "IndicEval-XL: Bridging Linguistic Diversity in Code Generation Across Indic Languages", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation from natural language prompts, revolutionizing software development workflows. As we advance towards agent-based development paradigms, these models form the cornerstone of next-generation software development lifecycles. However, current benchmarks for evaluating multilingual code generation capabilities are predominantly English-centric, limiting their applicability across the global developer community. To address this limitation, we present IndicEval-XL, a comprehensive benchmark for code generation that incorporates 6 major Indic languages, collectively spoken by approximately 14\\% of the world's population. Our benchmark bridges these languages with 12 programming languages, creating a robust evaluation framework. This work is particularly significant given India's representation of one-eighth of the global population and the crucial role Indic languages play in Indian society. IndicEval-XL represents a significant step toward expanding the linguistic diversity in code generation systems and evaluation frameworks. By developing resources that support multiple languages, we aim to make AI-powered development tools more inclusive and accessible to developers of various linguistic backgrounds. To facilitate further research and development in this direction, we make our dataset and evaluation benchmark publicly available at https://github.com/telekom/IndicEval-XL", "arxiv_url": "https://arxiv.org/abs/2502.19067", "authors": ["Ujjwal Singh", "Aditi Sharma", "Nikhil Gupta", "Deepakshi", "Vivek Kumar Jha"], "first_author": "Ujjwal Singh", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Prompting", "tags": ["Multilingual Benchmark", "Indic Languages", "NL-to-PL Parallelization", "Back-Translation Validation", "BERTScore Filtering", "CodeBERTScore Evaluation", "Translation Pipeline", "Low-Resource Language Support", "Pass@k Analysis"], "summary": "本文提出IndicEval-XL——一个覆盖6种印度语与英语、涉及12种编程语言、共6720道并行题目的多语言代码生成基准，并描述了翻译、回译与严格相似度与人工质检流程以保证数据质量与可访问性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2502.19067v1", "published": "2025-02-26", "update_time": "2025-02-26", "download_time": "2025-12-17 18:58:56"}
{"id": "2502.19149", "title": "Isolating Language-Coding from Problem-Solving: Benchmarking LLMs with PseudoEval", "abstract": "Existing code generation benchmarks for Large Language Models (LLMs) such as HumanEval and MBPP are designed to study LLMs' end-to-end performance, where the benchmarks feed a problem description in natural language as input and examine the generated code in specific programming languages. However, the evaluation scores revealed in this way provide a little hint as to the bottleneck of the code generation -- whether LLMs are struggling with their problem-solving capability or language-coding capability. To answer this question, we construct PseudoEval, a multilingual code generation benchmark that provides a solution written in pseudocode as input. By doing so, the bottleneck of code generation in various programming languages could be isolated and identified. Our study yields several interesting findings. For example, we identify that the bottleneck of LLMs in Python programming is problem-solving, while Rust is struggling relatively more in language-coding. Also, our study indicates that problem-solving capability may transfer across programming languages, while language-coding needs more language-specific effort, especially for undertrained programming languages. Finally, we release the pipeline of constructing PseudoEval to facilitate the extension to existing benchmarks. PseudoEval is available at: https://anonymous.4open.science/r/PseudocodeACL25-7B74.", "arxiv_url": "https://arxiv.org/abs/2502.19149", "authors": ["Jiarong Wu", "Songqiang Chen", "Jialun Cao", "Hau Ching Lo", "Shing-Chi Cheung"], "first_author": "Jiarong Wu", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Pseudocode-driven evaluation", "Problem vs Implementation", "Multilingual assessment", "Pseudocode extraction pipeline", "Cross-language transferability", "Language-specific coding errors", "Human-vs-auto pseudocode study", "Benchmark construction", "Inference strategy analysis", "Coding bottleneck diagnosis"], "summary": "本文提出一个多语言伪代码驱动的代码生成基准，通过将问题求解与语言实现分离并提供自动化伪代码提取流水线，以识别并分析不同编程语言中LLM在问题解决与语言编码方面的瓶颈。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2502.19149v1", "published": "2025-02-26", "update_time": "2025-02-26", "download_time": "2025-12-17 18:59:37"}
{"id": "2502.20868", "title": "ProBench: Benchmarking Large Language Models in Competitive Programming", "abstract": "With reasoning language models such as OpenAI-o3 and DeepSeek-R1 emerging, large language models (LLMs) have entered a new phase of development. However, existing benchmarks for coding evaluation are gradually inadequate to assess the capability of advanced LLMs in code reasoning. To bridge the gap for high-level code reasoning assessment, we propose ProBench to benchmark LLMs in competitive programming, drawing inspiration from the International Collegiate Programming Contest. ProBench collects a comprehensive set of competitive programming problems from Codeforces, Luogu, and Nowcoder platforms during the period from July to December 2024, obtaining real test results through online submissions to ensure the fairness and accuracy of the evaluation. We establish a unified problem attribute system, including difficulty grading and algorithm tagging. With carefully collected and annotated data in ProBench, we systematically assess 9 latest LLMs in competitive programming across multiple dimensions, including thought chain analysis, error type diagnosis, and reasoning depth evaluation. Experimental results show that QwQ-32B-Preview achieves the best score of 20.93 followed by DeepSeek-V3 with a score of 16.38, suggesting that models trained with specialized reasoning tasks significantly outperform general-purpose models (even larger than reasoning-oriented models) in programming. Further analysis also reveals key areas for programming capability enhancement, e.g., algorithm adaptability and reasoning sufficiency, providing important insights for the future development of reasoning models.", "arxiv_url": "https://arxiv.org/abs/2502.20868", "authors": ["Lei Yang", "Renren Jin", "Ling Shi", "Jianxiang Peng", "Yue Chen", "Deyi Xiong"], "first_author": "Lei Yang", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Competitive Programming", "Online Submission Evaluation", "Algorithm Tagging", "Difficulty Grading", "Chain-of-Thought Analysis", "Error Type Diagnosis", "Reasoning Depth Evaluation", "Robustness Testing", "Cross-Lingual Problems"], "summary": "本文提出ProBench——一个针对竞赛编程的在线提交评测基准，通过难度分级与算法标签并结合链式思维与错误诊断，对多款大型语言模型的代码推理能力进行全面而公平的评估与分析。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2502.20868v1", "published": "2025-02-28", "update_time": "2025-02-28", "download_time": "2025-12-17 19:00:03"}
{"id": "2503.04149", "title": "Dynamic Benchmarking of Reasoning Capabilities in Code Large Language Models Under Data Contamination", "abstract": "The rapid evolution of code largelanguage models underscores the need for effective and transparent benchmarking of their reasoning capabilities. However, the current benchmarking approach heavily depends on publicly available, human-created datasets. The widespread use of these fixed benchmark datasets makes the benchmarking process to be static and thus particularly susceptible to data contamination, an unavoidable consequence of the extensive data collection processes used to train Code LLMs. Existing approaches that address data contamination often suffer from human effort limitations and imbalanced problem complexity. To tackle these challenges, we propose \\tool, a novel benchmarking suite for evaluating Code LLMs under potential data contamination. Given a seed programming problem, \\tool employs multiple agents to extract and modify the context without altering the core logic, generating semantically equivalent variations. We introduce a dynamic data generation methods and conduct empirical studies on two seed datasets across 21 Code LLMs. Results show that \\tool effectively benchmarks reasoning capabilities under contamination risks while generating diverse problem sets to ensure consistent and reliable evaluations.", "arxiv_url": "https://arxiv.org/abs/2503.04149", "authors": ["Simin Chen", "Pranav Pusarla", "Baishakhi Ray"], "first_author": "Simin Chen", "category": ["Benchmark", "Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Metamorphic Testing", "Dynamic Problem Generation", "Complexity-Preserving Transformation", "Validation Agent", "Data Contamination", "Semantic Diversification", "Benchmark Robustness", "LLM-as-Judge"], "summary": "本文提出DyCodeEval：一种基于变形测试并使用多Agent LLM自动生成语义多样且保持复杂度不变的动态编程题基准，以在数据污染场景下可靠评估代码LLM的推理能力并通过验证Agent确保题目与测试用例的正确性。", "quality": "High", "conference": "ICML 2025", "pdf_url": "https://arxiv.org/pdf/2503.04149v2", "published": "2025-03-06", "update_time": "2025-06-03", "download_time": "2025-12-17 19:00:33"}
{"id": "2503.10452", "title": "DynaCode: A Dynamic Complexity-Aware Code Benchmark for Evaluating Large Language Models in Code Generation", "abstract": "The rapid advancement of large language models (LLMs) has significantly improved their performance in code generation tasks. However, existing code benchmarks remain static, consisting of fixed datasets with predefined problems. This makes them vulnerable to memorization during training, where LLMs recall specific test cases instead of generalizing to new problems, leading to data contamination and unreliable evaluation results. To address these issues, we introduce DynaCode, a dynamic, complexity-aware benchmark that overcomes the limitations of static datasets. DynaCode evaluates LLMs systematically using a complexity-aware metric, incorporating both code complexity and call-graph structures. DynaCode achieves large-scale diversity, generating up to 189 million unique nested code problems across four distinct levels of code complexity, referred to as units, and 16 types of call graphs. Results on 12 latest LLMs show an average performance drop of 16.8% to 45.7% compared to MBPP+, a static code generation benchmark, with performance progressively decreasing as complexity increases. This demonstrates DynaCode's ability to effectively differentiate LLMs. Additionally, by leveraging call graphs, we gain insights into LLM behavior, particularly their preference for handling subfunction interactions within nested code. Our benchmark and evaluation code are available at https://github.com/HWH-2000/DynaCode.", "arxiv_url": "https://arxiv.org/abs/2503.10452", "authors": ["Wenhao Hu", "Jinhao Duan", "Chunchen Wei", "Li Zhang", "Yue Zhang", "Kaidi Xu"], "first_author": "Wenhao Hu", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Generation Benchmarking", "tags": ["Dynamic Benchmark", "Complexity-aware Metric", "Call-Graph Construction", "Cyclomatic Complexity", "Data Contamination", "Nested Code Generation", "Execution-based Evaluation", "Error Analysis"], "summary": "DynaCode 提出一个基于调用图与复杂度感知的动态代码生成基准，通过自动构造多层嵌套问题生成大规模任务集合，结合静态与动态复杂度度量来更公平地评估并揭示 LLM 在不同代码与调用图复杂度下的弱点，同时缓解数据污染问题。", "quality": "High", "conference": "ACL", "pdf_url": "https://arxiv.org/pdf/2503.10452v2", "published": "2025-03-13", "update_time": "2025-05-29", "download_time": "2025-12-17 19:01:15"}
{"id": "2503.15242", "title": "BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space Complexity?", "abstract": "We introduce BigO(Bench), a novel coding benchmark designed to evaluate the capabilities of generative language models in understanding and generating code with specified time and space complexities. This benchmark addresses the gap in current evaluations that often overlook the ability of models to comprehend and produce code constrained by computational complexity. BigO(Bench) includes tooling to infer the algorithmic complexity of any Python function from profiling measurements, including human- or LLM-generated solutions. BigO(Bench) also includes of set of 3,105 coding problems and 1,190,250 solutions from Code Contests annotated with inferred (synthetic) time and space complexity labels from the complexity framework, as well as corresponding runtime and memory footprint values for a large set of input sizes. We present results from evaluating multiple state-of-the-art language models on this benchmark, highlighting their strengths and weaknesses in handling complexity requirements. In particular, token-space reasoning models are unrivaled in code generation but not in complexity understanding, hinting that they may not generalize well to tasks for which no reward was given at training time.", "arxiv_url": "https://arxiv.org/abs/2503.15242", "authors": ["Pierre Chambon", "Baptiste Roziere", "Benoit Sagot", "Gabriel Synnaeve"], "first_author": "Pierre Chambon", "category": ["Benchmark", "Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Performance Optimization", "tags": ["Dynamic Complexity Inference", "Complexity-conditioned Generation", "Complexity Prediction", "Runtime Profiling", "Memory Footprint Estimation", "Curve Fitting", "Fuzzing-based Input Scaling", "Benchmarking Code Complexity", "Sandboxed Execution", "LLM Complexity Evaluation"], "summary": "本文提出BigO(Bench)，一个包含3,105道题与约1.19M解法并附带动态时间/空间复杂度推断工具的基准，用以评估并比较LLM在满足指定复杂度约束下生成与理解代码的能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2503.15242v2", "published": "2025-03-19", "update_time": "2025-03-20", "download_time": "2025-12-17 19:01:47"}
{"id": "2504.15564", "title": "A Large-scale Class-level Benchmark Dataset for Code Generation with LLMs", "abstract": "Recent advancements in large language models (LLMs) have demonstrated promising capabilities in code generation tasks. However, most existing benchmarks focus on isolated functions and fail to capture the complexity of real-world, class-level software structures. To address this gap, we introduce a large-scale, Python class-level dataset curated from $13{,}174$ real-world open-source projects. The dataset contains over 842,000 class skeletons, each including class and method signatures, along with associated docstrings when available. We preserve structural and contextual dependencies critical to realistic software development scenarios and enrich the dataset with static code metrics to support downstream analysis. To evaluate the usefulness of this dataset, we use extracted class skeletons as prompts for GPT-4 to generate full class implementations. Results show that the LLM-generated classes exhibit strong lexical and structural similarity to human-written counterparts, with average ROUGE@L, BLEU, and TSED scores of 0.80, 0.59, and 0.73, respectively. These findings confirm that well-structured prompts derived from real-world class skeletons significantly enhance LLM performance in class-level code generation. This dataset offers a valuable resource for benchmarking, training, and improving LLMs in realistic software engineering contexts.", "arxiv_url": "https://arxiv.org/abs/2504.15564", "authors": ["Musfiqur Rahman", "SayedHassan Khatoonabadi", "Emad Shihab"], "first_author": "Musfiqur Rahman", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Prompting", "tags": ["Class-level Benchmark", "Class Skeletons", "AST Extraction", "Repository Context", "Static Code Metrics", "Prompt-driven Generation", "Docstring-aware Sampling", "Real-world Scalability"], "summary": "本文构建并公开了一个来自真实开源项目的大规模 Python 类级别基准数据集（约 842,656 个类骨架），保留类结构与上下文并附带静态代码度量，并通过将类骨架作为提示评估 LLM 的类级代码生成能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2504.15564v1", "published": "2025-04-22", "update_time": "2025-04-22", "download_time": "2025-12-17 19:02:18"}
{"id": "2505.04406", "title": "YABLoCo: Yet Another Benchmark for Long Context Code Generation", "abstract": "Large Language Models demonstrate the ability to solve various programming tasks, including code generation. Typically, the performance of LLMs is measured on benchmarks with small or medium-sized context windows of thousands of lines of code. At the same time, in real-world software projects, repositories can span up to millions of LoC. This paper closes this gap by contributing to the long context code generation benchmark (YABLoCo). The benchmark featured a test set of 215 functions selected from four large repositories with thousands of functions. The dataset contained metadata of functions, contexts of the functions with different levels of dependencies, docstrings, functions bodies, and call graphs for each repository. This paper presents three key aspects of the contribution. First, the benchmark aims at function body generation in large repositories in C and C++, two languages not covered by previous benchmarks. Second, the benchmark contains large repositories from 200K to 2,000K LoC. Third, we contribute a scalable evaluation pipeline for efficient computing of the target metrics and a tool for visual analysis of generated code. Overall, these three aspects allow for evaluating code generation in large repositories in C and C++.", "arxiv_url": "https://arxiv.org/abs/2505.04406", "authors": ["Aidar Valeev", "Roman Garaev", "Vadim Lomshakov", "Irina Piontkovskaya", "Vladimir Ivanov", "Israel Adewuyi"], "first_author": "Aidar Valeev", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Long-context", "Large-repository", "C/C++", "Function generation", "Call graph", "Context categorization", "Evaluation pipeline", "Visual analysis", "Docstring assessment", "Test-coverage"], "summary": "该论文提出了YABLoCo基准，用于评估在包含数十万至百万行代码的大型C/C++代码库中基于长上下文的函数体生成，提供了数据集、上下文/依赖分级、可扩展评估管道和可视化分析工具，并对若干现有LLM进行了评估。", "quality": "High", "conference": "LLM4Code 2025 Workshop (co-located with ICSE 2025) 2025", "pdf_url": "https://arxiv.org/pdf/2505.04406v1", "published": "2025-05-07", "update_time": "2025-05-07", "download_time": "2025-12-17 19:02:46"}
{"id": "2505.12331", "title": "OSS-Bench: Benchmark Generator for Coding LLMs", "abstract": "In light of the rapid adoption of AI coding assistants, LLM-assisted development has become increasingly prevalent, creating an urgent need for robust evaluation of generated code quality. Existing benchmarks often require extensive manual effort to create static datasets, rely on indirect or insufficiently challenging tasks, depend on non-scalable ground truth, or neglect critical low-level security evaluations, particularly memory-safety issues. In this work, we introduce OSS-Bench, a benchmark generator that automatically constructs large-scale, live evaluation tasks from real-world open-source software. OSS-Bench replaces functions with LLM-generated code and evaluates them using three natural metrics: compilability, functional correctness, and memory safety, leveraging robust signals like compilation failures, test-suite violations, and sanitizer alerts as ground truth. In our evaluation, the benchmark, instantiated as OSS-Bench(php) and OSS-Bench(sql), profiles 17 diverse LLMs, revealing insights such as intra-family behavioral patterns and inconsistencies between model size and performance. Our results demonstrate that OSS-Bench mitigates overfitting by leveraging the evolving complexity of OSS and highlights LLMs' limited understanding of low-level code security via extended fuzzing experiments. Overall, OSS-Bench offers a practical and scalable framework for benchmarking the real-world coding capabilities of LLMs.", "arxiv_url": "https://arxiv.org/abs/2505.12331", "authors": ["Yuancheng Jiang", "Roland Yap", "Zhenkai Liang"], "first_author": "Yuancheng Jiang", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Live benchmark", "Benchmark generator", "Function-level editing", "Compilability metric", "Test-suite evaluation", "Memory-safety analysis", "Sanitizer-based detection", "Fuzzing evaluation", "OSS-derived tasks", "Overfitting mitigation"], "summary": "本文提出 OSS-BENCH，一种从真实开源软件自动生成可持续更新的基准生成器，按可编译性、测试通过率和内存安全（通过 sanitizer 报告）对 LLM 生成的函数级代码进行评估，并在 PHP 与 SQLite 上对 17 种模型进行了大规模分析。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2505.12331v2", "published": "2025-05-18", "update_time": "2025-05-20", "download_time": "2025-12-17 19:03:14"}
{"id": "2505.15621", "title": "DSCodeBench: A Realistic Benchmark for Data Science Code Generation", "abstract": "We introduce DSCodeBench, a new benchmark designed to evaluate large language models (LLMs) on complicated and realistic data science code generation tasks. DSCodeBench consists of 1,000 carefully constructed problems sourced from realistic problems from GitHub across ten widely used Python data science libraries. DSCodeBench offers a more challenging and representative testbed, more complex code solutions, more comprehensive data science libraries, clearer and better structured problem descriptions, and stronger test suites. To construct the DSCodeBench, we develop a robust pipeline that combines task scope selection, code construction, test case generation, and problem description synthesis. The process is paired with rigorous manual editing to ensure alignment and enhance the reliability of the evaluation. Experimental result shows that DSCodeBench exhibits robust scaling behavior, where larger models systematically outperform smaller ones, validating its ability to distinguish model capabilities. The best LLM we test, GPT-4o, has a pass@1 of 0.392, indicating that LLMs still have a large room to improve for realistic data science code generation tasks. We believe DSCodeBench will serve as a rigorous and trustworthy foundation for advancing LLM-based data science programming.", "arxiv_url": "https://arxiv.org/abs/2505.15621", "authors": ["Shuyin Ouyang", "Dong Huang", "Jingwen Guo", "Zeyu Sun", "Qihao Zhu", "Jie M. Zhang"], "first_author": "Shuyin Ouyang", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Data Science Code", "Benchmark Construction", "Test-Case Script Generation", "Context Reconstruction", "Self-Repair", "Strong Test Suites", "API Coverage", "Manual Curation", "Scaling Behavior", "Data Leakage Mitigation"], "summary": "本文提出了DSCodeBench，一个包含1000个基于GitHub的真实数据科学代码生成任务的基准，结合自动化生成与人工校验的构建管道与强测试套件，并评估多款先进LLM以展示其区分能力与挑战性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2505.15621v3", "published": "2025-05-21", "update_time": "2025-11-16", "download_time": "2025-12-17 19:03:45"}
{"id": "2506.04894", "title": "ICPC-Eval: Probing the Frontiers of LLM Reasoning with Competitive Programming Contests", "abstract": "With the significant progress of large reasoning models in complex coding and reasoning tasks, existing benchmarks, like LiveCodeBench and CodeElo, are insufficient to evaluate the coding capabilities of large language models (LLMs) in real competition environments. Moreover, current evaluation metrics such as Pass@K fail to capture the reflective abilities of reasoning models. To address these challenges, we propose \\textbf{ICPC-Eval}, a top-level competitive coding benchmark designed to probing the frontiers of LLM reasoning. ICPC-Eval includes 118 carefully curated problems from 11 recent ICPC contests held in various regions of the world, offering three key contributions: 1) A challenging realistic ICPC competition scenario, featuring a problem type and difficulty distribution consistent with actual contests. 2) A robust test case generation method and a corresponding local evaluation toolkit, enabling efficient and accurate local evaluation. 3) An effective test-time scaling evaluation metric, Refine@K, which allows iterative repair of solutions based on execution feedback. The results underscore the significant challenge in evaluating complex reasoning abilities: top-tier reasoning models like DeepSeek-R1 often rely on multi-turn code feedback to fully unlock their in-context reasoning potential when compared to non-reasoning counterparts. Furthermore, despite recent advancements in code generation, these models still lag behind top-performing human teams. We release the benchmark at: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs", "arxiv_url": "https://arxiv.org/abs/2506.04894", "authors": ["Shiyi Xu", "Yiwen Hu", "Yingqian Min", "Zhipeng Chen", "Wayne Xin Zhao", "Ji-Rong Wen"], "first_author": "Shiyi Xu", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Competitive Programming", "LLM Test-Case Synthesis", "Local Evaluation Toolkit", "Special Judge Support", "Refine@K", "Iterative Refinement", "Execution Feedback", "Test-time Scaling", "Difficulty Calibration", "Zero False Positives"], "summary": "ICPC-Eval提出了一个包含118道ICPC高难度题目的基准，利用LLM生成并验证本地测试用例并引入Refine@K度量以评估基于执行反馈的迭代修正能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2506.04894v1", "published": "2025-06-05", "update_time": "2025-06-05", "download_time": "2025-12-17 19:04:10"}
{"id": "2512.14429", "title": "Seismology modeling agent: A smart assistant for geophysical researchers", "abstract": "To address the steep learning curve and reliance on complex manual file editing and command-line operations in the traditional workflow of the mainstream open-source seismic wave simulation software SPECFEM, this paper proposes an intelligent, interactive workflow powered by Large Language Models (LLMs). We introduce the first Model Context Protocol (MCP) server suite for SPECFEM (supporting 2D, 3D Cartesian, and 3D Globe versions), which decomposes the entire simulation process into discrete, agent-executable tools spanning from parameter generation and mesh partitioning to solver execution and visualization. This approach enables a paradigm shift from file-driven to intent-driven conversational interactions. The framework supports both fully automated execution and human-in-the-loop collaboration, allowing researchers to guide simulation strategies in real time and retain scientific decision-making authority while significantly reducing tedious low-level operations. Validated through multiple case studies, the workflow operates seamlessly in both autonomous and interactive modes, yielding high-fidelity results consistent with standard baselines. As the first application of MCP technology to computational seismology, this study significantly lowers the entry barrier, enhances reproducibility, and offers a promising avenue for advancing computational geophysics toward AI-assisted and automated scientific research. The complete source code is available at https://github.com/RenYukun1563/specfem-mcp.", "arxiv_url": "https://arxiv.org/abs/2512.14429", "authors": ["Yukun Ren", "Siwei Yu", "Kai Chen", "Jianwei Ma"], "first_author": "Yukun Ren", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Agents", "tags": ["Code Agents", "MCP Integration", "Agentic Workflow", "Seismic Simulation", "Human-in-the-loop", "Legacy Software Wrapping", "Intent-driven Interface", "Reproducibility"], "summary": "本文为SPECFEM构建了首个Model Context Protocol (MCP) 服务器套件，并结合LLM驱动的代理实现以自然语言为中心的自动化与交互式地震波模拟工作流，从而降低使用门槛并提升可重复性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.14429v1", "published": "2025-12-16", "update_time": "2025-12-16", "download_time": "2025-12-18 01:48:51"}
{"id": "2512.14233", "title": "PentestEval: Benchmarking LLM-based Penetration Testing with Modular and Stage-Level Design", "abstract": "Penetration testing is essential for assessing and strengthening system security against real-world threats, yet traditional workflows remain highly manual, expertise-intensive, and difficult to scale. Although recent advances in Large Language Models (LLMs) offer promising opportunities for automation, existing applications rely on simplistic prompting without task decomposition or domain adaptation, resulting in unreliable black-box behavior and limited insight into model capabilities across penetration testing stages. To address this gap, we introduce PentestEval, the first comprehensive benchmark for evaluating LLMs across six decomposed penetration testing stages: Information Collection, Weakness Gathering and Filtering, Attack Decision-Making, Exploit Generation and Revision. PentestEval integrates expert-annotated ground truth with a fully automated evaluation pipeline across 346 tasks covering all stages in 12 realistic vulnerable scenarios. Our stage-level evaluation of 9 widely used LLMs reveals generally weak performance and distinct limitations across the stages of penetration-testing workflow. End-to-end pipelines reach only 31% success rate, and existing LLM-powered systems such as PentestGPT, PentestAgent, and VulnBot exhibit similar limitations, with autonomous agents failing almost entirely. These findings highlight that autonomous penetration testing demands stronger structured reasoning, where modularization enhances each individual stage and improves overall performance. PentestEval provides the foundational benchmark needed for future research on fine-grained, stage-level evaluation, paving the way toward more reliable LLM-based automation.", "arxiv_url": "https://arxiv.org/abs/2512.14233", "authors": ["Ruozhao Yang", "Mingfei Cheng", "Gelei Deng", "Tianwei Zhang", "Junjie Wang", "Xiaofei Xie"], "first_author": "Ruozhao Yang", "category": ["Benchmark", "Empirical"], "field": "Security Testing", "task": "Penetration Testing Evaluation", "tags": ["Exploit Generation", "Exploit Revision", "Attack Decision-Making", "Weakness Gathering", "Weakness Filtering", "Stage-level Evaluation", "Modular Benchmark", "Ground-truth Annotations", "Autonomous Agent Failure", "Structured Reasoning", "End-to-End Evaluation"], "summary": "本文提出PentestEval——一个针对渗透测试六个分阶段的模块化基准，配备专家注释与自动化评测管线，用以细粒度评估LLM在信息收集、弱点过滤、攻击决策与利用生成等环节的能力并揭示现有模型与自动化系统的显著局限性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.14233v1", "published": "2025-12-16", "update_time": "2025-12-16", "download_time": "2025-12-18 01:49:18"}
{"id": "2512.14481", "title": "SASQ: Static Activation Scaling for Quantization-Aware Training in Large Language Models", "abstract": "Large language models (LLMs) excel at natural language tasks but face deployment challenges due to their growing size outpacing GPU memory advancements. Model quantization mitigates this issue by lowering weight and activation precision, but existing solutions face fundamental trade-offs: dynamic quantization incurs high computational overhead and poses deployment challenges on edge devices, while static quantization sacrifices accuracy. Existing approaches of quantization-aware training (QAT) further suffer from weight training costs. We propose SASQ: a lightweight QAT framework specifically tailored for activation quantization factors. SASQ exclusively optimizes only the quantization factors (without changing pre-trained weights), enabling static inference with high accuracy while maintaining deployment efficiency. SASQ adaptively truncates some outliers, thereby reducing the difficulty of quantization while preserving the distributional characteristics of the activations. SASQ not only surpasses existing SOTA quantization schemes but also outperforms the corresponding FP16 models. On LLaMA2-7B, it achieves 5.2% lower perplexity than QuaRot and 4.7% lower perplexity than the FP16 model on WikiText2.", "arxiv_url": "https://arxiv.org/abs/2512.14481", "authors": ["Shizhuo Mao", "Song Chen", "Yi Kang"], "first_author": "Shizhuo Mao", "category": ["Technical"], "field": "Model Compression & Deployment", "task": "Activation Quantization-Aware Training", "tags": ["Activation Quantization", "Quantization-Aware Training", "Static Quantization", "Outlier Truncation", "Phased Quantization", "Static Inference", "Inference Efficiency", "Per-token Quantization"], "summary": "本文提出SASQ，一种仅优化激活量化因子的轻量级量化感知训练方法，通过自适应截断异常值与分阶段量化实现静态推理，从而在不修改预训练权重的情况下提升大模型静态量化精度与部署效率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.14481v1", "published": "2025-12-16", "update_time": "2025-12-16", "download_time": "2025-12-18 01:52:36"}
{"id": "2512.14417", "title": "PortAgent: LLM-driven Vehicle Dispatching Agent for Port Terminals", "abstract": "Vehicle Dispatching Systems (VDSs) are critical to the operational efficiency of Automated Container Terminals (ACTs). However, their widespread commercialization is hindered due to their low transferability across diverse terminals. This transferability challenge stems from three limitations: high reliance on port operational specialists, a high demand for terminal-specific data, and time-consuming manual deployment processes. Leveraging the emergence of Large Language Models (LLMs), this paper proposes PortAgent, an LLM-driven vehicle dispatching agent that fully automates the VDS transferring workflow. It bears three features: (1) no need for port operations specialists; (2) low need of data; and (3) fast deployment. Specifically, specialist dependency is eliminated by the Virtual Expert Team (VET). The VET collaborates with four virtual experts, including a Knowledge Retriever, Modeler, Coder, and Debugger, to emulate a human expert team for the VDS transferring workflow. These experts specialize in the domain of terminal VDS via a few-shot example learning approach. Through this approach, the experts are able to learn VDS-domain knowledge from a few VDS examples. These examples are retrieved via a Retrieval-Augmented Generation (RAG) mechanism, mitigating the high demand for terminal-specific data. Furthermore, an automatic VDS design workflow is established among these experts to avoid extra manual interventions. In this workflow, a self-correction loop inspired by the LLM Reflexion framework is created", "arxiv_url": "https://arxiv.org/abs/2512.14417", "authors": ["Jia Hu", "Junqi Li", "Weimeng Lin", "Peng Jia", "Yuxiong Ji", "Jintao Lai"], "first_author": "Jia Hu", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Agents", "tags": ["Code Agents", "Virtual Expert Team", "RAG", "Few-shot Adaptation", "Self-Correction", "Automated Deployment", "Vehicle Dispatching", "AGV Scheduling", "Domain Transferability", "Human-Free Validation"], "summary": "本文提出PortAgent——一种基于大型语言模型的港口车辆调度迁移代理，通过构建虚拟专家团队、检索增强的少样本学习与自我修正的代码生成-执行-调试闭环，实现低数据、无专家且快速部署的VDS迁移，在未见场景上取得高成功率并显著缩短部署时间。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.14417v1", "published": "2025-12-16", "update_time": "2025-12-16", "download_time": "2025-12-18 01:54:02"}
{"id": "2512.15699", "title": "FrontierCS: Evolving Challenges for Evolving Intelligence", "abstract": "We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. Models solve these tasks by implementing executable programs rather than outputting a direct answer. FrontierCS includes algorithmic problems, which are often NP-hard variants of competitive programming problems with objective partial scoring, and research problems with the same property. For each problem we provide an expert reference solution and an automatic evaluator. Combining open-ended design, measurable progress, and expert curation, FrontierCS provides a benchmark at the frontier of computer-science difficulty. Empirically, we find that frontier reasoning models still lag far behind human experts on both the algorithmic and research tracks, that increasing reasoning budgets alone does not close this gap, and that models often over-optimize for generating merely workable code instead of discovering high-quality algorithms and system designs.", "arxiv_url": "https://arxiv.org/abs/2512.15699", "authors": ["Qiuyang Mang", "Wenhao Chai", "Zhifei Li", "Huanzhi Mao", "Shang Zhou", "Alexander Du", "Hanchen Li", "Shu Liu", "Edwin Chen", "Yichuan Wang", "Xieting Chu", "Zerui Cheng", "Yuan Xu", "Tian Xia", "Zirui Wang", "Tianneng Shi", "Jianzhu Yao", "Yilong Zhao", "Qizheng Zhang", "Charlie Ruan", "Zeyu Shen", "Kaiyuan Liu", "Runyuan He", "Dong Xing", "Zerui Li", "Zirong Zeng", "Yige Jiang", "Lufeng Cheng", "Ziyi Zhao", "Youran Sun", "Wesley Zheng", "Meiyuwang Zhang", "Ruyi Ji", "Xuechang Tu", "Zihan Zheng", "Zexing Chen", "Kangyang Zhou", "Zhaozi Wang", "Jingbang Chen", "Aleksandra Korolova", "Peter Henderson", "Pramod Viswanath", "Vijay Ganesh", "Saining Xie", "Zhuang Liu", "Dawn Song", "Sewon Min", "Ion Stoica", "Joseph E. Gonzalez", "Jingbo Shang", "Alvin Cheung"], "first_author": "Qiuyang Mang", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Algorithmic Optimization", "Open-Ended Benchmark", "Executable Solver Evaluation", "Automatic Scoring", "Parametric Instance Generation", "Expert Reference Solutions", "Human-Model Gap"], "summary": "FrontierCS 提出一个含 156 个多领域、开放式且可量化评分的计算机科学编程基准，要求模型生成可执行求解器并通过自动评估器在无已知最优解的问题上进行定量比较，实验证明当前模型在这些前沿问题上远落后于人类专家。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.15699v1", "published": "2025-12-17", "update_time": "2025-12-17", "download_time": "2025-12-19 01:51:14"}
{"id": "2512.15468", "title": "How Do Semantically Equivalent Code Transformations Impact Membership Inference on LLMs for Code?", "abstract": "The success of large language models for code relies on vast amounts of code data, including public open-source repositories, such as GitHub, and private, confidential code from companies. This raises concerns about intellectual property compliance and the potential unauthorized use of license-restricted code. While membership inference (MI) techniques have been proposed to detect such unauthorized usage, their effectiveness can be undermined by semantically equivalent code transformation techniques, which modify code syntax while preserving semantic.   In this work, we systematically investigate whether semantically equivalent code transformation rules might be leveraged to evade MI detection. The results reveal that model accuracy drops by only 1.5% in the worst case for each rule, demonstrating that transformed datasets can effectively serve as substitutes for fine-tuning. Additionally, we find that one of the rules (RenameVariable) reduces MI success by 10.19%, highlighting its potential to obscure the presence of restricted code. To validate these findings, we conduct a causal analysis confirming that variable renaming has the strongest causal effect in disrupting MI detection. Notably, we find that combining multiple transformations does not further reduce MI effectiveness. Our results expose a critical loophole in license compliance enforcement for training large language models for code, showing that MI detection can be substantially weakened by transformation-based obfuscation techniques.", "arxiv_url": "https://arxiv.org/abs/2512.15468", "authors": ["Hua Yang", "Alejandro Velasco", "Thanh Le-Cong", "Md Nazmul Haque", "Bowen Xu", "Denys Poshyvanyk"], "first_author": "Hua Yang", "category": ["Empirical", "Benchmark"], "field": "Training Data Auditing & Privacy", "task": "Membership Inference", "tags": ["Semantics-Preserving Obfuscation", "RenameVariable", "Membership Inference", "Causal Analysis", "Model Memorization", "Fine-tuning Robustness", "License Compliance"], "summary": "本文系统研究了23种语义等价代码变换对代码型大语言模型会员推断（MI）的影响，发现变换后模型性能几乎不降但MI成功率显著下降（尤其是变量重命名），并通过结构因果模型验证了该因果效应，揭示了基于变换的规避许可合规检测的风险。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.15468v1", "published": "2025-12-17", "update_time": "2025-12-17", "download_time": "2025-12-19 01:51:46"}
{"id": "2512.15688", "title": "BashArena: A Control Setting for Highly Privileged AI Agents", "abstract": "Future AI agents might run autonomously with elevated privileges. If these agents are misaligned, they might abuse these privileges to cause serious damage. The field of AI control develops techniques that make it harder for misaligned AIs to cause such damage, while preserving their usefulness. We introduce BashArena, a setting for studying AI control techniques in security-critical environments. BashArena contains 637 Linux system administration and infrastructure engineering tasks in complex, realistic environments, along with four sabotage objectives (execute malware, exfiltrate secrets, escalate privileges, and disable firewall) for a red team to target. We evaluate multiple frontier LLMs on their ability to complete tasks, perform sabotage undetected, and detect sabotage attempts. Claude Sonnet 4.5 successfully executes sabotage while evading monitoring by GPT-4.1 mini 26% of the time, at 4% trajectory-wise FPR. Our findings provide a baseline for designing more effective control protocols in BashArena. We release the dataset as a ControlArena setting and share our task generation pipeline.", "arxiv_url": "https://arxiv.org/abs/2512.15688", "authors": ["Adam Kaufman", "James Lucassen", "Tyler Tracy", "Cody Rushing", "Aryan Bhatt"], "first_author": "Adam Kaufman", "category": ["Benchmark", "Empirical"], "field": "AI Control & Security", "task": "Privileged-Agent Control Benchmarking", "tags": ["Red Teaming", "Control Protocols", "Monitoring Evasion", "Privileged Execution", "Multi-Step Interaction", "Docker Environments", "Adversarial Benchmark", "Side-Task Detection", "Security Testing", "Autonomous Agents"], "summary": "BashArena 提出一个包含637个复杂 Linux 运维主任务和四类破坏（下载恶意软件、泄露秘密、权限提升、禁用防火墙）的对抗性控制基准，用于评估在高权限环境中 AI 代理的破坏能力与监测防护效果并发布了任务生成管道与数据集。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.15688v1", "published": "2025-12-17", "update_time": "2025-12-17", "download_time": "2025-12-19 01:52:32"}
{"id": "2512.15466", "title": "On Assessing the Relevance of Code Reviews Authored by Generative Models", "abstract": "The use of large language models like ChatGPT in code review offers promising efficiency gains but also raises concerns about correctness and safety. Existing evaluation methods for code review generation either rely on automatic comparisons to a single ground truth, which fails to capture the variability of human perspectives, or on subjective assessments of \"usefulness\", a highly ambiguous concept. We propose a novel evaluation approach based on what we call multi-subjective ranking. Using a dataset of 280 self-contained code review requests and corresponding comments from CodeReview StackExchange, multiple human judges ranked the quality of ChatGPT-generated comments alongside the top human responses from the platform. Results show that ChatGPT's comments were ranked significantly better than human ones, even surpassing StackExchange's accepted answers. Going further, our proposed method motivates and enables more meaningful assessments of generative AI's performance in code review, while also raising awareness of potential risks of unchecked integration into review processes.", "arxiv_url": "https://arxiv.org/abs/2512.15466", "authors": ["Robert Heumüller", "Frank Ortmeier"], "first_author": "Robert Heumüller", "category": ["Empirical", "Benchmark"], "field": "Maintenance", "task": "Code Review", "tags": ["Multi-Subjective Ranking", "Human Annotation", "Code Review Comment Generation", "Evaluation Tooling", "Statistical Analysis", "Stylistic Bias"], "summary": "本文提出并应用多主观排序（multi-subjective ranking）的评估方法，对来自CodeReview StackExchange的280个实例由四名评审者对比排序，结果显示ChatGPT生成的代码审查评论在排名上显著优于人类评论，并发布了数据集与Rankr评估工具，同时讨论了生成评论因语体更为精炼导致的表面偏好风险。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.15466v1", "published": "2025-12-17", "update_time": "2025-12-17", "download_time": "2025-12-19 01:54:45"}
{"id": "2512.16816", "title": "Toward Systematic Counterfactual Fairness Evaluation of Large Language Models: The CAFFE Framework", "abstract": "Nowadays, Large Language Models (LLMs) are foundational components of modern software systems. As their influence grows, concerns about fairness have become increasingly pressing. Prior work has proposed metamorphic testing to detect fairness issues, applying input transformations to uncover inconsistencies in model behavior. This paper introduces an alternative perspective for testing counterfactual fairness in LLMs, proposing a structured and intent-aware framework coined CAFFE (Counterfactual Assessment Framework for Fairness Evaluation). Inspired by traditional non-functional testing, CAFFE (1) formalizes LLM-Fairness test cases through explicitly defined components, including prompt intent, conversational context, input variants, expected fairness thresholds, and test environment configuration, (2) assists testers by automatically generating targeted test data, and (3) evaluates model responses using semantic similarity metrics. Our experiments, conducted on three different architectural families of LLM, demonstrate that CAFFE achieves broader bias coverage and more reliable detection of unfair behavior than existing metamorphic approaches.", "arxiv_url": "https://arxiv.org/abs/2512.16816", "authors": ["Alessandra Parziale", "Gianmario Voria", "Valeria Pontillo", "Gemma Catolino", "Andrea De Lucia", "Fabio Palomba"], "first_author": "Alessandra Parziale", "category": ["Technical", "Empirical"], "field": "Software Testing", "task": "Test Generation", "tags": ["Counterfactual Fairness", "Intent-aware Testing", "Structured Test Cases", "Semantic Similarity Evaluation", "Stereotype-aware Prompt Generation", "Fairness Bug Detection", "Reproducibility"], "summary": "本文提出CAFFE框架，通过结构化的意图感知反事实测试用例生成和基于语义相似度的评估，系统性地检测并量化大语言模型的公平性缺陷，在多种模型上较现有变形测试方法提升了明显的检测覆盖与准确性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.16816v1", "published": "2025-12-18", "update_time": "2025-12-18", "download_time": "2025-12-20 01:45:28"}
{"id": "2512.16790", "title": "Inside Out: Uncovering How Comment Internalization Steers LLMs for Better or Worse", "abstract": "While comments are non-functional elements of source code, Large Language Models (LLM) frequently rely on them to perform Software Engineering (SE) tasks. Yet, where in the model this reliance resides, and how it affects performance, remains poorly understood. We present the first concept-level interpretability study of LLMs in SE, analyzing three tasks - code completion, translation, and refinement - through the lens of internal comment representation. Using Concept Activation Vectors (CAV), we show that LLMs not only internalize comments as distinct latent concepts but also differentiate between subtypes such as Javadocs, inline, and multiline comments. By systematically activating and deactivating these concepts in the LLMs' embedding space, we observed significant, model-specific, and task-dependent shifts in performance ranging from -90% to +67%. Finally, we conducted a controlled experiment using the same set of code inputs, prompting LLMs to perform 10 distinct SE tasks while measuring the activation of the comment concept within their latent representations. We found that code summarization consistently triggered the strongest activation of comment concepts, whereas code completion elicited the weakest sensitivity. These results open a new direction for building SE tools and models that reason about and manipulate internal concept representations rather than relying solely on surface-level input.", "arxiv_url": "https://arxiv.org/abs/2512.16790", "authors": ["Aaron Imani", "Mohammad Moshirpour", "Iftekhar Ahmed"], "first_author": "Aaron Imani", "category": ["Empirical"], "field": "Coding Assistant", "task": "Code Understanding", "tags": ["Interpretability", "Comment Internalization", "Concept Activation Vectors", "Comment Type Differentiation", "Latent Space Intervention", "Task-Conditional Sensitivity", "Model-Specific Effects"], "summary": "本文首次使用概念激活向量（CAV）在LLM的隐藏表示中实证揭示了代码注释作为独立概念的内部化及不同注释类型的区分，并通过激活/抑制这些概念展示了对代码补全、翻译和重构等任务性能可正可负的显著影响。", "quality": "High", "conference": "IEEE/ACM International Conference on Software Engineering (ICSE 2026)", "pdf_url": "https://arxiv.org/pdf/2512.16790v1", "published": "2025-12-18", "update_time": "2025-12-18", "download_time": "2025-12-20 01:45:49"}
{"id": "2512.16814", "title": "Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs", "abstract": "Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.", "arxiv_url": "https://arxiv.org/abs/2512.16814", "authors": ["William English", "Dominic Simon", "Sumit Kumar Jha", "Rickard Ewetz"], "first_author": "William English", "category": ["Technical"], "field": "Requirements & Design", "task": "Specification & Validation", "tags": ["Grammar Forcing", "Grammar-Constrained Decoding", "Atomic Predicate Lifting", "Masked AP Prediction", "Co-reference Handling", "Theoretical Learning Analysis", "Out-of-Distribution Robustness"], "summary": "本文提出GraFT框架，通过对原子命题使用掩码语言模型进行提升并在序列到序列翻译中基于时序逻辑语法动态约束输出词元，从而提高自然语言到时序逻辑的翻译准确性，并给出理论证明与多基准实证评估。", "quality": "High", "conference": "Proceedings of the 42nd International Conference on Machine Learning (ICML 2025)", "pdf_url": "https://arxiv.org/pdf/2512.16814v1", "published": "2025-12-18", "update_time": "2025-12-18", "download_time": "2025-12-20 01:48:23"}
{"id": "2512.16770", "title": "GinSign: Grounding Natural Language Into System Signatures for Temporal Logic Translation", "abstract": "Natural language (NL) to temporal logic (TL) translation enables engineers to specify, verify, and enforce system behaviors without manually crafting formal specifications-an essential capability for building trustworthy autonomous systems. While existing NL-to-TL translation frameworks have demonstrated encouraging initial results, these systems either explicitly assume access to accurate atom grounding or suffer from low grounded translation accuracy. In this paper, we propose a framework for Grounding Natural Language Into System Signatures for Temporal Logic translation called GinSign. The framework introduces a grounding model that learns the abstract task of mapping NL spans onto a given system signature: given a lifted NL specification and a system signature $\\mathcal{S}$, the classifier must assign each lifted atomic proposition to an element of the set of signature-defined atoms $\\mathcal{P}$. We decompose the grounding task hierarchically -- first predicting predicate labels, then selecting the appropriately typed constant arguments. Decomposing this task from a free-form generation problem into a structured classification problem permits the use of smaller masked language models and eliminates the reliance on expensive LLMs. Experiments across multiple domains show that frameworks which omit grounding tend to produce syntactically correct lifted LTL that is semantically nonequivalent to grounded target expressions, whereas our framework supports downstream model checking and achieves grounded logical-equivalence scores of $95.5\\%$, a $1.4\\times$ improvement over SOTA.", "arxiv_url": "https://arxiv.org/abs/2512.16770", "authors": ["William English", "Chase Walker", "Dominic Simon", "Rickard Ewetz"], "first_author": "William English", "category": ["Technical"], "field": "Requirements & Design", "task": "Specification & Validation", "tags": ["Predicate Grounding", "Argument Grounding", "System Signatures", "Lifted NL Translation", "Hierarchical Grounding", "Executable LTL", "Model Checking", "Logical Equivalence"], "summary": "本文提出GinSign框架，通过将自然语言中提升的原子命题分层地分类为系统签名中的谓词和类型化参数来实现可执行的时序逻辑（LTL）翻译，从而显著提升了有语义的地面化翻译准确率并支持下游模型检验。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.16770v1", "published": "2025-12-18", "update_time": "2025-12-18", "download_time": "2025-12-20 01:49:12"}
{"id": "2512.16272", "title": "Beyond Blind Spots: Analytic Hints for Mitigating LLM-Based Evaluation Pitfalls", "abstract": "Large Language Models are increasingly deployed as judges (LaaJ) in code generation pipelines. While attractive for scalability, LaaJs tend to overlook domain specific issues raising concerns about their reliability in critical evaluation tasks. To better understand these limitations in practice, we examine LaaJ behavior in a concrete industrial use case: legacy code modernization via COBOL code generation. In this setting, we find that even production deployed LaaJs can miss domain critical errors, revealing consistent blind spots in their evaluation capabilities.   To better understand these blind spots, we analyze generated COBOL programs and associated LaaJs judgments, drawing on expert knowledge to construct a preliminary taxonomy. Based on this taxonomy, we develop a lightweight analytic checker tool that flags over 30 domain specific issues observed in practice. We use its outputs as analytic hints, dynamically injecting them into the judges prompt to encourage LaaJ to revisit aspects it may have overlooked.   Experiments on a test set of 100 programs using four production level LaaJs show that LaaJ alone detects only about 45% of the errors present in the code (in all judges we tested), while the analytic checker alone lacks explanatory depth. When combined, the LaaJ+Hints configuration achieves up to 94% coverage (for the best performing judge and injection prompt) and produces qualitatively richer, more accurate explanations, demonstrating that analytic-LLM hybrids can substantially enhance evaluation reliability in deployed pipelines. We release the dataset and all used prompts.", "arxiv_url": "https://arxiv.org/abs/2512.16272", "authors": ["Ora Nova Fandina", "Eitan Farchi", "Shmulik Froimovich", "Raviv Gal", "Wesam Ibraheem", "Rami Katan", "Alice Podolsky"], "first_author": "Ora Nova Fandina", "category": ["Technical", "Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Analytic Hints", "LaaJ Blind Spots", "Rule-based Checker", "Hint Injection", "Evaluation Taxonomy", "COBOL Modernization", "Prompt Engineering", "Hybrid Evaluation", "Non-local Reasoning"], "summary": "本文在COBOL代码现代化评估中发现并分类了LLM作为评判器（LaaJ）的领域盲点，构建了含30+问题类型的基于规则的分析检查器，并通过将解析性提示注入评判模型的提示中显著提升了错误检测覆盖率，且发布了数据集与提示。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.16272v1", "published": "2025-12-18", "update_time": "2025-12-18", "download_time": "2025-12-21 01:58:24"}
{"id": "2512.16070", "title": "LLM4Perf: Large Language Models Are Effective Samplers for Multi-Objective Performance Modeling (Copy)", "abstract": "The performance of modern software systems is critically dependent on their complex configuration options. Building accurate performance models to navigate this vast space requires effective sampling strategies, yet existing methods often struggle with multi-objective optimization and cannot leverage semantic information from documentation. The recent success of Large Language Models (LLMs) motivates the central question of this work: Can LLMs serve as effective samplers for multi-objective performance modeling? To explore this, we present a comprehensive empirical study investigating the capabilities and characteristics of LLM-driven sampling. We design and implement LLM4Perf, a feedback-based framework, and use it to systematically evaluate the LLM-guided sampling process across four highly configurable, real-world systems. Our study reveals that the LLM-guided approach outperforms traditional baselines in most cases. Quantitatively, LLM4Perf achieves the best performance in nearly 68.8% (77 out of 112) of all evaluation scenarios, demonstrating its superior effectiveness. We find this effectiveness stems from the LLM's dual capabilities of configuration space pruning and feedback-driven strategy refinement. The effectiveness of this pruning is further validated by the fact that it also improves the performance of the baseline methods in nearly 91.5% (410 out of 448) of cases. Furthermore, we show how the LLM choices for each component and hyperparameters within LLM4Perf affect its effectiveness. Overall, this paper provides strong evidence for the effectiveness of LLMs in performance engineering and offers concrete insights into the mechanisms that drive their success.", "arxiv_url": "https://arxiv.org/abs/2512.16070", "authors": ["Xin Wang", "Zhenhao Li", "Zishuo Ding"], "first_author": "Xin Wang", "category": ["Technical", "Benchmark"], "field": "Performance Engineering", "task": "Multi-Objective Configuration Sampling & Performance Modeling", "tags": ["Configuration Pruning", "Feedback-Driven Sampling", "Multi-Objective Optimization", "Sampling Strategy Design", "Configuration Generation", "Configuration Voting", "Performance Trend Analysis", "Hyperparameter Sensitivity"], "summary": "本文提出LLM4Perf，一个利用大语言模型通过文档驱动的配置空间剪枝与迭代反馈引导多目标性能建模采样的框架，并构建了一个新的多目标性能数据集；实验证明LLM引导的采样在多数情景下优于传统基线，能提升性能模型的准确性与稳定性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.16070v1", "published": "2025-12-18", "update_time": "2025-12-18", "download_time": "2025-12-21 01:58:47"}
{"id": "2512.16465", "title": "cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution", "abstract": "Optimizing CUDA kernels is a challenging and labor-intensive task, given the need for hardware-software co-design expertise and the proprietary nature of high-performance kernel libraries. While recent large language models (LLMs) combined with evolutionary algorithms show promise in automatic kernel optimization, existing approaches often fall short in performance due to their suboptimal agent designs and mismatched evolution representations. This work identifies these mismatches and proposes cuPilot, a strategy-coordinated multi-agent framework that introduces strategy as an intermediate semantic representation for kernel evolution. Key contributions include a strategy-coordinated evolution algorithm, roofline-guided prompting, and strategy-level population initialization. Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09$\\times$ over PyTorch on a benchmark of 100 kernels. On the GEMM tasks, cuPilot showcases sophisticated optimizations and achieves high utilization of critical hardware units. The generated kernels are open-sourced at https://github.com/champloo2878/cuPilot-Kernels.git.", "arxiv_url": "https://arxiv.org/abs/2512.16465", "authors": ["Jinwu Chen", "Qidie Wu", "Bin Li", "Lin Ma", "Xin Si", "Yang Hu", "Shouyi Yin", "Jun Yang"], "first_author": "Jinwu Chen", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Performance Optimization", "tags": ["Strategy-Level Crossover", "Multi-Agent Framework", "Code Agents", "Roofline-Guided Prompting", "Strategy-Level Initialization", "Evolutionary Optimization", "CUDA Kernel Optimization", "Hardware-Aware Optimization", "Code RAG"], "summary": "本文提出 cuPilot——一种以“策略”作为中间表示的多智能体进化框架，通过策略级交叉与翻译、roofline 引导的提示和策略级种群初始化，有效提升 CUDA 内核性能，在 100 个 KernelBench 基准上对比 PyTorch 平均提速 3.09×。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.16465v1", "published": "2025-12-18", "update_time": "2025-12-18", "download_time": "2025-12-21 02:04:47"}
{"id": "2512.16262", "title": "Learning to Wait: Synchronizing Agents with the Physical World", "abstract": "Real-world agentic tasks, unlike synchronous Markov Decision Processes (MDPs), often involve non-blocking actions with variable latencies, creating a fundamental \\textit{Temporal Gap} between action initiation and completion. Existing environment-side solutions, such as blocking wrappers or frequent polling, either limit scalability or dilute the agent's context window with redundant observations. In this work, we propose an \\textbf{Agent-side Approach} that empowers Large Language Models (LLMs) to actively align their \\textit{Cognitive Timeline} with the physical world. By extending the Code-as-Action paradigm to the temporal domain, agents utilize semantic priors and In-Context Learning (ICL) to predict precise waiting durations (\\texttt{time.sleep(t)}), effectively synchronizing with asynchronous environment without exhaustive checking. Experiments in a simulated Kubernetes cluster demonstrate that agents can precisely calibrate their internal clocks to minimize both query overhead and execution latency, validating that temporal awareness is a learnable capability essential for autonomous evolution in open-ended environments.", "arxiv_url": "https://arxiv.org/abs/2512.16262", "authors": ["Yifei She", "Ping Zhang", "He Liu", "Yanmin Jia", "Yang Jing", "Zijun Liu", "Peng Sun", "Xiangbin Li", "Xiaohe Hu"], "first_author": "Yifei She", "category": ["Technical", "Empirical"], "field": "Agent Interaction", "task": "Temporal Synchronization", "tags": ["Code-as-Action", "Temporal Synchronization", "Active Wait", "In-Context Learning", "Asynchronous Environment", "Latency Modeling", "Regret Score", "Semantic Latency Estimation", "Context Efficiency"], "summary": "本文提出一种Agent端方法，通过将Code-as-Action扩展到时间域，让LLM代理利用语义先验与In-Context Learning预测time.sleep(t)以同步其认知时间线与物理世界的异步延迟，并在模拟Kubernetes集群中实验证明可在减少查询开销的同时降低执行延迟。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.16262v1", "published": "2025-12-18", "update_time": "2025-12-18", "download_time": "2025-12-21 02:08:10"}
{"id": "2512.15980", "title": "Embedding Software Intent: Lightweight Java Module Recovery", "abstract": "As an increasing number of software systems reach unprecedented scale, relying solely on code-level abstractions is becoming impractical. While architectural abstractions offer a means to manage these systems, maintaining their consistency with the actual code has been problematic. The Java Platform Module System (JPMS), introduced in Java 9, addresses this limitation by enabling explicit module specification at the language level. JPMS enhances architectural implementation through improved encapsulation and direct specification of ground-truth architectures within Java projects. Although many projects are written in Java, modularizing existing monolithic projects to JPMS modules is an open challenge due to ineffective module recovery by existing architecture recovery techniques. To address this challenge, this paper presents ClassLAR (Class-and Language model-based Architectural Recovery), a novel, lightweight, and efficient approach that recovers Java modules from monolithic Java systems using fully-qualified class names. ClassLAR leverages language models to extract semantic information from package and class names, capturing both structural and functional intent. In evaluations across 20 popular Java projects, ClassLAR outperformed all state-of-the-art techniques in architectural-level similarity metrics while achieving execution times that were 3.99 to 10.50 times faster.", "arxiv_url": "https://arxiv.org/abs/2512.15980", "authors": ["Yirui He", "Yuqi Huai", "Xingyu Chen", "Joshua Garcia"], "first_author": "Yirui He", "category": ["Technical", "Benchmark"], "field": "Architecture Recovery", "task": "Module Recovery", "tags": ["Module Recovery", "Class-name Semantics", "LM Topic Modeling", "JPMS Ground Truth", "Encapsulation", "Architecture Matching", "Lightweight", "Efficiency"], "summary": "本文提出ClassLAR，一种仅基于类的全限定名并利用语言模型的轻量级Java模块（JPMS）恢复方法，在20个开源项目上相比现有方法在架构相似性和封装性上表现更好且运行更快。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.15980v1", "published": "2025-12-17", "update_time": "2025-12-17", "download_time": "2025-12-22 01:57:54"}
{"id": "2512.15979", "title": "OLAF: Towards Robust LLM-Based Annotation Framework in Empirical Software Engineering", "abstract": "Large Language Models (LLMs) are increasingly used in empirical software engineering (ESE) to automate or assist annotation tasks such as labeling commits, issues, and qualitative artifacts. Yet the reliability and reproducibility of such annotations remain underexplored. Existing studies often lack standardized measures for reliability, calibration, and drift, and frequently omit essential configuration details. We argue that LLM-based annotation should be treated as a measurement process rather than a purely automated activity. In this position paper, we outline the \\textbf{Operationalization for LLM-based Annotation Framework (OLAF)}, a conceptual framework that organizes key constructs: \\textit{reliability, calibration, drift, consensus, aggregation}, and \\textit{transparency}. The paper aims to motivate methodological discussion and future empirical work toward more transparent and reproducible LLM-based annotation in software engineering research.", "arxiv_url": "https://arxiv.org/abs/2512.15979", "authors": ["Mia Mohammad Imran", "Tarannum Shaila Zaman"], "first_author": "Mia Mohammad Imran", "category": ["Survey", "Technical"], "field": "Empirical Methods", "task": "LLM-based Annotation & Measurement", "tags": ["Operationalization", "Annotator Reliability", "Consensus Measurement", "Calibration", "Drift Analysis", "Probabilistic Aggregation", "Prompt Sensitivity", "Human-in-the-Loop", "Transparency", "Annotation Workflows"], "summary": "本文提出了OLAF（一套面向软件工程中基于大模型的标注的操作化框架），将标注视为测量过程并定义了可靠性、共识、校准、漂移、聚合与透明性六个维度及相应的配置与报告指南，以提升LLM标注的可重复性与可审计性。", "quality": "Middle", "conference": "3rd International Workshop on Methodological Issues with Empirical Studies in Software Engineering (WSESE) 2026", "pdf_url": "https://arxiv.org/pdf/2512.15979v1", "published": "2025-12-17", "update_time": "2025-12-17", "download_time": "2025-12-22 01:58:12"}
{"id": "2512.16855", "title": "TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge", "abstract": "Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.", "arxiv_url": "https://arxiv.org/abs/2512.16855", "authors": ["Khurram Khalil", "Khaza Anuarul Hoque"], "first_author": "Khurram Khalil", "category": ["Technical"], "field": "Model Compression & Deployment", "task": "LLM Compression with Formal Guarantees", "tags": ["Temporal Logic", "STL Robustness", "Robustness-Guided Optimization", "Layer-wise Quantization", "Layer-wise Pruning", "Bayesian Optimization", "Formal Guarantees", "Edge Deployment", "Runtime Adaptability", "Coherence Preservation"], "summary": "本文提出TOGGLE，一种利用信号时序逻辑（STL）作为形式化约束、通过稳健性驱动的贝叶斯优化在无需重训练的情况下联合搜索层级量化与剪枝配置，从而在保证连贯性、长程依赖、上下文一致性和事实准确性等语言属性的前提下将LLM压缩并部署到边缘设备的框架。", "quality": "High", "conference": "IEEE ICCAD 2025", "pdf_url": "https://arxiv.org/pdf/2512.16855v1", "published": "2025-12-18", "update_time": "2025-12-18", "download_time": "2025-12-22 01:59:54"}
{"id": "2512.16626", "title": "Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game", "abstract": "We introduce Stackelberg Learning from Human Feedback (SLHF), a new framework for preference optimization. SLHF frames the alignment problem as a sequential-move game between two policies: a Leader, which commits to an action, and a Follower, which responds conditionally on the Leader's action. This approach decomposes preference optimization into a refinement problem for the Follower and an optimization problem against an adversary for the Leader. Unlike Reinforcement Learning from Human Feedback (RLHF), which assigns scalar rewards to actions, or Nash Learning from Human Feedback (NLHF), which seeks a simultaneous-move equilibrium, SLHF leverages the asymmetry of sequential play to capture richer preference structures. The sequential design of SLHF naturally enables inference-time refinement, as the Follower learns to improve the Leader's actions, and these refinements can be leveraged through iterative sampling. We compare the solution concepts of SLHF, RLHF, and NLHF, and lay out key advantages in consistency, data sensitivity, and robustness to intransitive preferences. Experiments on large language models demonstrate that SLHF achieves strong alignment across diverse preference datasets, scales from 0.5B to 8B parameters, and yields inference-time refinements that transfer across model families without further fine-tuning.", "arxiv_url": "https://arxiv.org/abs/2512.16626", "authors": ["Barna Pásztor", "Thomas Kleine Buening", "Andreas Krause"], "first_author": "Barna Pásztor", "category": ["Technical"], "field": "LLM Alignment & Preference Learning", "task": "Preference Optimization (Sequential Stackelberg Learning from Human Feedback)", "tags": ["Stackelberg Game", "Preference Optimization", "Inference-Time Refinement", "Follower Refinement", "Two-Timescale GDA", "Pairwise Preference Modeling", "Intransitive Preferences", "Transferable Refinements", "Human-LLM Interaction", "Robustness"], "summary": "本文提出了Stackelberg Learning from Human Feedback (SLHF)，将偏好优化建模为Leader-First、Follower-响应的序贯博弈，给出近似求解算法STACKELBERGGDA并展示其在推理时通过Follower迭代采样实现无微调的输出改进与跨模型迁移能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.16626v1", "published": "2025-12-18", "update_time": "2025-12-18", "download_time": "2025-12-22 02:11:57"}
{"id": "2512.17814", "title": "LLM-based Behaviour Driven Development for Hardware Design", "abstract": "Test and verification are essential activities in hardware and system design, but their complexity grows significantly with increasing system sizes. While Behavior Driven Development (BDD) has proven effective in software engineering, it is not yet well established in hardware design, and its practical use remains limited. One contributing factor is the manual effort required to derive precise behavioral scenarios from textual specifications.   Recent advances in Large Language Models (LLMs) offer new opportunities to automate this step. In this paper, we investigate the use of LLM-based techniques to support BDD in the context of hardware design.", "arxiv_url": "https://arxiv.org/abs/2512.17814", "authors": ["Rolf Drechsler", "Qian Liu"], "first_author": "Rolf Drechsler", "category": ["Technical"], "field": "Software Testing", "task": "Test Generation", "tags": ["Behavior-Driven Development", "Gherkin Scenario Generation", "NL-to-Verilog", "Hardware Verification", "BDD Automation", "Prompt Engineering", "Executable Test Scenarios", "Simulation"], "summary": "本文提出将大型语言模型应用于硬件设计的行为驱动开发（BDD），自动从自然语言规格生成可执行的Gherkin场景并生成对应的Verilog实现，以在ALU案例上进行仿真验证。", "quality": "Middle", "conference": "2nd International Symposium on Artificial Intelligence and Internet of Things (AIIoT-25) 2025", "pdf_url": "https://arxiv.org/pdf/2512.17814v1", "published": "2025-12-19", "update_time": "2025-12-19", "download_time": "2025-12-23 01:52:18"}
{"id": "2512.17540", "title": "SGCR: A Specification-Grounded Framework for Trustworthy LLM Code Review", "abstract": "Automating code review with Large Language Models (LLMs) shows immense promise, yet practical adoption is hampered by their lack of reliability, context-awareness, and control. To address this, we propose Specification-Grounded Code Review (SGCR), a framework that grounds LLMs in human-authored specifications to produce trustworthy and relevant feedback. SGCR features a novel dual-pathway architecture: an explicit path ensures deterministic compliance with predefined rules derived from these specifications, while an implicit path heuristically discovers and verifies issues beyond those rules. Deployed in a live industrial environment at HiThink Research, SGCR's suggestions achieved a 42% developer adoption rate-a 90.9% relative improvement over a baseline LLM (22%). Our work demonstrates that specification-grounding is a powerful paradigm for bridging the gap between the generative power of LLMs and the rigorous reliability demands of software engineering.", "arxiv_url": "https://arxiv.org/abs/2512.17540", "authors": ["Kai Wang", "Bingcheng Mao", "Shuai Jia", "Yujie Ding", "Dongming Han", "Tianyi Ma", "Bin Cao"], "first_author": "Kai Wang", "category": ["Technical", "Empirical"], "field": "Maintenance", "task": "Code Review", "tags": ["Specification Grounding", "Dual-Pathway", "Explicit Specification Injection", "Implicit Specification Discovery", "Code RAG", "Ensemble Aggregation", "Specification Segmentation", "Explainability", "Reliability", "Production Deployment"], "summary": "本文提出SGCR，一种将人工编写规范与双路径（显式规范注入与隐式规范发现）相结合的LLM驱动代码审查框架，结合RAG检索、模型集成与聚合以提高可控性与可信度，并在生产环境中验证了显著的开发者采纳率提升。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.17540v1", "published": "2025-12-19", "update_time": "2025-12-19", "download_time": "2025-12-23 01:52:38"}
{"id": "2512.17419", "title": "SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories", "abstract": "Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.", "arxiv_url": "https://arxiv.org/abs/2512.17419", "authors": ["Lilin Wang", "Lucas Ramalho", "Alan Celestino", "Phuc Anthony Pham", "Yu Liu", "Umang Kumar Sinha", "Andres Portillo", "Onassis Osunwa", "Gabriel Maduekwe"], "first_author": "Lilin Wang", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Repository-Level Coding", "tags": ["Repository-Level Coding", "Environment Synthesis", "Adaptive Log Parsing", "State-Differential Oracle", "Hint-Guided Trajectories", "Multilingual Benchmark", "Contamination-Aware Evaluation", "Automated Benchmark Generation"], "summary": "SWE-Bench++ 提出一个自动化、多语言且可扩展的框架，能够从真实 GitHub pull request 自动合成可复现的容器环境和自适应日志解析器、以三态差分判定区分缺陷与功能请求，并通过提示引导生成可用于训练的轨迹来构建大规模仓库级评测集。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.17419v1", "published": "2025-12-19", "update_time": "2025-12-19", "download_time": "2025-12-23 01:56:12"}
{"id": "2512.17259", "title": "Verifiability-First Agents: Provable Observability and Lightweight Audit Agents for Controlling Autonomous LLM Systems", "abstract": "As LLM-based agents grow more autonomous and multi-modal, ensuring they remain controllable, auditable, and faithful to deployer intent becomes critical. Prior benchmarks measured the propensity for misaligned behavior and showed that agent personalities and tool access significantly influence misalignment. Building on these insights, we propose a Verifiability-First architecture that (1) integrates run-time attestations of agent actions using cryptographic and symbolic methods, (2) embeds lightweight Audit Agents that continuously verify intent versus behavior using constrained reasoning, and (3) enforces challenge-response attestation protocols for high-risk operations. We introduce OPERA (Observability, Provable Execution, Red-team, Attestation), a benchmark suite and evaluation protocol designed to measure (i) detectability of misalignment, (ii) time to detection under stealthy strategies, and (iii) resilience of verifiability mechanisms to adversarial prompt and persona injection. Our approach shifts the evaluation focus from how likely misalignment is to how quickly and reliably misalignment can be detected and remediated.", "arxiv_url": "https://arxiv.org/abs/2512.17259", "authors": ["Abhivansh Gupta"], "first_author": "Abhivansh Gupta", "category": ["Technical", "Benchmark"], "field": "Autonomous Agents & Safety", "task": "Agent Verifiability and Audit", "tags": ["Action Attestation", "Intent Specification", "Audit Agents", "Provenance Log", "Challenge–Response Attestation", "Signed Receipts", "Runtime Verification", "Verifiability Benchmark", "Adversarial Prompt Robustness", "Remediation Controls"], "summary": "本文提出可验证性优先的智能体架构，通过意图规范、加密动作证明与轻量级审计代理以及挑战-响应协议实现运行时可观测性与可审计性，并引入评估检测速度与鲁棒性的OPERA基准。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.17259v1", "published": "2025-12-19", "update_time": "2025-12-19", "download_time": "2025-12-23 01:57:34"}
{"id": "2512.19644", "title": "More code, less validation: Risk factors for over-reliance on AI coding tools among scientists", "abstract": "Programming is essential to modern scientific research, yet most scientists report inadequate training for the software development their work demands. Generative AI tools capable of code generation may support scientific programmers, but user studies indicate risks of over-reliance, particularly among inexperienced users. We surveyed 868 scientists who program, examining adoption patterns, tool preferences, and factors associated with perceived productivity. Adoption is highest among students and less experienced programmers, with variation across fields. Scientific programmers overwhelmingly prefer general-purpose conversational interfaces like ChatGPT over developer-specific tools. Both inexperience and limited use of development practices (like testing, code review, and version control) are associated with greater perceived productivity-but these factors interact, suggesting formal practices may partially compensate for inexperience. The strongest predictor of perceived productivity is the number of lines of generated code typically accepted at once. These findings suggest scientific programmers using generative AI may gauge productivity by code generation rather than validation, raising concerns about research code integrity.", "arxiv_url": "https://arxiv.org/abs/2512.19644", "authors": ["Gabrielle O'Brien", "Alexis Parker", "Nasir Eisty", "Jeffrey Carver"], "first_author": "Gabrielle O'Brien", "category": ["Empirical"], "field": "Coding Assistant", "task": "Code Prompting", "tags": ["Human-LLM Interaction", "Over-reliance", "Scientific Software", "Survey Study", "Code Validation", "Testing Adoption", "Version Control Usage", "Perceived Productivity", "Experience Effects", "Tool Preference"], "summary": "本文基于对868名从事编程的科学家的问卷调查，发现学生与经验较少的研究人员更常使用以对话为主的生成式AI（如ChatGPT），且缺乏测试、代码审查与版本控制等实践与更高的自感生产力相关，尤其是一次性接受较多生成代码是感知生产力的最强预测因子，提示科研人员可能以生成量而非验证为依据过度依赖AI，威胁研究代码完整性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.19644v1", "published": "2025-12-22", "update_time": "2025-12-22", "download_time": "2025-12-24 01:50:13"}
{"id": "2512.19509", "title": "Beyond Language Boundaries: Uncovering Programming Language Families for Code Language Models", "abstract": "The rapid proliferation of diverse programming languages presents both opportunities and challenges for developing multilingual code LLMs. While existing techniques often train code LLMs by simply aggregating multilingual code data, few explore the deeper relationships between programming languages(PLs) and how such relationships can be utilized to optimize the training and inference of code LLMs. In this work, we investigate 2 fundamental questions: 1) What are the deep linguistic relationships among PLs? and 2) How can these relationships be leveraged to improve multilingual code LLMs? We propose an embedding-based framework to uncover the latent families of PLs. Our approach begins by defining 21 primary linguistic features of programming languages, such as variable definition, control structures, and method declarations, and then employs LLMs to generate feature-aligned code samples across multiple languages. By embedding these semantically parallel code snippets from 19 languages, we construct a similarity matrix and perform hierarchical clustering to uncover inherent language relationships. Our analysis reveals clear hierarchical structures among programming languages. Closely related languages form well-defined clusters (e.g., C, C++, Java, and Swift group together), while Go exhibits as a central language with the highest cross-language similarity. Building on the uncovered language families, we propose three strategies to enhance multilingual LLM training: transfer learning across linguistically related languages, linguistic proximity-guided curriculum learning, and centroid-based intermediary code translation. Experiments on 4 code intelligence tasks demonstrate that our methods significantly improve multilingual LLM performance. This work offers a universal perspective on programming languages and advances more effective strategies for multilingual code LLM training.", "arxiv_url": "https://arxiv.org/abs/2512.19509", "authors": ["Shangbo Yun", "Xiaodong Gu", "Jianghong Huang", "Beijun Shen"], "first_author": "Shangbo Yun", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Representation Learning", "tags": ["Code Representation Learning", "Programming Language Families", "Language Similarity", "Feature-aligned Generation", "Transfer Learning", "Curriculum Learning", "Intermediary Code Translation", "Centroid Language"], "summary": "本文通过定义21项编程语言特征并生成跨19种语言的语义对齐代码片段，基于代码模型嵌入发现编程语言族谱，并利用语言相似性提出迁移学习、相似度引导的课程学习和基于质心的中介代码翻译策略以提升多语言代码LLM性能。", "quality": "High", "conference": "FSE 2026", "pdf_url": "https://arxiv.org/pdf/2512.19509v1", "published": "2025-12-22", "update_time": "2025-12-22", "download_time": "2025-12-24 01:50:36"}
{"id": "2512.19481", "title": "A Dataset and Preliminary Study of Using GPT-5 for Code-change Impact Analysis", "abstract": "Understanding source code changes and their impact on other code entities is a crucial skill in software development. However, the analysis of code changes and their impact is often performed manually and therefore is time-consuming. Recent advancements in AI, and in particular large language models (LLMs) show promises to help developers in various code analysis tasks. However, the extent to which this potential can be utilized for understanding code changes and their impact is underexplored. To address this gap, we study the capabilities of GPT-5 and GPT-5-mini to predict the code entities impacted by given source code changes. We construct a dataset containing information about seed-changes, change pairs, and change types for each commit. Existing datasets lack crucial information about seed changes and impacted code entities. Our experiments evaluate the LLMs in two configurations: (1) seed-change information and the parent commit tree and (2) seed-change information, the parent commit tree, and the diff hunk of each seed change. We found that both LLMs perform poorly in the two experiments, whereas GPT-5 outperforms GPT-5-mini. Furthermore, the provision of the diff hunks helps both models to slightly improve their performance.", "arxiv_url": "https://arxiv.org/abs/2512.19481", "authors": ["Katharina Stengg", "Christian Macho", "Martin Pinzger"], "first_author": "Katharina Stengg", "category": ["Benchmark", "Empirical"], "field": "Maintenance", "task": "Change Impact Analysis", "tags": ["Change Impact Prediction", "Commit-level Dataset", "Seed-change Annotation", "Diff-hunk Prompting", "Prompt Optimization", "Manual Annotation", "Java Code Analysis", "LLM Evaluation"], "summary": "本文构建了一个扩展的代码变更影响数据集（包含种子变更、变更对与diff hunk），并对GPT-5与GPT-5-mini在代码变更影响预测任务上进行了初步评估，结果表明模型总体表现较差但在提供diff hunk时略有改善。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.19481v1", "published": "2025-12-22", "update_time": "2025-12-22", "download_time": "2025-12-24 01:52:55"}
{"id": "2512.19396", "title": "EchoTrail-GUI: Building Actionable Memory for GUI Agents via Critic-Guided Self-Exploration", "abstract": "Contemporary GUI agents, while increasingly capable due to advances in Large Vision-Language Models (VLMs), often operate with a critical limitation: they treat each task in isolation, lacking a mechanism to systematically learn from past successes. This digital ''amnesia'' results in sub-optimal performance, repeated errors, and poor generalization to novel challenges. To bridge this gap, we introduce EchoTrail-GUI, a novel framework designed to mimic human-like experiential learning by equipping agents with a dynamic, accessible memory. Our framework operates in three distinct stages. First, during Experience Exploration, an agent autonomously interacts with GUI environments to build a curated database of successful task trajectories, validated by a reward model. Crucially, the entire knowledge base construction is thus fully automated, requiring no human supervision. Second, in the Memory Injection stage, upon receiving a new task, our system efficiently retrieves the most relevant past trajectories to serve as actionable ''memories''. Finally, during GUI Task Inference, these memories are injected as in-context guidance to inform the agent's reasoning and decision-making process. We demonstrate the efficacy of our approach on benchmarks including Android World and AndroidLab. The results show that EchoTrail-GUI significantly improves the task success rate and operational efficiency of baseline agents, validating the power of structured memory in creating more robust and intelligent GUI automation.", "arxiv_url": "https://arxiv.org/abs/2512.19396", "authors": ["Runze Li", "Yuwen Zhai", "Bo Xu", "LiWu Xu", "Nian Shi", "Wei Zhang", "Ran Lin", "Liang Wang"], "first_author": "Runze Li", "category": ["Technical", "Benchmark"], "field": "GUI Interaction & Agents", "task": "Memory-Augmented GUI Automation", "tags": ["Memory-Augmented Agents", "Critic-Guided Exploration", "Autonomous Trajectory Collection", "Trajectory Curation", "Dense-Sparse Retrieval", "Memory Injection", "Retrieval-Augmented Inference", "GUI Automation"], "summary": "本文提出EchoTrail-GUI，通过评论家引导的自主探索自动构建高质量的GUI操作轨迹库，并采用稠密-稀疏检索将相关成功轨迹作为记忆注入到代理的推理过程中，从而在Android基准上显著提升任务成功率与效率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.19396v1", "published": "2025-12-22", "update_time": "2025-12-22", "download_time": "2025-12-24 01:53:43"}
{"id": "2512.20482", "title": "SweRank+: Multilingual, Multi-Turn Code Ranking for Software Issue Localization", "abstract": "Maintaining large-scale, multilingual codebases hinges on accurately localizing issues, which requires mapping natural-language error descriptions to the relevant functions that need to be modified. However, existing ranking approaches are often Python-centric and perform a single-pass search over the codebase. This work introduces SweRank+, a framework that couples SweRankMulti, a cross-lingual code ranking tool, with SweRankAgent, an agentic search setup, for iterative, multi-turn reasoning over the code repository. SweRankMulti comprises a code embedding retriever and a listwise LLM reranker, and is trained using a carefully curated large-scale issue localization dataset spanning multiple popular programming languages. SweRankAgent adopts an agentic search loop that moves beyond single-shot localization with a memory buffer to reason and accumulate relevant localization candidates over multiple turns. Our experiments on issue localization benchmarks spanning various languages demonstrate new state-of-the-art performance with SweRankMulti, while SweRankAgent further improves localization over single-pass ranking.", "arxiv_url": "https://arxiv.org/abs/2512.20482", "authors": ["Revanth Gangi Reddy", "Ye Liu", "Wenting Zhao", "JaeHyeok Doo", "Tarun Suresh", "Daniel Lee", "Caiming Xiong", "Yingbo Zhou", "Semih Yavuz", "Shafiq Joty"], "first_author": "Revanth Gangi Reddy", "category": ["Technical", "Benchmark"], "field": "Quality Management", "task": "Bug Localization", "tags": ["Multilingual Code", "Code Ranking", "Retriever-Reranker", "Listwise Reranking", "Agentic Search", "Multi-Turn Reasoning", "Memory Buffer", "Hard-Negative Mining", "Repository-Level Localization"], "summary": "本文提出SWERANK+，通过构建多语言数据集SWELOCMULTI并结合基于检索-重排序的SWERANKMULTI与具备记忆缓冲的多轮迭代搜索代理SWERANKAGENT，实现跨多种编程语言的高精度问题定位并刷新多项基线性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.20482v1", "published": "2025-12-23", "update_time": "2025-12-23", "download_time": "2025-12-25 01:52:43"}
{"id": "2512.20334", "title": "Comment Traps: How Defective Commented-out Code Augment Defects in AI-Assisted Code Generation", "abstract": "With the rapid development of large language models in code generation, AI-powered editors such as GitHub Copilot and Cursor are revolutionizing software development practices. At the same time, studies have identified potential defects in the generated code. Previous research has predominantly examined how code context influences the generation of defective code, often overlooking the impact of defects within commented-out code (CO code). AI coding assistants' interpretation of CO code in prompts affects the code they generate.   This study evaluates how AI coding assistants, GitHub Copilot and Cursor, are influenced by defective CO code. The experimental results show that defective CO code in the context causes AI coding assistants to generate more defective code, reaching up to 58.17 percent. Our findings further demonstrate that the tools do not simply copy the defective code from the context. Instead, they actively reason to complete incomplete defect patterns and continue to produce defective code despite distractions such as incorrect indentation or tags. Even with explicit instructions to ignore the defective CO code, the reduction in defects does not exceed 21.84 percent. These findings underscore the need for improved robustness and security measures in AI coding assistants.", "arxiv_url": "https://arxiv.org/abs/2512.20334", "authors": ["Yuan Huang", "Yukang Zhou", "Xiangping Chen", "Zibin Zheng"], "first_author": "Yuan Huang", "category": ["Empirical"], "field": "Coding Assistant", "task": "Code Prompting", "tags": ["Commented-out Code", "Defect Amplification", "Prompt Contamination", "Prompt Engineering", "Context Sparsity", "Robustness", "Security and Vulnerabilities", "Human-LLM Interaction"], "summary": "本研究在大规模真实Python代码库中注入带缺陷的注释掉代码并在GitHub Copilot与Cursor上测试，发现这些注释掉的缺陷代码会显著增加AI生成代码中的缺陷率，且通过提示工程难以完全缓解该问题。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.20334v1", "published": "2025-12-23", "update_time": "2025-12-23", "download_time": "2025-12-25 01:53:26"}
{"id": "2512.20387", "title": "Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems", "abstract": "We propose a Vision-Language Simulation Model (VLSM) that unifies visual and textual understanding to synthesize executable FlexScript from layout sketches and natural-language prompts, enabling cross-modal reasoning for industrial simulation systems. To support this new paradigm, the study constructs the first large-scale dataset for generative digital twins, comprising over 120,000 prompt-sketch-code triplets that enable multimodal learning between textual descriptions, spatial structures, and simulation logic. In parallel, three novel evaluation metrics, Structural Validity Rate (SVR), Parameter Match Rate (PMR), and Execution Success Rate (ESR), are proposed specifically for this task to comprehensively evaluate structural integrity, parameter fidelity, and simulator executability. Through systematic ablation across vision encoders, connectors, and code-pretrained language backbones, the proposed models achieve near-perfect structural accuracy and high execution robustness. This work establishes a foundation for generative digital twins that integrate visual reasoning and language understanding into executable industrial simulation systems.", "arxiv_url": "https://arxiv.org/abs/2512.20387", "authors": ["YuChe Hsu", "AnJui Wang", "TsaiChing Ni", "YuanFu Yang"], "first_author": "YuChe Hsu", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Multimodal code generation", "Layout-to-code", "Simulation DSL synthesis", "Sketch-conditioned synthesis", "Vision-Language Fusion", "Structural Validity", "Execution Robustness", "Parameter Fidelity", "Industrial Digital Twins", "Human-AI Prompting"], "summary": "本文提出视觉-语言仿真模型（VLSM）并构建首个大规模多模态数据集与专用评测指标，将布局草图和自然语言提示自动生成可执行的仿真脚本以实现工业数字孪生的自动化建模。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.20387v1", "published": "2025-12-23", "update_time": "2025-12-23", "download_time": "2025-12-25 01:55:57"}
{"id": "2512.20328", "title": "Toward Explaining Large Language Models in Software Engineering Tasks", "abstract": "Recent progress in Large Language Models (LLMs) has substantially advanced the automation of software engineering (SE) tasks, enabling complex activities such as code generation and code summarization. However, the black-box nature of LLMs remains a major barrier to their adoption in high-stakes and safety-critical domains, where explainability and transparency are vital for trust, accountability, and effective human supervision. Despite increasing interest in explainable AI for software engineering, existing methods lack domain-specific explanations aligned with how practitioners reason about SE artifacts. To address this gap, we introduce FeatureSHAP, the first fully automated, model-agnostic explainability framework tailored to software engineering tasks. Based on Shapley values, FeatureSHAP attributes model outputs to high-level input features through systematic input perturbation and task-specific similarity comparisons, while remaining compatible with both open-source and proprietary LLMs. We evaluate FeatureSHAP on two bi-modal SE tasks: code generation and code summarization. The results show that FeatureSHAP assigns less importance to irrelevant input features and produces explanations with higher fidelity than baseline methods. A practitioner survey involving 37 participants shows that FeatureSHAP helps practitioners better interpret model outputs and make more informed decisions. Collectively, FeatureSHAP represents a meaningful step toward practical explainable AI in software engineering. FeatureSHAP is available at https://github.com/deviserlab/FeatureSHAP.", "arxiv_url": "https://arxiv.org/abs/2512.20328", "authors": ["Antonio Vitale", "Khai-Nguyen Nguyen", "Denys Poshyvanyk", "Rocco Oliveto", "Simone Scalabrino", "Antonio Mastropaolo"], "first_author": "Antonio Vitale", "category": ["Technical", "Empirical"], "field": "Explainability & Interpretability", "task": "Feature-level Attribution", "tags": ["Feature-level Attribution", "Shapley Values", "Model-Agnostic Explanation", "Perturbation-based Attribution", "Prompt Feature Decomposition", "Actionable Explanations", "Faithfulness", "Human-LLM Interaction", "Code Generation", "Code Summarization", "User Study"], "summary": "本文提出FeatureSHAP，一种基于Shapley值的模型不可知的特征级可解释性框架，通过对语义层面输入特征进行系统性扰动与相似性比较，为代码生成和代码摘要任务提供更具可操作性与高保真性的解释，并通过定量实验与37位从业者调研验证其实用性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.20328v1", "published": "2025-12-23", "update_time": "2025-12-23", "download_time": "2025-12-25 01:56:42"}
{"id": "2512.21238", "title": "Assessing the Software Security Comprehension of Large Language Models", "abstract": "Large language models (LLMs) are increasingly used in software development, but their level of software security expertise remains unclear. This work systematically evaluates the security comprehension of five leading LLMs: GPT-4o-Mini, GPT-5-Mini, Gemini-2.5-Flash, Llama-3.1, and Qwen-2.5, using Blooms Taxonomy as a framework. We assess six cognitive dimensions: remembering, understanding, applying, analyzing, evaluating, and creating. Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments from an Introduction to Software Security course, real-world case studies (XBOW), and project-based creation tasks from a Secure Software Engineering course. Results show that while LLMs perform well on lower-level cognitive tasks such as recalling facts and identifying known vulnerabilities, their performance degrades significantly on higher-order tasks that require reasoning, architectural evaluation, and secure system creation. Beyond reporting aggregate accuracy, we introduce a software security knowledge boundary that identifies the highest cognitive level at which a model consistently maintains reliable performance. In addition, we identify 51 recurring misconception patterns exhibited by LLMs across Blooms levels.", "arxiv_url": "https://arxiv.org/abs/2512.21238", "authors": ["Mohammed Latif Siddiq", "Natalie Sekerak", "Antonio Karam", "Maria Leal", "Arvin Islam-Gomes", "Joanna C. S. Santos"], "first_author": "Mohammed Latif Siddiq", "category": ["Empirical", "Benchmark"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Security and Vulnerabilities", "Knowledge Boundary", "Bloom's Taxonomy", "Misconception Taxonomy", "Concept Inventory", "Evaluation Framework", "Higher-Order Reasoning", "Educational Assessment", "Hallucination"], "summary": "本文提出了基于布鲁姆认知分类的Basket评估框架，系统性地测评五种主流大模型在软件安全领域的理解能力，发现模型在记忆与识别层面表现良好但在分析、评估与创建等高阶认知任务上显著退化，并归纳出51类常见误解与知识边界。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.21238v1", "published": "2025-12-24", "update_time": "2025-12-24", "download_time": "2025-12-26 01:51:53"}
{"id": "2512.21236", "title": "Casting a SPELL: Sentence Pairing Exploration for LLM Limitation-breaking", "abstract": "Large language models (LLMs) have revolutionized software development through AI-assisted coding tools, enabling developers with limited programming expertise to create sophisticated applications. However, this accessibility extends to malicious actors who may exploit these powerful tools to generate harmful software. Existing jailbreaking research primarily focuses on general attack scenarios against LLMs, with limited exploration of malicious code generation as a jailbreak target. To address this gap, we propose SPELL, a comprehensive testing framework specifically designed to evaluate the weakness of security alignment in malicious code generation. Our framework employs a time-division selection strategy that systematically constructs jailbreaking prompts by intelligently combining sentences from a prior knowledge dataset, balancing exploration of novel attack patterns with exploitation of successful techniques. Extensive evaluation across three advanced code models (GPT-4.1, Claude-3.5, and Qwen2.5-Coder) demonstrates SPELL's effectiveness, achieving attack success rates of 83.75%, 19.38%, and 68.12% respectively across eight malicious code categories. The generated prompts successfully produce malicious code in real-world AI development tools such as Cursor, with outputs confirmed as malicious by state-of-the-art detection systems at rates exceeding 73%. These findings reveal significant security gaps in current LLM implementations and provide valuable insights for improving AI safety alignment in code generation applications.", "arxiv_url": "https://arxiv.org/abs/2512.21236", "authors": ["Yifan Huang", "Xiaojun Jia", "Wenbo Guo", "Yuqiang Sun", "Yihao Huang", "Chong Wang", "Yang Liu"], "first_author": "Yifan Huang", "category": ["Technical", "Benchmark"], "field": "Security & Misuse", "task": "Malicious Prompt Generation", "tags": ["Jailbreaking", "Sentence Pairing", "Prompt Component Discovery", "Exploit-Exploration Strategy", "Time-Division Selection", "Automated Attack Generation", "Defense Strategies", "Black-box Evaluation", "IDE Real-world Deployment", "Security and Vulnerabilities"], "summary": "本文提出SPELL框架，通过自动发现并组合提示句子组件以生成新的越狱提示，从而在多款代码LLM上高效生成可执行恶意代码并同时验证与提出可部署的防御策略。", "quality": "High", "conference": "ACM SIGSOFT International Symposium on the Foundations of Software Engineering (FSE 2026)", "pdf_url": "https://arxiv.org/pdf/2512.21236v1", "published": "2025-12-24", "update_time": "2025-12-24", "download_time": "2025-12-26 01:52:15"}
{"id": "2512.21336", "title": "Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty", "abstract": "Masked Diffusion Models (MDMs) offer flexible, non-autoregressive generation, but this freedom introduces a challenge: final output quality is highly sensitive to the decoding order. We are the first to formalize this issue, attributing the variability in output quality to the cumulative predictive uncertainty along a generative path. To quantify this uncertainty, we introduce Denoising Entropy, a computable metric that serves as an internal signal for evaluating generative process. Leveraging this metric, we propose two algorithms designed to optimize the decoding path: a post-hoc selection method and a real-time guidance strategy. Experiments demonstrate that our entropy-guided methods significantly improve generation quality, consistently boosting accuracy on challenging reasoning, planning, and code benchmarks. Our work establishes Denoising Entropy as a principled tool for understanding and controlling generation, effectively turning the uncertainty in MDMs from a liability into a key advantage for discovering high-quality solutions.", "arxiv_url": "https://arxiv.org/abs/2512.21336", "authors": ["Ziyu Chen", "Xinbei Jiang", "Peng Sun", "Tao Lin"], "first_author": "Ziyu Chen", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Path Uncertainty", "Denoising Entropy", "Entropy-guided Decoding", "Post-hoc Reranking", "Sequential Monte Carlo", "Masked Diffusion Models", "Non-autoregressive Generation", "Code Generation"], "summary": "本文提出了量化MDM解码路径不确定性的Denoising Entropy度量，并基于此设计后验重排和实时引导的熵驱动解码算法，以显著提升推理、规划与代码生成任务的生成质量。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.21336v1", "published": "2025-12-24", "update_time": "2025-12-24", "download_time": "2025-12-26 01:52:37"}
{"id": "2512.21332", "title": "C2LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention Pooling", "abstract": "We present C2LLM - Contrastive Code Large Language Models, a family of code embedding models in both 0.5B and 7B sizes. Building upon Qwen-2.5-Coder backbones, C2LLM adopts a Pooling by Multihead Attention (PMA) module for generating sequence embedding from token embeddings, effectively 1) utilizing the LLM's causal representations acquired during pretraining, while also 2) being able to aggregate information from all tokens in the sequence, breaking the information bottleneck in EOS-based sequence embeddings, and 3) supporting flexible adaptation of embedding dimension, serving as an alternative to MRL. Trained on three million publicly available data, C2LLM models set new records on MTEB-Code among models of similar sizes, with C2LLM-7B ranking 1st on the overall leaderboard.", "arxiv_url": "https://arxiv.org/abs/2512.21332", "authors": ["Jin Qin", "Zihan Liao", "Ziyin Zhang", "Hang Yu", "Peng Di", "Rui Wang"], "first_author": "Jin Qin", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Retrieval", "tags": ["Cross-Attention Pooling", "Contrastive Learning", "Causal Attention Preservation", "Embedding Dimension Adaptation", "Code Embeddings", "Retrieval Optimization"], "summary": "本文提出C2LLM——在Qwen-2.5-Coder基础上通过在LLM输出上加入Pooling by Multihead Attention（PMA）模块并采用对比学习训练的代码嵌入模型家族，从而在保持因果注意力的同时改进序列聚合与维度适配，在MTEB-Code上取得同规模模型的领先检索性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.21332v1", "published": "2025-12-24", "update_time": "2025-12-24", "download_time": "2025-12-26 01:52:58"}
{"id": "2512.21028", "title": "Artificial or Just Artful? Do LLMs Bend the Rules in Programming?", "abstract": "Large Language Models (LLMs) are widely used for automated code generation, yet their apparent successes often mask a tension between pretraining objectives and alignment choices. While pretraining encourages models to exploit all available signals to maximize success, alignment, whether through fine-tuning or prompting, may restrict their use. This conflict is especially salient in agentic AI settings, for instance when an agent has access to unit tests that, although intended for validation, act as strong contextual signals that can be leveraged regardless of explicit prohibitions. In this paper, we investigate how LLMs adapt their code generation strategies when exposed to test cases under different prompting conditions. Using the BigCodeBench (Hard) dataset, we design five prompting conditions that manipulate test visibility and impose explicit or implicit restrictions on their use. We evaluate five LLMs (four open-source and one closed-source) across correctness, code similarity, program size, and code churn, and analyze cross-model consistency to identify recurring adaptation strategies. Our results show that test visibility dramatically alters performance, correctness nearly doubles for some models, while explicit restrictions or partial exposure only partially mitigate this effect. Beyond raw performance, we identify four recurring adaptation strategies, with test-driven refinement emerging as the most frequent. These results highlight how LLMs adapt their behavior when exposed to contextual signals that conflict with explicit instructions, providing useful insight into how models reconcile pretraining objectives with alignment constraints.", "arxiv_url": "https://arxiv.org/abs/2512.21028", "authors": ["Oussama Ben Sghaier", "Kevin Delcourt", "Houari Sahraoui"], "first_author": "Oussama Ben Sghaier", "category": ["Empirical"], "field": "Coding Assistant", "task": "Code Prompting", "tags": ["Test-Driven Refinement", "Contextual Signal Exploitation", "Alignment Conflict", "Instruction Override", "Prompt Restrictions", "Correctness Improvement", "Code Similarity", "Code Churn", "Cross-Model Consistency", "Empirical Analysis"], "summary": "本文通过对五种提示条件下（完整/部分/显式禁止使用单元测试等）五个LLM在BigCodeBench上的对比实证研究，发现测试可见性显著提升正确率且模型常常在显式禁令下仍利用测试信号，表现出以测试为驱动的多种适应策略。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.21028v1", "published": "2025-12-24", "update_time": "2025-12-24", "download_time": "2025-12-27 01:49:29"}
{"id": "2512.20957", "title": "One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents", "abstract": "Locating the files and functions requiring modification in large open-source software (OSS) repositories is challenging due to their scale and structural complexity. Existing large language model (LLM)-based methods typically treat this as a repository-level retrieval task and rely on multiple auxiliary tools, which overlook code execution logic and complicate model control. We propose RepoNavigator, an LLM agent equipped with a single execution-aware tool-jumping to the definition of an invoked symbol. This unified design reflects the actual flow of code execution while simplifying tool manipulation. RepoNavigator is trained end-to-end via Reinforcement Learning (RL) directly from a pretrained model, without any closed-source distillation. Experiments demonstrate that RL-trained RepoNavigator achieves state-of-the-art performance, with the 7B model outperforming 14B baselines, the 14B model surpassing 32B competitors, and even the 32B model exceeding closed-source models such as Claude-3.7. These results confirm that integrating a single, structurally grounded tool with RL training provides an efficient and scalable solution for repository-level issue localization.", "arxiv_url": "https://arxiv.org/abs/2512.20957", "authors": ["Zhaoxi Zhang", "Yitong Duan", "Yanzhi Zhang", "Yiming Xu", "Jiyan He", "Yunfang Wu"], "first_author": "Zhaoxi Zhang", "category": ["Technical"], "field": "Quality Management", "task": "Bug Localization", "tags": ["Repository-Level Coding", "Issue Localization", "Jump Tool", "Language-Server Integration", "Reinforcement Learning", "Tool-Augmented Agent", "Execution-Aware Navigation", "End-to-End Training", "Model Distillation-Free"], "summary": "本文提出RepoNavigator，一种配备单一执行感知“jump”工具并通过在预训练模型上直接进行强化学习端到端训练的仓库级定位代理，可高效定位大型开源代码库中的相关文件与函数并在多项基准上取得SOTA性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.20957v1", "published": "2025-12-24", "update_time": "2025-12-24", "download_time": "2025-12-27 01:49:46"}
{"id": "2512.21132", "title": "AutoBaxBuilder: Bootstrapping Code Security Benchmarking", "abstract": "As LLMs see wide adoption in software engineering, the reliable assessment of the correctness and security of LLM-generated code is crucial. Notably, prior work has demonstrated that security is often overlooked, exposing that LLMs are prone to generating code with security vulnerabilities. These insights were enabled by specialized benchmarks, crafted through significant manual effort by security experts. However, relying on manually-crafted benchmarks is insufficient in the long term, because benchmarks (i) naturally end up contaminating training data, (ii) must extend to new tasks to provide a more complete picture, and (iii) must increase in difficulty to challenge more capable LLMs. In this work, we address these challenges and present AutoBaxBuilder, a framework that generates tasks and tests for code security benchmarking from scratch. We introduce a robust pipeline with fine-grained plausibility checks, leveraging the code understanding capabilities of LLMs to construct functionality tests and end-to-end security-probing exploits. To confirm the quality of the generated benchmark, we conduct both a qualitative analysis and perform quantitative experiments, comparing it against tasks constructed by human experts. We use AutoBaxBuilder to construct entirely new tasks and release them to the public as AutoBaxBench, together with a thorough evaluation of the security capabilities of LLMs on these tasks. We find that a new task can be generated in under 2 hours, costing less than USD 10.", "arxiv_url": "https://arxiv.org/abs/2512.21132", "authors": ["Tobias von Arx", "Niels Mündler", "Mark Vero", "Maximilian Baader", "Martin Vechev"], "first_author": "Tobias von Arx", "category": ["Technical", "Benchmark"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Security and Vulnerabilities", "Automated benchmark generation", "End-to-end exploit synthesis", "Functional test generation", "Scenario synthesis", "CWE classification", "Benchmark contamination mitigation", "Difficulty stratification", "Reproducibility", "Code Agents", "Efficiency"], "summary": "本文提出AUTOBAXBUILDER，一种基于LLM的自动化流水线，能从零生成后端安全基准任务（场景、功能测试和端到端漏洞利用脚本），并以此构建并公开AUTOBAXBENCH以评估模型的安全与正确性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.21132v1", "published": "2025-12-24", "update_time": "2025-12-24", "download_time": "2025-12-27 01:51:28"}
{"id": "2512.21024", "title": "Policy-Conditioned Policies for Multi-Agent Task Solving", "abstract": "In multi-agent tasks, the central challenge lies in the dynamic adaptation of strategies. However, directly conditioning on opponents' strategies is intractable in the prevalent deep reinforcement learning paradigm due to a fundamental ``representational bottleneck'': neural policies are opaque, high-dimensional parameter vectors that are incomprehensible to other agents. In this work, we propose a paradigm shift that bridges this gap by representing policies as human-interpretable source code and utilizing Large Language Models (LLMs) as approximate interpreters. This programmatic representation allows us to operationalize the game-theoretic concept of \\textit{Program Equilibrium}. We reformulate the learning problem by utilizing LLMs to perform optimization directly in the space of programmatic policies. The LLM functions as a point-wise best-response operator that iteratively synthesizes and refines the ego agent's policy code to respond to the opponent's strategy. We formalize this process as \\textit{Programmatic Iterated Best Response (PIBR)}, an algorithm where the policy code is optimized by textual gradients, using structured feedback derived from game utility and runtime unit tests. We demonstrate that this approach effectively solves several standard coordination matrix games and a cooperative Level-Based Foraging environment.", "arxiv_url": "https://arxiv.org/abs/2512.21024", "authors": ["Yue Lin", "Shuhui Zhu", "Wenhao Li", "Ang Li", "Dan Qiao", "Pascal Poupart", "Hongyuan Zha", "Baoxiang Wang"], "first_author": "Yue Lin", "category": ["Technical"], "field": "Multi-Agent Reinforcement Learning", "task": "Programmatic policy conditioning / Programmatic Iterated Best Response (PIBR)", "tags": ["Programmatic Policies", "Code Agents", "Program Equilibrium", "Textual Gradients", "Iterated Best Response", "Runtime Unit Tests", "Multi-Agent Coordination", "LLM Policy Synthesis", "Interpretability"], "summary": "本文提出将策略表示为可执行源代码并利用大语言模型作为近似解释器，提出Programmatic Iterated Best Response (PIBR)算法，通过文本梯度结合博弈效用与运行时单元测试在代码空间对策略进行迭代优化以实现多智能体协调。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.21024v1", "published": "2025-12-24", "update_time": "2025-12-24", "download_time": "2025-12-27 01:52:36"}
{"id": "2512.20732", "title": "FEM-Bench: A Structured Scientific Reasoning Benchmark for Evaluating Code-Generating LLMs", "abstract": "As LLMs advance their reasoning capabilities about the physical world, the absence of rigorous benchmarks for evaluating their ability to generate scientifically valid physical models has become a critical gap. Computational mechanics, which develops and applies mathematical models and numerical methods to predict the behavior of physical systems under forces, deformation, and constraints, provides an ideal foundation for structured scientific reasoning evaluation. Problems follow clear mathematical structure, enforce strict physical and numerical constraints, and support objective verification. The discipline requires constructing explicit models of physical systems and reasoning about geometry, spatial relationships, and material behavior, connecting directly to emerging AI goals in physical reasoning and world modeling. We introduce FEM-Bench, a computational mechanics benchmark designed to evaluate the ability of LLMs to generate correct finite element method (FEM) and related code. FEM-Bench 2025 contains a suite of introductory but nontrivial tasks aligned with material from a first graduate course on computational mechanics. These tasks capture essential numerical and physical modeling challenges while representing only a small fraction of the complexity present in the discipline. Despite their simplicity, state-of-the-art LLMs do not reliably solve all of them. In a five attempt run, the best performing model at function writing, Gemini 3 Pro, completed 30/33 tasks at least once and 26/33 tasks all five times. The best performing model at unit test writing, GPT-5, had an Average Joint Success Rate of 73.8%. Other popular models showed broad performance variation. FEM-Bench establishes a structured foundation for evaluating AI-generated scientific code, and future iterations will incorporate increasingly sophisticated tasks to track progress as models evolve.", "arxiv_url": "https://arxiv.org/abs/2512.20732", "authors": ["Saeed Mohammadzadeh", "Erfan Hamdi", "Joel Shor", "Emma Lejeune"], "first_author": "Saeed Mohammadzadeh", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["FEM Code Generation", "Scientific Computing", "Numerical Verification", "Mesh Convergence", "Physical Modeling", "Unit Test Generation", "Floating-point Sensitivity", "Structured Scientific Reasoning"], "summary": "本文提出FEM-Bench，一个用于评估大模型生成有限元与计算力学代码的结构化基准，包含入门但有挑战性的任务集、自动化评估工具和多模型基线测试以量化科学计算代码生成能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.20732v1", "published": "2025-12-23", "update_time": "2025-12-23", "download_time": "2025-12-28 02:03:21"}
{"id": "2512.20203", "title": "Well Begun is Half Done: Location-Aware and Trace-Guided Iterative Automated Vulnerability Repair", "abstract": "The advances of large language models (LLMs) have paved the way for automated software vulnerability repair approaches, which iteratively refine the patch until it becomes plausible. Nevertheless, existing LLM-based vulnerability repair approaches face notable limitations: 1) they ignore the concern of locations that need to be patched and focus solely on the repair content. 2) they lack quality assessment for generated candidate patches in the iterative process.   To tackle the two limitations, we propose \\sysname, an LLM-based approach that provides information about where should be patched first. Furthermore, \\sysname improves the iterative repair strategy by assessing the quality of test-failing patches and selecting the best patch for the next iteration. We introduce two dimensions to assess the quality of patches: whether they introduce new vulnerabilities and the taint statement coverage. We evaluated \\sysname on a real-world C/C++ vulnerability repair dataset VulnLoc+, which contains 40 vulnerabilities and their Proofs-of-Vulnerability. The experimental results demonstrate that \\sysname exhibits substantial improvements compared with the Neural Machine Translation-based, Program Analysis-based, and LLM-based state-of-the-art vulnerability repair approaches. Specifically, \\sysname is able to generate 27 plausible patches, which is comparable to or even 8 to 22 more plausible patches than the baselines. In terms of correct patch generation, \\sysname repairs 8 to 13 additional vulnerabilities compared with existing approaches.", "arxiv_url": "https://arxiv.org/abs/2512.20203", "authors": ["Zhenlei Ye", "Xiaobing Sun", "Sicong Cao", "Lili Bo", "Bin Li"], "first_author": "Zhenlei Ye", "category": ["Technical"], "field": "Quality Management", "task": "Vulnerability Repair", "tags": ["Program Repair", "Security and Vulnerabilities", "Location-Aware Patch Prediction", "Taint-Guided Ranking", "Iterative Repair", "Patch Quality Assessment", "Multi-Hunk Vulnerability", "PoV Verification"], "summary": "本文提出LoopRepair，一种结合补丁位置预测与污点追踪引导的迭代自动漏洞修复方法，通过预测需修复的代码行并基于是否引入新漏洞与污点语句覆盖对候选补丁排序选择，从而在多块（multi-hunk）C/C++漏洞修复上显著优于现有方法。", "quality": "High", "conference": "ICSE 2026", "pdf_url": "https://arxiv.org/pdf/2512.20203v1", "published": "2025-12-23", "update_time": "2025-12-23", "download_time": "2025-12-28 02:04:32"}
{"id": "2512.20968", "title": "Mesh-Attention: A New Communication-Efficient Distributed Attention with Improved Data Locality", "abstract": "Distributed attention is a fundamental problem for scaling context window for Large Language Models (LLMs). The state-of-the-art method, Ring-Attention, suffers from scalability limitations due to its excessive communication traffic. This paper proposes a new distributed attention algorithm, Mesh-Attention, by rethinking the design space of distributed attention with a new matrix-based model. Our method assigns a two-dimensional tile -- rather than one-dimensional row or column -- of computation blocks to each GPU to achieve higher efficiency through lower communication-computation (CommCom) ratio. The general approach covers Ring-Attention as a special case, and allows the tuning of CommCom ratio with different tile shapes. Importantly, we propose a greedy algorithm that can efficiently search the scheduling space within the tile with restrictions that ensure efficient communication among GPUs. The theoretical analysis shows that Mesh-Attention leads to a much lower communication complexity and exhibits good scalability comparing to other current algorithms.   Our extensive experiment results show that Mesh-Attention can achieve up to 3.4x speedup (2.9x on average) and reduce the communication volume by up to 85.4% (79.0% on average) on 256 GPUs. Our scalability results further demonstrate that Mesh-Attention sustains superior performance as the system scales, substantially reducing overhead in large-scale deployments. The results convincingly confirm the advantage of Mesh-Attention.", "arxiv_url": "https://arxiv.org/abs/2512.20968", "authors": ["Sirui Chen", "Jingji Chen", "Siqi Zhu", "Ziheng Jiang", "Yanghua Peng", "Xuehai Qian"], "first_author": "Sirui Chen", "category": ["Technical"], "field": "Model Infrastructure", "task": "Distributed Attention / Model Parallelism", "tags": ["Mesh Tiling", "Assignment Matrix", "CommCom Ratio", "Local Q-KV Property", "Greedy Scheduling", "Computation-Communication Overlap", "Communication Reduction", "GPU Scalability"], "summary": "本文提出Mesh-Attention，一种基于二维tile分配与赋值矩阵的分布式注意力算法，通过提高数据局部性和贪心调度最大化计算-通信重叠，在256 GPU上平均实现2.9×加速并显著降低通信量（最高85.4%）。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.20968v1", "published": "2025-12-24", "update_time": "2025-12-24", "download_time": "2025-12-28 02:07:59"}
{"id": "2512.20845", "title": "MAR:Multi-Agent Reflexion Improves Reasoning Abilities in LLMs", "abstract": "LLMs have shown the capacity to improve their performance on reasoning tasks through reflecting on their mistakes, and acting with these reflections in mind. However, continual reflections of the same LLM onto itself exhibit degeneration of thought, where the LLM continues to repeat the same errors again and again even with the knowledge that its wrong. To address this problem, we instead introduce multi-agent with multi-persona debators as the method to generate reflections. Through out extensive experimentation, we've found that the leads to better diversity of in the reflections generated by the llm agent. We demonstrate an accuracy of 47% EM HotPot QA (question answering) and 82.7% on HumanEval (programming), both performances surpassing reflection with a single llm.", "arxiv_url": "https://arxiv.org/abs/2512.20845", "authors": ["Onat Ozer", "Grace Wu", "Yuchen Wang", "Daniel Dosti", "Honghao Zhang", "Vivi De La Rue"], "first_author": "Onat Ozer", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Multi-Agent Debate", "Episodic Memory", "Self-Reflection", "Judge Aggregation", "Persona Diversity", "Iterative Refinement", "Code Agents", "Human-LLM Interaction", "Failure Mode Analysis", "Correctness"], "summary": "本文复现并分析了单代理 Reflexion 的失效模式，提出 Multi‑Agent Reflexion（MAR）——通过多样化人格的批评者和一个裁判聚合反思来减少认知盲点，从而在 HotPotQA 与 HumanEval 上提升了推理与代码生成正确率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.20845v1", "published": "2025-12-23", "update_time": "2025-12-23", "download_time": "2025-12-28 02:09:42"}
{"id": "2512.22113", "title": "Agentic Structured Graph Traversal for Root Cause Analysis of Code-related Incidents in Cloud Applications", "abstract": "Cloud incidents pose major operational challenges in production, with unresolved production cloud incidents cost on average over $2M per hour. Prior research identifies code- and configuration-related issues as the predominant category of root causes in cloud incidents. This paper introduces PRAXIS, an orchestrator that manages and deploys an agentic workflow for diagnosing code- and configuration-caused cloud incidents. PRAXIS employs an LLM-driven structured traversal over two types of graph: (1) a service dependency graph (SDG) that captures microservice-level dependencies; and (2) a hammock-block program dependence graph (PDG) that captures code-level dependencies for each microservice. Together, these graphs encode microservice- and code-level dependencies and the LLM acts as a traversal policy over these graphs, moving between services and code dependencies to localize and explain failures. Compared to state-of-the-art ReAct baselines, PRAXIS improves RCA accuracy by up to 3.1x while reducing token consumption by 3.8x. PRAXIS is demonstrated on a set of 30 comprehensive real-world incidents that is being compiled into an RCA benchmark.", "arxiv_url": "https://arxiv.org/abs/2512.22113", "authors": ["Shengkun Cui", "Rahul Krishna", "Saurabh Jha", "Ravishankar K. Iyer"], "first_author": "Shengkun Cui", "category": ["Technical", "Benchmark"], "field": "SRE & Incident Diagnosis", "task": "Root Cause Analysis", "tags": ["Agentic Orchestration", "Cross-SDG-PDG Traversal", "Service Dependency Graph", "Hammock-block PDG", "Structure-aware Reasoning", "Observability Integration", "Root-Cause Identification", "Token Efficiency"], "summary": "本文提出PRAXIS，一种基于LLM驱动的结构化图遍历代理化编排器，通过联合利用微服务依赖图和hammock块程序依赖图对云应用中代码与配置引起的故障进行跨层根因分析，在30个真实场景基准上显著提高诊断准确率并降低token消耗。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.22113v1", "published": "2025-12-26", "update_time": "2025-12-26", "download_time": "2025-12-29 02:01:25"}
{"id": "2512.21818", "title": "Analyzing Code Injection Attacks on LLM-based Multi-Agent Systems in Software Development", "abstract": "Agentic AI and Multi-Agent Systems are poised to dominate industry and society imminently. Powered by goal-driven autonomy, they represent a powerful form of generative AI, marking a transition from reactive content generation into proactive multitasking capabilities. As an exemplar, we propose an architecture of a multi-agent system for the implementation phase of the software engineering process. We also present a comprehensive threat model for the proposed system. We demonstrate that while such systems can generate code quite accurately, they are vulnerable to attacks, including code injection. Due to their autonomous design and lack of humans in the loop, these systems cannot identify and respond to attacks by themselves. This paper analyzes the vulnerability of multi-agent systems and concludes that the coder-reviewer-tester architecture is more resilient than both the coder and coder-tester architectures, but is less efficient at writing code. We find that by adding a security analysis agent, we mitigate the loss in efficiency while achieving even better resiliency. We conclude by demonstrating that the security analysis agent is vulnerable to advanced code injection attacks, showing that embedding poisonous few-shot examples in the injected code can increase the attack success rate from 0% to 71.95%.", "arxiv_url": "https://arxiv.org/abs/2512.21818", "authors": ["Brian Bowers", "Smita Khapre", "Jugal Kalita"], "first_author": "Brian Bowers", "category": ["Technical", "Empirical"], "field": "Maintenance", "task": "Code Review", "tags": ["Code Agents", "Code Injection", "Few-shot Poisoning", "Security Analysis Agent", "Threat Modeling", "Architectural Resilience", "Adversarial Robustness", "Data Exfiltration"], "summary": "本文提出并评估了用于软件实现阶段的多代理系统架构和相应的威胁模型，展示在代码注入攻击下不同架构（coder、coder-tester、coder-reviewer-tester）在安全性与效率间的权衡，并提出加入安全分析代理作为缓解措施但同时揭示该代理可被嵌入有害少样本示例的高级注入攻击显著绕过（成功率从0%提升到71.95%）。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.21818v1", "published": "2025-12-26", "update_time": "2025-12-26", "download_time": "2025-12-29 02:01:53"}
{"id": "2512.20861", "title": "Memory-Efficient Acceleration of Block Low-Rank Foundation Models on Resource Constrained GPUs", "abstract": "Recent advances in transformer-based foundation models have made them the default choice for many tasks, but their rapidly growing size makes fitting a full model on a single GPU increasingly difficult and their computational cost prohibitive. Block low-rank (BLR) compression techniques address this challenge by learning compact representations of weight matrices. While traditional low-rank (LR) methods often incur sharp accuracy drops, BLR approaches such as Monarch and BLAST can better capture the underlying structure, thus preserving accuracy while reducing computations and memory footprints. In this work, we use roofline analysis to show that, although BLR methods achieve theoretical savings and practical speedups for single-token inference, multi-token inference often becomes memory-bound in practice, increasing latency despite compiler-level optimizations in PyTorch. To address this, we introduce custom Triton kernels with partial fusion and memory layout optimizations for both Monarch and BLAST. On memory-constrained NVIDIA GPUs such as Jetson Orin Nano and A40, our kernels deliver up to $3.76\\times$ speedups and $3\\times$ model size compression over PyTorch dense baselines using CUDA backend and compiler-level optimizations, while supporting various models including Llama-7/1B, GPT2-S, DiT-XL/2, and ViT-B. Our code is available at https://github.com/pabillam/mem-efficient-blr .", "arxiv_url": "https://arxiv.org/abs/2512.20861", "authors": ["Pierre Abillama", "Changwoo Lee", "Juechu Dong", "David Blaauw", "Dennis Sylvester", "Hun-Seok Kim"], "first_author": "Pierre Abillama", "category": ["Technical"], "field": "Model Deployment & Systems", "task": "Inference Acceleration & Memory Optimization", "tags": ["Block Low-Rank", "Triton Kernels", "Roofline Analysis", "Memory Layout Optimization", "Partial Fusion", "Inference Acceleration", "Edge GPUs", "Model Compression", "Intermediate Data Movement", "Hardware-aware Optimization"], "summary": "本文通过屋顶线分析揭示了块低秩（BLR）分解在多令牌推理中因中间数据移动而变为内存瓶颈，并提出基于Triton的部分融合与内存布局优化内核，在受限GPU上显著加速并压缩模型。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.20861v1", "published": "2025-12-24", "update_time": "2025-12-24", "download_time": "2025-12-29 02:07:00"}
{"id": "2512.20823", "title": "NotSoTiny: A Large, Living Benchmark for RTL Code Generation", "abstract": "LLMs have shown early promise in generating RTL code, yet evaluating their capabilities in realistic setups remains a challenge. So far, RTL benchmarks have been limited in scale, skewed toward trivial designs, offering minimal verification rigor, and remaining vulnerable to data contamination. To overcome these limitations and to push the field forward, this paper introduces NotSoTiny, a benchmark that assesses LLM on the generation of structurally rich and context-aware RTL. Built from hundreds of actual hardware designs produced by the Tiny Tapeout community, our automated pipeline removes duplicates, verifies correctness and periodically incorporates new designs to mitigate contamination, matching Tiny Tapeout release schedule. Evaluation results show that NotSoTiny tasks are more challenging than prior benchmarks, emphasizing its effectiveness in overcoming current limitations of LLMs applied to hardware design, and in guiding the improvement of such promising technology.", "arxiv_url": "https://arxiv.org/abs/2512.20823", "authors": ["Razine Moundir Ghorab", "Emanuele Parisi", "Cristian Gutierrez", "Miquel Alberti-Binimelis", "Miquel Moreto", "Dario Garcia-Gasulla", "Gokcen Kestor"], "first_author": "Razine Moundir Ghorab", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["RTL Code Generation", "Contextual Module Completion", "Formal Equivalence Checking", "Simulation-based Evaluation", "Temporal Deduplication", "Near-duplicate Detection", "MinHash-LSH", "Contamination Mitigation", "Structural Complexity", "Living Benchmark"], "summary": "本文提出NotSoTiny——一个基于Tiny Tapeout真实硬件设计、经过去重与周期性刷新以降低数据污染的规模化RTL模块补全基准，并结合仿真与形式等价检查提供严格可扩展的评测流程以衡量LLM在复杂结构化硬件代码生成中的表现。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.20823v1", "published": "2025-12-23", "update_time": "2025-12-23", "download_time": "2025-12-29 02:07:41"}
{"id": "2512.21757", "title": "How Do Agents Perform Code Optimization? An Empirical Study", "abstract": "Performance optimization is a critical yet challenging aspect of software development, often requiring a deep understanding of system behavior, algorithmic tradeoffs, and careful code modifications. Although recent advances in AI coding agents have accelerated code generation and bug fixing, little is known about how these agents perform on real-world performance optimization tasks. We present the first empirical study comparing agent- and human-authored performance optimization commits, analyzing 324 agent-generated and 83 human-authored PRs from the AIDev dataset across adoption, maintainability, optimization patterns, and validation practices. We find that AI-authored performance PRs are less likely to include explicit performance validation than human-authored PRs (45.7\\% vs. 63.6\\%, $p=0.007$). In addition, AI-authored PRs largely use the same optimization patterns as humans. We further discuss limitations and opportunities for advancing agentic code optimization.", "arxiv_url": "https://arxiv.org/abs/2512.21757", "authors": ["Huiyun Peng", "Antonio Zhong", "Ricardo Andrés Calvo Méndez", "Kelechi G. Kalu", "James C. Davis"], "first_author": "Huiyun Peng", "category": ["Empirical"], "field": "Coding Assistant", "task": "Code Performance Optimization", "tags": ["Performance validation", "Optimization patterns", "LLM-assisted annotation", "Catalog refinement", "Agent-human comparison", "Merge/adoption metrics", "Static reasoning", "Profiling-based validation"], "summary": "本文基于 AIDev 数据集实证分析了 407 个性能相关 PR（含 324 个 AI 代理生成的 PR），比较了 AI 与人类在性能优化模式、验证实践、合并率与可维护性上的差异，发现 AI 提交虽然采用与人类相似的优化模式，但较少提供性能验证且更依赖静态推理，并据此扩展并修订了性能优化模式目录。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.21757v1", "published": "2025-12-25", "update_time": "2025-12-25", "download_time": "2025-12-30 01:53:39"}
{"id": "2512.21681", "title": "Exploring the Security Threats of Retriever Backdoors in Retrieval-Augmented Code Generation", "abstract": "Retrieval-Augmented Code Generation (RACG) is increasingly adopted to enhance Large Language Models for software development, yet its security implications remain dangerously underexplored. This paper conducts the first systematic exploration of a critical and stealthy threat: backdoor attacks targeting the retriever component, which represents a significant supply-chain vulnerability. It is infeasible to assess this threat realistically, as existing attack methods are either too ineffective to pose a real danger or are easily detected by state-of-the-art defense mechanisms spanning both latent-space analysis and token-level inspection, which achieve consistently high detection rates. To overcome this barrier and enable a realistic analysis, we first developed VenomRACG, a new class of potent and stealthy attack that serves as a vehicle for our investigation. Its design makes poisoned samples statistically indistinguishable from benign code, allowing the attack to consistently maintain low detectability across all evaluated defense mechanisms. Armed with this capability, our exploration reveals a severe vulnerability: by injecting vulnerable code equivalent to only 0.05% of the entire knowledge base size, an attacker can successfully manipulate the backdoored retriever to rank the vulnerable code in its top-5 results in 51.29% of cases. This translates to severe downstream harm, causing models like GPT-4o to generate vulnerable code in over 40% of targeted scenarios, while leaving the system's general performance intact. Our findings establish that retriever backdooring is not a theoretical concern but a practical threat to the software development ecosystem that current defenses are blind to, highlighting the urgent need for robust security measures.", "arxiv_url": "https://arxiv.org/abs/2512.21681", "authors": ["Tian Li", "Bo Lin", "Shangwen Wang", "Yusong Tan"], "first_author": "Tian Li", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Retrieval", "tags": ["Retriever Backdoor", "Supply-Chain Attack", "Semantic Disruption Injection", "Vulnerability-Aware Trigger", "Syntax-Semantic Injection", "Stealthy Poisoning", "Defense Evasion", "Transferability", "Downstream Vulnerability Induction", "ASR Evaluation"], "summary": "本文提出VenomRACG，一种针对检索器的高效且难以检测的后门攻击，通过语义扰动注入与脆弱性感知触发器在极少量投毒样本下使检索器优先返回易受攻击代码并诱导下游生成器输出脆弱代码，且能规避现有检测方法。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.21681v1", "published": "2025-12-25", "update_time": "2025-12-25", "download_time": "2025-12-30 01:53:55"}
{"id": "2512.21613", "title": "AMS-IO-Bench and AMS-IO-Agent: Benchmarking and Structured Reasoning for Analog and Mixed-Signal Integrated Circuit Input/Output Design", "abstract": "In this paper, we propose AMS-IO-Agent, a domain-specialized LLM-based agent for structure-aware input/output (I/O) subsystem generation in analog and mixed-signal (AMS) integrated circuits (ICs). The central contribution of this work is a framework that connects natural language design intent with industrial-level AMS IC design deliverables. AMS-IO-Agent integrates two key capabilities: (1) a structured domain knowledge base that captures reusable constraints and design conventions; (2) design intent structuring, which converts ambiguous user intent into verifiable logic steps using JSON and Python as intermediate formats. We further introduce AMS-IO-Bench, a benchmark for wirebond-packaged AMS I/O ring automation. On this benchmark, AMS-IO-Agent achieves over 70\\% DRC+LVS pass rate and reduces design turnaround time from hours to minutes, outperforming the baseline LLM. Furthermore, an agent-generated I/O ring was fabricated and validated in a 28 nm CMOS tape-out, demonstrating the practical effectiveness of the approach in real AMS IC design flows. To our knowledge, this is the first reported human-agent collaborative AMS IC design in which an LLM-based agent completes a nontrivial subtask with outputs directly used in silicon.", "arxiv_url": "https://arxiv.org/abs/2512.21613", "authors": ["Zhishuai Zhang", "Xintian Li", "Shilong Liu", "Aodong Zhang", "Lu Jie", "Nan Sun"], "first_author": "Zhishuai Zhang", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Code Agents", "Intent Graph", "EDA Script Generation", "I/O Ring Automation", "Domain-Specific Knowledge Base", "Design Intent Structuring", "Constraint Resolution", "DRC+LVS Validation", "Tape-out Validation", "Human-Agent Collaboration"], "summary": "本文提出AMS-IO-Agent，一种结合领域知识库与结构化设计意图推理的LLM智能体，并构建AMS-IO-Bench基准，实现自动生成可用于工业流程的AMS I/O环的可执行EDA脚本与版图，在28nm芯片流片中验证并取得70%+ DRC/LVS通过率与显著缩短设计周期。", "quality": "High", "conference": "AAAI 2026", "pdf_url": "https://arxiv.org/pdf/2512.21613v1", "published": "2025-12-25", "update_time": "2025-12-25", "download_time": "2025-12-30 01:58:59"}
{"id": "2512.21540", "title": "Leash: Adaptive Length Penalty and Reward Shaping for Efficient Large Reasoning Model", "abstract": "Existing approaches typically rely on fixed length penalties, but such penalties are hard to tune and fail to adapt to the evolving reasoning abilities of LLMs, leading to suboptimal trade-offs between accuracy and conciseness. To address this challenge, we propose Leash (adaptive LEngth penAlty and reward SHaping), a reinforcement learning framework for efficient reasoning in LLMs. We formulate length control as a constrained optimization problem and employ a Lagrangian primal-dual method to dynamically adjust the penalty coefficient. When generations exceed the target length, the penalty is intensified; when they are shorter, it is relaxed. This adaptive mechanism guides models toward producing concise reasoning without sacrificing task performance. Experiments on Deepseek-R1-Distill-Qwen-1.5B and Qwen3-4B-Thinking-2507 show that Leash reduces the average reasoning length by 60% across diverse tasks - including in-distribution mathematical reasoning and out-of-distribution domains such as coding and instruction following - while maintaining competitive performance. Our work thus presents a practical and effective paradigm for developing controllable and efficient LLMs that balance reasoning capabilities with computational budgets.", "arxiv_url": "https://arxiv.org/abs/2512.21540", "authors": ["Yanhao Li", "Lu Ma", "Jiaran Zhang", "Lexiang Tang", "Wentao Zhang", "Guibo Luo"], "first_author": "Yanhao Li", "category": ["Technical"], "field": "Model Efficiency & Control", "task": "Reasoning Length Control", "tags": ["Length Control", "Primal-Dual Optimization", "One-Sided Penalty", "Reward Shaping", "Chain-of-Thought Compression", "Constraint RL", "Efficiency", "Training Stability"], "summary": "本文提出LEASH，一种基于拉格朗日对偶的自适应长度惩罚与奖励塑形的强化学习方法，通过动态调整罚项并采用单向惩罚在训练中控制推理链长度，从而在不牺牲任务性能的前提下显著减少生成长度并提升推理效率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.21540v1", "published": "2025-12-25", "update_time": "2025-12-25", "download_time": "2025-12-30 02:00:03"}
{"id": "2512.23385", "title": "Securing the AI Supply Chain: What Can We Learn From Developer-Reported Security Issues and Solutions of AI Projects?", "abstract": "The rapid growth of Artificial Intelligence (AI) models and applications has led to an increasingly complex security landscape. Developers of AI projects must contend not only with traditional software supply chain issues but also with novel, AI-specific security threats. However, little is known about what security issues are commonly encountered and how they are resolved in practice. This gap hinders the development of effective security measures for each component of the AI supply chain. We bridge this gap by conducting an empirical investigation of developer-reported issues and solutions, based on discussions from Hugging Face and GitHub. To identify security-related discussions, we develop a pipeline that combines keyword matching with an optimal fine-tuned distilBERT classifier, which achieved the best performance in our extensive comparison of various deep learning and large language models. This pipeline produces a dataset of 312,868 security discussions, providing insights into the security reporting practices of AI applications and projects. We conduct a thematic analysis of 753 posts sampled from our dataset and uncover a fine-grained taxonomy of 32 security issues and 24 solutions across four themes: (1) System and Software, (2) External Tools and Ecosystem, (3) Model, and (4) Data. We reveal that many security issues arise from the complex dependencies and black-box nature of AI components. Notably, challenges related to Models and Data often lack concrete solutions. Our insights can offer evidence-based guidance for developers and researchers to address real-world security threats across the AI supply chain.", "arxiv_url": "https://arxiv.org/abs/2512.23385", "authors": ["The Anh Nguyen", "Triet Huynh Minh Le", "M. Ali Babar"], "first_author": "The Anh Nguyen", "category": ["Empirical", "Benchmark", "Technical"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Security and Vulnerabilities", "AI Supply Chain", "Developer Discussions", "Learning-based Classification", "Security Taxonomy", "Dependency Vulnerabilities", "Data Leakage", "Community-driven Detection"], "summary": "本文构建关键词+学习型分类器的检测管道，从 Hugging Face 与 GitHub 抽取并标注了约312,868条安全相关讨论，基于753条抽样帖进行主题分析，提出覆盖系统/工具/模型/数据四大类的32类安全问题与24类解决方案并开放复现包，为AI供应链安全提供实证证据与实践建议。", "quality": "High", "conference": "IEEE/ACM International Conference on Software Engineering (ICSE 2026)", "pdf_url": "https://arxiv.org/pdf/2512.23385v1", "published": "2025-12-29", "update_time": "2025-12-29", "download_time": "2026-01-01 02:04:42"}
{"id": "2512.23327", "title": "An Empirical Study of Generative AI Adoption in Software Engineering", "abstract": "Context. GenAI tools are being increasingly adopted by practitioners in SE, promising support for several SE activities. Despite increasing adoption, we still lack empirical evidence on how GenAI is used in practice, the benefits it provides, the challenges it introduces, and its broader organizational and societal implications. Objective. This study aims to provide an overview of the status of GenAI adoption in SE. It investigates the status of GenAI adoption, associated benefits and challenges, institutionalization of tools and techniques, and anticipated long term impacts on SE professionals and the community. Results. The results indicate a wide adoption of GenAI tools and how they are deeply integrated into daily SE work, particularly for implementation, verification and validation, personal assistance, and maintenance-related tasks. Practitioners report substantial benefits, most notably reduction in cycle time, quality improvements, enhanced support in knowledge work, and productivity gains. However, objective measurement of productivity and quality remains limited in practice. Significant challenges persist, including incorrect or unreliable outputs, prompt engineering difficulties, validation overhead, security and privacy concerns, and risks of overreliance. Institutionalization of tools and techniques seems to be common, but it varies considerably, with a strong focus on tool access and less emphasis on training and governance. Practitioners expect GenAI to redefine rather than replace their roles, while expressing moderate concern about job market contraction and skill shifts.", "arxiv_url": "https://arxiv.org/abs/2512.23327", "authors": ["Görkem Giray", "Onur Demirörs", "Marcos Kalinowski", "Daniel Mendez"], "first_author": "Görkem Giray", "category": ["Empirical"], "field": "Adoption & Socio-technical", "task": "GenAI adoption and organizational impact", "tags": ["Human-LLM Interaction", "Productivity Gains", "Cycle Time Reduction", "Quality Improvement", "Validation Overhead", "Prompt Engineering", "Security and Privacy", "Tool Institutionalization", "Governance", "Overreliance", "Job Market Impact", "Empirical Survey"], "summary": "本文基于对来自37个国家的204名软件工程从业者的问卷调查，实证分析了生成式AI在软件工程中的采用现状、带来的效益与挑战、工具制度化程度及对职业与社区的长期影响。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.23327v1", "published": "2025-12-29", "update_time": "2025-12-29", "download_time": "2026-01-01 02:05:01"}
{"id": "2512.23631", "title": "BOAD: Discovering Hierarchical Software Engineering Agents via Bandit Optimization", "abstract": "Large language models (LLMs) have shown strong reasoning and coding capabilities, yet they struggle to generalize to real-world software engineering (SWE) problems that are long-horizon and out of distribution. Existing systems often rely on a single agent to handle the entire workflow-interpreting issues, navigating large codebases, and implementing fixes-within one reasoning chain. Such monolithic designs force the model to retain irrelevant context, leading to spurious correlations and poor generalization. Motivated by how human engineers decompose complex problems, we propose structuring SWE agents as orchestrators coordinating specialized sub-agents for sub-tasks such as localization, editing, and validation. The challenge lies in discovering effective hierarchies automatically: as the number of sub-agents grows, the search space becomes combinatorial, and it is difficult to attribute credit to individual sub-agents within a team. We address these challenges by formulating hierarchy discovery as a multi-armed bandit (MAB) problem, where each arm represents a candidate sub-agent and the reward measures its helpfulness when collaborating with others. This framework, termed Bandit Optimization for Agent Design (BOAD), enables efficient exploration of sub-agent designs under limited evaluation budgets. On SWE-bench-Verified, BOAD outperforms single-agent and manually designed multi-agent systems. On SWE-bench-Live, featuring more recent and out-of-distribution issues, our 36B system ranks second on the leaderboard at the time of evaluation, surpassing larger models such as GPT-4 and Claude. These results demonstrate that automatically discovered hierarchical multi-agent systems significantly improve generalization on challenging long-horizon SWE tasks. Code is available at https://github.com/iamxjy/BOAD-SWE-Agent.", "arxiv_url": "https://arxiv.org/abs/2512.23631", "authors": ["Iris Xu", "Guangtao Zeng", "Zexue He", "Charles Jin", "Aldo Pareja", "Dan Gutfreund", "Chuang Gan", "Zhang-Wei Hong"], "first_author": "Iris Xu", "category": ["Technical"], "field": "Coding Assistant", "task": "Hierarchical Multi-Agent Orchestration", "tags": ["Code Agents", "Hierarchical Agents", "Bandit Optimization", "Helpfulness Estimation", "LLM-as-Judge", "Multi-Agent Coordination", "Orchestrator", "Sub-agent Specialization", "Sample Efficiency", "Long-Horizon Generalization"], "summary": "本文提出BOAD，一种将多臂赌博机用于自动发现分层软件工程多智能体体系结构的方法，通过评估子智能体的“有用性”并高效组合以提升在长时程、分布外的软件工程问题上的泛化能力，且在实际基准上取得优异成绩。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.23631v1", "published": "2025-12-29", "update_time": "2025-12-29", "download_time": "2026-01-01 02:05:32"}
{"id": "2512.23557", "title": "Toward Trustworthy Agentic AI: A Multimodal Framework for Preventing Prompt Injection Attacks", "abstract": "Powerful autonomous systems, which reason, plan, and converse using and between numerous tools and agents, are made possible by Large Language Models (LLMs), Vision-Language Models (VLMs), and new agentic AI systems, like LangChain and GraphChain. Nevertheless, this agentic environment increases the probability of the occurrence of multimodal prompt injection (PI) attacks, in which concealed or malicious instructions carried in text, pictures, metadata, or agent-to-agent messages may spread throughout the graph and lead to unintended behavior, a breach of policy, or corruption of state. In order to mitigate these risks, this paper suggests a Cross-Agent Multimodal Provenanc- Aware Defense Framework whereby all the prompts, either user-generated or produced by upstream agents, are sanitized and all the outputs generated by an LLM are verified independently before being sent to downstream nodes. This framework contains a Text sanitizer agent, visual sanitizer agent, and output validator agent all coordinated by a provenance ledger, which keeps metadata of modality, source, and trust level throughout the entire agent network. This architecture makes sure that agent-to-agent communication abides by clear trust frames such such that injected instructions are not propagated down LangChain or GraphChain-style-workflows. The experimental assessments show that multimodal injection detection accuracy is significantly enhanced, and the cross-agent trust leakage is minimized, as well as, agentic execution pathways become stable. The framework, which expands the concept of provenance tracking and validation to the multi-agent orchestration, enhances the establishment of secure, understandable and reliable agentic AI systems.", "arxiv_url": "https://arxiv.org/abs/2512.23557", "authors": ["Toqeer Ali Syed", "Mishal Ateeq Almutairi", "Mahmoud Abdel Moaty"], "first_author": "Toqeer Ali Syed", "category": ["Technical"], "field": "LLM Security & Safety", "task": "Prompt Injection Defense", "tags": ["Multimodal Sanitization", "Provenance Tracking", "Cross-Agent Validation", "Output Validation", "Trust Scoring", "Zero-Trust Agent Network", "Visual Prompt Injection Detection", "Agentic AI Security"], "summary": "本文提出了一个跨代理的多模态可溯源防御框架，通过文本与图像清洗器、信任评分与可溯源账本以及输出验证器，防止提示注入在LangChain/GraphChain式多代理管道中传播并提升系统安全性与执行稳定性。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.23557v1", "published": "2025-12-29", "update_time": "2025-12-29", "download_time": "2026-01-01 02:06:13"}
{"id": "2512.24636", "title": "How Do Agentic AI Systems Deal With Software Energy Concerns? A Pull Request-Based Study", "abstract": "As Software Engineering enters its new era (SE 3.0), AI coding agents increasingly automate software development workflows. However, it remains unclear how exactly these agents recognize and address software energy concerns-an issue growing in importance due to large-scale data centers, energy-hungry language models, and battery-constrained devices. In this paper, we examined the energy awareness of agent-authored pull requests (PRs) using a publicly available dataset. We identified 216 energy-explicit PRs and conducted a thematic analysis, deriving a taxonomy of energy-aware work. Our further analysis of the applied optimization techniques shows that most align with established research recommendations. Although building and running these agents is highly energy intensive, encouragingly, the results indicate that they exhibit energy awareness when generating software artifacts. However, optimization-related PRs are accepted less frequently than others, largely due to their negative impact on maintainability.", "arxiv_url": "https://arxiv.org/abs/2512.24636", "authors": ["Tanjum Motin Mitul", "Md. Masud Mazumder", "Md Nahidul Islam Opu", "Shaiful Chowdhury"], "first_author": "Tanjum Motin Mitul", "category": ["Empirical"], "field": "Coding Assistant", "task": "Code Performance Optimization", "tags": ["Energy-aware Code", "Energy Profiling", "Optimization Techniques", "Taxonomy", "Maintainability Tradeoff", "Pull Request Analysis", "Agentic AI", "Merge Time Impact"], "summary": "本文基于AIDev数据集对216个由AI代理提交的能耗相关Pull Request进行主题性分析，构建五类能耗关注点的分类并归纳出21种优化技术，发现代理具备能耗意识但优化类PR因降低可维护性而更易被拒绝且合并耗时更长。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.24636v1", "published": "2025-12-31", "update_time": "2025-12-31", "download_time": "2026-01-02 01:55:15"}
{"id": "2512.24635", "title": "DynaFix: Iterative Automated Program Repair Driven by Execution-Level Dynamic Information", "abstract": "Automated Program Repair (APR) aims to automatically generate correct patches for buggy programs. Recent approaches leveraging large language models (LLMs) have shown promise but face limitations. Most rely solely on static analysis, ignoring runtime behaviors. Some attempt to incorporate dynamic signals, but these are often restricted to training or fine-tuning, or injected only once into the repair prompt, without iterative use. This fails to fully capture program execution. Current iterative repair frameworks typically rely on coarse-grained feedback, such as pass/fail results or exception types, and do not leverage fine-grained execution-level information effectively. As a result, models struggle to simulate human stepwise debugging, limiting their effectiveness in multi-step reasoning and complex bug repair.   To address these challenges, we propose DynaFix, an execution-level dynamic information-driven APR method that iteratively leverages runtime information to refine the repair process. In each repair round, DynaFix captures execution-level dynamic information such as variable states, control-flow paths, and call stacks, transforming them into structured prompts to guide LLMs in generating candidate patches. If a patch fails validation, DynaFix re-executes the modified program to collect new execution information for the next attempt. This iterative loop incrementally improves patches based on updated feedback, similar to the stepwise debugging practices of human developers. We evaluate DynaFix on the Defects4J v1.2 and v2.0 benchmarks. DynaFix repairs 186 single-function bugs, a 10% improvement over state-of-the-art baselines, including 38 bugs previously unrepaired. It achieves correct patches within at most 35 attempts, reducing the patch search space by 70% compared with existing methods, thereby demonstrating both effectiveness and efficiency in repairing complex bugs.", "arxiv_url": "https://arxiv.org/abs/2512.24635", "authors": ["Zhili Huang", "Ling Xu", "Chao Liu", "Weifeng Sun", "Xu Zhang", "Yan Lei", "Meng Yan", "Hongyu Zhang"], "first_author": "Zhili Huang", "category": ["Technical"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Program Repair", "Dynamic Analysis", "Runtime Instrumentation", "Iterative Debugging", "Execution Traces", "Fault Localization", "LLM Prompting", "Search Space Reduction", "Java Instrumentation", "Patch Validation"], "summary": "DynaFix提出一种基于执行级动态信息的迭代自动程序修复框架，通过轻量级Java运行时插桩收集变量状态、控制流和调用栈并将其结构化为提示驱动LLM生成与验证补丁，从而在Defects4J上显著提升修复率并减少补丁搜索空间。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.24635v1", "published": "2025-12-31", "update_time": "2025-12-31", "download_time": "2026-01-02 01:55:30"}
{"id": "2512.25065", "title": "Vulcan: Instance-Optimal Systems Heuristics Through LLM-Driven Search", "abstract": "Resource-management tasks in modern operating and distributed systems continue to rely primarily on hand-designed heuristics for tasks such as scheduling, caching, or active queue management. Designing performant heuristics is an expensive, time-consuming process that we are forced to continuously go through due to the constant flux of hardware, workloads and environments.   We propose a new alternative: synthesizing instance-optimal heuristics -- specialized for the exact workloads and hardware where they will be deployed -- using code-generating large language models (LLMs). To make this synthesis tractable, Vulcan separates policy and mechanism through LLM-friendly, task-agnostic interfaces. With these interfaces, users specify the inputs and objectives of their desired policy, while Vulcan searches for performant policies via evolutionary search over LLM-generated code. This interface is expressive enough to capture a wide range of system policies, yet sufficiently constrained to allow even small, inexpensive LLMs to generate correct and executable code.   We use Vulcan to synthesize performant heuristics for cache eviction and memory tiering, and find that these heuristics outperform all human-designed state-of-the-art algorithms by upto 69% and 7.9% in performance for each of these tasks respectively.", "arxiv_url": "https://arxiv.org/abs/2512.25065", "authors": ["Rohit Dwivedula", "Divyanshu Saxena", "Sujay Yadalam", "Daehyeok Kim", "Aditya Akella"], "first_author": "Rohit Dwivedula", "category": ["Technical", "Empirical"], "field": "Systems & Resource Management", "task": "Instance-optimal Heuristic Synthesis", "tags": ["Instance-Optimal Heuristics", "Policy Synthesis", "VALUE-RANK Interface", "Evolutionary Search", "LLM Code Generation", "Interface Separation", "Cache Eviction", "Memory Tiering", "Interpretable Heuristics"], "summary": "本文提出VULCAN框架，通过将策略与机制分离并在受限的VALUE/RANK接口上结合LLM生成代码与进化搜索，自动合成针对具体工作负载和硬件的实例最优系统启发式策略，在缓存淘汰和内存分层任务上显著超过多个人类设计的基线。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.25065v1", "published": "2025-12-31", "update_time": "2025-12-31", "download_time": "2026-01-02 01:56:16"}
{"id": "2512.24873", "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem", "abstract": "Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.", "arxiv_url": "https://arxiv.org/abs/2512.24873", "authors": ["Weixun Wang", "XiaoXiao Xu", "Wanhe An", "Fangwen Dai", "Wei Gao", "Yancheng He", "Ju Huang", "Qiang Ji", "Hanqi Jin", "Xiaoyang Li", "Yang Li", "Zhongwen Li", "Shirong Lin", "Jiashun Liu", "Zenan Liu", "Tao Luo", "Dilxat Muhtar", "Yuanbin Qu", "Jiaqiang Shi", "Qinghui Sun", "Yingshui Tan", "Hao Tang", "Runze Wang", "Yi Wang", "Zhaoguo Wang", "Yanan Wu", "Shaopan Xiong", "Binchen Xu", "Xander Xu", "Yuchi Xu", "Qipeng Zhang", "Xixia Zhang", "Haizhou Zhao", "Jie Zhao", "Shuaibing Zhao", "Baihui Zheng", "Jianhui Zheng", "Suhang Zheng", "Yanni Zhu", "Mengze Cai", "Kerui Cao", "Xitong Chen", "Yue Dai", "Lifan Du", "Tao Feng", "Tao He", "Jin Hu", "Yijie Hu", "Ziyu Jiang", "Cheng Li", "Xiang Li", "Jing Liang", "Chonghuan Liu", "ZhenDong Liu", "Haodong Mi", "Yanhu Mo", "Junjia Ni", "Shixin Pei", "Jingyu Shen", "XiaoShuai Song", "Cecilia Wang", "Chaofan Wang", "Kangyu Wang", "Pei Wang", "Tao Wang", "Wei Wang", "Ke Xiao", "Mingyu Xu", "Tiange Xu", "Nan Ya", "Siran Yang", "Jianan Ye", "Yaxing Zang", "Duo Zhang", "Junbo Zhang", "Boren Zheng", "Wanxi Deng", "Ling Pan", "Lin Qu", "Wenbo Su", "Jiamang Wang", "Wei Wang", "Hu Wei", "Minggang Wu", "Cheng Yu", "Bing Zhao", "Zhicheng Zheng", "Bo Zheng"], "first_author": "Weixun Wang", "category": ["Technical", "Benchmark"], "field": "Agentic Systems", "task": "Agentic Reinforcement Learning", "tags": ["Code Agents", "Agentic Reinforcement Learning", "Chunk-level Credit Assignment", "Chunk-level Optimization", "Sandboxed Environment Orchestration", "Trajectory Synthesis", "Safety Verification", "Terminal Agent Benchmark", "End-to-end Training Pipeline", "Production Deployment"], "summary": "本文提出了面向多回合、工具化工作流的 Agentic Learning Ecosystem（ALE）（包含 ROLL、ROCK、iFlow CLI），基于百万级轨迹训练出开源代理模型 ROME，提出基于语义交互块的信用分配与策略优化方法以提升长时序稳定性，并发布更严格的 Terminal Bench Pro 基准与安全校验流程，展示了在终端代理任务上的竞争性表现与生产部署能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.24873v1", "published": "2025-12-31", "update_time": "2025-12-31", "download_time": "2026-01-02 01:58:19"}
{"id": "2512.24630", "title": "How Do Agentic AI Systems Address Performance Optimizations? A BERTopic-Based Analysis of Pull Requests", "abstract": "LLM-based software engineering is influencing modern software development. In addition to correctness, prior studies have also examined the performance of software artifacts generated by AI agents. However, it is unclear how exactly the agentic AI systems address performance concerns in practice. In this paper, we present an empirical study of performance-related pull requests generated by AI agents. Using LLM-assisted detection and BERTopic-based topic modeling, we identified 52 performance-related topics grouped into 10 higher-level categories. Our results show that AI agents apply performance optimizations across diverse layers of the software stack and that the type of optimization significantly affects pull request acceptance rates and review times. We also found that performance optimization by AI agents primarily occurs during the development phase, with less focus on the maintenance phase. Our findings provide empirical evidence that can support the evaluation and improvement of agentic AI systems with respect to their performance optimization behaviors and review outcomes.", "arxiv_url": "https://arxiv.org/abs/2512.24630", "authors": ["Md Nahidul Islam Opu", "Shahidul Islam", "Muhammad Asaduzzaman", "Shaiful Chowdhury"], "first_author": "Md Nahidul Islam Opu", "category": ["Empirical"], "field": "Coding Assistant", "task": "Code Performance Optimization", "tags": ["Performance Optimization Taxonomy", "BERTopic Clustering", "LLM-based Classification", "Pull Request Analysis", "PR Acceptance Rate", "Merge Time Analysis", "SDLC Activity Attribution", "Stack-level Optimizations", "Agentic AI Behavior"], "summary": "本文通过对AIDev数据集中1221个由代理AI生成的性能相关PR进行LLM辅助识别与BERTopic主题建模，归纳出52个性能主题并分为10类，实证分析了不同优化类型在PR接受率、合并时长和软件生命周期活动中的分布差异。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.24630v1", "published": "2025-12-31", "update_time": "2025-12-31", "download_time": "2026-01-03 01:49:29"}
{"id": "2512.24594", "title": "A Tale of 1001 LoC: Potential Runtime Error-Guided Specification Synthesis for Verifying Large-Scale Programs", "abstract": "Fully automated verification of large-scale software and hardware systems is arguably the holy grail of formal methods. Large language models (LLMs) have recently demonstrated their potential for enhancing the degree of automation in formal verification by, e.g., generating formal specifications as essential to deductive verification, yet exhibit poor scalability due to long-context reasoning limitations and, more importantly, the difficulty of inferring complex, interprocedural specifications. This paper presents Preguss -- a modular, fine-grained framework for automating the generation and refinement of formal specifications. Preguss synergizes between static analysis and deductive verification by steering two components in a divide-and-conquer fashion: (i) potential runtime error-guided construction and prioritization of verification units, and (ii) LLM-aided synthesis of interprocedural specifications at the unit level. We show that Preguss substantially outperforms state-of-the-art LLM-based approaches and, in particular, it enables highly automated RTE-freeness verification for real-world programs with over a thousand LoC, with a reduction of 80.6%~88.9% human verification effort.", "arxiv_url": "https://arxiv.org/abs/2512.24594", "authors": ["Zhongyi Wang", "Tengjie Lin", "Mingshuai Chen", "Haokun Li", "Mingqi Yang", "Xiao Yi", "Shengchao Qin", "Yixing Luo", "Xiaofeng Li", "Bin Gu", "Liqiang Lu", "Jianwei Yin"], "first_author": "Zhongyi Wang", "category": ["Technical", "Benchmark"], "field": "Requirements & Design", "task": "Specification & Validation", "tags": ["Specification Synthesis", "RTE-guided Synthesis", "Interprocedural Contracts", "Static-Analysis Integration", "Deductive Verification", "Modular Verification", "Scalability", "Verification Dataset"], "summary": "本文提出Preguss，一种结合静态分析与LLM的模块化细粒度规范合成框架，通过以潜在运行时错误引导的划分与优先级策略，在单元级合成跨过程规范，从而实现对千行级C程序的RTE无害性验证并大幅减少人工参与。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.24594v1", "published": "2025-12-31", "update_time": "2025-12-31", "download_time": "2026-01-03 01:49:48"}
{"id": "2512.24796", "title": "LeanCat: A Benchmark Suite for Formal Category Theory in Lean (Part I: 1-Categories)", "abstract": "Large language models (LLMs) have made rapid progress in formal theorem proving, yet current benchmarks under-measure the kind of abstraction and library-mediated reasoning that organizes modern mathematics. In parallel with FATE's emphasis on frontier algebra, we introduce LeanCat, a Lean benchmark for category-theoretic formalization -- a unifying language for mathematical structure and a core layer of modern proof engineering -- serving as a stress test of structural, interface-level reasoning. Part I: 1-Categories contains 100 fully formalized statement-level tasks, curated into topic families and three difficulty tiers via an LLM-assisted + human grading process. The best model solves 8.25% of tasks at pass@1 (32.50%/4.17%/0.00% by Easy/Medium/High) and 12.00% at pass@4 (50.00%/4.76%/0.00%). We also evaluate LeanBridge which use LeanExplore to search Mathlib, and observe consistent gains over single-model baselines. LeanCat is intended as a compact, reusable checkpoint for tracking both AI and human progress toward reliable, research-level formalization in Lean.", "arxiv_url": "https://arxiv.org/abs/2512.24796", "authors": ["Rongge Xu", "Hui Dai", "Yiming Fu", "Jiedong Jiang", "Tianjiao Nie", "Hongwei Wang", "Junkai Wang", "Holiverse Yang", "Jiatong Yang", "Zhi-Hao Zhang"], "first_author": "Rongge Xu", "category": ["Benchmark", "Empirical"], "field": "Formalization & Theorem Proving", "task": "Theorem Proving Benchmark (library-grounded formalization)", "tags": ["Formalization Benchmark", "Category Theory", "Proof Synthesis", "Library-Grounded Reasoning", "Search-Augmented Proving", "Retrieve-Generate-Verify", "Difficulty Annotation", "Natural-to-Formal Gap", "Math Library Navigation", "Tool-Augmented Proving"], "summary": "本文提出LeanCat基准（第一部分：1-范畴），包含100个在Lean中正式化的范畴论定理以测试模型在库驱动的抽象推理与证明合成能力，并给出难度标注、基线评估及基于检索的增强工作流。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.24796v1", "published": "2025-12-31", "update_time": "2025-12-31", "download_time": "2026-01-03 01:52:30"}
{"id": "2512.24615", "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization", "abstract": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose \\textbf{Youtu-Agent}, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a \\textbf{Workflow} mode for standard tasks and a \\textbf{Meta-Agent} mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an \\textbf{Agent Practice} module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an \\textbf{Agent RL} module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.", "arxiv_url": "https://arxiv.org/abs/2512.24615", "authors": ["Yuchen Shi", "Yuzheng Cai", "Siqi Cai", "Zihan Xu", "Lichao Chen", "Yulei Qin", "Zhijian Zhou", "Xiang Fei", "Chaofan Qiu", "Xiaoyu Tan", "Gang Li", "Zongyi Li", "Haojia Lin", "Guocan Cai", "Yong Mao", "Yunsheng Wu", "Ke Li", "Xing Sun"], "first_author": "Yuchen Shi", "category": ["Technical"], "field": "Agent Systems", "task": "Agent Construction & Optimization", "tags": ["Code Agents", "Automated Tool Synthesis", "Meta-Agent", "Workflow Automation", "In-Context Optimization", "Agent Reinforcement Learning", "YAML Configuration", "Context Management"], "summary": "Youtu-Agent 提出一个模块化的 LLM agent 框架，通过 YAML 结构化配置实现自动生成工具与代理，并结合基于上下文的 Practice 模块与可扩展的 RL 模块进行混合策略优化以持续提升代理性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.24615v1", "published": "2025-12-31", "update_time": "2025-12-31", "download_time": "2026-01-03 01:54:18"}
{"id": "2512.24570", "title": "On the Effectiveness of Training Data Optimization for LLM-based Code Generation: An Empirical Study", "abstract": "Large language models (LLMs) have achieved remarkable progress in code generation, largely driven by the availability of high-quality code datasets for effective training. To further improve data quality, numerous training data optimization techniques have been proposed; however, their overall effectiveness has not been systematically evaluated. To bridge this gap, we conduct the first large-scale empirical study, examining five widely-used training data optimization techniques and their pairwise combinations for LLM-based code generation across three benchmarks and four LLMs. Our results show that data synthesis is the most effective technique for improving functional correctness and reducing code smells, although it performs relatively worse on code maintainability compared to data refactoring, cleaning, and selection. Regarding combinations, we find that most combinations do not further improve functional correctness but can effectively enhance code quality (code smells and maintainability). Among all combinations, data synthesis combined with data refactoring achieves the strongest overall performance. Furthermore, our fine-grained analysis reinforces these findings and provides deeper insights into how individual techniques and their combinations influence code generation effectiveness. Overall, this work represents a first step toward a systematic understanding of training data optimization and combination strategies, offering practical guidance for future research and deployment in LLM-based code generation.", "arxiv_url": "https://arxiv.org/abs/2512.24570", "authors": ["Shiqi Kuang", "Zhao Tian", "Tao Xiao", "Dong Wang", "Junjie Chen"], "first_author": "Shiqi Kuang", "category": ["Empirical"], "field": "Coding Assistant", "task": "Code Pre-Training", "tags": ["Training Data Optimization", "Data Synthesis", "Data Refactoring", "Data Cleaning", "Data Selection", "Data Augmentation", "Technique Combination", "Correctness", "Code Smells", "Maintainability", "Complementarity", "Empirical Evaluation"], "summary": "本文通过大规模实证比较五类训练数据优化技术及其组合在多种LLM和基准上的效果，发现数据合成在提升功能正确性和减少代码异味方面最有效，而组合增益有限且互补性比数量更重要，为训练数据优化提供了实践指导。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.24570v1", "published": "2025-12-31", "update_time": "2025-12-31", "download_time": "2026-01-04 02:05:06"}
{"id": "2512.24560", "title": "Localized Calibrated Uncertainty in Code Language Models", "abstract": "Large Language models (LLMs) can generate complicated source code from natural language prompts. However, LLMs can generate output that deviates from what the user wants, requiring supervision and editing. To support this process, we offer techniques to localize where generations might be misaligned from user intent. We first create a dataset of \"Minimal Intent Aligning Patches\" of repaired LLM generated programs. Each program uses test cases to verify correctness. After creating a dataset of programs, we measure how well various techniques can assign a well-calibrated probability to indicate which parts of code will be edited in a minimal patch (i.e., give a probability that corresponds with empirical odds it is edited). We compare white-box probing (where we propose a technique for efficient arbitrary-span querying), against black-box reflective and self-consistency based approaches. We find probes with a small supervisor model can achieve low calibration error and Brier Skill Score of approx 0.2 estimating edited lines on code generated by models many orders of magnitude larger. We discuss the generalizability of the techniques, and the connections to AI oversight and control, finding a probe trained only on code shows some signs of generalizing to natural language errors if new probability scaling is allowed.", "arxiv_url": "https://arxiv.org/abs/2512.24560", "authors": ["David Gros", "Prem Devanbu"], "first_author": "David Gros", "category": ["Technical", "Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Alignment", "tags": ["Localized Calibration", "Line-level Confidence", "Token-level Confidence", "Minimal Patch", "White-box Probing", "Self-Consistency Sampling", "Reflective Confidence", "Brier Skill Score"], "summary": "本文构建了一个包含最小意图对齐补丁的数据集，并提出并比较了白盒探针、基于多次采样的一致性和反思式方法，以在行/标记级别产生校准良好的局部不确定性估计，辅助定位需修复的代码片段。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.24560v1", "published": "2025-12-31", "update_time": "2025-12-31", "download_time": "2026-01-04 02:05:23"}
{"id": "2512.24609", "title": "Reinforcement Learning-Augmented LLM Agents for Collaborative Decision Making and Performance Optimization", "abstract": "Large Language Models (LLMs) perform well in language tasks but often lack collaborative awareness and struggle to optimize global performance in multi-agent settings. We present a reinforcement learning-augmented LLM agent framework that formulates cooperation as a decentralized partially observable Markov decision process (Dec-POMDP) and adopts centralized training with decentralized execution (CTDE). We introduce Group Relative Policy Optimization (GRPO) to jointly optimize agent policies with access to global signals during training, together with a simplified joint reward that balances task quality, speed, and coordination cost. On collaborative writing and coding benchmarks, our framework delivers a 3x increase in task processing speed over single-agent baselines, 98.7% structural/style consistency in writing, and a 74.6% test pass rate in coding. The approach consistently outperforms strong multi-agent LLM baselines and provides a practical path toward reliable collaboration in complex workflows.", "arxiv_url": "https://arxiv.org/abs/2512.24609", "authors": ["Dong Qiu", "Duo Xu", "Limengxi Yue"], "first_author": "Dong Qiu", "category": ["Technical", "Empirical"], "field": "LLM Agents & Collaboration", "task": "Multi-Agent Coordination and Policy Optimization", "tags": ["Code Agents", "Dec-POMDP", "CTDE", "Group Relative Policy Optimization", "Credit Assignment", "Joint Reward Design", "Role-based Agents", "Throughput Optimization"], "summary": "本文提出一种基于强化学习的LLM多智能体协作框架，将协作建模为Dec‑POMDP、采用集中训练-分散执行并引入Group Relative Policy Optimization与联合奖励设计，从而在协作写作与角色分工编码任务上显著提升速度、质量与协调效率。", "quality": "High", "conference": "IEEE ICFTIC 2025", "pdf_url": "https://arxiv.org/pdf/2512.24609v1", "published": "2025-12-31", "update_time": "2025-12-31", "download_time": "2026-01-04 02:10:02"}
{"id": "2512.24571", "title": "SynRAG: A Large Language Model Framework for Executable Query Generation in Heterogeneous SIEM System", "abstract": "Security Information and Event Management (SIEM) systems are essential for large enterprises to monitor their IT infrastructure by ingesting and analyzing millions of logs and events daily. Security Operations Center (SOC) analysts are tasked with monitoring and analyzing this vast data to identify potential threats and take preventive actions to protect enterprise assets. However, the diversity among SIEM platforms, such as Palo Alto Networks Qradar, Google SecOps, Splunk, Microsoft Sentinel and the Elastic Stack, poses significant challenges. As these systems differ in attributes, architecture, and query languages, making it difficult for analysts to effectively monitor multiple platforms without undergoing extensive training or forcing enterprises to expand their workforce. To address this issue, we introduce SynRAG, a unified framework that automatically generates threat detection or incident investigation queries for multiple SIEM platforms from a platform-agnostic specification. SynRAG can generate platformspecific queries from a single high-level specification written by analysts. Without SynRAG, analysts would need to manually write separate queries for each SIEM platform, since query languages vary significantly across systems. This framework enables seamless threat detection and incident investigation across heterogeneous SIEM environments, reducing the need for specialized training and manual query translation. We evaluate SynRAG against state-of-the-art language models, including GPT, Llama, DeepSeek, Gemma, and Claude, using Qradar and SecOps as representative SIEM systems. Our results demonstrate that SynRAG generates significantly better queries for crossSIEM threat detection and incident investigation compared to the state-of-the-art base models.", "arxiv_url": "https://arxiv.org/abs/2512.24571", "authors": ["Md Hasan Saju", "Austin Page", "Akramul Azim", "Jeff Gardiner", "Farzaneh Abazari", "Frank Eargle"], "first_author": "Md Hasan Saju", "category": ["Technical", "Benchmark"], "field": "SIEM & Security Operations", "task": "SIEM Query Generation", "tags": ["SIEM Query Translation", "Retrieval-Augmented Generation", "YAML Threat Specification", "Syntax-aware Generation", "Cross-SIEM Interoperability", "Executable Query Synthesis", "Threat Detection", "Benchmark for SIEM queries"], "summary": "本文提出 SynRAG——一个基于 RAG 的统一框架，可从平台无关的 YAML 威胁规范自动生成各异 SIEM 平台（如 QRadar、SecOps）上可执行的查询，包含语法服务与向量检索并构建了用于评估的基准，实验显示优于若干主流大模型基线。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.24571v1", "published": "2025-12-31", "update_time": "2025-12-31", "download_time": "2026-01-04 02:10:30"}
{"id": "2601.00635", "title": "SEMODS: A Validated Dataset of Open-Source Software Engineering Models", "abstract": "Integrating Artificial Intelligence into Software Engineering (SE) requires having a curated collection of models suited to SE tasks. With millions of models hosted on Hugging Face (HF) and new ones continuously being created, it is infeasible to identify SE models without a dedicated catalogue. To address this gap, we present SEMODS: an SE-focused dataset of 3,427 models extracted from HF, combining automated collection with rigorous validation through manual annotation and large language model assistance. Our dataset links models to SE tasks and activities from the software development lifecycle, offering a standardized representation of their evaluation results, and supporting multiple applications such as data analysis, model discovery, benchmarking, and model adaptation.", "arxiv_url": "https://arxiv.org/abs/2601.00635", "authors": ["Alexandra González", "Xavier Franch", "Silverio Martínez-Fernández"], "first_author": "Alexandra González", "category": ["Benchmark"], "field": "Model Repositories & Catalogues", "task": "SE Model Cataloguing", "tags": ["Model Catalog", "SE Task Taxonomy", "Benchmark Harmonization", "LLM-assisted Annotation", "Model Card Normalization", "Automated Dataset Maintenance", "Repository Metadata Schema", "Model Discovery"], "summary": "本文提出SEMODS，一个从Hugging Face系统化收集并通过人工和大模型辅助验证的面向软件工程的模型数据集（3,427个模型），提供SE任务映射、标准化评测表示和自动化更新管道以支持模型发现与基准测试。", "quality": "High", "conference": "FORGE (ACM International Conference on AI Foundation Models and Software Engineering) 2026", "pdf_url": "https://arxiv.org/pdf/2601.00635v1", "published": "2026-01-02", "update_time": "2026-01-02", "download_time": "2026-01-05 02:05:13"}
{"id": "2601.00497", "title": "STELLAR: A Search-Based Testing Framework for Large Language Model Applications", "abstract": "Large Language Model (LLM)-based applications are increasingly deployed across various domains, including customer service, education, and mobility. However, these systems are prone to inaccurate, fictitious, or harmful responses, and their vast, high-dimensional input space makes systematic testing particularly challenging. To address this, we present STELLAR, an automated search-based testing framework for LLM-based applications that systematically uncovers text inputs leading to inappropriate system responses. Our framework models test generation as an optimization problem and discretizes the input space into stylistic, content-related, and perturbation features. Unlike prior work that focuses on prompt optimization or coverage heuristics, our work employs evolutionary optimization to dynamically explore feature combinations that are more likely to expose failures. We evaluate STELLAR on three LLM-based conversational question-answering systems. The first focuses on safety, benchmarking both public and proprietary LLMs against malicious or unsafe prompts. The second and third target navigation, using an open-source and an industrial retrieval-augmented system for in-vehicle venue recommendations. Overall, STELLAR exposes up to 4.3 times (average 2.5 times) more failures than the existing baseline approaches.", "arxiv_url": "https://arxiv.org/abs/2601.00497", "authors": ["Lev Sorokin", "Ivan Vasilev", "Ken E. Friedl", "Andrea Stocco"], "first_author": "Lev Sorokin", "category": ["Technical", "Empirical"], "field": "Software Testing", "task": "Test Generation", "tags": ["Search-based Testing", "Evolutionary Optimization", "Feature Discretization", "Retrieval-Augmented Generation", "Robustness Testing", "Safety Testing", "Conversational QA", "Perturbation Testing"], "summary": "本文提出STELLAR，一种将自然语言输入离散为风格、内容和扰动特征并通过进化搜索生成失败诱发测试用例的自动化搜索式测试框架，用以发现LLM应用（如车载对话系统）中的不当或错误响应并在多项用例中显著优于基线方法。", "quality": "High", "conference": "International Conference on Software Analysis, Evolution and Reengineering (SANER) 2026", "pdf_url": "https://arxiv.org/pdf/2601.00497v1", "published": "2026-01-01", "update_time": "2026-01-01", "download_time": "2026-01-05 02:06:23"}
{"id": "2512.24545", "title": "More Than Bits: Multi-Envelope Double Binary Factorization for Extreme Quantization", "abstract": "For extreme low-bit quantization of large language models (LLMs), Double Binary Factorization (DBF) is attractive as it enables efficient inference without sacrificing accuracy. However, the scaling parameters of DBF are too restrictive; after factoring out signs, all rank components share the same magnitude profile, resulting in performance saturation. We propose Multi-envelope DBF (MDBF), which retains a shared pair of 1-bit sign bases but replaces the single envelope with a rank-$l$ envelope. By sharing sign matrices among envelope components, MDBF effectively maintains a binary carrier and utilizes the limited memory budget for magnitude expressiveness. We also introduce a closed-form initialization and an alternating refinement method to optimize MDBF. Across the LLaMA and Qwen families, MDBF enhances perplexity and zero-shot accuracy over previous binary formats at matched bits per weight while preserving the same deployment-friendly inference primitive.", "arxiv_url": "https://arxiv.org/abs/2512.24545", "authors": ["Yuma Ichikawa", "Yoshihiko Fujisawa", "Yudai Fujimoto", "Akira Sakai", "Katsuki Fujisawa"], "first_author": "Yuma Ichikawa", "category": ["Technical"], "field": "Model Compression & Quantization", "task": "Extreme Low-bit Weight Quantization", "tags": ["Double Binary Factorization", "Multi-Envelope", "Envelope Rank", "ADMM Refinement", "Closed-form Initialization", "Per-layer PTQ", "Binary Inference Primitive", "Reconstruction Error Reduction"], "summary": "本文提出 Multi-Envelope DBF（MDBF），在保持共享二值符号基和部署友好二值推理路径的同时引入多重幅度包络，并通过闭式初始化与交替 ADMM 优化显著提升极低比特量化下的重建精度、困惑度和零射准确率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.24545v1", "published": "2025-12-31", "update_time": "2025-12-31", "download_time": "2026-01-05 02:16:25"}
{"id": "2512.24159", "title": "Developing controlled natural language for formal specification patterns using AI assistants", "abstract": "Using an AI assistant, we developed a method for systematically constructing controlled natural language for requirements based on formal specification patterns containing logical attributes. The method involves three stages: 1) compiling a generalized natural language requirement pattern that utilizes all attributes of the formal specification template; 2) generating, using the AI assistant, a corpus of natural language requirement patterns, reduced by partially evaluating attributes (the developed prompt utilizes the generalized template, attribute definitions, and specific formal semantics of the requirement patterns); and 3) formalizing the syntax of the controlled natural language based on an analysis of the grammatical structure of the resulting patterns. The method has been tested for event-driven temporal requirements.", "arxiv_url": "https://arxiv.org/abs/2512.24159", "authors": ["Natalia Garanina", "Vladimir Zyubin", "Igor Anureev"], "first_author": "Natalia Garanina", "category": ["Technical"], "field": "Requirements & Design", "task": "Specification & Validation", "tags": ["Controlled Natural Language", "Specification Patterns", "Formal Semantics", "AI-Assisted Generation", "Prompt Engineering", "Grammar Induction", "Attribute Partial Evaluation", "Event-Temporal Requirements"], "summary": "本文提出一种三阶段方法：以形式规范模式为基础构建通用自然语言模板，利用AI助手生成通过部分求值属性简化的模式语料，并基于语法分析将其形式化为具备内建语义的受控自然语言，已在事件-时序需求（EDTL/SUP）上验证。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.24159v1", "published": "2025-12-30", "update_time": "2025-12-30", "download_time": "2026-01-05 02:27:00"}
{"id": "2601.00753", "title": "Early-Stage Prediction of Review Effort in AI-Generated Pull Requests", "abstract": "As autonomous AI agents transition from code completion tools to full-fledged teammates capable of opening pull requests (PRs) at scale, software maintainers face a new challenge: not just reviewing code, but managing complex interaction loops with non-human contributors. This paradigm shift raises a critical question: can we predict which agent-generated PRs will consume excessive review effort before any human interaction begins?   Analyzing 33,707 agent-authored PRs from the AIDev dataset across 2,807 repositories, we uncover a striking two-regime behavioral pattern that fundamentally distinguishes autonomous agents from human developers. The first regime, representing 28.3 percent of all PRs, consists of instant merges (less than 1 minute), reflecting success on narrow automation tasks. The second regime involves iterative review cycles where agents frequently stall or abandon refinement (ghosting).   We propose a Circuit Breaker triage model that predicts high-review-effort PRs (top 20 percent) at creation time using only static structural features. A LightGBM model achieves AUC 0.957 on a temporal split, while semantic text features (TF-IDF, CodeBERT) provide negligible predictive value. At a 20 percent review budget, the model intercepts 69 percent of total review effort, enabling zero-latency governance.   Our findings challenge prevailing assumptions in AI-assisted code review: review burden is dictated by what agents touch, not what they say, highlighting the need for structural governance mechanisms in human-AI collaboration.", "arxiv_url": "https://arxiv.org/abs/2601.00753", "authors": ["Dao Sy Duy Minh", "Huynh Trung Kiet", "Tran Chi Nguyen", "Nguyen Lam Phu Quy", "Phu Hoa Pham", "Nguyen Dinh Ha Duong", "Truong Bao Tran"], "first_author": "Dao Sy Duy Minh", "category": ["Empirical", "Technical"], "field": "Maintenance", "task": "Code Review", "tags": ["Ghosting", "Review Triage", "Effort Prediction", "Creation-time Features", "Patch Complexity", "Structural Signals", "Agent-authored PRs", "Circuit Breaker", "Zero-latency Triage", "Model Interpretability"], "summary": "本文基于33,707个由AI代理提交的PR，发现即时合并与迭代失败两种行为模式，并提出基于创建时结构性特征的“断路器”LightGBM模型（AUC≈0.957）用于零延迟预测高审查成本的PR以辅助维护者分流。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.00753v1", "published": "2026-01-02", "update_time": "2026-01-02", "download_time": "2026-01-06 01:54:54"}
{"id": "2601.00482", "title": "Multi-Agent Coordinated Rename Refactoring", "abstract": "The primary value of AI agents in software development lies in their ability to extend the developer's capacity for reasoning and action, not to supplant human involvement. To showcase how to use agents working in tandem with developers, we designed a novel approach for carrying out coordinated renaming. Coordinated renaming, where a single rename refactoring triggers refactorings in multiple, related identifiers, is a frequent yet challenging task. Developers must manually propagate these rename refactorings across numerous files and contexts, a process that is both tedious and highly error-prone. State-of-the-art heuristic-based approaches produce an overwhelming number of false positives, while vanilla Large Language Models (LLMs) provide incomplete suggestions due to their limited context and inability to interact with refactoring tools. This leaves developers with incomplete refactorings or burdens them with filtering too many false positives. Coordinated renaming is exactly the kind of repetitive task that agents can significantly reduce the developers' burden while keeping them in the driver's seat.   We designed, implemented, and evaluated the first multi-agent framework that automates coordinated renaming. It operates on a key insight: a developer's initial refactoring is a clue to infer the scope of related refactorings. Our Scope Inference Agent first transforms this clue into an explicit, natural-language Declared Scope. The Planned Execution Agent then uses this as a strict plan to identify program elements that should undergo refactoring and safely executes the changes by invoking the IDE's own trusted refactoring APIs. Finally, the Replication Agent uses it to guide the project-wide search. We first conducted a formative study on the practice of coordinated renaming in 609K commits in 100 open-source projects and surveyed 205 developers ...", "arxiv_url": "https://arxiv.org/abs/2601.00482", "authors": ["Abhiram Bellur", "Mohammed Raihan Ullah", "Fraol Batole", "Mohit Kansara", "Masaharu Morimoto", "Kai Ishikawa", "Haifeng Chen", "Yaroslav Zharov", "Timofey Bryksin", "Tien N. Nguyen", "Hridesh Rajan", "Danny Dig"], "first_author": "Abhiram Bellur", "category": ["Technical", "Benchmark"], "field": "Maintenance", "task": "Refactoring", "tags": ["Code Agents", "Coordinated Rename", "Scope Inference", "IDE Refactoring API", "Program Slicing", "Semantic Search", "Developer-in-the-loop", "Episodic Memory", "Repository-level Propagation", "Pull Request Automation"], "summary": "本文提出CoRenameAgent——一个在IDE中运行的多智能体框架，通过范围推断、规划执行和复制传播三类代理结合开发者反馈与IDE可信重构API，自动、可靠地执行跨仓库的协同重命名，并在两个基准上显著优于现有方法且其自动生成的PR被社区接受。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.00482v1", "published": "2026-01-01", "update_time": "2026-01-01", "download_time": "2026-01-06 01:55:34"}
{"id": "2601.00743", "title": "An Agentic Framework for Neuro-Symbolic Programming", "abstract": "Integrating symbolic constraints into deep learning models could make them more robust, interpretable, and data-efficient. Still, it remains a time-consuming and challenging task. Existing frameworks like DomiKnowS help this integration by providing a high-level declarative programming interface, but they still assume the user is proficient with the library's specific syntax. We propose AgenticDomiKnowS (ADS) to eliminate this dependency. ADS translates free-form task descriptions into a complete DomiKnowS program using an agentic workflow that creates and tests each DomiKnowS component separately. The workflow supports optional human-in-the-loop intervention, enabling users familiar with DomiKnowS to refine intermediate outputs. We show how ADS enables experienced DomiKnowS users and non-users to rapidly construct neuro-symbolic programs, reducing development time from hours to 10-15 minutes.", "arxiv_url": "https://arxiv.org/abs/2601.00743", "authors": ["Aliakbar Nafar", "Chetan Chigurupati", "Danial Kamali", "Hamid Karimian", "Parisa Kordjamshidi"], "first_author": "Aliakbar Nafar", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Code Agents", "Code RAG", "Neuro-Symbolic", "Domain-Specific Language", "Human-LLM Interaction", "Interactive Coding", "Self-Refinement", "VLM Integration"], "summary": "本文提出AgenticDomiKnowS，一个将自然语言任务描述通过检索-生成-执行-审查的代理式工作流转化为完整DomiKnowS神经符号程序并支持人机交互与自我修正的系统，能将开发时间从数小时缩短到10–15分钟。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.00743v1", "published": "2026-01-02", "update_time": "2026-01-02", "download_time": "2026-01-06 01:56:06"}
{"id": "2601.00559", "title": "Cracking IoT Security: Can LLMs Outsmart Static Analysis Tools?", "abstract": "Smart home IoT platforms such as openHAB rely on Trigger Action Condition (TAC) rules to automate device behavior, but the interplay among these rules can give rise to interaction threats, unintended or unsafe behaviors emerging from implicit dependencies, conflicting triggers, or overlapping conditions. Identifying these threats requires semantic understanding and structural reasoning that traditionally depend on symbolic, constraint-driven static analysis. This work presents the first comprehensive evaluation of Large Language Models (LLMs) across a multi-category interaction threat taxonomy, assessing their performance on both the original openHAB (oHC/IoTB) dataset and a structurally challenging Mutation dataset designed to test robustness under rule transformations. We benchmark Llama 3.1 8B, Llama 70B, GPT-4o, Gemini-2.5-Pro, and DeepSeek-R1 across zero-, one-, and two-shot settings, comparing their results against oHIT's manually validated ground truth. Our findings show that while LLMs exhibit promising semantic understanding, particularly on action- and condition-related threats, their accuracy degrades significantly for threats requiring cross-rule structural reasoning, especially under mutated rule forms. Model performance varies widely across threat categories and prompt settings, with no model providing consistent reliability. In contrast, the symbolic reasoning baseline maintains stable detection across both datasets, unaffected by rule rewrites or structural perturbations. These results underscore that LLMs alone are not yet dependable for safety critical interaction-threat detection in IoT environments. We discuss the implications for tool design and highlight the potential of hybrid architectures that combine symbolic analysis with LLM-based semantic interpretation to reduce false positives while maintaining structural rigor.", "arxiv_url": "https://arxiv.org/abs/2601.00559", "authors": ["Jason Quantrill", "Noura Khajehnouri", "Zihan Guo", "Manar H. Alalfi"], "first_author": "Jason Quantrill", "category": ["Empirical", "Benchmark"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Rule Interaction Threats", "IoT Security", "Trigger-Action-Condition", "Mutation Testing", "Symbolic-LLM Hybrid", "Cross-rule Reasoning", "Structural Robustness", "Prompt Robustness", "False Positive Reduction"], "summary": "本文评估多种大语言模型在openHAB触发-条件-动作规则交互威胁检测上的表现，并通过一套结构变异数据集与符号静态分析工具对比，发现LLM在语义理解方面较好但在跨规则结构推理与变形规则下显著退化，提出结合符号分析与LLM的混合方案以提高检测可靠性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.00559v1", "published": "2026-01-02", "update_time": "2026-01-02", "download_time": "2026-01-06 01:57:35"}
{"id": "2601.02971", "title": "Few-shot learning for security bug report identification", "abstract": "Security bug reports require prompt identification to minimize the window of vulnerability in software systems. Traditional machine learning (ML) techniques for classifying bug reports to identify security bug reports rely heavily on large amounts of labeled data. However, datasets for security bug reports are often scarce in practice, leading to poor model performance and limited applicability in real-world settings. In this study, we propose a few-shot learning-based technique to effectively identify security bug reports using limited labeled data. We employ SetFit, a state-of-the-art few-shot learning framework that combines sentence transformers with contrastive learning and parameter-efficient fine-tuning. The model is trained on a small labeled dataset of bug reports and is evaluated on its ability to classify these reports as either security-related or non-security-related. Our approach achieves an AUC of 0.865, at best, outperforming traditional ML techniques (baselines) for all of the evaluated datasets. This highlights the potential of SetFit to effectively identify security bug reports. SetFit-based few-shot learning offers a promising alternative to traditional ML techniques to identify security bug reports. The approach enables efficient model development with minimal annotation effort, making it highly suitable for scenarios where labeled data is scarce.", "arxiv_url": "https://arxiv.org/abs/2601.02971", "authors": ["Muhammad Laiq"], "first_author": "Muhammad Laiq", "category": ["Empirical"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Issue Classification", "Few-shot Learning", "Contrastive Learning", "Parameter-efficient Fine-tuning", "Class Imbalance", "Security Bug Identification", "Empirical Comparison"], "summary": "本文在四个开源项目的数据集上实证评估了基于少样本学习的SetFit方法用于安全缺陷报告识别，实验表明在有限标注下可达到最高AUC 0.865并优于传统机器学习基线。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.02971v1", "published": "2026-01-06", "update_time": "2026-01-06", "download_time": "2026-01-08 01:55:02"}
{"id": "2601.02868", "title": "CodeMEM: AST-Guided Adaptive Memory for Repository-Level Iterative Code Generation", "abstract": "Large language models (LLMs) substantially enhance developer productivity in repository-level code generation through interactive collaboration. However, as interactions progress, repository context must be continuously preserved and updated to integrate newly validated information. Meanwhile, the expanding session history increases cognitive burden, often leading to forgetting and the reintroduction of previously resolved errors. Existing memory management approaches show promise but remain limited by natural language-centric representations. To overcome these limitations, we propose CodeMEM, an AST-guided dynamic memory management system tailored for repository-level iterative code generation. Specifically, CodeMEM introduces the Code Context Memory component that dynamically maintains and updates repository context through AST-guided LLM operations, along with the Code Session Memory that constructs a code-centric representation of interaction history and explicitly detects and mitigates forgetting through AST-based analysis. Experimental results on the instruction-following benchmark CodeIF-Bench and the code generation benchmark CoderEval demonstrate that CodeMEM achieves state-of-the-art performance, improving instruction following by 12.2% for the current turn and 11.5% for the session level, and reducing interaction rounds by 2-3, while maintaining competitive inference latency and token efficiency.", "arxiv_url": "https://arxiv.org/abs/2601.02868", "authors": ["Peiding Wang", "Li Zhang", "Fang Liu", "Chongyang Tao", "Yinghao Zhu"], "first_author": "Peiding Wang", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Retrieval", "tags": ["Repository-Level Coding", "AST-guided memory", "Code Context Memory", "Code Session Memory", "Forgetting Detection", "Code-change Analysis", "Interaction Efficiency"], "summary": "本文提出CODEMEM，一种基于AST引导的动态代码记忆管理系统，通过维护代码上下文记忆与会话记忆并基于AST分析检测与缓解遗忘，从而在仓库级多轮代码生成中显著提升指令遵循率并减少交互轮次。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.02868v1", "published": "2026-01-06", "update_time": "2026-01-06", "download_time": "2026-01-08 01:55:19"}
{"id": "2601.02941", "title": "SastBench: A Benchmark for Testing Agentic SAST Triage", "abstract": "SAST (Static Application Security Testing) tools are among the most widely used techniques in defensive cybersecurity, employed by commercial and non-commercial organizations to identify potential vulnerabilities in software. Despite their great utility, they generate numerous false positives, requiring costly manual filtering (aka triage). While LLM-powered agents show promise for automating cybersecurity tasks, existing benchmarks fail to emulate real-world SAST finding distributions. We introduce SastBench, a benchmark for evaluating SAST triage agents that combines real CVEs as true positives with filtered SAST tool findings as approximate false positives. SastBench features an agent-agnostic design. We evaluate different agents on the benchmark and present a comparative analysis of their performance, provide a detailed analysis of the dataset, and discuss the implications for future development.", "arxiv_url": "https://arxiv.org/abs/2601.02941", "authors": ["Jake Feiglin", "Guy Dar"], "first_author": "Jake Feiglin", "category": ["Benchmark", "Empirical"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["SAST Triage", "Agentic Benchmark", "Realistic False Positives", "CVE-based Ground Truth", "Agent Evaluation", "Security-Oriented Prompting", "Language Diversity", "Agent-Agnostic Evaluation"], "summary": "该论文提出并开源了SASTBENCH，一个以真实CVE作为真阳性并以过滤后的SAST工具报警作为近似假阳性构建的面向自动化SAST告警三分类（triage）的基准，并基于该基准对多种LLM驱动代理进行了比较评估与数据分析。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.02941v1", "published": "2026-01-06", "update_time": "2026-01-06", "download_time": "2026-01-08 01:59:25"}
{"id": "2601.02736", "title": "Hypothesize-Then-Verify: Speculative Root Cause Analysis for Microservices with Pathwise Parallelism", "abstract": "Microservice systems have become the backbone of cloud-native enterprise applications due to their resource elasticity, loosely coupled architecture, and lightweight deployment. Yet, the intrinsic complexity and dynamic runtime interactions of such systems inevitably give rise to anomalies. Ensuring system reliability therefore hinges on effective root cause analysis (RCA), which entails not only localizing the source of anomalies but also characterizing the underlying failures in a timely and interpretable manner. Recent advances in intelligent RCA techniques, particularly those powered by large language models (LLMs), have demonstrated promising capabilities, as LLMs reduce reliance on handcrafted features while offering cross-platform adaptability, task generalization, and flexibility. However, existing LLM-based methods still suffer from two critical limitations: (a) limited exploration diversity, which undermines accuracy, and (b) heavy dependence on large-scale LLMs, which results in slow inference. To overcome these challenges, we propose SpecRCA, a speculative root cause analysis framework for microservices that adopts a \\textit{hypothesize-then-verify} paradigm. SpecRCA first leverages a hypothesis drafting module to rapidly generate candidate root causes, and then employs a parallel root cause verifier to efficiently validate them. Preliminary experiments on the AIOps 2022 dataset demonstrate that SpecRCA achieves superior accuracy and efficiency compared to existing approaches, highlighting its potential as a practical solution for scalable and interpretable RCA in complex microservice environments.", "arxiv_url": "https://arxiv.org/abs/2601.02736", "authors": ["Lingzhe Zhang", "Tong Jia", "Yunpeng Zhai", "Leyi Pan", "Chiming Duan", "Minghua He", "Pei Xiao", "Ying Li"], "first_author": "Lingzhe Zhang", "category": ["Technical"], "field": "AIOps", "task": "Root Cause Analysis", "tags": ["Speculative Verification", "Hypothesize-then-Verify", "Pathwise Parallelism", "Modality Fusion", "Service Topology", "Trace Analysis", "Granger Causality", "Anomalous Metric Detection", "Parallel Verification", "Diagnosis Synthesis", "Efficiency", "Interpretability"], "summary": "本文提出 SpecRCA——一种面向微服务的假设-验证根因分析框架，通过轻量草拟模型生成多样化候选假设并并行用小型微调 LLM 验证，结合指标/链路/日志的拓扑融合与综合生成诊断报告，在 AIOps 2022 数据集上显著提升定位准确性并降低推理延迟。", "quality": "High", "conference": "ICSE", "pdf_url": "https://arxiv.org/pdf/2601.02736v1", "published": "2026-01-06", "update_time": "2026-01-06", "download_time": "2026-01-08 02:01:34"}
{"id": "2601.03988", "title": "Using Small Language Models to Reverse-Engineer Machine Learning Pipelines Structures", "abstract": "Background: Extracting the stages that structure Machine Learning (ML) pipelines from source code is key for gaining a deeper understanding of data science practices. However, the diversity caused by the constant evolution of the ML ecosystem (e.g., algorithms, libraries, datasets) makes this task challenging. Existing approaches either depend on non-scalable, manual labeling, or on ML classifiers that do not properly support the diversity of the domain. These limitations highlight the need for more flexible and reliable solutions.   Objective: We evaluate whether Small Language Models (SLMs) can leverage their code understanding and classification abilities to address these limitations, and subsequently how they can advance our understanding of data science practices.   Method: We conduct a confirmatory study based on two reference works selected for their relevance regarding current state-of-the-art's limitations. First, we compare several SLMs using Cochran's Q test. The best-performing model is then evaluated against the reference studies using two distinct McNemar's tests. We further analyze how variations in taxonomy definitions affect performance through an additional Cochran's Q test. Finally, a goodness-of-fit analysis is conducted using Pearson's chi-squared tests to compare our insights on data science practices with those from prior studies.", "arxiv_url": "https://arxiv.org/abs/2601.03988", "authors": ["Nicolas Lacroix", "Mireille Blay-Fornarino", "Sébastien Mosser", "Frederic Precioso"], "first_author": "Nicolas Lacroix", "category": ["Empirical"], "field": "Coding Assistant", "task": "Code Understanding", "tags": ["Pipeline Extraction", "Reverse Engineering", "Jupyter Notebook Analysis", "Small Language Models", "Instruction-level Classification", "Taxonomy Sensitivity", "Model Comparison", "Goodness-of-fit Analysis"], "summary": "本文通过对比多款开源小型语言模型，评估其在从 Jupyter 笔记本代码中反向工程提取机器学习流水线结构（阶段）上的准确性，分析分类法措辞对结果的影响并检验这些结果是否改变对数据科学家实践的既有理解。", "quality": "High", "conference": "SANER (IEEE International Conference on Software Analysis, Evolution and Reengineering) 2026", "pdf_url": "https://arxiv.org/pdf/2601.03988v1", "published": "2026-01-07", "update_time": "2026-01-07", "download_time": "2026-01-09 01:56:28"}
{"id": "2601.03878", "title": "Understanding Specification-Driven Code Generation with LLMs: An Empirical Study Design", "abstract": "Large Language Models (LLMs) are increasingly integrated into software development workflows, yet their behavior in structured, specification-driven processes remains poorly understood. This paper presents an empirical study design using CURRANTE, a Visual Studio Code extension that enables a human-in-the-loop workflow for LLM-assisted code generation. The tool guides developers through three sequential stages--Specification, Tests, and Function--allowing them to define requirements, generate and refine test suites, and produce functions that satisfy those tests. Participants will solve medium-difficulty problems from the LiveCodeBench dataset, while the tool records fine-grained interaction logs, effectiveness metrics (e.g., pass rate, all-pass completion), efficiency indicators (e.g., time-to-pass), and iteration behaviors. The study aims to analyze how human intervention in specification and test refinement influences the quality and dynamics of LLM-generated code. The results will provide empirical insights into the design of next-generation development environments that align human reasoning with model-driven code generation.", "arxiv_url": "https://arxiv.org/abs/2601.03878", "authors": ["Giovanni Rosa", "David Moreno-Lumbreras", "Gregorio Robles", "Jesús M. González-Barahona"], "first_author": "Giovanni Rosa", "category": ["Empirical", "Technical"], "field": "Software Testing", "task": "Test Generation", "tags": ["Specification-Driven Development", "Test-Driven Development", "Human-LLM Interaction", "IDE Plugin", "Test Suite Generation", "Test Refinement", "Interaction Logging", "Effectiveness Metrics", "Time-to-Pass", "Registered Study Protocol"], "summary": "本文提出并描述了一项基于 CURRANTE VSCode 插件的注册报告型实证研究设计，以评估在人类参与的规范（通过测试用例）驱动 TDD 工作流下，LLM 辅助代码生成的有效性、效率与迭代行为。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.03878v1", "published": "2026-01-07", "update_time": "2026-01-07", "download_time": "2026-01-09 01:56:58"}
{"id": "2601.04126", "title": "InfiniteWeb: Scalable Web Environment Synthesis for GUI Agent Training", "abstract": "GUI agents that interact with graphical interfaces on behalf of users represent a promising direction for practical AI assistants. However, training such agents is hindered by the scarcity of suitable environments. We present InfiniteWeb, a system that automatically generates functional web environments at scale for GUI agent training. While LLMs perform well on generating a single webpage, building a realistic and functional website with many interconnected pages faces challenges. We address these challenges through unified specification, task-centric test-driven development, and a combination of website seed with reference design image to ensure diversity. Our system also generates verifiable task evaluators enabling dense reward signals for reinforcement learning. Experiments show that InfiniteWeb surpasses commercial coding agents at realistic website construction, and GUI agents trained on our generated environments achieve significant performance improvements on OSWorld and Online-Mind2Web, demonstrating the effectiveness of proposed system.", "arxiv_url": "https://arxiv.org/abs/2601.04126", "authors": ["Ziyun Zhang", "Zezhou Wang", "Xiaoyi Zhang", "Zongyu Guo", "Jiahao Li", "Bin Li", "Yan Lu"], "first_author": "Ziyun Zhang", "category": ["Technical", "Benchmark"], "field": "Requirements & Design", "task": "Specification & Validation", "tags": ["Environment Synthesis", "Unified Specification", "Task-Centric TDD", "Evaluator Generation", "Design-Guided Frontend", "Visual-Functional Diversity", "Dense Reward Signals", "GUI Agent Training"], "summary": "本文提出INFINITEWEB，一个通过从任务导出统一数据与接口、采用面向任务的测试驱动开发以及设计图引导的前端生成，自动大规模合成功能性网页环境并生成可验证评估器以支持基于强化学习的GUI代理训练的系统。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.04126v1", "published": "2026-01-07", "update_time": "2026-01-07", "download_time": "2026-01-09 01:58:15"}
{"id": "2601.03731", "title": "From Laboratory to Real-World Applications: Benchmarking Agentic Code Reasoning at the Repository Level", "abstract": "As large language models (LLMs) evolve into autonomous agents, evaluating repository-level reasoning, the ability to maintain logical consistency across massive, real-world, interdependent file systems, has become critical. Current benchmarks typically fluctuate between isolated code snippets and black-box evaluations. We present RepoReason, a white-box diagnostic benchmark centered on abductive assertion verification. To eliminate memorization while preserving authentic logical depth, we implement an execution-driven mutation framework that utilizes the environment as a semantic oracle to regenerate ground-truth states. Furthermore, we establish a fine-grained diagnostic system using dynamic program slicing, quantifying reasoning via three orthogonal metrics: $ESV$ (reading load), $MCL$ (simulation depth), and $DFI$ (integration width). Comprehensive evaluations of frontier models (e.g., Claude-4.5-Sonnet, DeepSeek-v3.1-Terminus) reveal a prevalent aggregation deficit, where integration width serves as the primary cognitive bottleneck. Our findings provide granular white-box insights for optimizing the next generation of agentic software engineering.", "arxiv_url": "https://arxiv.org/abs/2601.03731", "authors": ["Jia Li", "Yuxin Su", "Michael R. Lyu"], "first_author": "Jia Li", "category": ["Benchmark", "Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Repository-Level Reasoning", "Abductive Verification", "Execution-Driven Mutation", "Deterministic Value Protocol", "Dynamic Program Slicing", "Reading Load (ESV)", "Simulation Depth (MCL)", "Integration Width (DFI)", "White-box Diagnostic", "Aggregation Deficit", "Long-chain State Tracking", "Cross-file Integration"], "summary": "本文提出了REPOREASON——一个以掩码断言验证为核心、结合执行驱动变异和动态程序切片的仓库级白盒基准，用于客观评估并细粒度诊断LLM代理在跨文件信息整合与长链状态追踪方面的推理能力，揭示了模型在整合宽度上的严重瓶颈（Aggregation Deficit）。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.03731v1", "published": "2026-01-07", "update_time": "2026-01-07", "download_time": "2026-01-09 02:08:24"}
{"id": "2601.04886", "title": "Analyzing Message-Code Inconsistency in AI Coding Agent-Authored Pull Requests", "abstract": "Pull request (PR) descriptions generated by AI coding agents are the primary channel for communicating code changes to human reviewers. However, the alignment between these messages and the actual changes remains unexplored, raising concerns about the trustworthiness of AI agents. To fill this gap, we analyzed 23,247 agentic PRs across five agents using PR message-code inconsistency (PR-MCI). We contributed 974 manually annotated PRs, found 406 PRs (1.7%) exhibited high PR-MCI, and identified eight PR-MCI types, revealing that descriptions claiming unimplemented changes was the most common issue (45.4%). Statistical tests confirmed that high-MCI PRs had 51.7% lower acceptance rates (28.3% vs. 80.0%) and took 3.5x longer to merge (55.8 vs. 16.0 hours). Our findings suggest that unreliable PR descriptions undermine trust in AI agents, highlighting the need for PR-MCI verification mechanisms and improved PR generation to enable trustworthy human-AI collaboration.", "arxiv_url": "https://arxiv.org/abs/2601.04886", "authors": ["Jingzhi Gong", "Giovanni Pinna", "Yixin Bian", "Jie M. Zhang"], "first_author": "Jingzhi Gong", "category": ["Empirical", "Benchmark"], "field": "Maintenance", "task": "Code Review", "tags": ["Message-Code Inconsistency", "PR Description Fidelity", "Human-LLM Interaction", "Hallucination", "Annotated Dataset", "Taxonomy", "Acceptance Rate", "Merge Delay"], "summary": "本文通过分析23,247个由AI代理提交的PR并发布974条人工标注，构建并量化了PR消息-代码不一致（PR-MCI）的分类与流行率，发现高不一致PR显著降低接受率并延长合并时间，强调需改进PR生成与校验以提升人机协作可信度。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.04886v1", "published": "2026-01-08", "update_time": "2026-01-08", "download_time": "2026-01-10 01:51:55"}
{"id": "2601.04556", "title": "4D-ARE: Bridging the Attribution Gap in LLM Agent Requirements Engineering", "abstract": "We deployed an LLM agent with ReAct reasoning and full data access. It executed flawlessly, yet when asked \"Why is completion rate 80%?\", it returned metrics instead of causal explanation. The agent knew how to reason but we had not specified what to reason about. This reflects a gap: runtime reasoning frameworks (ReAct, Chain-of-Thought) have transformed LLM agents, but design-time specification--determining what domain knowledge agents need--remains under-explored. We propose 4D-ARE (4-Dimensional Attribution-Driven Agent Requirements Engineering), a preliminary methodology for specifying attribution-driven agents. The core insight: decision-makers seek attribution, not answers. Attribution concerns organize into four dimensions (Results -> Process -> Support -> Long-term), motivated by Pearl's causal hierarchy. The framework operationalizes through five layers producing artifacts that compile directly to system prompts. We demonstrate the methodology through an industrial pilot deployment in financial services. 4D-ARE addresses what agents should reason about, complementing runtime frameworks that address how. We hypothesize systematic specification amplifies the power of these foundational advances. This paper presents a methodological proposal with preliminary industrial validation; rigorous empirical evaluation is planned for future work.", "arxiv_url": "https://arxiv.org/abs/2601.04556", "authors": ["Bo Yu", "Lei Zhao"], "first_author": "Bo Yu", "category": ["Technical"], "field": "Requirements & Design", "task": "Specification & Validation", "tags": ["Causal Attribution", "Agent Requirements", "Design-Time Specification", "Elicitation Methods", "Prompt Compilation", "Boundary Constraints", "Decision Support"], "summary": "本文提出4D-ARE——一种基于因果归因的LLM代理需求工程方法，通过四个归因维度与五层规范架构将领域知识编译为系统提示，并在金融服务的工业试点中展示了初步验证。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.04556v1", "published": "2026-01-08", "update_time": "2026-01-08", "download_time": "2026-01-10 01:52:22"}
{"id": "2601.05242", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "abstract": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "arxiv_url": "https://arxiv.org/abs/2601.05242", "authors": ["Shih-Yang Liu", "Xin Dong", "Ximing Lu", "Shizhe Diao", "Peter Belcak", "Mingjie Liu", "Min-Hung Chen", "Hongxu Yin", "Yu-Chiang Frank Wang", "Kwang-Ting Cheng", "Yejin Choi", "Jan Kautz", "Pavlo Molchanov"], "first_author": "Shih-Yang Liu", "category": ["Technical", "Empirical"], "field": "Reinforcement Learning & Alignment", "task": "Multi-reward Policy Optimization", "tags": ["Multi-objective RL", "Reward Decoupling", "Group-wise Normalization", "Advantage Normalization", "Reward Collapse Analysis", "Training Stability", "Convergence Improvement", "Tool Calling", "Math Reasoning", "Code Reasoning"], "summary": "本文提出GDPO，一种在多奖励强化学习中对每个奖励进行组内独立归一化并随后进行批次优势归一化的策略优化方法，以避免GRPO在多奖励设置下导致的奖励信号塌缩并显著提升训练稳定性与收敛性，在工具调用、数学与代码推理任务上均优于GRPO。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.05242v1", "published": "2026-01-08", "update_time": "2026-01-08", "download_time": "2026-01-10 01:52:49"}
{"id": "2601.05214", "title": "Internal Representations as Indicators of Hallucinations in Agent Tool Selection", "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in tool calling and tool usage, but suffer from hallucinations where they choose incorrect tools, provide malformed parameters and exhibit 'tool bypass' behavior by performing simulations and generating outputs instead of invoking specialized tools or external systems. This undermines the reliability of LLM based agents in production systems as it leads to inconsistent results, and bypasses security and audit controls. Such hallucinations in agent tool selection require early detection and error handling. Unlike existing hallucination detection methods that require multiple forward passes or external validation, we present a computationally efficient framework that detects tool-calling hallucinations in real-time by leveraging LLMs' internal representations during the same forward pass used for generation. We evaluate this approach on reasoning tasks across multiple domains, demonstrating strong detection performance (up to 86.4\\% accuracy) while maintaining real-time inference capabilities with minimal computational overhead, particularly excelling at detecting parameter-level hallucinations and inappropriate tool selections, critical for reliable agent deployment.", "arxiv_url": "https://arxiv.org/abs/2601.05214", "authors": ["Kait Healy", "Bharathi Srinivasan", "Visakh Madathil", "Jing Wu"], "first_author": "Kait Healy", "category": ["Technical"], "field": "Agent Systems & Reliability", "task": "Tool-Calling Hallucination Detection", "tags": ["Hallucination", "Internal Representations", "Real-time Detection", "Unsupervised Masking", "Tool Bypass", "Parameter Error Detection", "Function Selection", "Lightweight Classifier", "Inference Efficiency", "Agent Reliability"], "summary": "本文提出一种利用LLM最后一层内部表征进行实时、无监督的工具调用幻觉检测方法，通过对真实工具调用掩码重预测生成训练数据并训练轻量分类器，从而在多领域推理任务中高效识别不当工具选择和参数级错误，且推理开销极小。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.05214v1", "published": "2026-01-08", "update_time": "2026-01-08", "download_time": "2026-01-10 01:53:36"}
{"id": "2601.04540", "title": "AdaptEval: A Benchmark for Evaluating Large Language Models on Code Snippet Adaptation", "abstract": "Recent advancements in large language models (LLMs) have automated various software engineering tasks, with benchmarks emerging to evaluate their capabilities. However, for adaptation, a critical activity during code reuse, there is no benchmark to assess LLMs' performance, leaving their practical utility in this area unclear. To fill this gap, we propose AdaptEval, a benchmark designed to evaluate LLMs on code snippet adaptation. Unlike existing benchmarks, AdaptEval incorporates the following three distinctive features: First, Practical Context. Tasks in AdaptEval are derived from developers' practices, preserving rich contextual information from Stack Overflow and GitHub communities. Second, Multi-granularity Annotation. Each task is annotated with requirements at both task and adaptation levels, supporting the evaluation of LLMs across diverse adaptation scenarios. Third, Fine-grained Evaluation. AdaptEval includes a two-tier testing framework combining adaptation-level and function-level tests, which enables evaluating LLMs' performance across various individual adaptations. Based on AdaptEval, we conduct the first empirical study to evaluate six instruction-tuned LLMs and especially three reasoning LLMs on code snippet adaptation. Experimental results demonstrate that AdaptEval enables the assessment of LLMs' adaptation capabilities from various perspectives. It also provides critical insights into their current limitations, particularly their struggle to follow explicit instructions. We hope AdaptEval can facilitate further investigation and enhancement of LLMs' capabilities in code snippet adaptation, supporting their real-world applications.", "arxiv_url": "https://arxiv.org/abs/2601.04540", "authors": ["Tanghaoran Zhang", "Xinjun Mao", "Shangwen Wang", "Yuxin Zhao", "Yao Lu", "Jin Zhang", "Zhang Zhang", "Kang Yang", "Yue Yu"], "first_author": "Tanghaoran Zhang", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Code Snippet Adaptation", "Context-aware Adaptation", "Multi-Granularity Annotation", "Two-tier Evaluation", "Function-level Testing", "Adaptation-level Testing", "Instruction-following", "Reasoning LLMs"], "summary": "本文提出AdaptEval基准，收集自真实的Stack Overflow与GitHub适配实例，提供多粒度注释与两层（适配级与函数级）测试框架，并基于此评测多款指令调优与推理型LLM以揭示其在代码片段适配任务中的能力与局限。", "quality": "High", "conference": "ASE (IEEE/ACM International Conference on Automated Software Engineering) 2025", "pdf_url": "https://arxiv.org/pdf/2601.04540v1", "published": "2026-01-08", "update_time": "2026-01-08", "download_time": "2026-01-11 02:04:27"}
{"id": "2601.04526", "title": "Advancing Language Models for Code-related Tasks", "abstract": "Recent advances in language models (LMs) have driven significant progress in various software engineering tasks. However, existing LMs still struggle with complex programming scenarios due to limitations in data quality, model architecture, and reasoning capability. This research systematically addresses these challenges through three complementary directions: (1) improving code data quality with a code difference-guided adversarial augmentation technique (CODA) and a code denoising technique (CodeDenoise); (2) enhancing model architecture via syntax-guided code LMs (LEAM and LEAM++); and (3) advancing model reasoning with a prompting technique (muFiX) and an agent-based technique (Specine). These techniques aim to promote the practical adoption of LMs in software development and further advance intelligent software engineering.", "arxiv_url": "https://arxiv.org/abs/2601.04526", "authors": ["Zhao Tian"], "first_author": "Zhao Tian", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Adversarial Augmentation", "Input Denoising", "Syntax-Guided Generation", "AST Representation", "Specification Alignment", "Test-Guided Prompting", "Agent-Based Alignment", "Robustness", "Mutation Testing"], "summary": "本文提出一套系统性方法（CODA、CodeDenoise、LEAM/LEAM++、μFiX 与 Specine），通过改进代码数据质量、模型语法感知架构与推理/对齐机制，提升代码语言模型在生成与鲁棒性上的性能。", "quality": "High", "conference": "ICSE 2026", "pdf_url": "https://arxiv.org/pdf/2601.04526v1", "published": "2026-01-08", "update_time": "2026-01-08", "download_time": "2026-01-11 02:04:42"}
{"id": "2601.05187", "title": "SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning", "abstract": "Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.", "arxiv_url": "https://arxiv.org/abs/2601.05187", "authors": ["Yanchang Liang", "Xiaowei Zhao"], "first_author": "Yanchang Liang", "category": ["Technical", "Benchmark"], "field": "Requirements & Design", "task": "Specification & Validation", "tags": ["Graphical Modeling", "Simulink Integration", "Plan-Execute Agent", "Compact Model Representation", "Reflection-Guided RL", "Sparse-Reward Optimization", "Abstract-Reconstruct Augmentation", "In-Process Simulation", "On-Premise Training", "Benchmarking"], "summary": "本文提出SimuAgent——一个面向Simulink的基于LLM的建模与仿真助手，采用紧凑的Python字典表示、两阶段计划-执行框架、引入Reflection-GRPO强化学习与Abstract–Reconstruct自监督增强，并发布包含5300任务的SimuBench基准，在低成本本地硬件上实现高效且隐私保护的工业级建模自动化。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.05187v1", "published": "2026-01-08", "update_time": "2026-01-08", "download_time": "2026-01-11 02:05:38"}
{"id": "2601.05106", "title": "Token-Level LLM Collaboration via FusionRoute", "abstract": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.", "arxiv_url": "https://arxiv.org/abs/2601.05106", "authors": ["Nuoya Xiong", "Yuhang Zhou", "Hanqing Zeng", "Zhaorun Chen", "Furong Huang", "Shuchao Bi", "Lizhu Zhang", "Zhuokai Zhao"], "first_author": "Nuoya Xiong", "category": ["Technical"], "field": "Multi-LLM Collaboration", "task": "Token-level Routing & Logit Fusion", "tags": ["Token-level Routing", "Logit Fusion", "Complementary Generator", "Lightweight Router", "Mixture-of-Experts", "Router Post-training", "Theoretical Guarantees", "Robustness", "Efficiency", "Multi-Domain Coordination"], "summary": "本文提出FusionRoute，一种在每步解码时由轻量级路由器进行专家选择并通过对数几率相加提供补充生成信号的逐标记多LLM协作框架，理论上拓展了策略类并在数学推理、代码生成和指令跟随等多领域上实验证明了其鲁棒性与效率优势。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.05106v1", "published": "2026-01-08", "update_time": "2026-01-08", "download_time": "2026-01-11 02:06:46"}
{"id": "2601.05827", "title": "SSR: Safeguarding Staking Rewards by Defining and Detecting Logical Defects in DeFi Staking", "abstract": "Decentralized Finance (DeFi) staking is one of the most prominent applications within the DeFi ecosystem, where DeFi projects enable users to stake tokens on the platform and reward participants with additional tokens. However, logical defects in DeFi staking could enable attackers to claim unwarranted rewards by manipulating reward amounts, repeatedly claiming rewards, or engaging in other malicious actions. To mitigate these threats, we conducted the first study focused on defining and detecting logical defects in DeFi staking. Through the analysis of 64 security incidents and 144 audit reports, we identified six distinct types of logical defects, each accompanied by detailed descriptions and code examples. Building on this empirical research, we developed SSR (Safeguarding Staking Reward), a static analysis tool designed to detect logical defects in DeFi staking contracts. SSR utilizes a large language model (LLM) to extract fundamental information about staking logic and constructs a DeFi staking model. It then identifies logical defects by analyzing the model and the associated semantic features. We constructed a ground truth dataset based on known security incidents and audit reports to evaluate the effectiveness of SSR. The results indicate that SSR achieves an overall precision of 92.31%, a recall of 87.92%, and an F1-score of 88.85%. Additionally, to assess the prevalence of logical defects in real-world smart contracts, we compiled a large-scale dataset of 15,992 DeFi staking contracts. SSR detected that 3,557 (22.24%) of these contracts contained at least one logical defect.", "arxiv_url": "https://arxiv.org/abs/2601.05827", "authors": ["Zewei Lin", "Jiachi Chen", "Jingwen Zhang", "Zexu Wang", "Yuming Feng", "Weizhe Zhang", "Zibin Zheng"], "first_author": "Zewei Lin", "category": ["Empirical", "Technical", "Benchmark"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["DeFi Staking", "Logical Defect Classification", "LLM-guided Extraction", "Static Analysis", "Reward Manipulation", "State Update Omission", "Unauthorized Asset Access", "Single Pool Reliance", "Empirical Measurement", "Dataset Release"], "summary": "本文通过分析64起安全事件和144份审计报告，归纳出面向DeFi质押的六类逻辑缺陷，并提出SSR——一种结合LLM信息抽取与静态分析的检测工具，在人工构建的基准和1.6万余合约的大规模测评中表现出高精度与召回率并开放了数据与代码。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.05827v1", "published": "2026-01-09", "update_time": "2026-01-09", "download_time": "2026-01-12 02:01:09"}
{"id": "2601.05777", "title": "EET: Experience-Driven Early Termination for Cost-Efficient Software Engineering Agents", "abstract": "Software engineering (SE) agents powered by large language models are increasingly adopted in practice, yet they often incur substantial monetary cost. We introduce EET, an experience-driven early termination approach that reduces the cost of SE agents while preserving task performance. EET extracts structured experience from prior issue-resolution executions and leverages it to guide early termination during patch generation and selection, reducing unproductive iterations. We evaluate EET on the SWE-bench Verified benchmark across three representative SE agents. EET consistently reduces total cost by 19%-55% (32% on average), with negligible loss in resolution rate (at most 0.2%). These efficiency gains are achieved, on average, by identifying early-termination opportunities for 11% of issues and reducing API calls, input tokens, and output tokens by 21%, 30%, and 25%, respectively. We release the code, prompts, and data at https://github.com/EffiSEAgent/EET.", "arxiv_url": "https://arxiv.org/abs/2601.05777", "authors": ["Yaoqi Guo", "Ying Xiao", "Jie M. Zhang", "Mark Harman", "Yiling Lou", "Yang Liu", "Zhenpeng Chen"], "first_author": "Yaoqi Guo", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Program Repair", "Code Agents", "Early Stopping", "Experience Retrieval", "Execution Trajectory Abstraction", "Cost Efficiency", "Patch Selection"], "summary": "本文提出EET，一种将历史执行轨迹提炼为结构化经验并在补丁生成与选择阶段进行检索驱动的早停机制，能在几乎不损失修复率的情况下显著降低软件工程代理的成本与令牌使用。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.05777v1", "published": "2026-01-09", "update_time": "2026-01-09", "download_time": "2026-01-12 02:01:26"}
{"id": "2601.04996", "title": "AlgBench: To What Extent Do Large Reasoning Models Understand Algorithms?", "abstract": "Reasoning ability has become a central focus in the advancement of Large Reasoning Models (LRMs). Although notable progress has been achieved on several reasoning benchmarks such as MATH500 and LiveCodeBench, existing benchmarks for algorithmic reasoning remain limited, failing to answer a critical question: Do LRMs truly master algorithmic reasoning? To answer this question, we propose AlgBench, an expert-curated benchmark that evaluates LRMs under an algorithm-centric paradigm.   AlgBench consists of over 3,000 original problems spanning 27 algorithms, constructed by ACM algorithmic experts and organized under a comprehensive taxonomy, including Euclidean-structured, non-Euclidean-structured, non-optimized, local-optimized, global-optimized, and heuristic-optimized categories. Empirical evaluations on leading LRMs (e.g., Gemini-3-Pro, DeepSeek-v3.2-Speciale and GPT-o3) reveal substantial performance heterogeneity: while models perform well on non-optimized tasks (up to 92%), accuracy drops sharply to around 49% on globally optimized algorithms such as dynamic programming. Further analysis uncovers \\textbf{strategic over-shifts}, wherein models prematurely abandon correct algorithmic designs due to necessary low-entropy tokens. These findings expose fundamental limitations of problem-centric reinforcement learning and highlight the necessity of an algorithm-centric training paradigm for robust algorithmic reasoning.", "arxiv_url": "https://arxiv.org/abs/2601.04996", "authors": ["Henan Sun", "Kaichi Yu", "Yuyao Wang", "Bowen Liu", "Xunkai Li", "Rong-Hua Li", "Nuo Chen", "Jia Li"], "first_author": "Henan Sun", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Algorithm-centric Benchmark", "Algorithm Taxonomy", "Contamination Control", "Expert-curated Problems", "Performance Heterogeneity", "Dynamic Programming Weakness", "Strategic Over-shift", "Global Optimization"], "summary": "本文提出AlgBench——一个由算法竞赛/ACM专家手工构建的、包含3000+原始题目和新算法分类的算法推理基准，并在多款大型推理模型上实证评估，揭示模型在全局优化类算法（如动态规划）上的显著弱点、性能异质性以及因低熵必要token导致的策略性过早放弃问题。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.04996v1", "published": "2026-01-08", "update_time": "2026-01-08", "download_time": "2026-01-12 02:05:06"}
{"id": "2601.04920", "title": "Conversational AI for Rapid Scientific Prototyping: A Case Study on ESA's ELOPE Competition", "abstract": "Large language models (LLMs) are increasingly used as coding partners, yet their role in accelerating scientific discovery remains underexplored. This paper presents a case study of using ChatGPT for rapid prototyping in ESA's ELOPE (Event-based Lunar OPtical flow Egomotion estimation) competition. The competition required participants to process event camera data to estimate lunar lander trajectories. Despite joining late, we achieved second place with a score of 0.01282, highlighting the potential of human-AI collaboration in competitive scientific settings. ChatGPT contributed not only executable code but also algorithmic reasoning, data handling routines, and methodological suggestions, such as using fixed number of events instead of fixed time spans for windowing. At the same time, we observed limitations: the model often introduced unnecessary structural changes, gets confused by intermediate discussions about alternative ideas, occasionally produced critical errors and forgets important aspects in longer scientific discussions. By analyzing these strengths and shortcomings, we show how conversational AI can both accelerate development and support conceptual insight in scientific research. We argue that structured integration of LLMs into the scientific workflow can enhance rapid prototyping by proposing best practices for AI-assisted scientific work.", "arxiv_url": "https://arxiv.org/abs/2601.04920", "authors": ["Nils Einecke"], "first_author": "Nils Einecke", "category": ["Empirical"], "field": "Coding Assistant", "task": "Code Prompting", "tags": ["Human-LLM Interaction", "Interactive Coding", "Scientific Prototyping", "Algorithmic Reasoning", "Event-based Vision", "Egomotion Estimation", "Hallucination", "Context Drift", "Best Practices"], "summary": "本文以在ESA ELOPE竞赛中与ChatGPT协作为案例，探讨对话式AI在快速科学原型开发中的作用、优势（如算法建议与数据处理代码）与局限（如结构性改动、记忆衰减与关键错误），并提出整合LLM入科研流程的实践建议。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.04920v1", "published": "2026-01-08", "update_time": "2026-01-08", "download_time": "2026-01-12 02:05:57"}
{"id": "2601.05772", "title": "StriderSPD: Structure-Guided Joint Representation Learning for Binary Security Patch Detection", "abstract": "Vulnerabilities severely threaten software systems, making the timely application of security patches crucial for mitigating attacks. However, software vendors often silently patch vulnerabilities with limited disclosure, where Security Patch Detection (SPD) comes to protect software assets. Recently, most SPD studies have targeted Open-Source Software (OSS), yet a large portion of real-world software is closed-source, where patches are distributed as binaries without accessible source code. The limited binary SPD approaches often lift binaries to abstraction levels, i.e., assembly code or pseudo-code. However, assembly code is register-based instructions conveying limited semantics, while pseudo-code lacks parser-compatible grammar to extract structure, both hindering accurate vulnerability-fix representation learning. In addition, previous studies often obtain training and testing data from the same project for evaluation, which fails to reflect closed-source conditions. To alleviate the above challenges, we propose \\textbf{\\textit{StriderSPD}}, a \\underline{Str}ucture-gu\\underline{ide}d joint \\underline{r}epresentation \\underline{SPD} framework of binary code that integrates a graph branch into a large language model (LLM), leveraging structural information to guide the LLM in identifying security patches. Our novel design of the adapters in the graph branch effectively aligns the representations between assembly code and pseudo-code at the LLM's token level. We further present a two-stage training strategy to address the optimization imbalance caused by the large parameter disparity between StriderSPD's two branches, which enables proper branch fitting. To enable more realistic evaluation, we construct a binary SPD benchmark that is disjoint from prior datasets in both projects and domains and extensively evaluate StriderSPD on this benchmark.", "arxiv_url": "https://arxiv.org/abs/2601.05772", "authors": ["Qingyuan Li", "Chenchen Yu", "Chuanyi Li", "Xin-Cheng Wen", "Cheryl Lee", "Cuiyun Gao", "Bin Luo"], "first_author": "Qingyuan Li", "category": ["Technical", "Benchmark"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Binary Patch Classification", "Graph-guided LLM", "CFG GNN", "Adapter-mediated Alignment", "Two-stage Training", "Cross-project Benchmark", "Pseudo-code Semantics", "Inference Efficiency", "Cross-model Generalizability", "Security and Vulnerabilities"], "summary": "本文提出 StriderSPD，将基于控制流图的图分支与大模型分支通过三路适配器在 token 级别对齐，并采用两阶段训练与跨项目跨域的二进制补丁基准，实现了显著提升的安全补丁检测效果与高效推理性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.05772v1", "published": "2026-01-09", "update_time": "2026-01-09", "download_time": "2026-01-13 01:52:31"}
{"id": "2601.05752", "title": "AutoMonitor-Bench: Evaluating the Reliability of LLM-Based Misbehavior Monitor", "abstract": "We introduce AutoMonitor-Bench, the first benchmark designed to systematically evaluate the reliability of LLM-based misbehavior monitors across diverse tasks and failure modes. AutoMonitor-Bench consists of 3,010 carefully annotated test samples spanning question answering, code generation, and reasoning, with paired misbehavior and benign instances. We evaluate monitors using two complementary metrics: Miss Rate (MR) and False Alarm Rate (FAR), capturing failures to detect misbehavior and oversensitivity to benign behavior, respectively. Evaluating 12 proprietary and 10 open-source LLMs, we observe substantial variability in monitoring performance and a consistent trade-off between MR and FAR, revealing an inherent safety-utility tension. To further explore the limits of monitor reliability, we construct a large-scale training corpus of 153,581 samples and fine-tune Qwen3-4B-Instruction to investigate whether training on known, relatively easy-to-construct misbehavior datasets improves monitoring performance on unseen and more implicit misbehaviors. Our results highlight the challenges of reliable, scalable misbehavior monitoring and motivate future work on task-aware designing and training strategies for LLM-based monitors.", "arxiv_url": "https://arxiv.org/abs/2601.05752", "authors": ["Shu Yang", "Jingyu Hu", "Tong Li", "Hanqi Yan", "Wenxuan Wang", "Di Wang"], "first_author": "Shu Yang", "category": ["Benchmark", "Empirical"], "field": "Model Safety & Monitoring", "task": "Misbehavior Detection", "tags": ["Misbehavior Monitoring", "Miss Rate", "False Alarm Rate", "Paired Instances", "Cross-model Evaluation", "Monitor Fine-Tuning", "Generalization Gap", "Specification Gaming", "Sycophancy", "Safety Violations"], "summary": "本文提出了AutoMonitor-Bench——一个含3,010个（误行为/良性）配对样本的基准，用以系统评估基于LLM的误行为监控器，采用Miss Rate与False Alarm Rate衡量，在12款专有与10款开源模型上进行大规模评测并探讨了通过153K样本微调提升监控器泛化能力的局限性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.05752v1", "published": "2026-01-09", "update_time": "2026-01-09", "download_time": "2026-01-13 01:52:54"}
{"id": "2601.05755", "title": "VIGIL: Defending LLM Agents Against Tool Stream Injection via Verify-Before-Commit", "abstract": "LLM agents operating in open environments face escalating risks from indirect prompt injection, particularly within the tool stream where manipulated metadata and runtime feedback hijack execution flow. Existing defenses encounter a critical dilemma as advanced models prioritize injected rules due to strict alignment while static protection mechanisms sever the feedback loop required for adaptive reasoning. To reconcile this conflict, we propose \\textbf{VIGIL}, a framework that shifts the paradigm from restrictive isolation to a verify-before-commit protocol. By facilitating speculative hypothesis generation and enforcing safety through intent-grounded verification, \\textbf{VIGIL} preserves reasoning flexibility while ensuring robust control. We further introduce \\textbf{SIREN}, a benchmark comprising 959 tool stream injection cases designed to simulate pervasive threats characterized by dynamic dependencies. Extensive experiments demonstrate that \\textbf{VIGIL} outperforms state-of-the-art dynamic defenses by reducing the attack success rate by over 22\\% while more than doubling the utility under attack compared to static baselines, thereby achieving an optimal balance between security and utility. Code is available at https://anonymous.4open.science/r/VIGIL-378B/.", "arxiv_url": "https://arxiv.org/abs/2601.05755", "authors": ["Junda Lin", "Zhaomeng Zhou", "Zhi Zheng", "Shuochen Liu", "Tong Xu", "Yong Chen", "Enhong Chen"], "first_author": "Junda Lin", "category": ["Technical", "Benchmark"], "field": "Agent Security", "task": "Tool Stream Injection Defense", "tags": ["Tool Stream Injection", "Verify-Before-Commit", "Intent-Grounded Verification", "Speculative Reasoning", "Runtime Verification", "Perception Sanitization", "Adaptive Backtracking", "Dynamic Replanning", "Agent Robustness Evaluation", "Alignment-Driven Vulnerability"], "summary": "本文提出VIGIL——一种在执行前验证的LLM代理防御框架，通过意图约束、感知清洗、推测性回溯与运行时验证来抵御工具流注入，并构建了覆盖多向量注入的评测基准以验证其在降低攻击成功率同时保持实用性的效果。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.05755v1", "published": "2026-01-09", "update_time": "2026-01-09", "download_time": "2026-01-13 01:55:29"}
{"id": "2601.05587", "title": "HogVul: Black-box Adversarial Code Generation Framework Against LM-based Vulnerability Detectors", "abstract": "Recent advances in software vulnerability detection have been driven by Language Model (LM)-based approaches. However, these models remain vulnerable to adversarial attacks that exploit lexical and syntax perturbations, allowing critical flaws to evade detection. Existing black-box attacks on LM-based vulnerability detectors primarily rely on isolated perturbation strategies, limiting their ability to efficiently explore the adversarial code space for optimal perturbations. To bridge this gap, we propose HogVul, a black-box adversarial code generation framework that integrates both lexical and syntax perturbations under a unified dual-channel optimization strategy driven by Particle Swarm Optimization (PSO). By systematically coordinating two-level perturbations, HogVul effectively expands the search space for adversarial examples, enhancing the attack efficacy. Extensive experiments on four benchmark datasets demonstrate that HogVul achieves an average attack success rate improvement of 26.05\\% over state-of-the-art baseline methods. These findings highlight the potential of hybrid optimization strategies in exposing model vulnerabilities.", "arxiv_url": "https://arxiv.org/abs/2601.05587", "authors": ["Jingxiao Yang", "Ping He", "Tianyu Du", "Sun Bing", "Xuhong Zhang"], "first_author": "Jingxiao Yang", "category": ["Technical"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Adversarial Code Generation", "Black-box Attack", "Lexical Perturbation", "Syntax Perturbation", "Particle Swarm Optimization", "Dual-channel Optimization", "Semantic-aware Initialization", "Dead-code Injection", "Robustness Evaluation"], "summary": "本文提出 HogVul，一种基于粒子群优化的黑盒对抗代码生成框架，通过联合词法与语法级扰动并在双通道协同优化下生成可编译的对抗样本，从而显著提升对基于语言模型的漏洞检测器的绕过成功率并验证了方法的有效性与稳健性。", "quality": "High", "conference": "AAAI Conference on Artificial Intelligence 2026", "pdf_url": "https://arxiv.org/pdf/2601.05587v1", "published": "2026-01-09", "update_time": "2026-01-09", "download_time": "2026-01-13 01:57:38"}
{"id": "2601.07786", "title": "\"TODO: Fix the Mess Gemini Created\": Towards Understanding GenAI-Induced Self-Admitted Technical Debt", "abstract": "As large language models (LLMs) such as ChatGPT, Copilot, Claude, and Gemini become integrated into software development workflows, developers increasingly leave traces of AI involvement in their code comments. Among these, some comments explicitly acknowledge both the use of generative AI and the presence of technical shortcomings. Analyzing 6,540 LLM-referencing code comments from public Python and JavaScript-based GitHub repositories (November 2022-July 2025), we identified 81 that also self-admit technical debt(SATD). Developers most often describe postponed testing, incomplete adaptation, and limited understanding of AI-generated code, suggesting that AI assistance affects both when and why technical debt emerges. We term GenAI-Induced Self-admitted Technical debt (GIST) as a proposed conceptual lens to describe recurring cases where developers incorporate AI-generated code while explicitly expressing uncertainty about its behavior or correctness.", "arxiv_url": "https://arxiv.org/abs/2601.07786", "authors": ["Abdullah Al Mujahid", "Mia Mohammad Imran"], "first_author": "Abdullah Al Mujahid", "category": ["Empirical", "Benchmark"], "field": "Requirements & Design", "task": "Analysis", "tags": ["GenAI-Induced Technical Debt", "Self-Admitted Technical Debt", "Code Comment Mining", "AI Attribution", "Requirement Debt", "Test Debt", "Design Debt", "Open Coding"], "summary": "本文通过对公共GitHub中显式提及LLM的代码注释进行手工标注与分析，提出并实证化了“生成式AI引发的自我承认技术债（GIST）”概念，揭示了AI参与代码时债务类型的分布变化及开发者对AI角色的归因方式。", "quality": "Middle", "conference": "9th International Conference on Technical Debt (TechDebt 2026) 2026", "pdf_url": "https://arxiv.org/pdf/2601.07786v1", "published": "2026-01-12", "update_time": "2026-01-12", "download_time": "2026-01-14 01:59:42"}
{"id": "2601.07602", "title": "OODEval: Evaluating Large Language Models on Object-Oriented Design", "abstract": "Recent advances in large language models (LLMs) have driven extensive evaluations in software engineering. however, most prior work concentrates on code-level tasks, leaving software design capabilities underexplored. To fill this gap, we conduct a comprehensive empirical study evaluating 29 LLMs on object-oriented design (OOD) tasks. Owing to the lack of standardized benchmarks and metrics, we introduce OODEval, a manually constructed benchmark comprising 50 OOD tasks of varying difficulty, and OODEval-Human, the first human-rated OOD benchmark, which includes 940 undergraduate-submitted class diagrams evaluated by instructors. We further propose CLUE (Class Likeness Unified Evaluation), a unified metric set that assesses both global correctness and fine-grained design quality in class diagram generation. Using these benchmarks and metrics, we investigate five research questions: overall correctness, comparison with humans, model dimension analysis, task feature analysis, and bad case analysis. The results indicate that while LLMs achieve high syntactic accuracy, they exhibit substantial semantic deficiencies, particularly in method and relationship generation. Among the evaluated models, Qwen3-Coder-30B achieves the best overall performance, rivaling DeepSeek-R1 and GPT-4o, while Gemma3-4B-IT outperforms GPT-4o-Mini despite its smaller parameter scale. Although top-performing LLMs nearly match the average performance of undergraduates, they remain significantly below the level of the best human designers. Further analysis shows that parameter scale, code specialization, and instruction tuning strongly influence performance, whereas increased design complexity and lower requirement readability degrade it. Bad case analysis reveals common failure modes, including keyword misuse, missing classes or relationships, and omitted methods.", "arxiv_url": "https://arxiv.org/abs/2601.07602", "authors": ["Bingxu Xiao", "Yunwei Dong", "Yiqi Tang", "Manqing Zhang", "Yifan Zhou", "Chunyan Ma", "Yepang Liu"], "first_author": "Bingxu Xiao", "category": ["Benchmark", "Empirical", "Technical"], "field": "Requirements & Design", "task": "Specification & Validation", "tags": ["Software Modeling", "Class Diagram Evaluation", "Human-Rated Benchmark", "CLUE Metric", "Requirement-to-Design", "Method Generation", "Relationship Generation", "Semantic Correctness", "Syntactic Correctness", "Failure Modes", "Model Scaling Analysis", "Instruction Tuning Effects"], "summary": "本文提出了面向面向对象设计的基准OODEval及人工评分数据集OODEval-Human，并设计了CLUE评估指标，基于此对29个大模型进行系统实证评估，揭示模型在语法上较强但在方法与关系等语义层面存在明显不足。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.07602v1", "published": "2026-01-12", "update_time": "2026-01-12", "download_time": "2026-01-14 02:00:01"}
{"id": "2601.07790", "title": "Benchmarking Small Language Models and Small Reasoning Language Models on System Log Severity Classification", "abstract": "System logs are crucial for monitoring and diagnosing modern computing infrastructure, but their scale and complexity require reliable and efficient automated interpretation. Since severity levels are predefined metadata in system log messages, having a model merely classify them offers limited standalone practical value, revealing little about its underlying ability to interpret system logs. We argue that severity classification is more informative when treated as a benchmark for probing runtime log comprehension rather than as an end task. Using real-world journalctl data from Linux production servers, we evaluate nine small language models (SLMs) and small reasoning language models (SRLMs) under zero-shot, few-shot, and retrieval-augmented generation (RAG) prompting. The results reveal strong stratification. Qwen3-4B achieves the highest accuracy at 95.64% with RAG, while Gemma3-1B improves from 20.25% under few-shot prompting to 85.28% with RAG. Notably, the tiny Qwen3-0.6B reaches 88.12% accuracy despite weak performance without retrieval. In contrast, several SRLMs, including Qwen3-1.7B and DeepSeek-R1-Distill-Qwen-1.5B, degrade substantially when paired with RAG. Efficiency measurements further separate models: most Gemma and Llama variants complete inference in under 1.2 seconds per log, whereas Phi-4-Mini-Reasoning exceeds 228 seconds per log while achieving <10% accuracy. These findings suggest that (1) architectural design, (2) training objectives, and (3) the ability to integrate retrieved context under strict output constraints jointly determine performance. By emphasizing small, deployable models, this benchmark aligns with real-time requirements of digital twin (DT) systems and shows that severity classification serves as a lens for evaluating model competence and real-time deployability, with implications for root cause analysis (RCA) and broader DT integration.", "arxiv_url": "https://arxiv.org/abs/2601.07790", "authors": ["Yahya Masri", "Emily Ma", "Zifu Wang", "Joseph Rogers", "Chaowei Yang"], "first_author": "Yahya Masri", "category": ["Empirical", "Benchmark"], "field": "AIOps", "task": "Log Parsing", "tags": ["Severity Classification", "Retrieval-Augmented Reasoning", "Small Model Evaluation", "Inference Efficiency", "Model Stratification", "Grounded Log Understanding", "Zero/Few-shot Robustness", "Digital Twin Integration"], "summary": "该论文使用真实生产服务器日志，以零/少样本和检索增强提示评估多款小型与小型推理语言模型在日志严重性分类上的准确性与推理效率，揭示结构、训练目标与检索集成能力对性能与实时可部署性的影响。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.07790v1", "published": "2026-01-12", "update_time": "2026-01-12", "download_time": "2026-01-14 02:00:41"}
{"id": "2601.07782", "title": "Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning", "abstract": "LLM agents operating over massive, dynamic tool libraries rely on effective retrieval, yet standard single-shot dense retrievers struggle with complex requests. These failures primarily stem from the disconnect between abstract user goals and technical documentation, and the limited capacity of fixed-size embeddings to model combinatorial tool compositions. To address these challenges, we propose TOOLQP, a lightweight framework that models retrieval as iterative query planning. Instead of single-shot matching, TOOLQP decomposes instructions into sub-tasks and dynamically generates queries to interact with the retriever, effectively bridging the semantic gap by targeting the specific sub-tasks required for composition. We train TOOLQP using synthetic query trajectories followed by optimization via Reinforcement Learning with Verifiable Rewards (RLVR). Experiments demonstrate that TOOLQP achieves state-of-the-art performance, exhibiting superior zero-shot generalization, robustness across diverse retrievers, and significant improvements in downstream agentic execution.", "arxiv_url": "https://arxiv.org/abs/2601.07782", "authors": ["Wei Fang", "James Glass"], "first_author": "Wei Fang", "category": ["Technical"], "field": "Agent Tooling", "task": "Tool Retrieval (Query Planning)", "tags": ["Query Planning", "Iterative Retrieval", "Tool Composition", "Semantic Gap Bridging", "Retriever-Agnostic", "Reinforcement Learning for Retrieval", "Environment Interaction", "Downstream Agent Grounding", "Zero-shot Generalization"], "summary": "本文提出TOOLQP，一种将工具检索建模为迭代查询规划的轻量框架，通过将复杂请求分解为子任务、动态生成子查询并结合合成轨迹与可验证奖励的强化学习优化，从而在大规模动态工具库中显著提升检索准确性和下游代理执行效果。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.07782v1", "published": "2026-01-12", "update_time": "2026-01-12", "download_time": "2026-01-14 02:01:01"}
{"id": "2601.08806", "title": "APEX-SWE", "abstract": "We introduce the AI Productivity Index for Software Engineering (APEX-SWE), a benchmark for assessing whether frontier AI models can execute economically valuable software engineering work. Unlike existing evaluations that focus on narrow, well-defined tasks, APEX-SWE assesses two novel task types that reflect real-world software engineering work: (1) Integration tasks (n=100), which require constructing end-to-end systems across heterogeneous cloud primitives, business applications, and infrastructure-as-code services, and (2) Observability tasks (n=100), which require debugging production failures using telemetry signals such as logs and dashboards, as well as unstructured context. We evaluated eight frontier models on APEX-SWE. Gemini 3 Pro (Thinking = High) performs best, with a Pass@1 score of 25\\%. Our analysis shows that strong performance is primarily driven by epistemic reasoning, defined as the ability to distinguish between assumptions and verified facts, combined with agency to resolve uncertainty prior to acting. We open-source the APEX-SWE evaluation harness and a dev set (n=50).", "arxiv_url": "https://arxiv.org/abs/2601.08806", "authors": ["Abhi Kottamasu", "Akul Datta", "Aakash Barthwal", "Chirag Mahapatra", "Ajay Arun", "Adarsh Hiremath", "Brendan Foody", "Bertie Vidgen"], "first_author": "Abhi Kottamasu", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Repository-Level Coding", "tags": ["Repository-Level Coding", "Code Agents", "Production Debugging", "Observability", "End-to-End Integration", "Infrastructure-as-Code", "Telemetry Analysis", "Epistemic Reasoning"], "summary": "APEX–SWE提出了一个包含100个集成任务和100个可观测性任务的基准，用于评估前沿AI模型在真实生产级软件工程（跨云原语、业务应用、基础设施即代码与生产故障排查）中的实际执行能力，并开源了开发集与评估工具，发现顶级模型Pass@1约为25%，成功往往依赖于区分假设与已验证事实的认识型推理及在行动前化解不确定性的能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.08806v1", "published": "2026-01-13", "update_time": "2026-01-13", "download_time": "2026-01-15 01:56:27"}
{"id": "2601.08773", "title": "Reliable Graph-RAG for Codebases: AST-Derived Graphs vs LLM-Extracted Knowledge Graphs", "abstract": "Retrieval-Augmented Generation for software engineering often relies on vector similarity search, which captures topical similarity but can fail on multi-hop architectural reasoning such as controller to service to repository chains, interface-driven wiring, and inheritance. This paper benchmarks three retrieval pipelines on Java codebases (Shopizer, with additional runs on ThingsBoard and OpenMRS Core): (A) vector-only No-Graph RAG, (B) an LLM-generated knowledge graph RAG (LLM-KB), and (C) a deterministic AST-derived knowledge graph RAG (DKB) built with Tree-sitter and bidirectional traversal.   Using 15 architecture and code-tracing queries per repository, we measure indexing time, query latency, corpus coverage, cost, and answer correctness. DKB builds its graph in seconds, while LLM-KB requires much longer graph generation. LLM-KB also shows indexing incompleteness: on Shopizer, 377 files are skipped or missed, reducing embedded chunk coverage and graph size compared to DKB. End-to-end cost is modest for DKB relative to the vector-only baseline but much higher for LLM-KB, especially as repository scale increases. Query latency is similar for No-Graph and DKB, while LLM-KB is slower and more variable. On the Shopizer question suite, DKB achieves the highest correctness, LLM-KB is close behind, and the vector-only baseline performs worst on upstream architectural queries and has the highest hallucination risk. Overall, deterministic AST-derived graphs provide more reliable coverage and multi-hop grounding than LLM-extracted graphs at substantially lower indexing cost.", "arxiv_url": "https://arxiv.org/abs/2601.08773", "authors": ["Manideep Reddy Chinthareddy"], "first_author": "Manideep Reddy Chinthareddy", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Retrieval", "tags": ["AST-derived Graph", "LLM-extracted Graph", "Graph-RAG", "Multi-hop Reasoning", "Repository-Level Retrieval", "Indexing Coverage", "Cost Analysis", "Deterministic Indexing", "Tree-sitter Parsing", "Hallucination"], "summary": "本文在多个 Java 代码库上对比了向量检索、LLM 生成知识图谱和基于 AST 的确定性知识图谱三种 RAG 管线，结果表明基于 AST 的 DKB 在索引覆盖率、答案正确性与成本-延迟权衡上更优且更可复现，而 LLM 生成的图谱存在提取不完整、随机性高和成本显著更大的问题。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.08773v1", "published": "2026-01-13", "update_time": "2026-01-13", "download_time": "2026-01-15 01:56:54"}
{"id": "2601.08778", "title": "Pervasive Annotation Errors Break Text-to-SQL Benchmarks and Leaderboards", "abstract": "Researchers have proposed numerous text-to-SQL techniques to streamline data analytics and accelerate the development of database-driven applications. To compare these techniques and select the best one for deployment, the community depends on public benchmarks and their leaderboards. Since these benchmarks heavily rely on human annotations during question construction and answer evaluation, the validity of the annotations is crucial.   In this paper, we conduct an empirical study that (i) benchmarks annotation error rates for two widely used text-to-SQL benchmarks, BIRD and Spider 2.0-Snow, and (ii) corrects a subset of the BIRD development (Dev) set to measure the impact of annotation errors on text-to-SQL agent performance and leaderboard rankings. Through expert analysis, we show that BIRD Mini-Dev and Spider 2.0-Snow have error rates of 52.8% and 62.8%, respectively. We re-evaluate all 16 open-source agents from the BIRD leaderboard on both the original and the corrected BIRD Dev subsets. We show that performance changes range from -7% to 31% (in relative terms) and rank changes range from $-9$ to $+9$ positions. We further assess whether these impacts generalize to the full BIRD Dev set. We find that the rankings of agents on the uncorrected subset correlate strongly with those on the full Dev set (Spearman's $r_s$=0.85, $p$=3.26e-5), whereas they correlate weakly with those on the corrected subset (Spearman's $r_s$=0.32, $p$=0.23). These findings show that annotation errors can significantly distort reported performance and rankings, potentially misguiding research directions or deployment choices. Our code and data are available at https://github.com/uiuc-kang-lab/text_to_sql_benchmarks.", "arxiv_url": "https://arxiv.org/abs/2601.08778", "authors": ["Tengjun Jin", "Yoojin Choi", "Yuxuan Zhu", "Daniel Kang"], "first_author": "Tengjun Jin", "category": ["Empirical", "Technical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Annotation Errors", "SQL Verification", "Agent-based Review", "Annotation Pipeline", "Diagnostic Reporting", "Human-in-the-loop", "Leaderboard Reliability", "Ranking Sensitivity", "Schema-aware Validation", "Execution-based Verification"], "summary": "本文通过开发基于代理的审查工具并对 BIRD 与 Spider 2.0-Snow 进行人工验证，发现两大 text-to-SQL 基准分别存在 52.8% 和 62.8% 的标注错误，进而显著扭曲模型性能与排行榜，并提出 SAR-Agent 与 SAPAR 来检测与修正这些注释错误。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.08778v1", "published": "2026-01-13", "update_time": "2026-01-13", "download_time": "2026-01-15 02:06:24"}
{"id": "2601.08777", "title": "Asymptotic Universal Alignment: A New Alignment Framework via Test-Time Scaling", "abstract": "Aligning large language models (LLMs) to serve users with heterogeneous and potentially conflicting preferences is a central challenge for personalized and trustworthy AI. We formalize an ideal notion of universal alignment through test-time scaling: for each prompt, the model produces $k\\ge 1$ candidate responses and a user selects their preferred one. We introduce $(k,f(k))$-robust alignment, which requires the $k$-output model to have win rate $f(k)$ against any other single-output model, and asymptotic universal alignment (U-alignment), which requires $f(k)\\to 1$ as $k\\to\\infty$. Our main result characterizes the optimal convergence rate: there exists a family of single-output policies whose $k$-sample product policies achieve U-alignment at rate $f(k)=\\frac{k}{k+1}$, and no method can achieve a faster rate in general.   We show that popular post-training methods, including Nash learning from human feedback (NLHF), can fundamentally underutilize the benefits of test-time scaling. Even though NLHF is optimal for $k=1$, sampling from the resulting (often deterministic) policy cannot guarantee win rates above $\\tfrac{1}{2}$ except for an arbitrarily small slack. This stems from a lack of output diversity: existing alignment methods can collapse to a single majority-preferred response, making additional samples redundant. In contrast, our approach preserves output diversity and achieves the optimal test-time scaling rate. In particular, we propose a family of symmetric multi-player alignment games and prove that any symmetric Nash equilibrium policy of the $(k+1)$-player alignment game achieves the optimal $(k,\\frac{k}{k+1})$-robust alignment. Finally, we provide theoretical convergence guarantees for self-play learning dynamics in these games and extend the framework to opponents that also generate multiple responses.", "arxiv_url": "https://arxiv.org/abs/2601.08777", "authors": ["Yang Cai", "Weiqiang Zheng"], "first_author": "Yang Cai", "category": ["Technical"], "field": "Model Alignment & Personalization", "task": "Asymptotic Universal Alignment (Test-Time Scaling)", "tags": ["Test-time Scaling", "Asymptotic Universal Alignment", "Robust Alignment", "Output Diversity", "Alignment Games", "Nash Equilibrium", "Self-Play Convergence", "Condorcet Analysis"], "summary": "本文通过在测试时生成多候选回复的框架形式化了渐近普适对齐（U‑alignment），证明存在单输出策略家族通过测试时扩展以最优速率k/(k+1)趋近普适对齐，提出对称多玩家对齐博弈以保持输出多样性并给出自我博弈的收敛性保证，同时指出现有方法在测试时扩展下会因输出塌缩而丧失潜在优势。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.08777v1", "published": "2026-01-13", "update_time": "2026-01-13", "download_time": "2026-01-15 02:09:17"}
{"id": "2601.09703", "title": "ShortCoder: Knowledge-Augmented Syntax Optimization for Token-Efficient Code Generation", "abstract": "Code generation tasks aim to automate the conversion of user requirements into executable code, significantly reducing manual development efforts and enhancing software productivity. The emergence of large language models (LLMs) has significantly advanced code generation, though their efficiency is still impacted by certain inherent architectural constraints. Each token generation necessitates a complete inference pass, requiring persistent retention of contextual information in memory and escalating resource consumption. While existing research prioritizes inference-phase optimizations such as prompt compression and model quantization, the generation phase remains underexplored. To tackle these challenges, we propose a knowledge-infused framework named ShortCoder, which optimizes code generation efficiency while preserving semantic equivalence and readability. In particular, we introduce: (1) ten syntax-level simplification rules for Python, derived from AST-preserving transformations, achieving 18.1% token reduction without functional compromise; (2) a hybrid data synthesis pipeline integrating rule-based rewriting with LLM-guided refinement, producing ShorterCodeBench, a corpus of validated tuples of original code and simplified code with semantic consistency; (3) a fine-tuning strategy that injects conciseness awareness into the base LLMs. Extensive experimental results demonstrate that ShortCoder consistently outperforms state-of-the-art methods on HumanEval, achieving an improvement of 18.1%-37.8% in generation efficiency over previous methods while ensuring the performance of code generation.", "arxiv_url": "https://arxiv.org/abs/2601.09703", "authors": ["Sicong Liu", "Yanxian Huang", "Mingwei Liu", "Jiachi Chen", "Ensheng Shi", "Yuchi Ma", "Hongyu Zhang", "Yin Zhang", "Yanlin Wang"], "first_author": "Sicong Liu", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Performance Optimization", "tags": ["Token Efficiency", "Syntax Simplification", "AST-preserving Transformation", "Rule-based Rewriting", "LLM-guided Refinement", "Conciseness-aware Fine-tuning", "Data Synthesis", "Generation Latency Reduction", "Readability Preservation"], "summary": "ShortCoder提出一种知识增强的Python语法简化与微调框架，基于10条AST保持的简化规则和混合数据合成（规则重写+LLM精化）生成简洁代码对并注入简洁性意识至模型，从而在保持语义正确性的前提下显著减少生成token数并提升代码生成效率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.09703v1", "published": "2026-01-14", "update_time": "2026-01-14", "download_time": "2026-01-16 01:56:29"}
{"id": "2601.09695", "title": "How well LLM-based test generation techniques perform with newer LLM versions?", "abstract": "The rapid evolution of Large Language Models (LLMs) has strongly impacted software engineering, leading to a growing number of studies on automated unit test generation. However, the standalone use of LLMs without post-processing has proven insufficient, often producing tests that fail to compile or achieve high coverage. Several techniques have been proposed to address these issues, reporting improvements in test compilation and coverage. While important, LLM-based test generation techniques have been evaluated against relatively weak baselines (for todays' standards), i.e., old LLM versions and relatively weak prompts, which may exacerbate the performance contribution of the approaches. In other words, stronger (newer) LLMs may obviate any advantage these techniques bring. We investigate this issue by replicating four state-of-the-art LLM-based test generation tools, HITS, SymPrompt, TestSpark, and CoverUp that include engineering components aimed at guiding the test generation process through compilation and execution feedback, and evaluate their relative effectiveness and efficiency over a plain LLM test generation method. We integrate current LLM versions in all approaches and run an experiment on 393 classes and 3,657 methods. Our results show that the plain LLM approach can outperform previous state-of-the-art approaches in all test effectiveness metrics we used: line coverage (by 17.72%), branch coverage (by 19.80%) and mutation score (by 20.92%), and it does so at a comparable cost (LLM queries). We also observe that the granularity at which the plain LLM is applied has a significant impact on the cost. We therefore propose targeting first the program classes, where test generation is more efficient, and then the uncovered methods to reduce the number of LLM requests. This strategy achieves comparable (slightly higher) effectiveness while requiring about 20% fewer LLM requests.", "arxiv_url": "https://arxiv.org/abs/2601.09695", "authors": ["Michael Konstantinou", "Renzo Degiovanni", "Mike Papadakis"], "first_author": "Michael Konstantinou", "category": ["Empirical"], "field": "Software Testing", "task": "Test Generation", "tags": ["Mutation Testing", "Class-level Generation", "Method-level Generation", "Hybrid Granularity", "LLM Cost Analysis", "Compilation Failures", "Coverage Evaluation", "Replication Study", "LLM Version Comparison"], "summary": "本文复现并比较了四种基于LLM的单元测试生成工具与简单的零样本LLM提示在新一代LLM（如gpt-4o-mini、Llama 3.3、DeepSeek V3）上的效果与成本，结果表明Plain-LLM在覆盖率和变异得分上超越了这些方法，并提出先按类再针对未覆盖方法的混合粒度策略可在保持效果的同时节省约20%请求成本。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.09695v1", "published": "2026-01-14", "update_time": "2026-01-14", "download_time": "2026-01-16 01:56:52"}
{"id": "2601.09282", "title": "Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing", "abstract": "Cluster workload allocation often requires complex configurations, creating a usability gap. This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. The system employs a Large Language Model (LLM) integrated via a Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. A prototype featuring a cluster state cache and an intent analyzer (using AWS Bedrock) was developed. Empirical evaluation demonstrated high LLM parsing accuracy (>95% Subset Accuracy on an evaluation ground-truth dataset) for top-tier models like Amazon Nova Pro/Premier and Mistral Pixtral Large, significantly outperforming a baseline engine. Scheduling quality tests across six scenarios showed the prototype achieved superior or equivalent placement compared to standard Kubernetes configurations, particularly excelling in complex and quantitative scenarios and handling conflicting soft preferences. The results validate using LLMs for accessible scheduling but highlight limitations like synchronous LLM latency, suggesting asynchronous processing for production readiness. This work confirms the viability of semantic soft affinity for simplifying workload orchestration.", "arxiv_url": "https://arxiv.org/abs/2601.09282", "authors": ["Leszek Sliwko", "Jolanta Mizeria-Pietraszko"], "first_author": "Leszek Sliwko", "category": ["Technical", "Empirical"], "field": "Cluster Management & Scheduling", "task": "Semantic Soft-Affinity Scheduling", "tags": ["Semantic Soft-Affinity", "Natural-Language Intent Parsing", "Kubernetes Scheduler Extender", "Cluster State Cache", "Score Scalarization", "Placement Quality Evaluation", "Synchronous LLM Latency"], "summary": "本文提出并实现了一个基于大语言模型的语义意图驱动Kubernetes调度原型，将自然语言的分配提示解析为软亲和性偏好以优化节点放置，并在评估中表现出高解析准确率和优越或相当的调度质量，同时指出同步LLM延迟为部署限制。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.09282v1", "published": "2026-01-14", "update_time": "2026-01-14", "download_time": "2026-01-16 02:02:14"}
{"id": "2601.09097", "title": "Programming over Thinking: Efficient and Robust Multi-Constraint Planning", "abstract": "Multi-constraint planning involves identifying, evaluating, and refining candidate plans while satisfying multiple, potentially conflicting constraints. Existing large language model (LLM) approaches face fundamental limitations in this domain. Pure reasoning paradigms, which rely on long natural language chains, are prone to inconsistency, error accumulation, and prohibitive cost as constraints compound. Conversely, LLMs combined with coding- or solver-based strategies lack flexibility: they often generate problem-specific code from scratch or depend on fixed solvers, failing to capture generalizable logic across diverse problems. To address these challenges, we introduce the Scalable COde Planning Engine (SCOPE), a framework that disentangles query-specific reasoning from generic code execution. By separating reasoning from execution, SCOPE produces solver functions that are consistent, deterministic, and reusable across queries while requiring only minimal changes to input parameters. SCOPE achieves state-of-the-art performance while lowering cost and latency. For example, with GPT-4o, it reaches 93.1% success on TravelPlanner, a 61.6% gain over the best baseline (CoT) while cutting inference cost by 1.4x and time by ~4.67x. Code is available at https://github.com/DerrickGXD/SCOPE.", "arxiv_url": "https://arxiv.org/abs/2601.09097", "authors": ["Derrick Goh Xin Deik", "Quanyu Long", "Zhengyuan Liu", "Nancy F. Chen", "Wenya Wang"], "first_author": "Derrick Goh Xin Deik", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Code Agents", "Reusable Solver", "Problem Formalization", "Parameter Extraction", "Structured Representation", "Combination Enumeration", "Output Reflection", "Deterministic Execution", "Efficiency", "Robustness"], "summary": "本文提出SCOPE，一种将查询特定的结构化问题形式化与通用可重用求解器生成分离的多代理框架，能从单个示例自动抽取参数化表示并生成确定性、可复用的求解函数，从而在多约束规划任务上实现更高的鲁棒性与更低的成本与延迟。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.09097v1", "published": "2026-01-14", "update_time": "2026-01-14", "download_time": "2026-01-16 02:05:26"}
{"id": "2601.10496", "title": "Model See, Model Do? Exposure-Aware Evaluation of Bug-vs-Fix Preference in Code LLMs", "abstract": "Large language models are increasingly used for code generation and debugging, but their outputs can still contain bugs, that originate from training data. Distinguishing whether an LLM prefers correct code, or a familiar incorrect version might be influenced by what it's been exposed to during training. We introduce an exposure-aware evaluation framework that quantifies how prior exposure to buggy versus fixed code influences a model's preference. Using the ManySStuBs4J benchmark, we apply Data Portraits for membership testing on the Stack-V2 corpus to estimate whether each buggy and fixed variant was seen during training. We then stratify examples by exposure and compare model preference using code completion as well as multiple likelihood-based scoring metrics We find that most examples (67%) have neither variant in the training data, and when only one is present, fixes are more frequently present than bugs. In model generations, models reproduce buggy lines far more often than fixes, with bug-exposed examples amplifying this tendency and fix-exposed examples showing only marginal improvement. In likelihood scoring, minimum and maximum token-probability metrics consistently prefer the fixed code across all conditions, indicating a stable bias toward correct fixes. In contrast, metrics like the Gini coefficient reverse preference when only the buggy variant was seen. Our results indicate that exposure can skew bug-fix evaluations and highlight the risk that LLMs may propagate memorised errors in practice.", "arxiv_url": "https://arxiv.org/abs/2601.10496", "authors": ["Ali Al-Kaswan", "Claudio Spiess", "Prem Devanbu", "Arie van Deursen", "Maliheh Izadi"], "first_author": "Ali Al-Kaswan", "category": ["Empirical", "Technical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Exposure-aware evaluation", "Membership inference", "Bug propagation", "Memorisation", "Likelihood-based scoring", "Generation matching", "Metric sensitivity", "Model bias"], "summary": "本文提出一种面向训练数据曝光的评估框架，基于成员资格推断判断SStuBs中的bug/修复是否出现在训练语料，进而通过似然评分与生成匹配分析曝光如何影响代码LLM对bug与修复的偏好，发现生成端更易复现bug且曝光会放大此现象，而某些似然指标则稳定偏好修复。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.10496v1", "published": "2026-01-15", "update_time": "2026-01-15", "download_time": "2026-01-17 01:50:45"}
{"id": "2601.10258", "title": "Evolving with AI: A Longitudinal Analysis of Developer Logs", "abstract": "AI-powered coding assistants are rapidly becoming fixtures in professional IDEs, yet their sustained influence on everyday development remains poorly understood. Prior research has focused on short-term use or self-reported perceptions, leaving open questions about how sustained AI use reshapes actual daily coding practices in the long term. We address this gap with a mixed-method study of AI adoption in IDEs, combining longitudinal two-year fine-grained telemetry from 800 developers with a survey of 62 professionals. We analyze five dimensions of workflow change: productivity, code quality, code editing, code reuse, and context switching. Telemetry reveals that AI users produce substantially more code but also delete significantly more. Meanwhile, survey respondents report productivity gains and perceive minimal changes in other dimensions. Our results offer empirical insights into the silent restructuring of software workflows and provide implications for designing future AI-augmented tooling.", "arxiv_url": "https://arxiv.org/abs/2601.10258", "authors": ["Agnia Sergeyuk", "Eric Huang", "Dariia Karaeva", "Anastasiia Serova", "Yaroslav Golubev", "Iftekhar Ahmed"], "first_author": "Agnia Sergeyuk", "category": ["Empirical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Longitudinal Telemetry", "IDE Interaction", "Productivity", "Code Quality", "Code Reuse", "Context Switching", "Human-LLM Interaction", "Survey Triangulation", "Workflow Fragmentation", "External Code Copying"], "summary": "本文通过对800名开发者两年IDE遥测日志与62份问卷的混合方法纵向分析，发现使用AI助手的开发者产出更多代码但删除也更多、更频繁复制外部代码且切换上下文更频繁，而受访者自我感知主要表现为生产力提升。", "quality": "High", "conference": "ICSE (International Conference on Software Engineering) 2026", "pdf_url": "https://arxiv.org/pdf/2601.10258v1", "published": "2026-01-15", "update_time": "2026-01-15", "download_time": "2026-01-17 01:51:19"}
{"id": "2601.10402", "title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering", "abstract": "The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable blueprint for AI capable of autonomous exploration beyond human-precedent complexities.", "arxiv_url": "https://arxiv.org/abs/2601.10402", "authors": ["Xinyu Zhu", "Yuzhu Cai", "Zexi Liu", "Bingyang Zheng", "Cheng Wang", "Rui Ye", "Jiaao Chen", "Hanrui Wang", "Wei-Chen Wang", "Yuzhi Zhang", "Linfeng Zhang", "Weinan E", "Di Jin", "Siheng Chen"], "first_author": "Xinyu Zhu", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Agents", "tags": ["Ultra-Long-Horizon", "Hierarchical Memory", "Context Management", "Agentic Science", "Cognitive Caching", "Experiment Planning", "Long-Running Automation", "Benchmark Evaluation"], "summary": "论文提出面向超长周期自主科研的智能体ML‑Master 2.0，通过层级化认知缓存（HCC）将短期经验提炼为稳定知识与跨任务“智慧”，在OpenAI的MLE‑Bench上24小时预算下取得SOTA表现，显著提升机器学习工程任务的长期探索与策略一致性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.10402v1", "published": "2026-01-15", "update_time": "2026-01-15", "download_time": "2026-01-17 01:55:22"}
{"id": "2601.10343", "title": "OctoBench: Benchmarking Scaffold-Aware Instruction Following in Repository-Grounded Agentic Coding", "abstract": "Modern coding scaffolds turn LLMs into capable software agents, but their ability to follow scaffold-specified instructions remains under-examined, especially when constraints are heterogeneous and persist across interactions. To fill this gap, we introduce OctoBench, which benchmarks scaffold-aware instruction following in repository-grounded agentic coding. OctoBench includes 34 environments and 217 tasks instantiated under three scaffold types, and is paired with 7,098 objective checklist items. To disentangle solving the task from following the rules, we provide an automated observation-and-scoring toolkit that captures full trajectories and performs fine-grained checks. Experiments on eight representative models reveal a systematic gap between task-solving and scaffold-aware compliance, underscoring the need for training and evaluation that explicitly targets heterogeneous instruction following. We release the benchmark to support reproducible benchmarking and to accelerate the development of more scaffold-aware coding agents.", "arxiv_url": "https://arxiv.org/abs/2601.10343", "authors": ["Deming Ding", "Shichun Liu", "Enhui Yang", "Jiahang Lin", "Ziying Chen", "Shihan Dou", "Honglin Guo", "Weiyu Cheng", "Pengyu Zhao", "Chengjun Xiao", "Qunhong Zeng", "Qi Zhang", "Xuanjing Huang", "Qidi Xu", "Tao Gui"], "first_author": "Deming Ding", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Code Agents", "Repository-Level Coding", "Scaffold Compliance", "Checklist-based Evaluation", "LLM-as-a-Judge", "Observation Harness", "Instruction Prioritization", "Cross-Scaffold Robustness", "Conflict Testing"], "summary": "本文提出OCTOBENCH——一个面向仓库驱动的代理化编程脚手架的指令遵循基准，包含34个环境、217个任务与7098个可判定检查项，并配套轨迹记录与自动化打分工具以评估模型在异构、持久约束与冲突情形下的合规性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.10343v1", "published": "2026-01-15", "update_time": "2026-01-15", "download_time": "2026-01-17 01:56:20"}
{"id": "2601.10338", "title": "Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale", "abstract": "The rise of AI agent frameworks has introduced agent skills, modular packages containing instructions and executable code that dynamically extend agent capabilities. While this architecture enables powerful customization, skills execute with implicit trust and minimal vetting, creating a significant yet uncharacterized attack surface. We conduct the first large-scale empirical security analysis of this emerging ecosystem, collecting 42,447 skills from two major marketplaces and systematically analyzing 31,132 using SkillScan, a multi-stage detection framework integrating static analysis with LLM-based semantic classification. Our findings reveal pervasive security risks: 26.1% of skills contain at least one vulnerability, spanning 14 distinct patterns across four categories: prompt injection, data exfiltration, privilege escalation, and supply chain risks. Data exfiltration (13.3%) and privilege escalation (11.8%) are most prevalent, while 5.2% of skills exhibit high-severity patterns strongly suggesting malicious intent. We find that skills bundling executable scripts are 2.12x more likely to contain vulnerabilities than instruction-only skills (OR=2.12, p<0.001). Our contributions include: (1) a grounded vulnerability taxonomy derived from 8,126 vulnerable skills, (2) a validated detection methodology achieving 86.7% precision and 82.5% recall, and (3) an open dataset and detection toolkit to support future research. These results demonstrate an urgent need for capability-based permission systems and mandatory security vetting before this attack vector is further exploited.", "arxiv_url": "https://arxiv.org/abs/2601.10338", "authors": ["Yi Liu", "Weizhe Wang", "Ruitao Feng", "Yao Zhang", "Guangquan Xu", "Gelei Deng", "Yuekang Li", "Leo Zhang"], "first_author": "Yi Liu", "category": ["Empirical", "Benchmark"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Agent Skills", "Supply Chain Risks", "Prompt Injection", "Data Exfiltration", "Privilege Escalation", "Static Analysis", "Semantic Classification", "Detection Framework", "Security Taxonomy", "Open Dataset"], "summary": "本文对两大市场收集的3万余个AI代理技能进行大规模实证安全分析，提出结合静态分析与LLM语义分类的SkillScan框架，揭示四大类14种广泛存在的漏洞并公开数据与工具以促进防护研究。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.10338v1", "published": "2026-01-15", "update_time": "2026-01-15", "download_time": "2026-01-18 02:03:20"}
{"id": "2601.10253", "title": "Developer Interaction Patterns with Proactive AI: A Five-Day Field Study", "abstract": "Current in-IDE AI coding tools typically rely on time-consuming manual prompting and context management, whereas proactive alternatives that anticipate developer needs without explicit invocation remain underexplored. Understanding when humans are receptive to such proactive AI assistance during their daily work remains an open question in human-AI interaction research. We address this gap through a field study of proactive AI assistance in professional developer workflows. We present a five-day in-the-wild study with 15 developers who interacted with a proactive feature of an AI assistant integrated into a production-grade IDE that offers code quality suggestions based on in-IDE developer activity. We examined 229 AI interventions across 5,732 interaction points to understand how proactive suggestions are received across workflow stages, how developers experience them, and their perceived impact. Our findings reveal systematic patterns in human receptivity to proactive suggestions: interventions at workflow boundaries (e.g., post-commit) achieved 52% engagement rates, while mid-task interventions (e.g., on declined edit) were dismissed 62% of the time. Notably, well-timed proactive suggestions required significantly less interpretation time than reactive suggestions (45.4s versus 101.4s, W = 109.00, r = 0.533, p = 0.0016), indicating enhanced cognitive alignment. This study provides actionable implications for designing proactive coding assistants, including how to time interventions, align them with developer context, and strike a balance between AI agency and user control in production IDEs.", "arxiv_url": "https://arxiv.org/abs/2601.10253", "authors": ["Nadine Kuo", "Agnia Sergeyuk", "Valerie Chen", "Maliheh Izadi"], "first_author": "Nadine Kuo", "category": ["Empirical"], "field": "Coding Assistant", "task": "Human-LLM Interaction", "tags": ["Proactive Assistance", "In-IDE Integration", "Timing of Interventions", "Workflow Boundaries", "User Control", "Cognitive Load", "Code Quality Suggestions", "Field Study"], "summary": "论文通过将主动式代码质量建议集成到生产级IDE并进行为期五天的实地研究，揭示开发者对LLM主动干预的时机依赖型接受模式（如提交后更易接受），并提出改进主动助理设计的实践启示。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.10253v1", "published": "2026-01-15", "update_time": "2026-01-15", "download_time": "2026-01-18 02:03:36"}
{"id": "2601.10681", "title": "Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems", "abstract": "Large language model (LLM) contexts are typically constructed using retrieval-augmented generation (RAG), which involves ranking and selecting the top-k passages. The approach causes fragmentation in information graphs in document structures, over-retrieval, and duplication of content alongside insufficient query context, including 2nd and 3rd order facets. In this paper, a structure-informed and diversity-constrained context bubble construction framework is proposed that assembles coherent, citable bundles of spans under a strict token budget. The method preserves and exploits inherent document structure by organising multi-granular spans (e.g., sections and rows) and using task-conditioned structural priors to guide retrieval. Starting from high-relevance anchor spans, a context bubble is constructed through constrained selection that balances query relevance, marginal coverage, and redundancy penalties. It will explicitly constrain diversity and budget, producing compact and informative context sets, unlike top-k retrieval. Moreover, a full retrieval is emitted that traces the scoring and selection choices of the records, thus providing auditability and deterministic tuning. Experiments on enterprise documents demonstrate the efficiency of context bubble as it significantly reduces redundant context, is better able to cover secondary facets and has a better answer quality and citation faithfulness within a limited context window. Ablation studies demonstrate that both structural priors as well as diversity constraint selection are necessary; removing either component results in a decline in coverage and an increase in redundant or incomplete context.", "arxiv_url": "https://arxiv.org/abs/2601.10681", "authors": ["Amir Khurshid", "Abhishek Sehgal"], "first_author": "Amir Khurshid", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Retrieval", "tags": ["Context Window Optimization", "Structure-Aware Retrieval", "Diversity-Constrained Selection", "MMR-style Selection", "Auditability", "Token Budgeting", "Enterprise Documents", "Citation Faithfulness"], "summary": "该论文提出一种结构感知与多样性约束的“上下文气泡”构造方法，在严格token预算下从企业文档中选择多粒度片段以兼顾相关性、覆盖与去冗余，并通过可审计的选择流程显著提升RAG的答案质量与引用可信度。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.10681v1", "published": "2026-01-15", "update_time": "2026-01-15", "download_time": "2026-01-18 02:04:16"}
{"id": "2601.10498", "title": "Projected Microbatch Accumulation yields reference-free proximal policy updates for reinforcement learning", "abstract": "This note introduces Projected Microbatch Accumulation (PROMA), a proximal policy update method for large language model fine-tuning. PROMA accumulates policy gradients across microbatches by projecting out sequence-wise gradient components before microbatch aggregation. The projection is applied layer-wise during the backward pass, enabling efficient implementation without additional forward or backward passes. Empirically, PROMA enforces tighter control of local KL divergence than GRPO, resulting in more stable policy learning. Unlike PPO and GRPO, PROMA achieves proximal updates without inducing entropy collapse and does not rely on a reference policy or likelihood-ratio clipping.", "arxiv_url": "https://arxiv.org/abs/2601.10498", "authors": ["Nilin Abrahamsen"], "first_author": "Nilin Abrahamsen", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Alignment", "tags": ["RLHF Optimization", "Proximal Updates", "Reference-Free RL", "Gradient Projection", "KL Control", "Entropy Preservation", "Microbatch Accumulation", "Stability"], "summary": "本文提出PROMA方法，通过在反向传播中对微批次累积梯度进行序列梯度正交投影，实现无需参考策略与比值截断的近端策略更新，从而在LLM强化学习微调中更稳健地控制KL并保持更高熵与平滑更新。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.10498v1", "published": "2026-01-15", "update_time": "2026-01-15", "download_time": "2026-01-18 02:05:45"}
{"id": "2601.11362", "title": "RITA: A Tool for Automated Requirements Classification and Specification from Online User Feedback", "abstract": "Context and motivation. Online user feedback is a valuable resource for requirements engineering, but its volume and noise make analysis difficult. Existing tools support individual feedback analysis tasks, but their capabilities are rarely integrated into end-to-end support. Problem. The lack of end-to-end integration limits the practical adoption of existing RE tools and makes it difficult to assess their real-world usefulness. Solution. To address this challenge, we present RITA, a tool that integrates lightweight open-source large language models into a unified workflow for feedback-driven RE. RITA supports automated request classification, non-functional requirement identification, and natural-language requirements specification generation from online feedback via a user-friendly interface, and integrates with Jira for seamless transfer of requirements specifications to development tools. Results and conclusions. RITA exploits previously evaluated LLM-based RE techniques to efficiently transform raw user feedback into requirements artefacts, helping bridge the gap between research and practice. A demonstration is available at: https://youtu.be/8meCLpwQWV8.", "arxiv_url": "https://arxiv.org/abs/2601.11362", "authors": ["Manjeshwar Aniruddh Mallya", "Alessio Ferrari", "Mohammad Amin Zadenoori", "Jacek Dąbrowski"], "first_author": "Manjeshwar Aniruddh Mallya", "category": ["Technical"], "field": "Requirements & Design", "task": "Specification & Validation", "tags": ["Requirements Classification", "Non-Functional Requirements", "User Feedback Mining", "Requirements Specification", "User Stories", "Jira Integration", "Prompt Engineering", "Lightweight LLMs", "End-to-End Tool"], "summary": "本文介绍了RITA工具，将轻量开源LLM整合到端到端流程中，从在线用户反馈自动进行请求分类、非功能需求识别与需求规格/用户故事生成，并支持一键同步至Jira。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.11362v1", "published": "2026-01-16", "update_time": "2026-01-16", "download_time": "2026-01-19 02:01:11"}
{"id": "2601.11077", "title": "ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development", "abstract": "The evolution of Large Language Models (LLMs) into autonomous agents has expanded the scope of AI coding from localized code generation to complex, repository-level, and execution-driven problem solving. However, current benchmarks predominantly evaluate code logic in static contexts, neglecting the dynamic, full-process requirements of real-world engineering, particularly in backend development which demands rigorous environment configuration and service deployment. To address this gap, we introduce ABC-Bench, a benchmark explicitly designed to evaluate agentic backend coding within a realistic, executable workflow. Using a scalable automated pipeline, we curated 224 practical tasks spanning 8 languages and 19 frameworks from open-source repositories. Distinct from previous evaluations, ABC-Bench require the agents to manage the entire development lifecycle from repository exploration to instantiating containerized services and pass the external end-to-end API tests. Our extensive evaluation reveals that even state-of-the-art models struggle to deliver reliable performance on these holistic tasks, highlighting a substantial disparity between current model capabilities and the demands of practical backend engineering. Our code is available at https://github.com/OpenMOSS/ABC-Bench.", "arxiv_url": "https://arxiv.org/abs/2601.11077", "authors": ["Jie Yang", "Honglin Guo", "Li Ji", "Jiazheng Zhou", "Rui Zheng", "Zhikai Lei", "Shuo Zhang", "Zhiheng Xi", "Shichun Liu", "Yuxin Wang", "Bo Wang", "Yining Zheng", "Tao Gui", "Xipeng Qiu"], "first_author": "Jie Yang", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Repository-Level Coding", "tags": ["Repository-Level Coding", "Code Agents", "Containerization", "Environment Configuration", "End-to-End API Testing", "Task Generation Pipeline", "Deployment Robustness", "Backend Framework Diversity"], "summary": "本文提出ABC-Bench，一个由224个可执行后端全生命周期任务构成的基准与自动化构建流水线，要求代理完成仓库探索、代码修改、环境配置与容器化部署并通过端到端API测试，评估显示环境配置与部署为当前模型的主要瓶颈。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.11077v1", "published": "2026-01-16", "update_time": "2026-01-16", "download_time": "2026-01-19 02:01:37"}
{"id": "2601.10398", "title": "LatentRefusal: Latent-Signal Refusal for Unanswerable Text-to-SQL Queries", "abstract": "In LLM-based text-to-SQL systems, unanswerable and underspecified user queries may generate not only incorrect text but also executable programs that yield misleading results or violate safety constraints, posing a major barrier to safe deployment. Existing refusal strategies for such queries either rely on output-level instruction following, which is brittle due to model hallucinations, or estimate output uncertainty, which adds complexity and overhead. To address this challenge, we formalize safe refusal in text-to-SQL systems as an answerability-gating problem and propose LatentRefusal, a latent-signal refusal mechanism that predicts query answerability from intermediate hidden activations of a large language model. We introduce the Tri-Residual Gated Encoder, a lightweight probing architecture, to suppress schema noise and amplify sparse, localized cues of question-schema mismatch that indicate unanswerability. Extensive empirical evaluations across diverse ambiguous and unanswerable settings, together with ablation studies and interpretability analyses, demonstrate the effectiveness of the proposed approach and show that LatentRefusal provides an attachable and efficient safety layer for text-to-SQL systems. Across four benchmarks, LatentRefusal improves average F1 to 88.5 percent on both backbones while adding approximately 2 milliseconds of probe overhead.", "arxiv_url": "https://arxiv.org/abs/2601.10398", "authors": ["Xuancheng Ren", "Shijing Hu", "Zhihui Lu", "Jiangqi Huang", "Qiang Duan"], "first_author": "Xuancheng Ren", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Alignment", "tags": ["Pre-generation refusal", "Latent-signal probing", "Gated encoder probe", "Question-schema mismatch", "Schema-noise suppression", "Single-pass gating", "Execution-free safety", "Low-latency"], "summary": "本文提出LatentRefusal，通过在冻结的大型模型中对中间隐藏态进行轻量探针检测并放大问题与数据库模式不匹配的稀疏信号，从而在生成或执行任何SQL之前以单次前向、低延迟且无执行的方式判断Text-to-SQL查询是否可回答并进行安全拒绝。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.10398v1", "published": "2026-01-15", "update_time": "2026-01-15", "download_time": "2026-01-19 02:07:00"}
{"id": "2601.10011", "title": "Memo-SQL: Structured Decomposition and Experience-Driven Self-Correction for Training-Free NL2SQL", "abstract": "Existing NL2SQL systems face two critical limitations: (1) they rely on in-context learning with only correct examples, overlooking the rich signal in historical error-fix pairs that could guide more robust self-correction; and (2) test-time scaling approaches often decompose questions arbitrarily, producing near-identical SQL candidates across runs and diminishing ensemble gains. Moreover, these methods suffer from a stark accuracy-efficiency trade-off: high performance demands excessive computation, while fast variants compromise quality. We present Memo-SQL, a training-free framework that addresses these issues through two simple ideas: structured decomposition and experience-aware self-correction. Instead of leaving decomposition to chance, we apply three clear strategies, entity-wise, hierarchical, and atomic sequential, to encourage diverse reasoning. For correction, we build a dynamic memory of both successful queries and historical error-fix pairs, and use retrieval-augmented prompting to bring relevant examples into context at inference time, no fine-tuning or external APIs required. On BIRD, Memo-SQL achieves 68.5% execution accuracy, setting a new state of the art among open, zero-fine-tuning methods, while using over 10 times fewer resources than prior TTS approaches.", "arxiv_url": "https://arxiv.org/abs/2601.10011", "authors": ["Zerui Yang", "Weichuan Wang", "Yanwei Xu", "Linqi Song", "Yudai Matsuda", "Wei Han", "Bo Bai"], "first_author": "Zerui Yang", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Structured Decomposition", "Error-Correction Memory", "Retrieval-Augmented ICL", "Experience-Guided Self-Correction", "ReAct+Reflection", "Test-Time Scaling", "Ensemble Diversity", "Execution-Guided Repair", "NL2SQL"], "summary": "本文提出Memo-SQL，一种无需训练的NL2SQL框架，通过实体/层次/原子三种结构化分解策略结合基于历史错误—修正对的检索增强即时自我纠正（ReAct+Reflection），在显著降低计算开销的同时提升执行准确率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.10011v1", "published": "2026-01-15", "update_time": "2026-01-15", "download_time": "2026-01-19 02:15:31"}
{"id": "2601.10942", "title": "Change And Cover: Last-Mile, Pull Request-Based Regression Test Augmentation", "abstract": "Software is in constant evolution, with developers frequently submitting pull requests (PRs) to introduce new features or fix bugs. Testing PRs is critical to maintaining software quality. Yet, even in projects with extensive test suites, some PR-modified lines remain untested, leaving a \"last-mile\" regression test gap. Existing test generators typically aim to improve overall coverage, but do not specifically target the uncovered lines in PRs. We present Change And Cover (ChaCo), an LLM-based test augmentation technique that addresses this gap. It makes three contributions: (i) ChaCo considers the PR-specific patch coverage, offering developers augmented tests for code just when it is on the developers' mind. (ii) We identify providing suitable test context as a crucial challenge for an LLM to generate useful tests, and present two techniques to extract relevant test content, such as existing test functions, fixtures, and data generators. (iii) To make augmented tests acceptable for developers, ChaCo carefully integrates them into the existing test suite, e.g., by matching the test's structure and style with the existing tests, and generates a summary of the test addition for developer review. We evaluate ChaCo on 145 PRs from three popular and complex open-source projects - SciPy, Qiskit, and Pandas. The approach successfully helps 30% of PRs achieve full patch coverage, at the cost of $0.11, showing its effectiveness and practicality. Human reviewers find the tests to be worth adding (4.53/5.0), well integrated (4.2/5.0), and relevant to the PR (4.7/5.0). Ablations show test context is crucial for context-aware test generation, leading to 2x coverage. We submitted 12 tests, of which 8 have already been merged, and two previously unknown bugs were exposed and fixed. We envision our approach to be integrated into CI workflows, automating the last mile of regression test augmentation.", "arxiv_url": "https://arxiv.org/abs/2601.10942", "authors": ["Zitong Zhou", "Matteo Paltenghi", "Miryung Kim", "Michael Pradel"], "first_author": "Zitong Zhou", "category": ["Technical"], "field": "Software Testing", "task": "Test Generation", "tags": ["PR-based testing", "Patch coverage", "Context-aware generation", "Test suite integration", "Fixture reuse", "Runtime feedback", "Developer acceptability"], "summary": "本文提出ChaCo，一种基于大语言模型的PR特定回归测试增强方法，通过提取PR与现有测试的上下文、迭代运行反馈并将生成的测试按项目风格整合到测试套件中，从而有效弥补补丁覆盖率的“最后一公里”缺口并被维护者接受。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.10942v1", "published": "2026-01-16", "update_time": "2026-01-16", "download_time": "2026-01-20 01:56:44"}
{"id": "2601.10865", "title": "Multi-Agent Taint Specification Extraction for Vulnerability Detection", "abstract": "Static Application Security Testing (SAST) tools using taint analysis are widely viewed as providing higher-quality vulnerability detection results compared to traditional pattern-based approaches. However, performing static taint analysis for JavaScript poses two major challenges. First, JavaScript's dynamic features complicate data flow extraction required for taint tracking. Second, npm's large library ecosystem makes it difficult to identify relevant sources/sinks and establish taint propagation across dependencies. In this paper, we present SemTaint, a multi-agent system that strategically combines the semantic understanding of Large Language Models (LLMs) with traditional static program analysis to extract taint specifications, including sources, sinks, call edges, and library flow summaries tailored to each package. Conceptually, SemTaint uses static program analysis to calculate a call graph and defers to an LLM to resolve call edges that cannot be resolved statically. Further, it uses the LLM to classify sources and sinks for a given CWE. The resulting taint specification is then provided to a SAST tool, which performs vulnerability analysis. We integrate SemTaint with CodeQL, a state-of-the-art SAST tool, and demonstrate its effectiveness by detecting 106 of 162 vulnerabilities previously undetectable by CodeQL. Furthermore, we find 4 novel vulnerabilities in 4 popular npm packages. In doing so, we demonstrate that LLMs can practically enhance existing static program analysis algorithms, combining the strengths of both symbolic reasoning and semantic understanding for improved vulnerability detection.", "arxiv_url": "https://arxiv.org/abs/2601.10865", "authors": ["Jonah Ghebremichael", "Saastha Vasan", "Saad Ullah", "Greg Tystahl", "David Adei", "Christopher Kruegel", "Giovanni Vigna", "William Enck", "Alexandros Kapravelos"], "first_author": "Jonah Ghebremichael", "category": ["Technical"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Taint Specification Extraction", "Call Graph Repair", "Taint-Informed Callee Resolution", "Flow Summaries", "Demand-Driven Dependency Modeling", "Multi-Agent Reasoning", "LLM-Augmented Static Analysis", "CodeQL Integration"], "summary": "SemTaint 提出一种静态驱动、LLM 增强的多代理系统，通过有选择地修复调用图、识别源/汇并生成库流摘要，为 CodeQL 提供可复用的污点规范，从而显著提升对 JavaScript/npm 包中漏洞的静态检测能力并发现若干新漏洞。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.10865v1", "published": "2026-01-15", "update_time": "2026-01-15", "download_time": "2026-01-20 01:57:15"}
{"id": "2601.11124", "title": "Learn Before Represent: Bridging Generative and Contrastive Learning for Domain-Specific LLM Embeddings", "abstract": "Large Language Models (LLMs) adapted via contrastive learning excel in general representation learning but struggle in vertical domains like chemistry and law, primarily due to a lack of domain-specific knowledge. This work identifies a core bottleneck: the prevailing ``LLM+CL'' paradigm focuses on semantic alignment but cannot perform knowledge acquisition, leading to failures on specialized terminology. To bridge this gap, we propose Learn Before Represent (LBR), a novel two-stage framework. LBR first injects domain knowledge via an Information Bottleneck-Constrained Generative Learning stage, preserving the LLM's causal attention to maximize knowledge acquisition while compressing semantics. It then performs Generative-Refined Contrastive Learning on the compressed representations for alignment. This approach maintains architectural consistency and resolves the objective conflict between generative and contrastive learning. Extensive experiments on medical, chemistry, and code retrieval tasks show that LBR significantly outperforms strong baselines. Our work establishes a new paradigm for building accurate and robust representations in vertical domains.", "arxiv_url": "https://arxiv.org/abs/2601.11124", "authors": ["Xiaoyu Liang", "Yuchen Peng", "Jiale Luo", "Wenhao Wang", "Haoji Hu", "Xincheng Zhou"], "first_author": "Xiaoyu Liang", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Representation Learning", "tags": ["Information Bottleneck", "Contrastive Learning", "Generative Learning", "Knowledge Injection", "Causal Attention", "Representation Collapse Mitigation", "Vertical Domain Adaptation", "Code Retrieval"], "summary": "论文提出两阶段的“先学再表征”框架，通过信息瓶颈约束的生成学习先注入领域知识、再进行对比学习对齐表示，在保持自回归结构一致性的同时缓解生成与对比目标冲突，显著提升医疗、化学与代码检索等垂直领域的嵌入效果。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.11124v1", "published": "2026-01-16", "update_time": "2026-01-16", "download_time": "2026-01-20 02:03:42"}
{"id": "2601.10955", "title": "Beyond Max Tokens: Stealthy Resource Amplification via Tool Calling Chains in LLM Agents", "abstract": "The agent-tool communication loop is a critical attack surface in modern Large Language Model (LLM) agents. Existing Denial-of-Service (DoS) attacks, primarily triggered via user prompts or injected retrieval-augmented generation (RAG) context, are ineffective for this new paradigm. They are fundamentally single-turn and often lack a task-oriented approach, making them conspicuous in goal-oriented workflows and unable to exploit the compounding costs of multi-turn agent-tool interactions. We introduce a stealthy, multi-turn economic DoS attack that operates at the tool layer under the guise of a correctly completed task. Our method adjusts text-visible fields and a template-governed return policy in a benign, Model Context Protocol (MCP)-compatible tool server, optimizing these edits with a Monte Carlo Tree Search (MCTS) optimizer. These adjustments leave function signatures unchanged and preserve the final payload, steering the agent into prolonged, verbose tool-calling sequences using text-only notices. This compounds costs across turns, escaping single-turn caps while keeping the final answer correct to evade validation. Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. It drives GPU KV cache occupancy from <1% to 35-74% and cuts co-running throughput by approximately 50%. Because the server remains protocol-compatible and task outcomes are correct, conventional checks fail. These results elevate the agent-tool interface to a first-class security frontier, demanding a paradigm shift from validating final answers to monitoring the economic and computational cost of the entire agentic process.", "arxiv_url": "https://arxiv.org/abs/2601.10955", "authors": ["Kaiyu Zhou", "Yongsen Zheng", "Yicheng He", "Meng Xue", "Xueluan Gong", "Yuji Wang", "Kwok-Yan Lam"], "first_author": "Kaiyu Zhou", "category": ["Empirical", "Technical"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Agent Security", "Tool Calling", "Economic DoS", "MCP Compatibility", "Monte Carlo Tree Search", "Stealth Attack", "Resource Exhaustion", "KV Cache Pressure"], "summary": "论文提出一种在MCP工具层通过文本可见字段与模板化返回策略触发的多轮隐蔽经济型DoS攻击，并用MCTS自动优化以在保持任务正确性的同时极大放大代理工具调用链的算力与成本开销，实证表明在多LLM与工具基准上可显著膨胀token与能耗。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.10955v1", "published": "2026-01-16", "update_time": "2026-01-16", "download_time": "2026-01-20 02:06:09"}
{"id": "2601.10773", "title": "LogicLens: Leveraging Semantic Code Graph to explore Multi Repository large systems", "abstract": "Understanding large software systems is a challenging task, especially when code is distributed across multiple repositories and microservices. Developers often need to reason not only about the structure of the code, but also about its domain logic and runtime behaviors, which are typically implicit and scattered. We introduce LogicLens, a reactive conversational agent that assists developers in exploring complex software systems through a semantic multi-repository graph. This graph is built in a preprocessing step by combining syntactic code analysis, via AST parsing and repository traversal, with semantic enrichment using Large Language Models (LLMs). The resulting graph captures both structural elements, such as files, classes, and functions, as well as functional abstractions like domain entities, operations, and workflows. Once the graph is constructed, LogicLens enables developers to interact with it via natural language, dynamically retrieving relevant subgraphs and answering technical or functional queries. We present the architecture of the system, discuss emergent behaviors, and evaluate its effectiveness on real-world multi-repository scenarios. We demonstrate emergent capabilities including impact analysis and symptom-based debugging that arise naturally from the semantic graph structure.", "arxiv_url": "https://arxiv.org/abs/2601.10773", "authors": ["Niko Usai", "Dario Montagnini", "Kristian Ilianov Iliev", "Raffaele Camanzo"], "first_author": "Niko Usai", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Understanding", "tags": ["Software Knowledge Graph", "Cross-Repository Reasoning", "Graph RAG", "ReAct Agent", "Semantic Enrichment", "AST-based Extraction", "Conversational Code Exploration", "Impact Analysis", "Symptom-based Debugging"], "summary": "本文提出LogicLens，一种将AST解析构建结构图并用LLM进行语义增强以生成跨多仓库语义知识图的交互式对话代理，支持基于图的检索、影响分析与故障定位以帮助开发者理解大型分布式系统。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.10773v1", "published": "2026-01-15", "update_time": "2026-01-15", "download_time": "2026-01-21 02:00:12"}
{"id": "2601.10220", "title": "Agentic Pipelines in Embedded Software Engineering: Emerging Practices and Challenges", "abstract": "A new transformation is underway in software engineering, driven by the rapid adoption of generative AI in development workflows. Similar to how version control systems once automated manual coordination, AI tools are now beginning to automate many aspects of programming. For embedded software engineering organizations, however, this marks their first experience integrating AI into safety-critical and resource-constrained environments. The strict demands for determinism, reliability, and traceability pose unique challenges for adopting generative technologies.   In this paper, we present findings from a qualitative study with ten senior experts from four companies who are evaluating generative AI-augmented development for embedded software. Through semi-structured focus group interviews and structured brainstorming sessions, we identified eleven emerging practices and fourteen challenges related to the orchestration, responsible governance, and sustainable adoption of generative AI tools. Our results show how embedded software engineering teams are rethinking workflows, roles, and toolchains to enable a sustainable transition toward agentic pipelines and generative AI-augmented development.", "arxiv_url": "https://arxiv.org/abs/2601.10220", "authors": ["Simin Sun", "Miroslaw Staron"], "first_author": "Simin Sun", "category": ["Empirical"], "field": "Embedded SE & AI Governance", "task": "Generative AI Integration & Governance", "tags": ["Agentic Pipelines", "Human-LLM Interaction", "Determinism", "Traceability", "Compiler-in-the-loop", "AI Governance", "Toolchain Integration", "Certification"], "summary": "本文通过对来自四家公司十位嵌入式软件专家的焦点小组访谈与结构化头脑风暴，识别了将生成式 AI 融入嵌入式软件工程以保持确定性、可靠性与可追溯性时出现的11项新兴实践与14项挑战，并讨论了相关治理与工作流重构的影响。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.10220v1", "published": "2026-01-15", "update_time": "2026-01-15", "download_time": "2026-01-21 02:01:15"}
{"id": "2601.10820", "title": "Towards Reliable ML Feature Engineering via Planning in Constrained-Topology of LLM Agents", "abstract": "Recent advances in code generation models have unlocked unprecedented opportunities for automating feature engineering, yet their adoption in real-world ML teams remains constrained by critical challenges: (i) the scarcity of datasets capturing the iterative and complex coding processes of production-level feature engineering, (ii) limited integration and personalization of widely used coding agents, such as CoPilot and Devin, with a team's unique tools, codebases, workflows, and practices, and (iii) suboptimal human-AI collaboration due to poorly timed or insufficient feedback. We address these challenges with a planner-guided, constrained-topology multi-agent framework that generates code for repositories in a multi-step fashion. The LLM-powered planner leverages a team's environment, represented as a graph, to orchestrate calls to available agents, generate context-aware prompts, and use downstream failures to retroactively correct upstream artifacts. It can request human intervention at critical steps, ensuring generated code is reliable, maintainable, and aligned with team expectations. On a novel in-house dataset, our approach achieves 38% and 150% improvement in the evaluation metric over manually crafted and unplanned workflows respectively. In practice, when building features for recommendation models serving over 120 million users, our approach has delivered real-world impact by reducing feature engineering cycles from three weeks to a single day.", "arxiv_url": "https://arxiv.org/abs/2601.10820", "authors": ["Himanshu Thakur", "Anusha Kamath", "Anurag Muthyala", "Dhwani Sanmukhani", "Smruthi Mukund", "Jay Katukuri"], "first_author": "Himanshu Thakur", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Repository-Level Coding", "tags": ["Repository-Level Coding", "Code Agents", "Planner-Guided Orchestration", "Constrained Topology", "Human-LLM Interaction", "PySpark Feature Engineering", "Failure-driven Repair", "Robustness"], "summary": "本文提出一种基于规划器的受约束拓扑多智能体框架，用于仓库级别的PySpark特征工程，通过动态协调现有代理、利用下游失败进行回溯修复并支持人机交互来提升代码生成的可靠性与可维护性，并发布了对应的基准数据集。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.10820v1", "published": "2026-01-15", "update_time": "2026-01-15", "download_time": "2026-01-21 02:11:56"}
{"id": "2601.11960", "title": "R$^2$PO: Decoupling Training Trajectories from Inference Responses for LLM Reasoning", "abstract": "Reinforcement learning has become a central paradigm for improving LLM reasoning. However, existing methods use a single policy to produce both inference responses and training optimization trajectories. The objective conflict between generating stable inference responses and diverse training trajectories leads to insufficient exploration, which harms reasoning capability. In this paper, to address the problem, we propose R$^2$PO (Residual Rollout Policy Optimization), which introduces a lightweight Residual Rollout-Head atop the policy to decouple training trajectories from inference responses, enabling controlled trajectory diversification during training while keeping inference generation stable. Experiments across multiple benchmarks show that our method consistently outperforms baselines, achieving average accuracy gains of 3.1% on MATH-500 and 2.4% on APPS, while also reducing formatting errors and mitigating length bias for stable optimization. Our code is publicly available at https://github.com/RRPO-ARR/Code.", "arxiv_url": "https://arxiv.org/abs/2601.11960", "authors": ["Jingchu Wang", "Bingbing Xu", "Yige Yuan", "Bin Xie", "Xiaoqian Sun", "Huawei Shen"], "first_author": "Jingchu Wang", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Residual Rollout Head", "Decoupled Exploration", "Trajectory Diversification", "Group Inverse-Frequency Reward", "Training Stability", "Length-Bias Mitigation", "Formatting Error Reduction", "Inference-Discardable Module", "Low Overhead"], "summary": "本文提出R2PO，通过在冻结主干上加入轻量级残差Rollout-Head将训练轨迹与推理响应解耦，实现可控的轨迹多样化以增强RL微调的探索性，从而在数学与代码推理任务上提升性能并改善训练稳定性与格式/长度偏差问题，推理时可丢弃该模块以保持生成稳定性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.11960v1", "published": "2026-01-17", "update_time": "2026-01-17", "download_time": "2026-01-21 02:15:11"}
{"id": "2601.14132", "title": "Toward self-coding information systems", "abstract": "In this extended abstract, we propose a novel research topic in the field of agentic AI, which we refer to as self-coding information systems. These systems will be able to dynamically adapt their structure or behavior by evaluating potential adaptation decisions, generate source code, test, and (re)deploy their source code autonomously, at runtime, reducing the time to market of new features. Here we motivate the topic, provide a formal definition of self-coding information systems, discuss some expected impacts of the new technology, and indicate potential research directions.", "arxiv_url": "https://arxiv.org/abs/2601.14132", "authors": ["Rodrigo Falcão", "Frank Elberzhager", "Karthik Vaidhyanathan"], "first_author": "Rodrigo Falcão", "category": ["Survey"], "field": "Software Architecture & Runtime Adaptation", "task": "Runtime code generation / Self-coding systems", "tags": ["Runtime code generation", "Self-adaptation", "Code Agents", "Reference architectures", "Modifiability", "Reliability strategies", "Maintainability", "Human-LLM Interaction", "Architectural trade-offs", "Economic evaluation"], "summary": "本文提出并形式化定义了“自我编码信息系统”这一研究方向，讨论其架构权衡、潜在影响及若干关键研究议题（如运行时代码生成的可靠性、可维护性、参考架构与经济性评估）。", "quality": "Middle", "conference": "ICSE", "pdf_url": "https://arxiv.org/pdf/2601.14132v1", "published": "2026-01-20", "update_time": "2026-01-20", "download_time": "2026-01-22 02:00:06"}
{"id": "2601.13943", "title": "RepoGenesis: Benchmarking End-to-End Microservice Generation from Readme to Repository", "abstract": "Large language models and agents have achieved remarkable progress in code generation. However, existing benchmarks focus on isolated function/class-level generation (e.g., ClassEval) or modifications to existing codebases (e.g., SWE-Bench), neglecting complete microservice repository generation that reflects real-world 0-to-1 development workflows. To bridge this gap, we introduce RepoGenesis, the first multilingual benchmark for repository-level end-to-end web microservice generation, comprising 106 repositories (60 Python, 46 Java) across 18 domains and 11 frameworks, with 1,258 API endpoints and 2,335 test cases verified through a \"review-rebuttal\" quality assurance process. We evaluate open-source agents (e.g., DeepCode) and commercial IDEs (e.g., Cursor) using Pass@1, API Coverage (AC), and Deployment Success Rate (DSR). Results reveal that despite high AC (up to 73.91%) and DSR (up to 100%), the best-performing system achieves only 23.67% Pass@1 on Python and 21.45% on Java, exposing deficiencies in architectural coherence, dependency management, and cross-file consistency. Notably, GenesisAgent-8B, fine-tuned on RepoGenesis (train), achieves performance comparable to GPT-5 mini, demonstrating the quality of RepoGenesis for advancing microservice generation. We release our benchmark at https://github.com/pzy2000/RepoGenesis.", "arxiv_url": "https://arxiv.org/abs/2601.13943", "authors": ["Zhiyuan Peng", "Xin Yin", "Pu Zhao", "Fangkai Yang", "Lu Wang", "Ran Jia", "Xu Chen", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "first_author": "Zhiyuan Peng", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Repository-Level Coding", "Microservice Generation", "NL2Repo", "API Coverage", "Deployment Success Rate", "Black-box Testing", "Review-Rebuttal QA", "Cross-file Consistency", "Architectural Coherence", "Dependency Management", "Multilingual"], "summary": "RepoGenesis 提出首个面向从需求文档到完整仓库的多语言微服务生成基准（106 个仓库、1,258 个 API、2,335 个测试用例），并通过 Pass@1、API 覆盖率和部署成功率评估多种代理与 IDE，揭示架构一致性、跨文件一致性和依赖管理等关键挑战。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.13943v1", "published": "2026-01-20", "update_time": "2026-01-20", "download_time": "2026-01-22 02:00:46"}
{"id": "2601.14027", "title": "Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics", "abstract": "Agentic systems have recently become the dominant paradigm for formal theorem proving, achieving strong performance by coordinating multiple models and tools. However, existing approaches often rely on task-specific pipelines and trained formal provers, limiting their flexibility and reproducibility. In this paper, we propose the paradigm that directly uses a general coding agent as a formal math reasoner. This paradigm is motivated by (1) A general coding agent provides a natural interface for diverse reasoning tasks beyond proving, (2) Performance can be improved by simply replacing the underlying base model, without training, and (3) MCP enables flexible extension and autonomous calling of specialized tools, avoiding complex design. Based on this paradigm, we introduce Numina-Lean-Agent, which combines Claude Code with Numina-Lean-MCP to enable autonomous interaction with Lean, retrieval of relevant theorems, informal proving and auxiliary reasoning tools. Using Claude Opus 4.5 as the base model, Numina-Lean-Agent solves all problems in Putnam 2025 (12 / 12), matching the best closed-source system. Beyond benchmark evaluation, we further demonstrate its generality by interacting with mathematicians to successfully formalize the Brascamp-Lieb theorem. We release Numina-Lean-Agent and all solutions at https://github.com/project-numina/numina-lean-agent.", "arxiv_url": "https://arxiv.org/abs/2601.14027", "authors": ["Junqi Liu", "Zihao Zhou", "Zekai Zhu", "Marco Dos Santos", "Weikun He", "Jiawei Liu", "Ran Wang", "Yunzhou Xie", "Junqiao Zhao", "Qiufeng Wang", "Lihong Zhi", "Jia Li", "Wenda Li"], "first_author": "Junqi Liu", "category": ["Technical"], "field": "Formal Methods & Theorem Proving", "task": "Agentic Formal Theorem Proving", "tags": ["Code Agents", "Agentic Reasoning", "Lean Integration", "Theorem Retrieval", "Informal Proof Generation", "Multi-Model Collaboration", "Autonomous Tool Invocation", "Formalization", "Human-LLM Interaction"], "summary": "本文提出Numina‑Lean‑Agent：以通用编程代理为核心、通过MCP插件化调用Lean交互、定理检索与非正式证明等工具，构建可扩展的自治形式数学推理系统， 在Putnam 2025上实现12/12并辅助形式化Brascamp–Lieb定理，且已开源。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.14027v1", "published": "2026-01-20", "update_time": "2026-01-20", "download_time": "2026-01-22 02:04:04"}
{"id": "2601.13864", "title": "HardSecBench: Benchmarking the Security Awareness of LLMs for Hardware Code Generation", "abstract": "Large language models (LLMs) are being increasingly integrated into practical hardware and firmware development pipelines for code generation. Existing studies have primarily focused on evaluating the functional correctness of LLM-generated code, yet paid limited attention to its security issues. However, LLM-generated code that appears functionally sound may embed security flaws which could induce catastrophic damages after deployment. This critical research gap motivates us to design a benchmark for assessing security awareness under realistic specifications. In this work, we introduce HardSecBench, a benchmark with 924 tasks spanning Verilog Register Transfer Level (RTL) and firmware-level C, covering 76 hardware-relevant Common Weakness Enumeration (CWE) entries. Each task includes a structured specification, a secure reference implementation, and executable tests. To automate artifact synthesis, we propose a multi-agent pipeline that decouples synthesis from verification and grounds evaluation in execution evidence, enabling reliable evaluation. Using HardSecBench, we evaluate a range of LLMs on hardware and firmware code generation and find that models often satisfy functional requirements while still leaving security risks. We also find that security results vary with prompting. These findings highlight pressing challenges and offer actionable insights for future advancements in LLM-assisted hardware design. Our data and code will be released soon.", "arxiv_url": "https://arxiv.org/abs/2601.13864", "authors": ["Qirui Chen", "Jingxian Shuai", "Shuangwu Chen", "Shenghao Ye", "Zijian Wen", "Xufei Su", "Jie Jin", "Jiangming Li", "Jun Chen", "Xiaobin Tan", "Jian Yang"], "first_author": "Qirui Chen", "category": ["Benchmark", "Technical", "Empirical"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["RTL Security", "Firmware Security", "CWE-Guided Benchmark", "Multi-Agent Construction", "Requirement-Level Test Harnesses", "Simulation-Based Evaluation", "Prompt Sensitivity", "Coverage & Mutation Filtering"], "summary": "本文提出 HardSecBench：一个包含924个 Verilog 与固件 C 任务、覆盖76类 CWE 的硬件安全基准，采用多代理流水线自动合成结构化规范、金牌实现与可执行测试并基于仿真证据评估 LLM 生成代码的安全意识与提示敏感性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.13864v1", "published": "2026-01-20", "update_time": "2026-01-20", "download_time": "2026-01-22 02:05:51"}
{"id": "2601.15232", "title": "When Agents Fail: A Comprehensive Study of Bugs in LLM Agents with Automated Labeling", "abstract": "Large Language Models (LLMs) have revolutionized intelligent application development. While standalone LLMs cannot perform any actions, LLM agents address the limitation by integrating tools. However, debugging LLM agents is difficult and costly as the field is still in it's early stage and the community is underdeveloped. To understand the bugs encountered during agent development, we present the first comprehensive study of bug types, root causes, and effects in LLM agent-based software. We collected and analyzed 1,187 bug-related posts and code snippets from Stack Overflow, GitHub, and Hugging Face forums, focused on LLM agents built with seven widely used LLM frameworks as well as custom implementations. For a deeper analysis, we have also studied the component where the bug occurred, along with the programming language and framework. This study also investigates the feasibility of automating bug identification. For that, we have built a ReAct agent named BugReAct, equipped with adequate external tools to determine whether it can detect and annotate the bugs in our dataset. According to our study, we found that BugReAct equipped with Gemini 2.5 Flash achieved a remarkable performance in annotating bug characteristics with an average cost of 0.01 USD per post/code snippet.", "arxiv_url": "https://arxiv.org/abs/2601.15232", "authors": ["Niful Islam", "Ragib Shahriar Ayon", "Deepak George Thomas", "Shibbir Ahmed", "Mohammad Wardat"], "first_author": "Niful Islam", "category": ["Empirical", "Benchmark", "Technical"], "field": "Quality Management", "task": "Bug Localization", "tags": ["Agent Bug Taxonomy", "Automated Labeling", "ReAct Agent", "Forum Mining", "Component-level Analysis", "Cross-framework Analysis", "Dataset Release", "Cost-efficient Annotation"], "summary": "本文收集并分析了来自 Stack Overflow、GitHub 与 Hugging Face 的 1,187 条 LLM 代理相关 bug 报告，构建代理 bug 分类法并提出 BugReAct（一种基于 ReAct 的自动标注代理）用于自动识别与注释这些 bug，并发布数据集以支持后续研究。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.15232v1", "published": "2026-01-21", "update_time": "2026-01-21", "download_time": "2026-01-23 01:55:41"}
{"id": "2601.15195", "title": "Where Do AI Coding Agents Fail? An Empirical Study of Failed Agentic Pull Requests in GitHub", "abstract": "AI coding agents are now submitting pull requests (PRs) to software projects, acting not just as assistants but as autonomous contributors. As these agentic contributions are rapidly increasing across real repositories, little is known about how they behave in practice and why many of them fail to be merged. In this paper, we conduct a large-scale study of 33k agent-authored PRs made by five coding agents across GitHub. (RQ1) We first quantitatively characterize merged and not-merged PRs along four broad dimensions: 1) merge outcomes across task types, 2) code changes, 3) CI build results, and 4) review dynamics. We observe that tasks related to documentation, CI, and build update achieve the highest merge success, whereas performance and bug-fix tasks perform the worst. Not-merged PRs tend to involve larger code changes, touch more files, and often do not pass the project's CI/CD pipeline validation. (RQ2) To further investigate why some agentic PRs are not merged, we qualitatively analyze 600 PRs to derive a hierarchical taxonomy of rejection patterns. This analysis complements the quantitative findings in RQ1 by uncovering rejection reasons not captured by quantitative metrics, including lack of meaningful reviewer engagement, duplicate PRs, unwanted feature implementations, and agent misalignment. Together, our findings highlight key socio-technical and human-AI collaboration factors that are critical to improving the success of future agentic workflows.", "arxiv_url": "https://arxiv.org/abs/2601.15195", "authors": ["Ramtin Ehsani", "Sakshi Pathak", "Shriya Rawal", "Abdullah Al Mujahid", "Mia Mohammad Imran", "Preetha Chatterjee"], "first_author": "Ramtin Ehsani", "category": ["Empirical"], "field": "Version Control & Collaboration", "task": "Git VCS", "tags": ["Agentic PRs", "Reviewer Abandonment", "CI Failures", "Duplicate PRs", "Unwanted Feature Implementation", "Agent Misalignment", "Large PRs", "Review Iteration", "Merge-rate by Task", "Human-LLM Interaction"], "summary": "本文基于超过33k个由五种代码代理在GitHub提交的拉取请求，定量比较了被合并与未被合并PR在任务类型、代码变更、CI结果和评审互动上的差异，并通过对600个未合并PR的定性分析构建了包含评审放弃、重复提交、构建失败与代理错位等拒绝模式的分层分类法。", "quality": "High", "conference": "International Mining Software Repositories Conference (MSR) 2026", "pdf_url": "https://arxiv.org/pdf/2601.15195v1", "published": "2026-01-21", "update_time": "2026-01-21", "download_time": "2026-01-23 01:55:58"}
{"id": "2601.15188", "title": "Benchmarking Large Language Models for ABAP Code Generation: An Empirical Study on Iterative Improvement by Compiler Feedback", "abstract": "This work investigates the performance of Large Language Models (LLMs) in generating ABAP code. Despite successful applications of generative AI in many programming languages, there are hardly any systematic analyses of ABAP code generation to date. The aim of the study is to empirically analyze to what extent various LLMs can generate syntactically correct and functional ABAP code, how effectively they use compiler feedback for iterative improvement, and which task types pose special challenges. For this purpose, a benchmark with 180 tasks is conducted, consisting of adapted HumanEval tasks and practical SAP scenarios. The results show significant performance differences between the models: more powerful LLMs achieve success rates of around 75% after several iterations and benefit greatly from compiler feedback, while smaller models perform significantly weaker. Overall, the study highlights the high potential of powerful LLMs for ABAP development processes, especially in iterative error correction.", "arxiv_url": "https://arxiv.org/abs/2601.15188", "authors": ["Stephan Wallraven", "Tim Köhne", "Hartmut Westenberger", "Andreas Moser"], "first_author": "Stephan Wallraven", "category": ["Empirical", "Benchmark"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["ABAP", "Low-Resource Language", "Compiler Feedback", "Iterative Refinement", "Algorithmic Task Adaptation", "Automated Unit Tests", "Error Analysis", "Survival Analysis", "SAP Scenarios", "Functional Correctness", "ABAP Benchmark"], "summary": "本文构建了包含164个改编算法题与16个SAP场景共180题的ABAP基准，实证评估多种LLM在生成ABAP代码的语法与功能正确性，并研究了利用ABAP编译器错误反馈进行最多五轮迭代改进对成功率的提升（强模型迭代后成功率约75%）。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.15188v1", "published": "2026-01-21", "update_time": "2026-01-21", "download_time": "2026-01-23 01:57:20"}
{"id": "2601.15165", "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models", "abstract": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap", "arxiv_url": "https://arxiv.org/abs/2601.15165", "authors": ["Zanlin Ni", "Shenzhi Wang", "Yang Yue", "Tianyu Yu", "Weilin Zhao", "Yeguo Hua", "Tianyi Chen", "Jun Song", "Cheng Yu", "Bo Zheng", "Gao Huang"], "first_author": "Zanlin Ni", "category": ["Technical", "Empirical"], "field": "Diffusion LLMs & Decoding", "task": "Arbitrary-order decoding and RL for reasoning elicitation", "tags": ["Entropy Degradation", "Arbitrary-Order Decoding", "Decoding Order", "Diffusion LLMs", "Parallel Decoding", "RL for Reasoning", "Pass@k Evaluation", "Exploration Suppression"], "summary": "本文发现扩散式语言模型的任意生成顺序会通过绕过高不确定性标记压缩推理解空间（称为熵退化），并提出在训练阶段回归自回归顺序并使用标准GRPO（JustGRPO）来更有效地激发推理能力，同时保留并行解码优势，显著提升了GSM8K等基准的表现。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.15165v1", "published": "2026-01-21", "update_time": "2026-01-21", "download_time": "2026-01-23 01:57:50"}
{"id": "2601.15879", "title": "Evaluating and Achieving Controllable Code Completion in Code LLM", "abstract": "Code completion has become a central task, gaining significant attention with the rise of large language model (LLM)-based tools in software engineering. Although recent advances have greatly improved LLMs' code completion abilities, evaluation methods have not advanced equally. Most current benchmarks focus solely on functional correctness of code completions based on given context, overlooking models' ability to follow user instructions during completion-a common scenario in LLM-assisted programming. To address this limitation, we present the first instruction-guided code completion benchmark, Controllable Code Completion Benchmark (C3-Bench), comprising 2,195 carefully designed completion tasks. Through comprehensive evaluation of over 40 mainstream LLMs across C3-Bench and conventional benchmarks, we reveal substantial gaps in instruction-following capabilities between open-source and advanced proprietary models during code completion tasks. Moreover, we develop a straightforward data synthesis pipeline that leverages Qwen2.5-Coder to generate high-quality instruction-completion pairs for supervised fine-tuning (SFT). The resulting model, Qwen2.5-Coder-C3, achieves state-of-the-art performance on C3-Bench. Our findings provide valuable insights for enhancing LLMs' code completion and instruction-following capabilities, establishing new directions for future research in code LLMs. To facilitate reproducibility and foster further research in code LLMs, we open-source all code, datasets, and models.", "arxiv_url": "https://arxiv.org/abs/2601.15879", "authors": ["Jiajun Zhang", "Zeyu Cui", "Lei Zhang", "Jian Yang", "Jiaxi Yang", "Qiang Liu", "Zilei Wang", "Binyuan Hui", "Liang Wang", "Junyang Lin"], "first_author": "Jiajun Zhang", "category": ["Benchmark", "Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Controllable Code Completion", "Instruction-Following", "Implementation-Control", "Scale-Control", "Instruction Synthesis", "Synthetic SFT", "Automated Evaluation", "Cross-Model Benchmarking"], "summary": "本文提出了首个可控代码补全基准C3-Bench（含2195个指令引导的补全任务）并基于自动化评分评估40+模型，随后通过合成指令-补全数据对模型进行监督微调得到性能提升的Qwen2.5-Coder-C3。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.15879v1", "published": "2026-01-22", "update_time": "2026-01-22", "download_time": "2026-01-24 01:53:23"}
{"id": "2601.15728", "title": "Benchmarking Text-to-Python against Text-to-SQL: The Impact of Explicit Logic and Ambiguity", "abstract": "While Text-to-SQL remains the dominant approach for database interaction, real-world analytics increasingly require the flexibility of general-purpose programming languages such as Python or Pandas to manage file-based data and complex analytical workflows. Despite this growing need, the reliability of Text-to-Python in core data retrieval remains underexplored relative to the mature SQL ecosystem. To address this gap, we introduce BIRD-Python, a benchmark designed for cross-paradigm evaluation. We systematically refined the original dataset to reduce annotation noise and align execution semantics, thereby establishing a consistent and standardized baseline for comparison. Our analysis reveals a fundamental paradigmatic divergence: whereas SQL leverages implicit DBMS behaviors through its declarative structure, Python requires explicit procedural logic, making it highly sensitive to underspecified user intent. To mitigate this challenge, we propose the Logic Completion Framework (LCF), which resolves ambiguity by incorporating latent domain knowledge into the generation process. Experimental results show that (1) performance differences primarily stem from missing domain context rather than inherent limitations in code generation, and (2) when these gaps are addressed, Text-to-Python achieves performance parity with Text-to-SQL. These findings establish Python as a viable foundation for analytical agents-provided that systems effectively ground ambiguous natural language inputs in executable logical specifications. Resources are available at https://anonymous.4open.science/r/Bird-Python-43B7/.", "arxiv_url": "https://arxiv.org/abs/2601.15728", "authors": ["Hangle Hu", "Chenyu Hou", "Bin Cao", "Ruizhe Li"], "first_author": "Hangle Hu", "category": ["Benchmark", "Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["File-based Data Processing", "Explicit Procedural Logic", "Ambiguity Resolution", "Logic Completion", "Context Grounding", "Cross-paradigm Evaluation", "Dataset Curation", "Execution-based Evaluation"], "summary": "本文提出BIRD-Python基准并引入逻辑补全框架LCF，通过修正数据集并在文件化环境下对比Text-to-Python与Text-to-SQL，发现性能差异主要源于缺失领域上下文，补全后Text-to-Python可达到与Text-to-SQL相当的性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.15728v1", "published": "2026-01-22", "update_time": "2026-01-22", "download_time": "2026-01-24 01:53:56"}
{"id": "2601.16175", "title": "Learning to Discover at Test Time", "abstract": "How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.", "arxiv_url": "https://arxiv.org/abs/2601.16175", "authors": ["Mert Yuksekgonul", "Daniel Koceja", "Xinhao Li", "Federico Bianchi", "Jed McCaleb", "Xiaolong Wang", "Jan Kautz", "Yejin Choi", "James Zou", "Carlos Guestrin", "Yu Sun"], "first_author": "Mert Yuksekgonul", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Performance Optimization", "tags": ["Test-Time RL", "Single-Problem Optimization", "Reward-Shaped Objective", "Learning-to-Discover", "Kernel Optimization", "Algorithm Discovery", "Mathematical Conjecture Search", "Single-Cell Denoising", "Thinking Tokens", "Open-Model Reproducibility"], "summary": "本文提出TTT-Discover，一种在测试时对LLM进行强化学习以持续针对单一科学问题训练并优先探索高回报解，从而在数学、GPU内核优化、算法竞赛和生物去噪等多项任务上实现可复现且低成本的新SOTA结果。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.16175v1", "published": "2026-01-22", "update_time": "2026-01-22", "download_time": "2026-01-24 01:54:44"}
{"id": "2601.16172", "title": "Structured Hints for Sample-Efficient Lean Theorem Proving", "abstract": "State-of-the-art neural theorem provers like DeepSeek-Prover-V1.5 combine large language models with reinforcement learning, achieving impressive results through sophisticated training. We ask: do these highly-trained models still benefit from simple structural guidance at inference time? We evaluate a lightweight intervention -- a fixed prompt schedule over 15 common tactic skeletons -- on the miniF2F benchmark. This simple approach yields 21.7% pass@16 compared to 15.2% for standard sampling from the same model, a 43% relative improvement using the same number of samples (k=16) and same maximum generation length (1024 tokens). Our results suggest that even capable RL-trained provers underutilize structural priors available in the tactic language, and that simple inference-time guidance remains a cheap, complementary boost.", "arxiv_url": "https://arxiv.org/abs/2601.16172", "authors": ["Zachary Burton"], "first_author": "Zachary Burton", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Tactic Skeletons", "Structured Intermediate Representation", "Fixed Prompt Schedule", "Inference-Time Guidance", "Proof Assistant", "Sample Efficiency", "Structural Priors", "Failure-Mode Analysis"], "summary": "本文提出在推理时使用结构化中间表示与固定提示日程的轻量级方法，通过对15种战术骨架进行调度，在相同采样预算下于miniF2F上将通过率从15.2%提升到21.7%。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.16172v1", "published": "2026-01-22", "update_time": "2026-01-22", "download_time": "2026-01-24 01:55:07"}
{"id": "2601.16839", "title": "AI builds, We Analyze: An Empirical Study of AI-Generated Build Code Quality", "abstract": "The rapid adoption of AI coding agents for software development has raised important questions about the quality and maintainability of the code they produce. While prior studies have examined AI-generated source code, the impact of AI coding agents on build systems-a critical yet understudied component of the software lifecycle-remains largely unexplored. This data mining challenge focuses on AIDev, the first large-scale, openly available dataset capturing agent-authored pull requests (Agentic-PRs) from real-world GitHub repositories. Our paper leverages this dataset to investigate (RQ1) whether AI coding agents generate build code with quality issues (e.g., code smells), (RQ2) to what extent AI agents can eliminate code smells from build code, and (RQ3) to what extent Agentic-PRs are accepted by developers. We identified 364 maintainability and security-related build smells across varying severity levels, indicating that AI-generated build code can introduce quality issues-such as lack of error handling, and hardcoded paths or URLs-while also, in some cases, removing existing smells through refactorings (e.g., Pull Up Module and Externalize Properties). Notably, more than 61\\% of Agentic-PRs are approved and merged with minimal human intervention. This dual impact underscores the need for future research on AI-aware build code quality assessment to systematically evaluate, guide, and govern AI-generated build systems code.", "arxiv_url": "https://arxiv.org/abs/2601.16839", "authors": ["Anwar Ghammam", "Mohamed Almukhtar"], "first_author": "Anwar Ghammam", "category": ["Empirical", "Benchmark"], "field": "Maintenance", "task": "Refactoring", "tags": ["Build Code Quality", "Build Smells", "Build Refactoring", "Dependency Management", "Hardcoded Credentials", "PR Acceptance", "Agent Behavior Variability", "Static Analysis"], "summary": "本文基于 AIDev 数据集和 Sniffer 静态分析器，对 AI 生成的 Maven/Gradle/CMake/Make 构建脚本进行实证分析，发现 AI 既会引入多种可维护性与安全相关的构建代码气味（如通配符依赖、缺失错误处理、硬编码凭证），也在部分情况下通过重构消除气味，且超过61%的 AI 发起的 PR 被开发者合并。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.16839v1", "published": "2026-01-23", "update_time": "2026-01-23", "download_time": "2026-01-27 02:02:29"}
{"id": "2601.16809", "title": "Will It Survive? Deciphering the Fate of AI-Generated Code in Open Source", "abstract": "The integration of AI agents as coding assistants into software development has raised questions about the long-term viability of AI agent-generated code. A prevailing hypothesis within the software engineering community suggests this code is \"disposable\", meaning it is merged quickly but discarded shortly thereafter. If true, organizations risk shifting maintenance burden from generation to post-deployment remediation. We investigate this hypothesis through survival analysis of 201 open-source projects, tracking over 200,000 code units authored by AI agents versus humans. Contrary to the disposable code narrative, agent-authored code survives significantly longer: at the line level, it exhibits a 15.8 percentage-point lower modification rate and 16% lower hazard of modification (HR = 0.842, p < 0.001). However, modification profiles differ. Agent-authored code shows modestly elevated corrective rates (26.3% vs. 23.0%), while human code shows higher adaptive rates. However, the effect sizes are small (Cramér's V = 0.116), and per-agent variation exceeds the agent-human gap. Turning to prediction, textual features can identify modification-prone code (AUC-ROC = 0.671), but predicting when modifications occur remains challenging (Macro F1 = 0.285), suggesting timing depends on external organizational dynamics. The bottleneck for agent-generated code may not be generation quality, but the organizational practices that govern its long-term evolution.", "arxiv_url": "https://arxiv.org/abs/2601.16809", "authors": ["Musfiqur Rahman", "Emad Shihab"], "first_author": "Musfiqur Rahman", "category": ["Empirical"], "field": "Software Evolution", "task": "Code Survival Analysis", "tags": ["Survival Analysis", "Line Attribution", "Modification Intent", "Temporal Prediction", "Textual Features", "Mining Software Repositories", "Agent Heterogeneity", "Organizational Practices"], "summary": "本文通过对201个开源项目中超过20万条由AI代理与人工编写的代码行进行生存分析，发现AI生成代码在行级别上存活时间更长且修改倾向不同（更多为纠正性修改），并验证了基于文本特征预测代码是否会被修改的可行性但难以准确预测修改时间。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.16809v1", "published": "2026-01-23", "update_time": "2026-01-23", "download_time": "2026-01-27 02:02:46"}
{"id": "2601.16863", "title": "Mixture-of-Models: Unifying Heterogeneous Agents via N-Way Self-Evaluating Deliberation", "abstract": "This paper introduces the N-Way Self-Evaluating Deliberation (NSED) protocol, a Runtime Mixture-of-Models (MoM) architecture that constructs emergent composite models from a plurality of distinct expert agents. Unlike traditional Mixture-of-Experts (MoE) which rely on static gating networks, NSED employs a Dynamic Expertise Broker - a runtime optimization engine that treats model selection as a variation of the Knapsack Problem, binding heterogeneous checkpoints to functional roles based on live telemetry and cost constraints. At the execution layer, we formalize deliberation as a Macro-Scale Recurrent Neural Network (RNN), where the consensus state loops back through a semantic forget gate to enable iterative refinement without proportional VRAM scaling. Key components include an orchestration fabric for trustless N-to-N peer review, a Quadratic Voting activation function for non-linear consensus, and a feedback-driven state update. Empirical validation on challenging benchmarks (AIME 2025, LiveCodeBench) demonstrates that this topology allows ensembles of small (less than 20B) consumer-grade models to match or exceed the performance of state-of-the-art 100B+ parameter models, establishing a new hardware arbitrage efficiency frontier. Furthermore, testing on the DarkBench safety suite reveals intrinsic alignment properties, with peer-mediated correction reducing sycophancy scores below that of any individual agent.", "arxiv_url": "https://arxiv.org/abs/2601.16863", "authors": ["Tims Pecerskis", "Aivars Smirnovs"], "first_author": "Tims Pecerskis", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Agents", "tags": ["Code Agents", "Recurrent Deliberation", "Dynamic Brokerage", "Trustless Consensus", "Quadratic Voting", "Inference-Time Compute", "Efficiency-Fatigue Model", "Optimal Stopping", "Human-in-the-Loop", "Interpretability"], "summary": "本文提出 N-Way Self-Evaluating Deliberation (NSED)，通过运行时的异构模型混合（Runtime Mixture-of-Models）、递归的宏神经元拓扑和无信任投票机制，实现小型模型集群在推理时迭代自评与共识形成，从而在成本受限下匹配或超越超大模型性能并给出最优停止的数学模型。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.16863v1", "published": "2026-01-23", "update_time": "2026-01-23", "download_time": "2026-01-27 02:04:08"}
{"id": "2601.16700", "title": "Adoption of Generative Artificial Intelligence in the German Software Engineering Industry: An Empirical Study", "abstract": "Generative artificial intelligence (GenAI) tools have seen rapid adoption among software developers. While adoption rates in the industry are rising, the underlying factors influencing the effective use of these tools, including the depth of interaction, organizational constraints, and experience-related considerations, have not been thoroughly investigated. This issue is particularly relevant in environments with stringent regulatory requirements, such as Germany, where practitioners must address the GDPR and the EU AI Act while balancing productivity gains with intellectual property considerations. Despite the significant impact of GenAI on software engineering, to the best of our knowledge, no empirical study has systematically examined the adoption dynamics of GenAI tools within the German context. To address this gap, we present a comprehensive mixed-methods study on GenAI adoption among German software engineers. Specifically, we conducted 18 exploratory interviews with practitioners, followed by a developer survey with 109 participants. We analyze patterns of tool adoption, prompting strategies, and organizational factors that influence effectiveness. Our results indicate that experience level moderates the perceived benefits of GenAI tools, and productivity gains are not evenly distributed among developers. Further, organizational size affects both tool selection and the intensity of tool use. Limited awareness of the project context is identified as the most significant barrier. We summarize a set of actionable implications for developers, organizations, and tool vendors seeking to advance artificial intelligence (AI) assisted software development.", "arxiv_url": "https://arxiv.org/abs/2601.16700", "authors": ["Ludwig Felder", "Tobias Eisenreich", "Mahsa Fischer", "Stefan Wagner", "Chunyang Chen"], "first_author": "Ludwig Felder", "category": ["Empirical"], "field": "Coding Assistant", "task": "Code Prompting", "tags": ["Human-LLM Interaction", "Prompt Engineering", "Developer Productivity", "Regulatory Compliance", "Data Privacy", "SME Constraints", "Experience Gap", "Context Awareness"], "summary": "本文通过18次半结构化访谈与109份问卷的混合方法实证研究，分析了德国软件工程师对生成式AI工具的采纳、提示策略及组织与合规性障碍，发现经验水平、组织规模与缺乏项目上下文显著影响工具效果。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.16700v1", "published": "2026-01-23", "update_time": "2026-01-23", "download_time": "2026-01-27 02:05:26"}
{"id": "2601.18749", "title": "Let's Make Every Pull Request Meaningful: An Empirical Analysis of Developer and Agentic Pull Requests", "abstract": "The automatic generation of pull requests (PRs) using AI agents has become increasingly common. Although AI-generated PRs are fast and easy to create, their merge rates have been reported to be lower than those created by humans. In this study, we conduct a large-scale empirical analysis of 40,214 PRs collected from the AIDev dataset. We extract 64 features across six families and fit statistical regression models to compare PR merge outcomes for human and agentic PRs, as well as across three AI agents. Our results show that submitter attributes dominate merge outcomes for both groups, while review-related features exhibit contrasting effects between human and agentic PRs. The findings of this study provide insights into improving PR quality through human-AI collaboration.", "arxiv_url": "https://arxiv.org/abs/2601.18749", "authors": ["Haruhiko Yoshioka", "Takahiro Monno", "Haruka Tokumasu", "Taiki Wakamatsu", "Yuki Ota", "Nimmi Weeraddana", "Kenichi Matsumoto"], "first_author": "Haruhiko Yoshioka", "category": ["Empirical"], "field": "Version Control & Collaboration", "task": "Git VCS", "tags": ["PR Merge Prediction", "Human-LLM Interaction", "Review Dynamics", "Submitter Reputation", "Agent Behavior Comparison", "Feature Importance", "Logistic Regression"], "summary": "本文基于AIDev数据集对40,214个由人类与AI代理提交的Pull Request进行大规模实证分析，提取64项特征并用可解释的回归模型比较影响合并率的因素，发现提交者属性主导合并结果且审查相关特征对人工与代理PR的影响相反。", "quality": "High", "conference": "International Conference on Mining Software Repositories (MSR) 2026", "pdf_url": "https://arxiv.org/pdf/2601.18749v1", "published": "2026-01-26", "update_time": "2026-01-26", "download_time": "2026-01-28 01:58:12"}
{"id": "2601.18418", "title": "daVinci-Dev: Agent-native Mid-training for Software Engineering", "abstract": "Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model's agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...", "arxiv_url": "https://arxiv.org/abs/2601.18418", "authors": ["Ji Zeng", "Dayuan Fu", "Tiantian Mi", "Yumin Zhuang", "Yaxing Huang", "Xuefeng Li", "Lyumanshan Ye", "Muhang Xie", "Qishuo Hua", "Zhen Huang", "Mohan Jiang", "Hanning Wang", "Jifan Lin", "Yang Xiao", "Jie Sun", "Yunze Wu", "Pengfei Liu"], "first_author": "Ji Zeng", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Pre-Training", "tags": ["Agentic Mid-Training", "Contextually-Native Trajectories", "Environmentally-Native Trajectories", "Repository-Level Coding", "Test-Driven Iteration", "Execution Feedback", "Data Synthesis", "Scalability"], "summary": "本文提出一种面向软件工程的 agent-native 中期训练方法，通过构建大规模的上下文本地与环境本地轨迹数据并用于中期训练，从而显著提升代码代理在仓库级问题定位、编辑与测试驱动修复等任务的表现，并计划开源数据与模型工件。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.18418v1", "published": "2026-01-26", "update_time": "2026-01-26", "download_time": "2026-01-28 01:58:43"}
{"id": "2601.18381", "title": "AI Agent for Reverse-Engineering Legacy Finite-Difference Code and Translating to Devito", "abstract": "To facilitate the transformation of legacy finite difference implementations into the Devito environment, this study develops an integrated AI agent framework. Retrieval-Augmented Generation (RAG) and open-source Large Language Models are combined through multi-stage iterative workflows in the system's hybrid LangGraph architecture. The agent constructs an extensive Devito knowledge graph through document parsing, structure-aware segmentation, extraction of entity relationships, and Leiden-based community detection. GraphRAG optimisation enhances query performance across semantic communities that include seismic wave simulation, computational fluid dynamics, and performance tuning libraries. A reverse engineering component derives three-level query strategies for RAG retrieval through static analysis of Fortran source code. To deliver precise contextual information for language model guidance, the multi-stage retrieval pipeline performs parallel searching, concept expansion, community-scale retrieval, and semantic similarity analysis. Code synthesis is governed by Pydantic-based constraints to guarantee structured outputs and reliability. A comprehensive validation framework integrates conventional static analysis with the G-Eval approach, covering execution correctness, structural soundness, mathematical consistency, and API compliance. The overall agent workflow is implemented on the LangGraph framework and adopts concurrent processing to support quality-based iterative refinement and state-aware dynamic routing. The principal contribution lies in the incorporation of feedback mechanisms motivated by reinforcement learning, enabling a transition from static code translation toward dynamic and adaptive analytical behavior.", "arxiv_url": "https://arxiv.org/abs/2601.18381", "authors": ["Yinghan Hou", "Zongyou Yang"], "first_author": "Yinghan Hou", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["GraphRAG", "Knowledge Graph Construction", "Reverse Engineering", "Fortran Static Analysis", "Multi-stage Retrieval", "Structured Output Constraints", "Validation Framework", "Multi-Agent Orchestration", "Finite-Difference Translation"], "summary": "本文提出一个基于GraphRAG与多阶段检索的AI代理框架，通过构建Devito知识图谱、对Fortran源代码进行静态分析并采用受约束的代码生成与验证流程，实现将遗留有限差分Fortran代码自动逆向并翻译为Devito实现，辅以多代理决策与质量驱动的迭代优化。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.18381v1", "published": "2026-01-26", "update_time": "2026-01-26", "download_time": "2026-01-28 02:03:16"}
{"id": "2601.18282", "title": "Think-Augmented Function Calling: Improving LLM Parameter Accuracy Through Embedded Reasoning", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in function calling for autonomous agents, yet current mechanisms lack explicit reasoning transparency during parameter generation, particularly for complex functions with interdependent parameters. While existing approaches like chain-of-thought prompting operate at the agent level, they fail to provide fine-grained reasoning guidance for individual function parameters. To address these limitations, we propose Think-Augmented Function Calling (TAFC), a novel framework that enhances function calling accuracy through explicit reasoning at both function and parameter levels. Our method introduces a universal \"think\" parameter augmentation that enables models to articulate their decision-making process, with dynamic optimization for parameter descriptions to improve reasoning quality. For complex parameters, TAFC automatically triggers granular reasoning based on complexity scoring, ensuring appropriate justification for critical decisions. Additionally, we propose reasoning-guided optimization to align generated reasoning with human expectations. TAFC requires no architectural modifications to existing LLMs while maintaining full API compatibility. Evaluation on ToolBench across proprietary and open-source models demonstrates significant improvements in parameter generation accuracy and reasoning coherence for multi-parameter functions, while providing enhanced interpretability for debugging AI agent behaviors.", "arxiv_url": "https://arxiv.org/abs/2601.18282", "authors": ["Lei Wei", "Jinpeng Ou", "Xiao Peng", "Bin Wang"], "first_author": "Lei Wei", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Parameter-Level Reasoning", "Function Calling Augmentation", "Complexity Scoring", "Dynamic Description Tuning", "Tool Description Optimization", "Code Agents", "Interpretability"], "summary": "本文提出TAFC（Think‑Augmented Function Calling），通过在函数签名中加入“think”推理字段、基于复杂度触发的参数级细粒度推理及动态描述优化，在无需改动模型架构的前提下显著提升多参数函数调用的参数生成准确性与可解释性，并在ToolBench上验证了性能提升。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.18282v1", "published": "2026-01-26", "update_time": "2026-01-26", "download_time": "2026-01-28 02:04:17"}
{"id": "2601.20755", "title": "ProfInfer: An eBPF-based Fine-Grained LLM Inference Profiler", "abstract": "As large language models (LLMs) move from research to production, understanding how inference engines behave in real time has become both essential and elusive. Unlike general-purpose engines such as ONNX Runtime, today's LLM inference systems offer little operator-level visibility, leaving developers blind to where time and resources go. Even basic questions -- is this workload memory-bound or compute-bound? -- often remain unanswered. To close this gap, we develop a fine-grained, non-intrusive profiling framework for modern LLM inference engines, exemplified by llama.cpp but applicable to similar runtime architectures. Built on extended Berkeley Packet Filter (eBPF) technology, our system dynamically attaches probes to runtime functions across multiple layers -- without modifying or recompiling the source. It transforms collected traces into rich visualizations of operators, graphs, timelines, and hardware counter trends, exposing how dense inference, Mixture-of-Experts routing, and operator offloading behave in practice. With less than 4% runtime overhead and high profiling fidelity, our framework makes LLM inference both transparent and diagnosable, turning performance profiling into a practical tool for optimization, scheduling, and resource-aware deployment.", "arxiv_url": "https://arxiv.org/abs/2601.20755", "authors": ["Bohua Zou", "Debayan Roy", "Dhimankumar Yogesh Airao", "Weihao Xu", "Binqi Sun", "Yutao Liu", "Haibo Chen"], "first_author": "Bohua Zou", "category": ["Technical"], "field": "Inference Systems & Profiling", "task": "LLM Inference Profiling", "tags": ["eBPF Tracing", "Operator-level Profiling", "On-device LLMs", "Hardware Performance Counters", "Dynamic DAG Reconstruction", "KV-cache Analysis", "Mixture-of-Experts Monitoring", "Low-overhead Profiling", "Timeline & DAG Visualization"], "summary": "本文提出ProfInfer，一种基于eBPF的轻量级、非侵入性且细粒度的LLM推理性能分析框架，能够在不修改源码的情况下采集算子级执行、硬件计数器和动态计算图信息，并通过时间线和DAG可视化帮助诊断移动与边缘设备上的推理瓶颈。", "quality": "High", "conference": "9th Annual Conference on Machine Learning and Systems (MLSys) 2026", "pdf_url": "https://arxiv.org/pdf/2601.20755v1", "published": "2026-01-28", "update_time": "2026-01-28", "download_time": "2026-01-29 02:12:42"}
{"id": "2601.20615", "title": "DRAINCODE: Stealthy Energy Consumption Attacks on Retrieval-Augmented Code Generation via Context Poisoning", "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in code generation by leveraging retrieval-augmented generation (RAG) methods. However, the computational costs associated with LLM inference, particularly in terms of latency and energy consumption, have received limited attention in the security context. This paper introduces DrainCode, the first adversarial attack targeting the computational efficiency of RAG-based code generation systems. By strategically poisoning retrieval contexts through a mutation-based approach, DrainCode forces LLMs to produce significantly longer outputs, thereby increasing GPU latency and energy consumption. We evaluate the effectiveness of DrainCode across multiple models. Our experiments show that DrainCode achieves up to an 85% increase in latency, a 49% increase in energy consumption, and more than a 3x increase in output length compared to the baseline. Furthermore, we demonstrate the generalizability of the attack across different prompting strategies and its effectiveness compared to different defenses. The results highlight DrainCode as a potential method for increasing the computational overhead of LLMs, making it useful for evaluating LLM security in resource-constrained environments. We provide code and data at https://github.com/DeepSoftwareAnalytics/DrainCode.", "arxiv_url": "https://arxiv.org/abs/2601.20615", "authors": ["Yanlin Wang", "Jiadong Wu", "Tianyue Jiang", "Mingwei Liu", "Jiachi Chen", "Chong Wang", "Ensheng Shi", "Xilin Liu", "Yuchi Ma", "Zibin Zheng"], "first_author": "Yanlin Wang", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Retrieval", "tags": ["Context Poisoning", "Energy-Consumption Attack", "Retrieval-Augmented Generation", "Hypothetical Query Generation", "Gradient-Guided Mutation", "Multi-Position Mutation", "Defense Evasion"], "summary": "本文提出DRAINCODE，通过在RAG检索语料中注入经过梯度引导变异的隐蔽触发器，使代码生成模型在不破坏功能的前提下产生大幅冗长输出，从而显著增加推理延迟和能耗并能绕过多种防御。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.20615v1", "published": "2026-01-28", "update_time": "2026-01-28", "download_time": "2026-01-29 02:13:18"}
{"id": "2601.19825", "title": "Routing End User Queries to Enterprise Databases", "abstract": "We address the task of routing natural language queries in multi-database enterprise environments. We construct realistic benchmarks by extending existing NL-to-SQL datasets. Our study shows that routing becomes increasingly challenging with larger, domain-overlapping DB repositories and ambiguous queries, motivating the need for more structured and robust reasoning-based solutions. By explicitly modelling schema coverage, structural connectivity, and fine-grained semantic alignment, the proposed modular, reasoning-driven reranking strategy consistently outperforms embedding-only and direct LLM-prompting baselines across all the metrics.", "arxiv_url": "https://arxiv.org/abs/2601.19825", "authors": ["Saikrishna Sudarshan", "Tanay Kulkarni", "Manasi Patwardhan", "Lovekesh Vig", "Ashwin Srinivasan", "Tanmay Tulsidas Verlekar"], "first_author": "Saikrishna Sudarshan", "category": ["Technical", "Benchmark"], "field": "Natural Language Database Access", "task": "Query Routing to Enterprise Databases", "tags": ["Query Routing", "Schema Entity Recognition", "Embedding Retrieval", "LLM Re-ranking", "Schema Coverage", "Table Connectivity", "Phrase-Schema Mapping", "Training-free Method", "Prompt Design", "Cross-domain Robustness", "Benchmark Construction"], "summary": "该工作为企业多数据库场景构建了更现实的路由基准（将跨域NL-to-SQL数据集扩展为路由数据集），并提出一种无训练、模块化的推理驱动重排序方法——结合模式实体识别、嵌入检索、表连接结构和细粒度语义对齐——在路由任务上超越了基线。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.19825v1", "published": "2026-01-27", "update_time": "2026-01-27", "download_time": "2026-01-29 02:16:54"}
{"id": "2601.19793", "title": "CASTER: Breaking the Cost-Performance Barrier in Multi-Agent Orchestration via Context-Aware Strategy for Task Efficient Routing", "abstract": "Graph-based Multi-Agent Systems (MAS) enable complex cyclic workflows but suffer from inefficient static model allocation, where deploying strong models uniformly wastes computation on trivial sub-tasks. We propose CASTER (Context-Aware Strategy for Task Efficient Routing), a lightweight router for dynamic model selection in graph-based MAS. CASTER employs a Dual-Signal Router that combines semantic embeddings with structural meta-features to estimate task difficulty. During training, the router self-optimizes through a Cold Start to Iterative Evolution paradigm, learning from its own routing failures via on-policy negative feedback. Experiments using LLM-as-a-Judge evaluation across Software Engineering, Data Analysis, Scientific Discovery, and Cybersecurity demonstrate that CASTER reduces inference cost by up to 72.4% compared to strong-model baselines while matching their success rates, and consistently outperforms both heuristic routing and FrugalGPT across all domains.", "arxiv_url": "https://arxiv.org/abs/2601.19793", "authors": ["Shanyv Liu", "Xuyang Yuan", "Tao Chen", "Zijun Zhan", "Zhu Han", "Danyang Zheng", "Weishan Zhang", "Shaohua Cao"], "first_author": "Shanyv Liu", "category": ["Technical", "Benchmark"], "field": "Multi-Agent Orchestration", "task": "Model Routing & Resource Allocation", "tags": ["Context-Aware Routing", "Task Difficulty Estimation", "Dual-Branch Feature Fusion", "On-Policy Iterative Evolution", "Cost-Efficient Inference", "Multi-Agent Orchestration", "Model Selection", "Cross-Domain Benchmarking"], "summary": "本文提出CASTER，一种用于图式多智能体系统的轻量级上下文感知路由器，通过结合语义嵌入与结构元特征并采用冷启动到迭代进化的在线训练策略，动态在强/弱模型之间分配子任务，从而在跨软件工程、数据分析、科学发现与网络安全的基准上在保持成功率的同时将推理成本最高降低72.4%。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.19793v1", "published": "2026-01-27", "update_time": "2026-01-27", "download_time": "2026-01-29 02:17:30"}
{"id": "2601.20810", "title": "Context-Augmented Code Generation Using Programming Knowledge Graphs", "abstract": "Large Language Models (LLMs) excel at code generation but struggle with complex problems. Retrieval-Augmented Generation (RAG) mitigates this issue by integrating external knowledge, yet retrieval models often miss relevant context, and generation models hallucinate with irrelevant data. We propose Programming Knowledge Graph (PKG) for semantic representation and fine-grained retrieval of code and text. Our approach enhances retrieval precision through tree pruning and mitigates hallucinations via a re-ranking mechanism that integrates non-RAG solutions. Structuring external data into finer-grained nodes improves retrieval granularity. Evaluations on HumanEval and MBPP show up to 20% pass@1 accuracy gains and a 34% improvement over baselines on MBPP. Our findings demonstrate that our proposed PKG approach along with re-ranker effectively address complex problems while maintaining minimal negative impact on solutions that are already correct without RAG. The replication package is published at https://github.com/iamshahd/ProgrammingKnowledgeGraph", "arxiv_url": "https://arxiv.org/abs/2601.20810", "authors": ["Shahd Seddik", "Fahd Seddik", "Iman Saberi", "Fatemeh Fard", "Minh Hieu Huynh", "Patanamon Thongtanunam"], "first_author": "Shahd Seddik", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Retrieval", "tags": ["Programming Knowledge Graph", "AST segmentation", "Block-level retrieval", "Function-level retrieval", "Text-centric DAG", "Structure-aware pruning", "Re-ranking", "Hallucination mitigation", "Granularity tradeoff"], "summary": "本文提出编程知识图（PKG），将代码与文档结构化为细粒度检索单元并结合树裁剪与候选重排序以提升检索增强的代码生成，在HumanEval和MBPP上显著提高了通过率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.20810v1", "published": "2026-01-28", "update_time": "2026-01-28", "download_time": "2026-01-30 02:12:53"}
{"id": "2601.20789", "title": "SERA: Soft-Verified Efficient Repository Agents", "abstract": "Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of frontier open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. Our method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with cost-efficiency, this enables specialization to private codebases. Beyond repository specialization, we apply SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. We use this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, we believe our work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. We release SERA as the first model in Ai2's Open Coding Agents series, along with all our code, data, and Claude Code integration to support the research community.", "arxiv_url": "https://arxiv.org/abs/2601.20789", "authors": ["Ethan Shen", "Danny Tormoen", "Saurabh Shah", "Ali Farhadi", "Tim Dettmers"], "first_author": "Ethan Shen", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Instruction-Tuning", "tags": ["Soft Verification", "Repository Specialization", "Synthetic Pull Requests", "Synthetic Trajectories", "Supervised Finetuning", "Vague Instructions", "Cost Efficiency", "Scaling Laws"], "summary": "本文提出SERA与SVG方法，通过从代码库生成大量软验证的合成PR轨迹并以监督微调训练仓库专用编码代理，实现以极低成本高效地将开源模型专门化到私有代码库并达到或超过教师模型性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.20789v1", "published": "2026-01-28", "update_time": "2026-01-28", "download_time": "2026-01-30 02:13:11"}
{"id": "2601.20802", "title": "Reinforcement Learning via Self-Distillation", "abstract": "Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model's ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts.", "arxiv_url": "https://arxiv.org/abs/2601.20802", "authors": ["Jonas Hübotter", "Frederike Lübeck", "Lejs Behric", "Anton Baumann", "Marco Bagatella", "Daniel Marta", "Ido Hakimi", "Idan Shenfeld", "Thomas Kleine Buening", "Carlos Guestrin", "Andreas Krause"], "first_author": "Jonas Hübotter", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Self-Distillation", "Rich Feedback", "Credit Assignment", "In-Context Learning", "Policy Optimization", "Test-Time Training", "Sample Efficiency", "Runtime-Error Feedback"], "summary": "本文提出Self-Distillation Policy Optimization (SDPO)，在具有丰富文本反馈的可验证环境中将模型自身在反馈条件下的下一令牌分布作为“自教师”进行蒸馏，从而将反馈转化为密集的令牌级学习信号以显著提升代码和推理任务的样本效率与最终准确率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.20802v1", "published": "2026-01-28", "update_time": "2026-01-28", "download_time": "2026-01-30 02:14:29"}
{"id": "2601.20745", "title": "HESTIA: A Hessian-Guided Differentiable Quantization-Aware Training Framework for Extremely Low-Bit LLMs", "abstract": "As large language models (LLMs) continue to scale, deployment is increasingly bottlenecked by the memory wall, motivating a shift toward extremely low-bit quantization. However, most quantization-aware training (QAT) methods apply hard rounding and the straight-through estimator (STE) from the beginning of the training, which prematurely discretizes the optimization landscape and induces persistent gradient mismatch between latent weights and quantized weights, hindering effective optimization of quantized models. To address this, we propose Hestia, a Hessian-guided differentiable QAT framework for extremely low-bit LLMs, which replaces the rigid step function with a temperature-controlled softmax relaxation to maintain gradient flow early in training while progressively hardening quantization. Furthermore, Hestia leverages a tensor-wise Hessian trace metric as a lightweight curvature signal to drive fine-grained temperature annealing, enabling sensitivity-aware discretization across the model. Evaluations on Llama-3.2 show that Hestia consistently outperforms existing ternary QAT baselines, yielding average zero-shot improvements of 5.39% and 4.34% for the 1B and 3B models. These results indicate that Hessian-guided relaxation effectively recovers representational capacity, establishing a more robust training path for 1.58-bit LLMs. The code is available at https://github.com/hestia2026/Hestia.", "arxiv_url": "https://arxiv.org/abs/2601.20745", "authors": ["Guoan Wang", "Feiyu Wang", "Zongwei Lv", "Yikun Zong", "Tong Yang"], "first_author": "Guoan Wang", "category": ["Technical"], "field": "Model Compression & Deployment", "task": "Quantization-Aware Training", "tags": ["Differentiable Quantization", "Hessian-guided Annealing", "Softmax Relaxation", "Ternary Quantization", "Dead-Zone Mitigation", "Temperature Scheduling", "Curvature-Aware Sensitivity", "Low-Bit LLM Optimization", "Gradient Flow Preservation", "Quantization-Aware Training"], "summary": "本文提出 HESTIA，一种针对极低位（如 1.58-bit）大语言模型的 Hessian 指导可微量化训练框架，通过温度控制的 Softmax 松弛维持早期梯度流并利用张量级 Hessian 跟踪进行灵敏度感知的自适应退火，从而显著提升 ternary 量化下的模型性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.20745v1", "published": "2026-01-28", "update_time": "2026-01-28", "download_time": "2026-01-30 02:15:07"}
{"id": "2601.22136", "title": "StepShield: When, Not Whether to Intervene on Rogue Agents", "abstract": "Existing agent safety benchmarks report binary accuracy, conflating early intervention with post-mortem analysis. A detector that flags a violation at step 8 enables intervention; one that reports it at step 48 provides only forensic value. This distinction is critical, yet current benchmarks cannot measure it. We introduce StepShield, the first benchmark to evaluate when violations are detected, not just whether. StepShield contains 9,213 code agent trajectories, including 1,278 meticulously annotated training pairs and a 7,935-trajectory test set with a realistic 8.1% rogue rate. Rogue behaviors are grounded in real-world security incidents across six categories. We propose three novel temporal metrics: Early Intervention Rate (EIR), Intervention Gap, and Tokens Saved. Surprisingly, our evaluation reveals that an LLM-based judge achieves 59% EIR while a static analyzer achieves only 26%, a 2.3x performance gap that is entirely invisible to standard accuracy metrics. We further show that early detection has direct economic benefits: our cascaded HybridGuard detector reduces monitoring costs by 75% and projects to $108M in cumulative savings over five years at enterprise scale. By shifting the focus of evaluation from whether to when, StepShield provides a new foundation for building safer and more economically viable AI agents. The code and data are released under an Apache 2.0 license.", "arxiv_url": "https://arxiv.org/abs/2601.22136", "authors": ["Gloria Felicia", "Michael Eniolade", "Jinfeng He", "Zitha Sasindran", "Hemant Kumar", "Milan Hussain Angati", "Sandeep Bandarupalli"], "first_author": "Gloria Felicia", "category": ["Benchmark", "Technical", "Empirical"], "field": "Agent Safety & Monitoring", "task": "Rogue Agent Detection (Temporal/Step-level)", "tags": ["Step-level Annotation", "Early Intervention", "Rogue Detection", "Temporal Metrics", "LLM-as-Judge", "Hybrid Detection", "Guardrails", "Economic Cost Analysis"], "summary": "StepShield 提出并发布了首个侧重“何时而非是否”检测代理违例的步级时间性基准与数据集、三项时间度量（EIR、Intervention Gap、Tokens Saved），并基于多种检测范式（包括混合检测器）展示早期检测在实用与经济上的显著价值。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.22136v1", "published": "2026-01-29", "update_time": "2026-01-29", "download_time": "2026-01-31 02:08:53"}
{"id": "2601.22130", "title": "World of Workflows: a Benchmark for Bringing World Models to Enterprise Systems", "abstract": "Frontier large language models (LLMs) excel as autonomous agents in many domains, yet they remain untested in complex enterprise systems where hidden workflows create cascading effects across interconnected databases. Existing enterprise benchmarks evaluate surface-level agentic task completion similar to general consumer benchmarks, ignoring true challenges in enterprises, such as limited observability, large database state, and hidden workflows with cascading side effects. We introduce World of Workflows (WoW), a realistic ServiceNow-based environment incorporating 4,000+ business rules and 55 active workflows embedded in the system, alongside WoW-bench, a benchmark of 234 tasks evaluating constrained agentic task completion and enterprise dynamics modeling capabilities. We reveal two major takeaways: (1) Frontier LLMs suffer from dynamics blindness, consistently failing to predict the invisible, cascading side effects of their actions, which leads to silent constraint violations, and (2) reliability in opaque systems requires grounded world modeling, where agents must mentally simulate hidden state transitions to bridge the observability gap when high-fidelity feedback is unavailable. For reliable and useful enterprise agents, WoW motivates a new paradigm to explicitly learn system dynamics. We release our GitHub for setting up and evaluating WoW.", "arxiv_url": "https://arxiv.org/abs/2601.22130", "authors": ["Lakshya Gupta", "Litao Li", "Yizhe Liu", "Sriram Ganapathi Subramanian", "Kaheer Suleman", "Zichen Zhang", "Haoye Lu", "Sumit Pasupalak"], "first_author": "Lakshya Gupta", "category": ["Benchmark", "Empirical"], "field": "Enterprise Systems & Agents", "task": "Workflow-driven World Modeling", "tags": ["World Modeling", "Hidden Workflows", "Partial Observability", "Dynamics Prediction", "Constraint Satisfaction", "Audit-Log Observations", "Enterprise Simulation", "Agentic Task Completion", "State Tracking", "Dynamics Blindness"], "summary": "本文提出WoW与WoW-bench——一个基于ServiceNow的高保真企业环境与234项评测任务，用以评估LLM在有限观测、隐藏工作流与数据库级联更新下的世界建模与代理执行能力，并揭示前沿LLM在预测隐蔽状态变化与满足约束方面存在严重“动力盲点”。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.22130v1", "published": "2026-01-29", "update_time": "2026-01-29", "download_time": "2026-01-31 02:09:14"}
{"id": "2601.22139", "title": "Reasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers", "abstract": "Reasoning-oriented Large Language Models (LLMs) have achieved remarkable progress with Chain-of-Thought (CoT) prompting, yet they remain fundamentally limited by a \\emph{blind self-thinking} paradigm: performing extensive internal reasoning even when critical information is missing or ambiguous. We propose Proactive Interactive Reasoning (PIR), a new reasoning paradigm that transforms LLMs from passive solvers into proactive inquirers that interleave reasoning with clarification. Unlike existing search- or tool-based frameworks that primarily address knowledge uncertainty by querying external environments, PIR targets premise- and intent-level uncertainty through direct interaction with the user. PIR is implemented via two core components: (1) an uncertainty-aware supervised fine-tuning procedure that equips models with interactive reasoning capability, and (2) a user-simulator-based policy optimization framework driven by a composite reward that aligns model behavior with user intent. Extensive experiments on mathematical reasoning, code generation, and document editing demonstrate that PIR consistently outperforms strong baselines, achieving up to 32.70\\% higher accuracy, 22.90\\% higher pass rate, and 41.36 BLEU improvement, while reducing nearly half of the reasoning computation and unnecessary interaction turns. Further reliability evaluations on factual knowledge, question answering, and missing-premise scenarios confirm the strong generalization and robustness of PIR. Model and code are publicly available at: \\href{https://github.com/SUAT-AIRI/Proactive-Interactive-R1}", "arxiv_url": "https://arxiv.org/abs/2601.22139", "authors": ["Xin Chen", "Feng Jiang", "Yiqian Zhang", "Hardy Chen", "Shuo Yan", "Wenya Xie", "Min Yang", "Shujian Huang"], "first_author": "Xin Chen", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Proactive Inquiry", "Uncertainty Detection", "Think-and-Ask", "User Simulator", "Group Relative Policy Optimization", "Intent Alignment", "Interaction Efficiency", "Reasoning Trace Augmentation", "Reinforcement Learning", "Hallucination Reduction"], "summary": "本文提出PIR（一种通过不确定性感知的数据构建与基于用户模拟器的策略优化，将推理与主动澄清交织，使推理型LLM在推理过程中主动询问以对齐用户意图、提高正确性并减少计算与交互开销）的新范式与实现方法，并在数学推理、代码生成和文档编辑上验证其有效性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.22139v1", "published": "2026-01-29", "update_time": "2026-01-29", "download_time": "2026-01-31 02:10:01"}
{"id": "2601.22129", "title": "SWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents", "abstract": "Test-time scaling has been widely adopted to enhance the capabilities of Large Language Model (LLM) agents in software engineering (SWE) tasks. However, the standard approach of repeatedly sampling trajectories from scratch is computationally expensive. While recent methods have attempted to mitigate costs using specialized value agents, they can suffer from model miscalibration and fail to generalize to modern agents that synthesize custom bash scripts as tools. In this paper, we introduce SWE-Replay, the first efficient and generalizable test-time scaling technique for modern agents without reliance on potentially noisy value estimates. SWE-Replay optimizes the scaling process by recycling trajectories from prior trials, dynamically choosing to either explore from scratch or exploit archived experience by branching at critical intermediate steps. This selection of intermediate steps is driven by the potential and reasoning significance of repository exploration, rather than external LLM-based quality estimates. Our evaluation shows that, on SWE-Bench Verified, SWE-Replay consistently outperforms naive scaling, reducing costs by up to 17.4% while maintaining or even improving performance by up to 3.8%. Further evaluation on SWE-Bench Pro and Multilingual validates the generalizability of SWE-Replay, establishing it as a robust foundation for efficient test-time scaling of software engineering agents.", "arxiv_url": "https://arxiv.org/abs/2601.22129", "authors": ["Yifeng Ding", "Lingming Zhang"], "first_author": "Yifeng Ding", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Performance Optimization", "tags": ["Trajectory Replay", "Replay-Based Exploration", "Step Selection", "Environment State Restoration", "Exploration-Exploitation", "Test-Time Scaling", "Repository Long-Tail Exploration", "Value-Estimator-Free", "Cost-Efficient Inference"], "summary": "SWE-Replay 提出通过维护轨迹归档并在选定的中间步骤恢复并分叉探索，从而在无需依赖 LLM 评分器的情况下显著降低软件工程代理测试时采样成本并保持或提升任务性能的方法，并在多套 SWE-Bench 上验证了其效果与泛化性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.22129v1", "published": "2026-01-29", "update_time": "2026-01-29", "download_time": "2026-01-31 02:10:39"}
{"id": "2601.22025", "title": "When \"Better\" Prompts Hurt: Evaluation-Driven Iteration for LLM Applications", "abstract": "Evaluating Large Language Model (LLM) applications differs from traditional software testing because outputs are stochastic, high-dimensional, and sensitive to prompt and model changes. We present an evaluation-driven workflow - Define, Test, Diagnose, Fix - that turns these challenges into a repeatable engineering loop.   We introduce the Minimum Viable Evaluation Suite (MVES), a tiered set of recommended evaluation components for (i) general LLM applications, (ii) retrieval-augmented generation (RAG), and (iii) agentic tool-use workflows. We also synthesize common evaluation methods (automated checks, human rubrics, and LLM-as-judge) and discuss known judge failure modes.   In reproducible local experiments (Ollama; Llama 3 8B Instruct and Qwen 2.5 7B Instruct), we observe that a generic \"improved\" prompt template can trade off behaviors: on our small structured suites, extraction pass rate decreased from 100% to 90% and RAG compliance from 93.3% to 80% for Llama 3 when replacing task-specific prompts with generic rules, while instruction-following improved. These findings motivate evaluation-driven prompt iteration and careful claim calibration rather than universal prompt recipes.   All test suites, harnesses, and results are included for reproducibility.", "arxiv_url": "https://arxiv.org/abs/2601.22025", "authors": ["Daniel Commey"], "first_author": "Daniel Commey", "category": ["Survey", "Benchmark", "Empirical"], "field": "Software Testing", "task": "Test Generation", "tags": ["Evaluation-Driven Iteration", "Minimum Viable Evaluation Suite", "RAG Evaluation", "LLM-as-Judge", "Judge Failure Modes", "Prompt Trade-offs", "Test Set Design", "Reproducibility", "Regression Testing", "Evaluation Harnesses"], "summary": "本文提出了基于“Define–Test–Diagnose–Fix”的评价驱动迭代流程与分层最小可行评估套件(MVES)，系统化LLM应用的测试方法并通过可复现的本地实验展示了通用“更好”提示可能导致行为权衡与回归，从而强调要以评估结果指导提示迭代而非盲目通用化。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.22025v1", "published": "2026-01-29", "update_time": "2026-01-29", "download_time": "2026-02-01 02:38:43"}
{"id": "2601.21787", "title": "Assessing the Business Process Modeling Competences of Large Language Models", "abstract": "The creation of Business Process Model and Notation (BPMN) models is a complex and time-consuming task requiring both domain knowledge and proficiency in modeling conventions. Recent advances in large language models (LLMs) have significantly expanded the possibilities for generating BPMN models directly from natural language, building upon earlier text-to-process methods with enhanced capabilities in handling complex descriptions. However, there is a lack of systematic evaluations of LLM-generated process models. Current efforts either use LLM-as-a-judge approaches or do not consider established dimensions of model quality. To this end, we introduce BEF4LLM, a novel LLM evaluation framework comprising four perspectives: syntactic quality, pragmatic quality, semantic quality, and validity. Using BEF4LLM, we conduct a comprehensive analysis of open-source LLMs and benchmark their performance against human modeling experts. Results indicate that LLMs excel in syntactic and pragmatic quality, while humans outperform in semantic aspects; however, the differences in scores are relatively modest, highlighting LLMs' competitive potential despite challenges in validity and semantic quality. The insights highlight current strengths and limitations of using LLMs for BPMN modeling and guide future model development and fine-tuning. Addressing these areas is essential for advancing the practical deployment of LLMs in business process modeling.", "arxiv_url": "https://arxiv.org/abs/2601.21787", "authors": ["Chantale Lauer", "Peter Pfeiffer", "Alexander Rombach", "Nijat Mehdiyev"], "first_author": "Chantale Lauer", "category": ["Benchmark", "Empirical"], "field": "Requirements & Design", "task": "Specification & Validation", "tags": ["Business Process Modeling", "BPMN Validation", "Model Quality Metrics", "Syntactic Evaluation", "Semantic Evaluation", "Pragmatic Evaluation", "BPMN-XML Validity", "LLM Evaluation Framework", "Human-LLM Comparison", "Conversational Process Modeling"], "summary": "本文提出BEF4LLM评估框架（39项指标）用于系统评估LLM的文本到BPMN建模能力，并在大规模开源LLM基准上比较其与人类建模专家在句法、语用、语义和有效性维度的表现，发现LLM在句法与语用上表现良好但在语义一致性和生成有效BPMN-XML方面仍有明显挑战。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.21787v1", "published": "2026-01-29", "update_time": "2026-01-29", "download_time": "2026-02-01 02:39:24"}
{"id": "2601.22159", "title": "RedSage: A Cybersecurity Generalist LLM", "abstract": "Cybersecurity operations demand assistant LLMs that support diverse workflows without exposing sensitive data. Existing solutions either rely on proprietary APIs with privacy risks or on open models lacking domain adaptation. To bridge this gap, we curate 11.8B tokens of cybersecurity-focused continual pretraining data via large-scale web filtering and manual collection of high-quality resources, spanning 28.6K documents across frameworks, offensive techniques, and security tools. Building on this, we design an agentic augmentation pipeline that simulates expert workflows to generate 266K multi-turn cybersecurity samples for supervised fine-tuning. Combined with general open-source LLM data, these resources enable the training of RedSage, an open-source, locally deployable cybersecurity assistant with domain-aware pretraining and post-training. To rigorously evaluate the models, we introduce RedSage-Bench, a benchmark with 30K multiple-choice and 240 open-ended Q&A items covering cybersecurity knowledge, skills, and tool expertise. RedSage is further evaluated on established cybersecurity benchmarks (e.g., CTI-Bench, CyberMetric, SECURE) and general LLM benchmarks to assess broader generalization. At the 8B scale, RedSage achieves consistently better results, surpassing the baseline models by up to +5.59 points on cybersecurity benchmarks and +5.05 points on Open LLM Leaderboard tasks. These findings demonstrate that domain-aware agentic augmentation and pre/post-training can not only enhance cybersecurity-specific expertise but also help to improve general reasoning and instruction-following. All models, datasets, and code are publicly available.", "arxiv_url": "https://arxiv.org/abs/2601.22159", "authors": ["Naufal Suryanto", "Muzammal Naseer", "Pengfei Li", "Syed Talal Wasim", "Jinhui Yi", "Juergen Gall", "Paolo Ceravolo", "Ernesto Damiani"], "first_author": "Naufal Suryanto", "category": ["Technical", "Benchmark"], "field": "Cybersecurity", "task": "Domain-specific LLM pretraining and benchmarking for cybersecurity assistants", "tags": ["Agentic Augmentation", "Domain-Aware Pretraining", "Continual Pretraining", "Simulated Expert Workflows", "Tool Proficiency Assessment", "Multi-turn Security Dialogs", "Instruction Preference Alignment", "Open-data Open-model Release"], "summary": "本文提出RedSage——一款面向网络安全的开源本地可部署大模型，通过11.8B令牌的领域持续预训练、266K条代理生成的多轮增强监督数据与后训练的偏好对齐，并构建30K+240项的RedSage-Bench基准，实现了对安全知识、实操技能和工具能力的全面评估并在多项基准上取得领先表现，同时公开数据与代码。", "quality": "High", "conference": "ICLR 2026", "pdf_url": "https://arxiv.org/pdf/2601.22159v1", "published": "2026-01-29", "update_time": "2026-01-29", "download_time": "2026-02-01 02:39:48"}
{"id": "2601.21972", "title": "Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic", "abstract": "Recent work has explored optimizing LLM collaboration through Multi-Agent Reinforcement Learning (MARL). However, most MARL fine-tuning approaches rely on predefined execution protocols, which often require centralized execution. Decentralized LLM collaboration is more appealing in practice, as agents can run inference in parallel with flexible deployments. Also, current approaches use Monte Carlo methods for fine-tuning, which suffer from high variance and thus require more samples to train effectively. Actor-critic methods are prevalent in MARL for dealing with these issues, so we developed Multi-Agent Actor-Critic (MAAC) methods to optimize decentralized LLM collaboration. In this paper, we analyze when and why these MAAC methods are beneficial. We propose 2 MAAC approaches, \\textbf{CoLLM-CC} with a \\textbf{C}entralized \\textbf{C}ritic and \\textbf{CoLLM-DC} with \\textbf{D}ecentralized \\textbf{C}ritics. Our experiments across writing, coding, and game-playing domains show that Monte Carlo methods and CoLLM-DC can achieve performance comparable to CoLLM-CC in short-horizon and dense-reward settings. However, they both underperform CoLLM-CC on long-horizon or sparse-reward tasks, where Monte Carlo methods require substantially more samples and CoLLM-DC struggles to converge. Our code is available at https://github.com/OpenMLRL/CoMLRL/releases/tag/v1.3.2.", "arxiv_url": "https://arxiv.org/abs/2601.21972", "authors": ["Shuo Liu", "Tianle Chen", "Ryan Amiri", "Christopher Amato"], "first_author": "Shuo Liu", "category": ["Technical", "Empirical"], "field": "Multi-Agent LLM Collaboration", "task": "Decentralized MARL Fine-Tuning (Actor-Critic)", "tags": ["Multi-Agent RL", "Decentralized Collaboration", "Centralized Critic", "Decentralized Critic", "Actor-Critic", "RL Fine-Tuning", "Sample Efficiency", "Variance Reduction", "Sparse Rewards", "Long-Horizon Tasks", "Dec-POMDP"], "summary": "本文提出两种多智能体actor-critic方法（基于中心化评论家与去中心化评论家）用于优化去中心化的LLM协作，并在写作、编码与游戏等任务上实证表明中心化评论家在长时序或稀疏奖励场景下优于蒙特卡洛与去中心化评论家方法。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.21972v1", "published": "2026-01-29", "update_time": "2026-01-29", "download_time": "2026-02-01 02:43:30"}
{"id": "2601.23059", "title": "On the Impact of Code Comments for Automated Bug-Fixing: An Empirical Study", "abstract": "Large Language Models (LLMs) are increasingly relevant in Software Engineering research and practice, with Automated Bug Fixing (ABF) being one of their key applications. ABF involves transforming a buggy method into its fixed equivalent. A common preprocessing step in ABF involves removing comments from code prior to training. However, we hypothesize that comments may play a critical role in fixing certain types of bugs by providing valuable design and implementation insights. In this study, we investigate how the presence or absence of comments, both during training and at inference time, impacts the bug-fixing capabilities of LLMs. We conduct an empirical evaluation comparing two model families, each evaluated under all combinations of training and inference conditions (with and without comments), and thereby revisiting the common practice of removing comments during training. To address the limited availability of comments in state-of-the-art datasets, we use an LLM to automatically generate comments for methods lacking them. Our findings show that comments improve ABF accuracy by up to threefold when present in both phases, while training with comments does not degrade performance when instances lack them. Additionally, an interpretability analysis identifies that comments detailing method implementation are particularly effective in aiding LLMs to fix bugs accurately.", "arxiv_url": "https://arxiv.org/abs/2601.23059", "authors": ["Antonio Vitale", "Emanuela Guglielmi", "Simone Scalabrino", "Rocco Oliveto"], "first_author": "Antonio Vitale", "category": ["Empirical", "Benchmark"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Comment-aware ABF", "Auto-generated Comments", "Comment Taxonomy", "Training-Inference Consistency", "SHAP Interpretability", "Empirical Evaluation", "Dataset Re-extraction", "Accuracy Improvement"], "summary": "本文通过构建并公开一个保留并补充注释的自动修复数据集，采用两类模型在训练/推理阶段有无注释的四种配置进行对比实证研究，发现注释（尤其是描述实现细节的注释）在训练与推理均存在时可将基于LLM的自动修复准确率提升最多三倍。", "quality": "High", "conference": "IEEE/ACM International Conference on Program Comprehension (ICPC 2026)", "pdf_url": "https://arxiv.org/pdf/2601.23059v1", "published": "2026-01-30", "update_time": "2026-01-30", "download_time": "2026-02-02 02:28:53"}
{"id": "2601.23009", "title": "SolAgent: A Specialized Multi-Agent Framework for Solidity Code Generation", "abstract": "Smart contracts are the backbone of the decentralized web, yet ensuring their functional correctness and security remains a critical challenge. While Large Language Models (LLMs) have shown promise in code generation, they often struggle with the rigorous requirements of smart contracts, frequently producing code that is buggy or vulnerable. To address this, we propose SolAgent, a novel tool-augmented multi-agent framework that mimics the workflow of human experts. SolAgent integrates a \\textbf{dual-loop refinement mechanism}: an inner loop using the \\textit{Forge} compiler to ensure functional correctness, and an outer loop leveraging the \\textit{Slither} static analyzer to eliminate security vulnerabilities. Additionally, the agent is equipped with file system capabilities to resolve complex project dependencies. Experiments on the SolEval+ Benchmark, a rigorous suite derived from high-quality real-world projects, demonstrate that SolAgent achieves a Pass@1 rate of up to \\textbf{64.39\\%}, significantly outperforming state-of-the-art LLMs ($\\sim$25\\%), AI IDEs (e.g., GitHub Copilot), and existing agent frameworks. Moreover, it reduces security vulnerabilities by up to \\textbf{39.77\\%} compared to human-written baselines. Finally, we demonstrate that the high-quality trajectories generated by SolAgent can be used to distill smaller, open-source models, democratizing access to secure smart contract generation. We release our data and code at https://github.com/openpaperz/SolAgent.", "arxiv_url": "https://arxiv.org/abs/2601.23009", "authors": ["Wei Chen", "Zhiyuan Peng", "Xin Yin", "Chao Ni", "Chenhao Ying", "Bang Xie", "Yuan Luo"], "first_author": "Wei Chen", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Code Agents", "Smart Contract", "Compiler-in-the-loop", "Static-Analysis-in-the-loop", "Dual-Loop Refinement", "Dependency Resolution", "Agent Trajectory Distillation", "Executable Synthesis"], "summary": "SolAgent提出了一个面向Solidity的工具增强多代理框架，通过整合Forge编译器和Slither静态分析器的双循环迭代精炼、文件系统能力和项目依赖解析，实现高可执行性与更低漏洞率的智能合约代码生成，并基于构建的SolEval+基准进行评估及蒸馏小型模型。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.23009v1", "published": "2026-01-30", "update_time": "2026-01-30", "download_time": "2026-02-02 02:29:15"}
{"id": "2601.21947", "title": "ToolWeaver: Weaving Collaborative Semantics for Scalable Tool Use in Large Language Models", "abstract": "Prevalent retrieval-based tool-use pipelines struggle with a dual semantic challenge: their retrievers often employ encoders that fail to capture complex semantics, while the Large Language Model (LLM) itself lacks intrinsic tool knowledge from its natural language pretraining. Generative methods offer a powerful alternative by unifying selection and execution, tasking the LLM to directly learn and generate tool identifiers. However, the common practice of mapping each tool to a unique new token introduces substantial limitations: it creates a scalability and generalization crisis, as the vocabulary size explodes and each tool is assigned a semantically isolated token. This approach also creates a semantic bottleneck that hinders the learning of collaborative tool relationships, as the model must infer them from sparse co-occurrences of monolithic tool IDs within a vast library. To address these limitations, we propose ToolWeaver, a novel generative tool learning framework that encodes tools into hierarchical sequences. This approach makes vocabulary expansion logarithmic to the number of tools. Crucially, it enables the model to learn collaborative patterns from the dense co-occurrence of shared codes, rather than the sparse co-occurrence of monolithic tool IDs. We generate these structured codes through a novel tokenization process designed to weave together a tool's intrinsic semantics with its extrinsic co-usage patterns. These structured codes are then integrated into the LLM through a generative alignment stage, where the model is fine-tuned to produce the hierarchical code sequences. Evaluation results with nearly 47,000 tools show that ToolWeaver significantly outperforms state-of-the-art methods, establishing a more scalable, generalizable, and semantically-aware foundation for advanced tool-augmented agents.", "arxiv_url": "https://arxiv.org/abs/2601.21947", "authors": ["Bowen Fang", "Wen Ye", "Yunyue Su", "Jinghao Zhang", "Qiang Liu", "Yesheng Liu", "Xin Sun", "Shu Wu", "Jiabing Yang", "Baole Wei", "Liang Wang"], "first_author": "Bowen Fang", "category": ["Technical"], "field": "Tool Use & Agents", "task": "Tool Representation & Selection", "tags": ["Compositional Tool Codes", "Hierarchical Tokenization", "Collaborative Semantics", "Collaborative-aware Vector Quantization", "Generative Alignment", "Scalability", "Tool Invocation", "API Usage"], "summary": "ToolWeaver提出将工具表示为分层组合的离散代码，通过协作感知的矢量量化和生成式对齐让LLM学习工具间的协同语义，从而在近47,000个工具规模上实现更可扩展且更具通用性的工具选择与调用。", "quality": "High", "conference": "ICLR 2026", "pdf_url": "https://arxiv.org/pdf/2601.21947v1", "published": "2026-01-29", "update_time": "2026-01-29", "download_time": "2026-02-02 02:33:06"}
{"id": "2601.21879", "title": "astra-langchain4j: Experiences Combining LLMs and Agent Programming", "abstract": "Given the emergence of Generative AI over the last two years and the increasing focus on Agentic AI as a form of Multi-Agent System it is important to explore both how such technologies can impact the use of traditional Agent Toolkits and how the wealth of experience encapsulated in those toolkits can influence the design of the new agentic platforms. This paper presents an overview of our experience developing a prototype large language model (LLM) integration for the ASTRA programming language. It presents a brief overview of the toolkit, followed by three example implementations, concluding with a discussion of the experiences garnered through the examples.", "arxiv_url": "https://arxiv.org/abs/2601.21879", "authors": ["Rem Collier", "Katharine Beaumont", "Andrei Ciortea"], "first_author": "Rem Collier", "category": ["Technical"], "field": "Agent Programming & Agentic AI", "task": "LLM Integration for Agent Programming", "tags": ["Belief RAG", "Agent-LM Integration", "Prompt Templates", "Agent Modules", "Symbolic-LLM Hybrid", "Interactive Agents", "Agent Tooling"], "summary": "本文介绍了将LLM集成到ASTRA智能体编程语言的库astra-langchain4j，提供LLM调用、可重用提示模板及利用智能体信念进行检索增强生成（BeliefRAG）等机制，并通过示例总结实践经验。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2601.21879v1", "published": "2026-01-29", "update_time": "2026-01-29", "download_time": "2026-02-02 02:34:27"}
{"id": "2602.00933", "title": "MCP-Atlas: A Large-Scale Benchmark for Tool-Use Competency with Real MCP Servers", "abstract": "The Model Context Protocol (MCP) is rapidly becoming the standard interface for Large Language Models (LLMs) to discover and invoke external tools. However, existing evaluations often fail to capture the complexity of real-world scenarios, relying on restricted toolsets, simplistic workflows, or subjective LLM-as-a-judge metrics. We introduce MCP-Atlas, a large-scale benchmark for evaluating tool-use competency, comprising 36 real MCP servers and 220 tools. It includes 1,000 tasks designed to assess tool-use competency in realistic, multi-step workflows. Tasks use natural language prompts that avoid naming specific tools or servers, requiring agents to identify and orchestrate 3-6 tool calls across multiple servers. We score tasks using a claims-based rubric that awards partial credit based on the factual claims satisfied in the model's final answer, complemented by internal diagnostics on tool discovery, parameterization, syntax, error recovery, and efficiency. Evaluation results on frontier models reveal that top models achieve pass rates exceeding 50%, with primary failures arising from inadequate tool usage and task understanding. We release the task schema, containerized harness, and a 500-task public subset of the benchmark dataset to facilitate reproducible comparisons and advance the development of robust, tool-augmented agents.", "arxiv_url": "https://arxiv.org/abs/2602.00933", "authors": ["Chaithanya Bandi", "Ben Hertzberg", "Geobio Boo", "Tejas Polakam", "Jeff Da", "Sami Hassaan", "Manasi Sharma", "Andrew Park", "Ernesto Hernandez", "Dan Rambado", "Ivan Salazar", "Rafael Cruz", "Chetan Rane", "Ben Levin", "Brad Kenstler", "Bing Liu"], "first_author": "Chaithanya Bandi", "category": ["Benchmark", "Empirical"], "field": "Tool Use & Agents", "task": "Tool orchestration evaluation", "tags": ["Code Agents", "Tool-Orchestration", "Multi-Step Workflows", "Claims-Based Scoring", "Real MCP Servers", "Tool Discovery", "Distractor Tools", "Partial Credit", "Cross-Server Composition", "Evaluation Harness"], "summary": "MCP-Atlas 提出并开源了一个大规模、基于真实 MCP 服务器的工具使用能力基准（1,000 题，36 个服务器，220 个工具），并通过基于事实命题的评分与内部诊断评估多步跨服务器工具编排的性能与失败模式。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2602.00933v1", "published": "2026-01-31", "update_time": "2026-01-31", "download_time": "2026-02-03 02:27:12"}
{"id": "2602.00840", "title": "Code Quality Analysis of Translations from C to Rust", "abstract": "C/C++ is a prevalent programming language. Yet, it suffers from significant memory and thread-safety issues. Recent studies have explored automated translation of C/C++ to safer languages, such as Rust. However, these studies focused mostly on the correctness and safety of the translated code, which are indeed critical, but they left other important quality concerns (e.g., performance, robustness, and maintainability) largely unexplored. This work investigates strengths and weaknesses of three C-to-Rust translators, namely C2Rust (a transpiler), C2SaferRust (an LLM-guided transpiler), and TranslationGym (an LLM-based direct translation). We perform an in-depth quantitative and qualitative analysis of several important quality attributes for the translated Rust code of the popular GNU coreutils, using human-based translation as a baseline. To assess the internal and external quality of the Rust code, we: (i) apply Clippy, a rule-based state-of-the-practice Rust static analysis tool; (ii) investigate the capability of an LLM (GPT-4o) to identify issues potentially overlooked by Clippy; and (iii) perform a manual analysis of the issues reported by Clippy and GPT-4o. Our results show that while newer techniques reduce some unsafe and non-idiomatic patterns, they frequently introduce new issues, revealing systematic trade-offs that are not visible under existing evaluation practices. Notably, none of the automated techniques consistently match or exceed human-written translations across all quality dimensions, yet even human-written Rust code exhibits persistent internal quality issues such as readability and non-idiomatic patterns. Together, these findings show that translation quality remains a multi-dimensional challenge, requiring systematic evaluation and targeted tool support beyond both naive automation and manual rewriting.", "arxiv_url": "https://arxiv.org/abs/2602.00840", "authors": ["Biruk Tadesse", "Vikram Nitin", "Mazin Salah", "Baishakhi Ray", "Marcelo d'Amorim", "Wesley Assunção"], "first_author": "Biruk Tadesse", "category": ["Empirical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["C-to-Rust Migration", "Idiomaticity", "Runtime Panics", "Performance Regression", "Thread-Safety", "Static Analysis", "Clippy Limitations", "LLM Linter", "Quality Taxonomy", "Human Baseline"], "summary": "本文对三种自动C→Rust翻译器（C2Rust、C2SaferRust、TranslationGym）与人工翻译在GNU coreutils上的翻译结果进行了定量与定性研究，基于Clippy与GPT-4o及人工分析提出了18类质量问题的分类，揭示了安全、性能、可维护性等多维质量间的系统性权衡并给出改进建议。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2602.00840v1", "published": "2026-01-31", "update_time": "2026-01-31", "download_time": "2026-02-03 02:27:48"}
{"id": "2602.00929", "title": "Learning Abstractions for Hierarchical Planning in Program-Synthesis Agents", "abstract": "Humans learn abstractions and use them to plan efficiently to quickly generalize across tasks -- an ability that remains challenging for state-of-the-art large language model (LLM) agents and deep reinforcement learning (RL) systems. Inspired by the cognitive science of how people form abstractions and intuitive theories of their world knowledge, Theory-Based RL (TBRL) systems, such as TheoryCoder, exhibit strong generalization through effective use of abstractions. However, they heavily rely on human-provided abstractions and sidestep the abstraction-learning problem. We introduce TheoryCoder-2, a new TBRL agent that leverages LLMs' in-context learning ability to actively learn reusable abstractions rather than relying on hand-specified ones, by synthesizing abstractions from experience and integrating them into a hierarchical planning process. We conduct experiments on diverse environments, including BabyAI, Minihack and VGDL games like Sokoban. We find that TheoryCoder-2 is significantly more sample-efficient than baseline LLM agents augmented with classical planning domain construction, reasoning-based planning, and prior program-synthesis agents such as WorldCoder. TheoryCoder-2 is able to solve complex tasks that the baselines fail, while only requiring minimal human prompts, unlike prior TBRL systems.", "arxiv_url": "https://arxiv.org/abs/2602.00929", "authors": ["Zergham Ahmed", "Kazuki Irie", "Joshua B. Tenenbaum", "Christopher J. Bates", "Samuel J. Gershman"], "first_author": "Zergham Ahmed", "category": ["Technical"], "field": "Program Synthesis & Hierarchical Planning", "task": "Abstraction Learning for Hierarchical Planning", "tags": ["Code Agents", "Abstraction Learning", "PDDL Synthesis", "Hierarchical Planning", "Theory-Based RL", "World-Model Synthesis", "In-Context Learning", "Symbolic Planning", "Transfer Learning", "Sample Efficiency"], "summary": "本文提出TheoryCoder-2，利用大语言模型的in‑context学习自动合成PDDL抽象与低级世界模型并将其整合入双层规划，从而在BabyAI、MiniHack和VGDL等环境中显著提升样本效率与泛化能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2602.00929v1", "published": "2026-01-31", "update_time": "2026-01-31", "download_time": "2026-02-03 02:29:58"}
{"id": "2602.00851", "title": "Persuasion Propagation in LLM Agents", "abstract": "Modern AI agents increasingly combine conversational interaction with autonomous task execution, such as coding and web research, raising a natural question: what happens when an agent engaged in long-horizon tasks is subjected to user persuasion? We study how belief-level intervention can influence downstream task behavior, a phenomenon we name \\emph{persuasion propagation}. We introduce a behavior-centered evaluation framework that distinguishes between persuasion applied during or prior to task execution. Across web research and coding tasks, we find that on-the-fly persuasion induces weak and inconsistent behavioral effects. In contrast, when the belief state is explicitly specified at task time, belief-prefilled agents conduct on average 26.9\\% fewer searches and visit 16.9\\% fewer unique sources than neutral-prefilled agents. These results suggest that persuasion, even in prior interaction, can affect the agent's behavior, motivating behavior-level evaluation in agentic systems.", "arxiv_url": "https://arxiv.org/abs/2602.00851", "authors": ["Hyejun Jeong", "Amir Houmansadr", "Shlomo Zilberstein", "Eugene Bagdasarian"], "first_author": "Hyejun Jeong", "category": ["Empirical"], "field": "Coding Assistant", "task": "Code Alignment", "tags": ["Persuasion Propagation", "Belief Conditioning", "Behavioral Drift", "Trace-based Evaluation", "Agent Auditing", "Long-Horizon Agents", "On-the-fly Persuasion", "Web Research"], "summary": "本文提出并实证研究了“说服传播”现象，即在多步骤LLM代理中无关的说服如何形成持久信念并在后续编码与网络检索任务中改变执行轨迹（如搜索广度、修订频率），并提出基于执行轨迹的受控评估框架来度量该效应。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2602.00851v1", "published": "2026-01-31", "update_time": "2026-01-31", "download_time": "2026-02-03 02:31:42"}
{"id": "2602.03462", "title": "RAL-Bench: Benchmarking for Application-Level Functional Correctness and Non-Functional Quality Attributes", "abstract": "Code generation has advanced rapidly with code-focused large language models (LLMs), especially on snippet-level tasks. However, application-level generation requires producing a runnable multi-file repository with correct structure, dependencies, and end-to-end executability, and real-world software must satisfy both functional correctness and non-functional quality (e.g., maintainability, security). Existing benchmarks provide a limited execution-based assessment of these requirements at the application level. We ask: Can current LLMs generate application-level repositories that meet both functional and non-functional criteria? We propose RAL-Bench, a benchmark and evaluation framework for application-level code generation. For each task, we distill a concise natural-language requirement from a high-quality reference project, build black-box system tests covering functional and non-functional attributes, and keep only tests that pass on the reference repository to ensure a sound oracle and an end-to-end executable suite. Functional correctness is measured by system-test pass rate. Non-functional quality is measured along five ISO/IEC 25010-inspired dimensions and aggregated with an Analytic Hierarchy Process (AHP)-derived weight vector, with per-dimension diagnostics and baseline-normalized scoring using reference measurements. Across 16 LLMs evaluated zero-shot with greedy decoding, functional correctness is the dominant bottleneck: no model exceeds a 45% functional pass rate under our requirement-driven, reference-validated tests. We release RAL-Bench at https://github.com/Wwstarry/RAL-Bench. .", "arxiv_url": "https://arxiv.org/abs/2602.03462", "authors": ["Ruwei Pan", "Yakun Zhang", "Qingyuan Liang", "Yueheng Zhu", "Chao Liu", "Lu Zhang", "Hongyu Zhang"], "first_author": "Ruwei Pan", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Application-Level Code Generation", "tags": ["Application-Level Generation", "System Testing", "Non-functional Quality", "ISO25010", "AHP Weighting", "Requirement Distillation", "Reference-validated Tests", "Baseline-normalized Scoring", "End-to-End Executability", "Zero-shot Evaluation"], "summary": "本文提出 RAL-Bench，一个用于评估 LLM 从自然语言需求生成可运行应用仓库的基准框架，通过对参考实现验证的黑盒系统测试同时度量功能正确性与基于 ISO/IEC 25010 的多维非功能质量（并用 AHP 加权聚合），并在零样本条件下评估了 16 款 LLM，发现功能正确性是主要瓶颈。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2602.03462v1", "published": "2026-02-03", "update_time": "2026-02-03", "download_time": "2026-02-04 02:21:26"}
{"id": "2602.03419", "title": "SWE-World: Building Software Engineering Agents in Docker-Free Environments", "abstract": "Recent advances in large language models (LLMs) have enabled software engineering agents to tackle complex code modification tasks. Most existing approaches rely on execution feedback from containerized environments, which require dependency-complete setup and physical execution of programs and tests. While effective, this paradigm is resource-intensive and difficult to maintain, substantially complicating agent training and limiting scalability. We propose SWE-World, a Docker-free framework that replaces physical execution environments with a learned surrogate for training and evaluating software engineering agents. SWE-World leverages LLM-based models trained on real agent-environment interaction data to predict intermediate execution outcomes and final test feedback, enabling agents to learn without interacting with physical containerized environments. This design preserves the standard agent-environment interaction loop while eliminating the need for costly environment construction and maintenance during agent optimization and evaluation. Furthermore, because SWE-World can simulate the final evaluation outcomes of candidate trajectories without real submission, it enables selecting the best solution among multiple test-time attempts, thereby facilitating effective test-time scaling (TTS) in software engineering tasks. Experiments on SWE-bench Verified demonstrate that SWE-World raises Qwen2.5-Coder-32B from 6.2\\% to 52.0\\% via Docker-free SFT, 55.0\\% with Docker-free RL, and 68.2\\% with further TTS. The code is available at https://github.com/RUCAIBox/SWE-World", "arxiv_url": "https://arxiv.org/abs/2602.03419", "authors": ["Shuang Sun", "Huatong Song", "Lisheng Huang", "Jinhao Jiang", "Ran Le", "Zhihao Lv", "Zongchao Chen", "Yiwen Hu", "Wenyang Luo", "Wayne Xin Zhao", "Yang Song", "Hongteng Xu", "Tao Zhang", "Ji-Rong Wen"], "first_author": "Shuang Sun", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Code Simulation", "Code Agents", "Surrogate Execution", "World Modeling", "Docker-Free Training", "Test-Time Scaling", "Transition Model", "Reward Modeling", "Repository-Level Coding", "Agentic Interaction"], "summary": "本文提出SWE-World：用训练得到的LLM作为环境的替代（步骤级转移模型与终端奖励模型），以在无Docker容器的条件下训练和评估软件工程代理，从而显著降低资源开销并在SFT、RL与测试时扩展中提升修复性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2602.03419v1", "published": "2026-02-03", "update_time": "2026-02-03", "download_time": "2026-02-04 02:21:44"}
{"id": "2602.02475", "title": "AgentRx: Diagnosing AI Agent Failures from Execution Trajectories", "abstract": "AI agents often fail in ways that are difficult to localize because executions are probabilistic, long-horizon, multi-agent, and mediated by noisy tool outputs. We address this gap by manually annotating failed agent runs and release a novel benchmark of 115 failed trajectories spanning structured API workflows, incident management, and open-ended web/file tasks. Each trajectory is annotated with a critical failure step and a category from a grounded-theory derived, cross-domain failure taxonomy. To mitigate the human cost of failure attribution, we present AGENTRX, an automated domain-agnostic diagnostic framework that pinpoints the critical failure step in a failed agent trajectory. It synthesizes constraints, evaluates them step-by-step, and produces an auditable validation log of constraint violations with associated evidence; an LLM-based judge uses this log to localize the critical step and category. Our framework improves step localization and failure attribution over existing baselines across three domains.", "arxiv_url": "https://arxiv.org/abs/2602.02475", "authors": ["Shraddha Barke", "Arnav Goyal", "Alind Khare", "Avaljot Singh", "Suman Nath", "Chetan Bansal"], "first_author": "Shraddha Barke", "category": ["Benchmark", "Technical"], "field": "AIOps", "task": "Agent Failure Localization", "tags": ["Failure Taxonomy", "Execution Trajectories", "Constraint Synthesis", "Validation Log", "LLM Adjudication", "Critical Failure Localization", "Grounded Theory Annotation", "Multi-Agent Diagnostics", "Tool Invocation Validation"], "summary": "该论文发布了包含115个失败代理执行轨迹的基准与跨域失败分类法，并提出AGENTRX框架通过从工具模式与策略合成约束、逐步验证并由基于LLM的裁判利用可审计的违约日志来定位和归因关键失败步骤。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2602.02475v1", "published": "2026-02-02", "update_time": "2026-02-02", "download_time": "2026-02-04 02:22:32"}
{"id": "2602.02419", "title": "SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration", "abstract": "Graphical User Interface (GUI) grounding aims to translate natural language instructions into executable screen coordinates, enabling automated GUI interaction. Nevertheless, incorrect grounding can result in costly, hard-to-reverse actions (e.g., erroneous payment approvals), raising concerns about model reliability. In this paper, we introduce SafeGround, an uncertainty-aware framework for GUI grounding models that enables risk-aware predictions through calibrations before testing. SafeGround leverages a distribution-aware uncertainty quantification method to capture the spatial dispersion of stochastic samples from outputs of any given model. Then, through the calibration process, SafeGround derives a test-time decision threshold with statistically guaranteed false discovery rate (FDR) control. We apply SafeGround on multiple GUI grounding models for the challenging ScreenSpot-Pro benchmark. Experimental results show that our uncertainty measure consistently outperforms existing baselines in distinguishing correct from incorrect predictions, while the calibrated threshold reliably enables rigorous risk control and potentials of substantial system-level accuracy improvements. Across multiple GUI grounding models, SafeGround improves system-level accuracy by up to 5.38\\% percentage points over Gemini-only inference.", "arxiv_url": "https://arxiv.org/abs/2602.02419", "authors": ["Qingni Wang", "Yue Fan", "Xin Eric Wang"], "first_author": "Qingni Wang", "category": ["Technical"], "field": "GUI Interaction", "task": "GUI Grounding", "tags": ["Uncertainty Calibration", "Selective Prediction", "False Discovery Rate Control", "Distribution-aware Uncertainty", "Cascaded Inference", "Spatial Uncertainty", "Black-box Models"], "summary": "SAFEGROUND 提出了一种无需访问模型内部、基于随机采样的空间分布不确定性量化与学习-然后-测试校准方法，以在 GUI 定位任务中实现可控错误率的选择性预测与级联推理，从而提升系统级准确性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2602.02419v1", "published": "2026-02-02", "update_time": "2026-02-02", "download_time": "2026-02-04 02:23:45"}
{"id": "2602.04640", "title": "Towards Structured, State-Aware, and Execution-Grounded Reasoning for Software Engineering Agents", "abstract": "Software Engineering (SE) agents have shown promising abilities in supporting various SE tasks. Current SE agents remain fundamentally reactive, making decisions mainly based on conversation history and the most recent response. However, this reactive design provides no explicit structure or persistent state within the agent's memory, making long-horizon reasoning challenging. As a result, SE agents struggle to maintain a coherent understanding across reasoning steps, adapt their hypotheses as new evidence emerges, or incorporate execution feedback into the mental reasoning model of the system state.   In this position paper, we argue that, to further advance SE agents, we need to move beyond reactive behavior toward a structured, state-aware, and execution-grounded reasoning. We outline how explicit structure, persistent and evolving state, and the integration of execution-grounded feedback can help SE agents perform more coherent and reliable reasoning in long-horizon tasks. We also provide an initial roadmap for developing next-generation SE agents that can more effectively perform real-world tasks.", "arxiv_url": "https://arxiv.org/abs/2602.04640", "authors": ["Tse-Hsun", "Chen"], "first_author": "Tse-Hsun", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Stateful Reasoning", "Structured Memory", "Execution-Grounded Feedback", "Hypothesis Tracking", "Pre/Post Conditions", "Finite-State Abstraction", "Long-Horizon Reasoning", "Tool Integration"], "summary": "本文主张将软件工程代理从基于对话的被动反应式设计，转向具有显式结构、持久可演化状态和执行反馈驱动的推理机制，并提出实现该目标的初步路线图。", "quality": "Middle", "conference": "BoatSE", "pdf_url": "https://arxiv.org/pdf/2602.04640v1", "published": "2026-02-04", "update_time": "2026-02-04", "download_time": "2026-02-05 02:22:50"}
{"id": "2602.04449", "title": "What's in a Benchmark? The Case of SWE-Bench in Automated Program Repair", "abstract": "The rapid progress in Automated Program Repair (APR) has been fueled by advances in AI, particularly large language models (LLMs) and agent-based systems. SWE-Bench is a benchmark designed to evaluate repair systems using real issues mined from popular open-source Python repositories. Its public leaderboards-SWE-Bench Lite and Verified-have become central platforms for tracking progress and comparing solutions. In this paper, we present the first comprehensive study of these two leaderboards, examining who is submitting solutions, the products behind the submissions, the LLMs employed, and the openness of the approaches. We analyze 79 entries submitted to Lite leaderboard and 133 to Verified. Our results show that most entries on both leaderboards originate from industry, particularly small companies and large publicly traded companies. These submissions often achieve top results, although academic contributions-typically open source-also remain competitive. We also find a clear dominance of proprietary LLMs, especially Claude family, with state-of-the-art results on both leaderboards currently achieved by Claude 4 Sonnet. These findings offer insights into the SWE-Bench ecosystem that can guide greater transparency and diversity in future benchmark-driven research.", "arxiv_url": "https://arxiv.org/abs/2602.04449", "authors": ["Matias Martinez", "Xavier Franch"], "first_author": "Matias Martinez", "category": ["Empirical"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Program Repair", "Benchmark Analysis", "Leaderboard Dynamics", "Submitter Profiling", "LLM Adoption", "Proprietary Model Dominance", "Open-source Availability", "Patch Correctness"], "summary": "本文对SWE-Bench Lite和Verified排行榜上的79和133条提交进行系统性实证分析，揭示了提交者构成、产品可用性、开源程度与所用大型语言模型（以专有模型占优）对自动程序修复结果的影响，并讨论了基准的透明性与演化问题。", "quality": "High", "conference": "IEEE/ACM 48th International Conference on Software Engineering (ICSE)", "pdf_url": "https://arxiv.org/pdf/2602.04449v1", "published": "2026-02-04", "update_time": "2026-02-04", "download_time": "2026-02-05 02:23:11"}
{"id": "2602.04572", "title": "From Competition to Collaboration: Designing Sustainable Mechanisms Between LLMs and Online Forums", "abstract": "While Generative AI (GenAI) systems draw users away from (Q&A) forums, they also depend on the very data those forums produce to improve their performance. Addressing this paradox, we propose a framework of sequential interaction, in which a GenAI system proposes questions to a forum that can publish some of them. Our framework captures several intricacies of such a collaboration, including non-monetary exchanges, asymmetric information, and incentive misalignment. We bring the framework to life through comprehensive, data-driven simulations using real Stack Exchange data and commonly used LLMs. We demonstrate the incentive misalignment empirically, yet show that players can achieve roughly half of the utility in an ideal full-information scenario. Our results highlight the potential for sustainable collaboration that preserves effective knowledge sharing between AI systems and human knowledge platforms.", "arxiv_url": "https://arxiv.org/abs/2602.04572", "authors": ["Niv Fono", "Yftah Ziser", "Omer Ben-Porat"], "first_author": "Niv Fono", "category": ["Technical", "Empirical"], "field": "Requirements & Design", "task": "Management", "tags": ["Human-LLM Interaction", "Incentive Misalignment", "Game-Theoretic Modeling", "Acceptance-Aware Mechanism", "Asymmetric Information", "Non-Monetary Exchange", "Data-Driven Simulation", "Forum Sustainability"], "summary": "本文提出了一个博弈论框架，并通过基于真实Stack Exchange数据与开源LLM的大规模仿真实验，设计并验证了在非货币交换与信息不对称条件下，GenAI与问答社区之间能够部分恢复双方效用的可持续协作机制。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2602.04572v1", "published": "2026-02-04", "update_time": "2026-02-04", "download_time": "2026-02-05 02:24:42"}
{"id": "2602.04466", "title": "Is Micro Domain-Adaptive Pre-Training Effective for Real-World Operations? Multi-Step Evaluation Reveals Potential and Bottlenecks", "abstract": "When applying LLMs to real-world enterprise operations, LLMs need to handle proprietary knowledge in small domains of specific operations ($\\textbf{micro domains}$). A previous study shows micro domain-adaptive pre-training ($\\textbf{mDAPT}$) with fewer documents is effective, similarly to DAPT in larger domains. However, it evaluates mDAPT only on multiple-choice questions; thus, its effectiveness for generative tasks in real-world operations remains unknown. We aim to reveal the potential and bottlenecks of mDAPT for generative tasks. To this end, we disentangle the answering process into three subtasks and evaluate the performance of each subtask: (1) $\\textbf{eliciting}$ facts relevant to questions from an LLM's own knowledge, (2) $\\textbf{reasoning}$ over the facts to obtain conclusions, and (3) $\\textbf{composing}$ long-form answers based on the conclusions. We verified mDAPT on proprietary IT product knowledge for real-world questions in IT technical support operations. As a result, mDAPT resolved the elicitation task that the base model struggled with but did not resolve other subtasks. This clarifies mDAPT's effectiveness in the knowledge aspect and its bottlenecks in other aspects. Further analysis empirically shows that resolving the elicitation and reasoning tasks ensures sufficient performance (over 90%), emphasizing the need to enhance reasoning capability.", "arxiv_url": "https://arxiv.org/abs/2602.04466", "authors": ["Masaya Tsunokake", "Yuta Koreeda", "Terufumi Morishita", "Koichi Nagatsuka", "Hikaru Tomonari", "Yasuhiro Sogawa"], "first_author": "Masaya Tsunokake", "category": ["Empirical", "Technical"], "field": "Domain Adaptation & Knowledge Grounding", "task": "Micro Domain-Adaptive Pre-Training Evaluation for Generative QA", "tags": ["Micro-domain Adaptation", "Multi-step Oracle Evaluation", "Knowledge Elicitation", "Knowledge Memorization", "Reasoning Bottleneck", "Answer Composition", "Enterprise IT QA"], "summary": "本文通过提出多步oracle评估与知识评估方法，实证研究在企业微域（micro-domain）上进行领域自适应预训练（mDAPT）对生成式技术支持问答的效果，发现mDAPT能显著提升知识提取能力但在推理与长文生成上仍存瓶颈。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2602.04466v1", "published": "2026-02-04", "update_time": "2026-02-04", "download_time": "2026-02-05 02:25:42"}
{"id": "2602.05762", "title": "RocqSmith: Can Automatic Optimization Forge Better Proof Agents?", "abstract": "This work studies the applicability of automatic AI agent optimization methods to real-world agents in formal verification settings, focusing on automated theorem proving in Rocq as a representative and challenging domain. We evaluate how different automatic agent optimizers perform when applied to the task of optimizing a Rocq proof-generation agent, and assess whether parts of the fine-grained tuning of agentic systems, such as prompt design, contextual knowledge, and control strategies, can be automated. Our results show that while several optimizers yield measurable improvements, simple few-shot bootstrapping is the most consistently effective; however, none of the studied methods matches the performance of a carefully engineered state-of-the-art proof agent.", "arxiv_url": "https://arxiv.org/abs/2602.05762", "authors": ["Andrei Kozyrev", "Nikita Khramov", "Denis Lochmelis", "Valerio Morelli", "Gleb Solovev", "Anton Podkopaev"], "first_author": "Andrei Kozyrev", "category": ["Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Program Proof", "Proof Agents", "Agent Optimization", "Prompt Optimization", "Few-shot Bootstrapping", "Contextual Knowledge Retrieval", "Control Flow Optimization", "Optimization Efficiency"], "summary": "本文在Rocq自动定理证明场景中系统评估了多种自动化智能体优化方法，发现简单的few-shot自举最为稳定但仍未能超越精心设计的手工证明智能体。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2602.05762v1", "published": "2026-02-05", "update_time": "2026-02-05", "download_time": "2026-02-06 02:22:52"}
{"id": "2602.05721", "title": "A Dual-Loop Agent Framework for Automated Vulnerability Reproduction", "abstract": "Automated vulnerability reproduction from CVE descriptions requires generating executable Proof-of-Concept (PoC) exploits and validating them in target environments. This process is critical in software security research and practice, yet remains time-consuming and demands specialized expertise when performed manually. While LLM agents show promise for automating this task, existing approaches often conflate exploring attack directions with fixing implementation details, which leads to unproductive debugging loops when reproduction fails. To address this, we propose Cve2PoC, an LLM-based dual-loop agent framework following a plan-execute-evaluate paradigm. The Strategic Planner analyzes vulnerability semantics and target code to produce structured attack plans. The Tactical Executor generates PoC code and validates it through progressive verification. The Adaptive Refiner evaluates execution results and routes failures to different loops: the \\textit{Tactical Loop} for code-level refinement, while the \\textit{Strategic Loop} for attack strategy replanning. This dual-loop design enables the framework to escape ineffective debugging by matching remediation to failure type. Evaluation on two benchmarks covering 617 real-world vulnerabilities demonstrates that Cve2PoC achieves 82.9\\% and 54.3\\% reproduction success rates on SecBench.js and PatchEval, respectively, outperforming the best baseline by 11.3\\% and 20.4\\%. Human evaluation confirms that generated PoCs achieve comparable code quality to human-written exploits in readability and reusability.", "arxiv_url": "https://arxiv.org/abs/2602.05721", "authors": ["Bin Liu", "Yanjie Zhao", "Zhenpeng Chen", "Guoai Xu", "Haoyu Wang"], "first_author": "Bin Liu", "category": ["Technical"], "field": "Quality Management", "task": "Vulnerability Reproduction", "tags": ["Code Agents", "Security and Vulnerabilities", "PoC Generation", "Dual-Loop Agents", "Strategic Planning", "Tactical Execution", "Progressive Verification", "Adaptive Refinement", "Differential Testing", "Experience Retrieval"], "summary": "本文提出Cve2PoC——一种将战略规划与战术执行分离的基于LLM的双环代理框架，通过结构化攻击计划、渐进式多层验证与自适应精炼，实现从CVE描述自动生成并验证可执行PoC，显著提升漏洞复现成功率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2602.05721v1", "published": "2026-02-05", "update_time": "2026-02-05", "download_time": "2026-02-06 02:23:26"}
{"id": "2602.04837", "title": "Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing", "abstract": "Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods.", "arxiv_url": "https://arxiv.org/abs/2602.04837", "authors": ["Zhaotian Weng", "Antonis Antoniades", "Deepak Nathani", "Zhen Zhang", "Xiao Pu", "Xin Eric Wang"], "first_author": "Zhaotian Weng", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Group Evolution", "Experience Sharing", "Open-Ended Self-Improvement", "Meta-Learning", "Performance-Novelty Selection", "Diversity Consolidation", "Agent Robustness", "Transferability"], "summary": "本文提出Group-Evolving Agents（GEA），通过将群体作为进化单元并在群体内显式共享与复用经验，实现开放式自我改进，在多个编码基准上显著优于现有自我进化方法并展现更强的鲁棒性与可迁移性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2602.04837v1", "published": "2026-02-04", "update_time": "2026-02-04", "download_time": "2026-02-06 02:24:45"}
{"id": "2602.04811", "title": "SE-Bench: Benchmarking Self-Evolution with Knowledge Internalization", "abstract": "True self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hindered by two obstacles: the entanglement of prior knowledge, where ``new'' knowledge may appear in pre-training data, and the entanglement of reasoning complexity, where failures may stem from problem difficulty rather than an inability to recall learned knowledge. We introduce SE-Bench, a diagnostic environment that obfuscates the NumPy library and its API doc into a pseudo-novel package with randomized identifiers. Agents are trained to internalize this package and evaluated on simple coding tasks without access to documentation, yielding a clean setting where tasks are trivial with the new API doc but impossible for base models without it. Our investigation reveals three insights: (1) the Open-Book Paradox, where training with reference documentation inhibits retention, requiring \"Closed-Book Training\" to force knowledge compression into weights; (2) the RL Gap, where standard RL fails to internalize new knowledge completely due to PPO clipping and negative gradients; and (3) the viability of Self-Play for internalization, proving models can learn from self-generated, noisy tasks when coupled with SFT, but not RL. Overall, SE-Bench establishes a rigorous diagnostic platform for self-evolution with knowledge internalization. Our code and dataset can be found at https://github.com/thunlp/SE-Bench.", "arxiv_url": "https://arxiv.org/abs/2602.04811", "authors": ["Jiarui Yuan", "Tailin Jin", "Weize Chen", "Zeyuan Liu", "Zhiyuan Liu", "Maosong Sun"], "first_author": "Jiarui Yuan", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Instruction-Tuning", "tags": ["Knowledge Internalization", "API Obfuscation", "Closed-Book Training", "Open-Book Paradox", "Self-Play Learning", "RL Gap", "PPO Clipping", "Compositional Generalization", "Diagnostic Benchmark", "Consensus Filtering"], "summary": "该论文提出SE-BENCH，通过将NumPy函数和文档混淆为伪新库并构建可训练与测试题集，提供一个“必须内化才能解题”的诊断基准，用以评估和分析SFT、RL与自我博弈在将新API知识压缩入模型权重中的效果，并揭示了开卷悖论、RL内化缺陷与基于SFT的自我博弈可行性等重要发现。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2602.04811v1", "published": "2026-02-04", "update_time": "2026-02-04", "download_time": "2026-02-06 02:25:38"}
