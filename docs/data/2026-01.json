[
  {
    "id": "2601.00635",
    "title": "SEMODS: A Validated Dataset of Open-Source Software Engineering Models",
    "abstract": "Integrating Artificial Intelligence into Software Engineering (SE) requires having a curated collection of models suited to SE tasks. With millions of models hosted on Hugging Face (HF) and new ones continuously being created, it is infeasible to identify SE models without a dedicated catalogue. To address this gap, we present SEMODS: an SE-focused dataset of 3,427 models extracted from HF, combining automated collection with rigorous validation through manual annotation and large language model assistance. Our dataset links models to SE tasks and activities from the software development lifecycle, offering a standardized representation of their evaluation results, and supporting multiple applications such as data analysis, model discovery, benchmarking, and model adaptation.",
    "arxiv_url": "https://arxiv.org/abs/2601.00635",
    "authors": [
      "Alexandra González",
      "Xavier Franch",
      "Silverio Martínez-Fernández"
    ],
    "first_author": "Alexandra González",
    "category": [
      "Benchmark"
    ],
    "field": "Model Repositories & Catalogues",
    "task": "SE Model Cataloguing",
    "tags": [
      "Model Catalog",
      "SE Task Taxonomy",
      "Benchmark Harmonization",
      "LLM-assisted Annotation",
      "Model Card Normalization",
      "Automated Dataset Maintenance",
      "Repository Metadata Schema",
      "Model Discovery"
    ],
    "summary": "本文提出SEMODS，一个从Hugging Face系统化收集并通过人工和大模型辅助验证的面向软件工程的模型数据集（3,427个模型），提供SE任务映射、标准化评测表示和自动化更新管道以支持模型发现与基准测试。",
    "quality": "High",
    "conference": "FORGE (ACM International Conference on AI Foundation Models and Software Engineering) 2026",
    "pdf_url": "https://arxiv.org/pdf/2601.00635v1",
    "published": "2026-01-02",
    "update_time": "2026-01-02",
    "download_time": "2026-01-05 02:05:13"
  },
  {
    "id": "2601.00497",
    "title": "STELLAR: A Search-Based Testing Framework for Large Language Model Applications",
    "abstract": "Large Language Model (LLM)-based applications are increasingly deployed across various domains, including customer service, education, and mobility. However, these systems are prone to inaccurate, fictitious, or harmful responses, and their vast, high-dimensional input space makes systematic testing particularly challenging. To address this, we present STELLAR, an automated search-based testing framework for LLM-based applications that systematically uncovers text inputs leading to inappropriate system responses. Our framework models test generation as an optimization problem and discretizes the input space into stylistic, content-related, and perturbation features. Unlike prior work that focuses on prompt optimization or coverage heuristics, our work employs evolutionary optimization to dynamically explore feature combinations that are more likely to expose failures. We evaluate STELLAR on three LLM-based conversational question-answering systems. The first focuses on safety, benchmarking both public and proprietary LLMs against malicious or unsafe prompts. The second and third target navigation, using an open-source and an industrial retrieval-augmented system for in-vehicle venue recommendations. Overall, STELLAR exposes up to 4.3 times (average 2.5 times) more failures than the existing baseline approaches.",
    "arxiv_url": "https://arxiv.org/abs/2601.00497",
    "authors": [
      "Lev Sorokin",
      "Ivan Vasilev",
      "Ken E. Friedl",
      "Andrea Stocco"
    ],
    "first_author": "Lev Sorokin",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Software Testing",
    "task": "Test Generation",
    "tags": [
      "Search-based Testing",
      "Evolutionary Optimization",
      "Feature Discretization",
      "Retrieval-Augmented Generation",
      "Robustness Testing",
      "Safety Testing",
      "Conversational QA",
      "Perturbation Testing"
    ],
    "summary": "本文提出STELLAR，一种将自然语言输入离散为风格、内容和扰动特征并通过进化搜索生成失败诱发测试用例的自动化搜索式测试框架，用以发现LLM应用（如车载对话系统）中的不当或错误响应并在多项用例中显著优于基线方法。",
    "quality": "High",
    "conference": "International Conference on Software Analysis, Evolution and Reengineering (SANER) 2026",
    "pdf_url": "https://arxiv.org/pdf/2601.00497v1",
    "published": "2026-01-01",
    "update_time": "2026-01-01",
    "download_time": "2026-01-05 02:06:23"
  },
  {
    "id": "2601.00753",
    "title": "Early-Stage Prediction of Review Effort in AI-Generated Pull Requests",
    "abstract": "As autonomous AI agents transition from code completion tools to full-fledged teammates capable of opening pull requests (PRs) at scale, software maintainers face a new challenge: not just reviewing code, but managing complex interaction loops with non-human contributors. This paradigm shift raises a critical question: can we predict which agent-generated PRs will consume excessive review effort before any human interaction begins?   Analyzing 33,707 agent-authored PRs from the AIDev dataset across 2,807 repositories, we uncover a striking two-regime behavioral pattern that fundamentally distinguishes autonomous agents from human developers. The first regime, representing 28.3 percent of all PRs, consists of instant merges (less than 1 minute), reflecting success on narrow automation tasks. The second regime involves iterative review cycles where agents frequently stall or abandon refinement (ghosting).   We propose a Circuit Breaker triage model that predicts high-review-effort PRs (top 20 percent) at creation time using only static structural features. A LightGBM model achieves AUC 0.957 on a temporal split, while semantic text features (TF-IDF, CodeBERT) provide negligible predictive value. At a 20 percent review budget, the model intercepts 69 percent of total review effort, enabling zero-latency governance.   Our findings challenge prevailing assumptions in AI-assisted code review: review burden is dictated by what agents touch, not what they say, highlighting the need for structural governance mechanisms in human-AI collaboration.",
    "arxiv_url": "https://arxiv.org/abs/2601.00753",
    "authors": [
      "Dao Sy Duy Minh",
      "Huynh Trung Kiet",
      "Tran Chi Nguyen",
      "Nguyen Lam Phu Quy",
      "Phu Hoa Pham",
      "Nguyen Dinh Ha Duong",
      "Truong Bao Tran"
    ],
    "first_author": "Dao Sy Duy Minh",
    "category": [
      "Empirical",
      "Technical"
    ],
    "field": "Maintenance",
    "task": "Code Review",
    "tags": [
      "Ghosting",
      "Review Triage",
      "Effort Prediction",
      "Creation-time Features",
      "Patch Complexity",
      "Structural Signals",
      "Agent-authored PRs",
      "Circuit Breaker",
      "Zero-latency Triage",
      "Model Interpretability"
    ],
    "summary": "本文基于33,707个由AI代理提交的PR，发现即时合并与迭代失败两种行为模式，并提出基于创建时结构性特征的“断路器”LightGBM模型（AUC≈0.957）用于零延迟预测高审查成本的PR以辅助维护者分流。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.00753v1",
    "published": "2026-01-02",
    "update_time": "2026-01-02",
    "download_time": "2026-01-06 01:54:54"
  },
  {
    "id": "2601.00482",
    "title": "Multi-Agent Coordinated Rename Refactoring",
    "abstract": "The primary value of AI agents in software development lies in their ability to extend the developer's capacity for reasoning and action, not to supplant human involvement. To showcase how to use agents working in tandem with developers, we designed a novel approach for carrying out coordinated renaming. Coordinated renaming, where a single rename refactoring triggers refactorings in multiple, related identifiers, is a frequent yet challenging task. Developers must manually propagate these rename refactorings across numerous files and contexts, a process that is both tedious and highly error-prone. State-of-the-art heuristic-based approaches produce an overwhelming number of false positives, while vanilla Large Language Models (LLMs) provide incomplete suggestions due to their limited context and inability to interact with refactoring tools. This leaves developers with incomplete refactorings or burdens them with filtering too many false positives. Coordinated renaming is exactly the kind of repetitive task that agents can significantly reduce the developers' burden while keeping them in the driver's seat.   We designed, implemented, and evaluated the first multi-agent framework that automates coordinated renaming. It operates on a key insight: a developer's initial refactoring is a clue to infer the scope of related refactorings. Our Scope Inference Agent first transforms this clue into an explicit, natural-language Declared Scope. The Planned Execution Agent then uses this as a strict plan to identify program elements that should undergo refactoring and safely executes the changes by invoking the IDE's own trusted refactoring APIs. Finally, the Replication Agent uses it to guide the project-wide search. We first conducted a formative study on the practice of coordinated renaming in 609K commits in 100 open-source projects and surveyed 205 developers ...",
    "arxiv_url": "https://arxiv.org/abs/2601.00482",
    "authors": [
      "Abhiram Bellur",
      "Mohammed Raihan Ullah",
      "Fraol Batole",
      "Mohit Kansara",
      "Masaharu Morimoto",
      "Kai Ishikawa",
      "Haifeng Chen",
      "Yaroslav Zharov",
      "Timofey Bryksin",
      "Tien N. Nguyen",
      "Hridesh Rajan",
      "Danny Dig"
    ],
    "first_author": "Abhiram Bellur",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Maintenance",
    "task": "Refactoring",
    "tags": [
      "Code Agents",
      "Coordinated Rename",
      "Scope Inference",
      "IDE Refactoring API",
      "Program Slicing",
      "Semantic Search",
      "Developer-in-the-loop",
      "Episodic Memory",
      "Repository-level Propagation",
      "Pull Request Automation"
    ],
    "summary": "本文提出CoRenameAgent——一个在IDE中运行的多智能体框架，通过范围推断、规划执行和复制传播三类代理结合开发者反馈与IDE可信重构API，自动、可靠地执行跨仓库的协同重命名，并在两个基准上显著优于现有方法且其自动生成的PR被社区接受。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.00482v1",
    "published": "2026-01-01",
    "update_time": "2026-01-01",
    "download_time": "2026-01-06 01:55:34"
  },
  {
    "id": "2601.00743",
    "title": "An Agentic Framework for Neuro-Symbolic Programming",
    "abstract": "Integrating symbolic constraints into deep learning models could make them more robust, interpretable, and data-efficient. Still, it remains a time-consuming and challenging task. Existing frameworks like DomiKnowS help this integration by providing a high-level declarative programming interface, but they still assume the user is proficient with the library's specific syntax. We propose AgenticDomiKnowS (ADS) to eliminate this dependency. ADS translates free-form task descriptions into a complete DomiKnowS program using an agentic workflow that creates and tests each DomiKnowS component separately. The workflow supports optional human-in-the-loop intervention, enabling users familiar with DomiKnowS to refine intermediate outputs. We show how ADS enables experienced DomiKnowS users and non-users to rapidly construct neuro-symbolic programs, reducing development time from hours to 10-15 minutes.",
    "arxiv_url": "https://arxiv.org/abs/2601.00743",
    "authors": [
      "Aliakbar Nafar",
      "Chetan Chigurupati",
      "Danial Kamali",
      "Hamid Karimian",
      "Parisa Kordjamshidi"
    ],
    "first_author": "Aliakbar Nafar",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Editing",
    "tags": [
      "Code Agents",
      "Code RAG",
      "Neuro-Symbolic",
      "Domain-Specific Language",
      "Human-LLM Interaction",
      "Interactive Coding",
      "Self-Refinement",
      "VLM Integration"
    ],
    "summary": "本文提出AgenticDomiKnowS，一个将自然语言任务描述通过检索-生成-执行-审查的代理式工作流转化为完整DomiKnowS神经符号程序并支持人机交互与自我修正的系统，能将开发时间从数小时缩短到10–15分钟。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.00743v1",
    "published": "2026-01-02",
    "update_time": "2026-01-02",
    "download_time": "2026-01-06 01:56:06"
  },
  {
    "id": "2601.00559",
    "title": "Cracking IoT Security: Can LLMs Outsmart Static Analysis Tools?",
    "abstract": "Smart home IoT platforms such as openHAB rely on Trigger Action Condition (TAC) rules to automate device behavior, but the interplay among these rules can give rise to interaction threats, unintended or unsafe behaviors emerging from implicit dependencies, conflicting triggers, or overlapping conditions. Identifying these threats requires semantic understanding and structural reasoning that traditionally depend on symbolic, constraint-driven static analysis. This work presents the first comprehensive evaluation of Large Language Models (LLMs) across a multi-category interaction threat taxonomy, assessing their performance on both the original openHAB (oHC/IoTB) dataset and a structurally challenging Mutation dataset designed to test robustness under rule transformations. We benchmark Llama 3.1 8B, Llama 70B, GPT-4o, Gemini-2.5-Pro, and DeepSeek-R1 across zero-, one-, and two-shot settings, comparing their results against oHIT's manually validated ground truth. Our findings show that while LLMs exhibit promising semantic understanding, particularly on action- and condition-related threats, their accuracy degrades significantly for threats requiring cross-rule structural reasoning, especially under mutated rule forms. Model performance varies widely across threat categories and prompt settings, with no model providing consistent reliability. In contrast, the symbolic reasoning baseline maintains stable detection across both datasets, unaffected by rule rewrites or structural perturbations. These results underscore that LLMs alone are not yet dependable for safety critical interaction-threat detection in IoT environments. We discuss the implications for tool design and highlight the potential of hybrid architectures that combine symbolic analysis with LLM-based semantic interpretation to reduce false positives while maintaining structural rigor.",
    "arxiv_url": "https://arxiv.org/abs/2601.00559",
    "authors": [
      "Jason Quantrill",
      "Noura Khajehnouri",
      "Zihan Guo",
      "Manar H. Alalfi"
    ],
    "first_author": "Jason Quantrill",
    "category": [
      "Empirical",
      "Benchmark"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "Rule Interaction Threats",
      "IoT Security",
      "Trigger-Action-Condition",
      "Mutation Testing",
      "Symbolic-LLM Hybrid",
      "Cross-rule Reasoning",
      "Structural Robustness",
      "Prompt Robustness",
      "False Positive Reduction"
    ],
    "summary": "本文评估多种大语言模型在openHAB触发-条件-动作规则交互威胁检测上的表现，并通过一套结构变异数据集与符号静态分析工具对比，发现LLM在语义理解方面较好但在跨规则结构推理与变形规则下显著退化，提出结合符号分析与LLM的混合方案以提高检测可靠性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.00559v1",
    "published": "2026-01-02",
    "update_time": "2026-01-02",
    "download_time": "2026-01-06 01:57:35"
  },
  {
    "id": "2601.02971",
    "title": "Few-shot learning for security bug report identification",
    "abstract": "Security bug reports require prompt identification to minimize the window of vulnerability in software systems. Traditional machine learning (ML) techniques for classifying bug reports to identify security bug reports rely heavily on large amounts of labeled data. However, datasets for security bug reports are often scarce in practice, leading to poor model performance and limited applicability in real-world settings. In this study, we propose a few-shot learning-based technique to effectively identify security bug reports using limited labeled data. We employ SetFit, a state-of-the-art few-shot learning framework that combines sentence transformers with contrastive learning and parameter-efficient fine-tuning. The model is trained on a small labeled dataset of bug reports and is evaluated on its ability to classify these reports as either security-related or non-security-related. Our approach achieves an AUC of 0.865, at best, outperforming traditional ML techniques (baselines) for all of the evaluated datasets. This highlights the potential of SetFit to effectively identify security bug reports. SetFit-based few-shot learning offers a promising alternative to traditional ML techniques to identify security bug reports. The approach enables efficient model development with minimal annotation effort, making it highly suitable for scenarios where labeled data is scarce.",
    "arxiv_url": "https://arxiv.org/abs/2601.02971",
    "authors": [
      "Muhammad Laiq"
    ],
    "first_author": "Muhammad Laiq",
    "category": [
      "Empirical"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "Issue Classification",
      "Few-shot Learning",
      "Contrastive Learning",
      "Parameter-efficient Fine-tuning",
      "Class Imbalance",
      "Security Bug Identification",
      "Empirical Comparison"
    ],
    "summary": "本文在四个开源项目的数据集上实证评估了基于少样本学习的SetFit方法用于安全缺陷报告识别，实验表明在有限标注下可达到最高AUC 0.865并优于传统机器学习基线。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.02971v1",
    "published": "2026-01-06",
    "update_time": "2026-01-06",
    "download_time": "2026-01-08 01:55:02"
  },
  {
    "id": "2601.02868",
    "title": "CodeMEM: AST-Guided Adaptive Memory for Repository-Level Iterative Code Generation",
    "abstract": "Large language models (LLMs) substantially enhance developer productivity in repository-level code generation through interactive collaboration. However, as interactions progress, repository context must be continuously preserved and updated to integrate newly validated information. Meanwhile, the expanding session history increases cognitive burden, often leading to forgetting and the reintroduction of previously resolved errors. Existing memory management approaches show promise but remain limited by natural language-centric representations. To overcome these limitations, we propose CodeMEM, an AST-guided dynamic memory management system tailored for repository-level iterative code generation. Specifically, CodeMEM introduces the Code Context Memory component that dynamically maintains and updates repository context through AST-guided LLM operations, along with the Code Session Memory that constructs a code-centric representation of interaction history and explicitly detects and mitigates forgetting through AST-based analysis. Experimental results on the instruction-following benchmark CodeIF-Bench and the code generation benchmark CoderEval demonstrate that CodeMEM achieves state-of-the-art performance, improving instruction following by 12.2% for the current turn and 11.5% for the session level, and reducing interaction rounds by 2-3, while maintaining competitive inference latency and token efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2601.02868",
    "authors": [
      "Peiding Wang",
      "Li Zhang",
      "Fang Liu",
      "Chongyang Tao",
      "Yinghao Zhu"
    ],
    "first_author": "Peiding Wang",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Retrieval",
    "tags": [
      "Repository-Level Coding",
      "AST-guided memory",
      "Code Context Memory",
      "Code Session Memory",
      "Forgetting Detection",
      "Code-change Analysis",
      "Interaction Efficiency"
    ],
    "summary": "本文提出CODEMEM，一种基于AST引导的动态代码记忆管理系统，通过维护代码上下文记忆与会话记忆并基于AST分析检测与缓解遗忘，从而在仓库级多轮代码生成中显著提升指令遵循率并减少交互轮次。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.02868v1",
    "published": "2026-01-06",
    "update_time": "2026-01-06",
    "download_time": "2026-01-08 01:55:19"
  },
  {
    "id": "2601.02941",
    "title": "SastBench: A Benchmark for Testing Agentic SAST Triage",
    "abstract": "SAST (Static Application Security Testing) tools are among the most widely used techniques in defensive cybersecurity, employed by commercial and non-commercial organizations to identify potential vulnerabilities in software. Despite their great utility, they generate numerous false positives, requiring costly manual filtering (aka triage). While LLM-powered agents show promise for automating cybersecurity tasks, existing benchmarks fail to emulate real-world SAST finding distributions. We introduce SastBench, a benchmark for evaluating SAST triage agents that combines real CVEs as true positives with filtered SAST tool findings as approximate false positives. SastBench features an agent-agnostic design. We evaluate different agents on the benchmark and present a comparative analysis of their performance, provide a detailed analysis of the dataset, and discuss the implications for future development.",
    "arxiv_url": "https://arxiv.org/abs/2601.02941",
    "authors": [
      "Jake Feiglin",
      "Guy Dar"
    ],
    "first_author": "Jake Feiglin",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "SAST Triage",
      "Agentic Benchmark",
      "Realistic False Positives",
      "CVE-based Ground Truth",
      "Agent Evaluation",
      "Security-Oriented Prompting",
      "Language Diversity",
      "Agent-Agnostic Evaluation"
    ],
    "summary": "该论文提出并开源了SASTBENCH，一个以真实CVE作为真阳性并以过滤后的SAST工具报警作为近似假阳性构建的面向自动化SAST告警三分类（triage）的基准，并基于该基准对多种LLM驱动代理进行了比较评估与数据分析。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.02941v1",
    "published": "2026-01-06",
    "update_time": "2026-01-06",
    "download_time": "2026-01-08 01:59:25"
  },
  {
    "id": "2601.02736",
    "title": "Hypothesize-Then-Verify: Speculative Root Cause Analysis for Microservices with Pathwise Parallelism",
    "abstract": "Microservice systems have become the backbone of cloud-native enterprise applications due to their resource elasticity, loosely coupled architecture, and lightweight deployment. Yet, the intrinsic complexity and dynamic runtime interactions of such systems inevitably give rise to anomalies. Ensuring system reliability therefore hinges on effective root cause analysis (RCA), which entails not only localizing the source of anomalies but also characterizing the underlying failures in a timely and interpretable manner. Recent advances in intelligent RCA techniques, particularly those powered by large language models (LLMs), have demonstrated promising capabilities, as LLMs reduce reliance on handcrafted features while offering cross-platform adaptability, task generalization, and flexibility. However, existing LLM-based methods still suffer from two critical limitations: (a) limited exploration diversity, which undermines accuracy, and (b) heavy dependence on large-scale LLMs, which results in slow inference. To overcome these challenges, we propose SpecRCA, a speculative root cause analysis framework for microservices that adopts a \\textit{hypothesize-then-verify} paradigm. SpecRCA first leverages a hypothesis drafting module to rapidly generate candidate root causes, and then employs a parallel root cause verifier to efficiently validate them. Preliminary experiments on the AIOps 2022 dataset demonstrate that SpecRCA achieves superior accuracy and efficiency compared to existing approaches, highlighting its potential as a practical solution for scalable and interpretable RCA in complex microservice environments.",
    "arxiv_url": "https://arxiv.org/abs/2601.02736",
    "authors": [
      "Lingzhe Zhang",
      "Tong Jia",
      "Yunpeng Zhai",
      "Leyi Pan",
      "Chiming Duan",
      "Minghua He",
      "Pei Xiao",
      "Ying Li"
    ],
    "first_author": "Lingzhe Zhang",
    "category": [
      "Technical"
    ],
    "field": "AIOps",
    "task": "Root Cause Analysis",
    "tags": [
      "Speculative Verification",
      "Hypothesize-then-Verify",
      "Pathwise Parallelism",
      "Modality Fusion",
      "Service Topology",
      "Trace Analysis",
      "Granger Causality",
      "Anomalous Metric Detection",
      "Parallel Verification",
      "Diagnosis Synthesis",
      "Efficiency",
      "Interpretability"
    ],
    "summary": "本文提出 SpecRCA——一种面向微服务的假设-验证根因分析框架，通过轻量草拟模型生成多样化候选假设并并行用小型微调 LLM 验证，结合指标/链路/日志的拓扑融合与综合生成诊断报告，在 AIOps 2022 数据集上显著提升定位准确性并降低推理延迟。",
    "quality": "High",
    "conference": "ICSE",
    "pdf_url": "https://arxiv.org/pdf/2601.02736v1",
    "published": "2026-01-06",
    "update_time": "2026-01-06",
    "download_time": "2026-01-08 02:01:34"
  },
  {
    "id": "2601.03988",
    "title": "Using Small Language Models to Reverse-Engineer Machine Learning Pipelines Structures",
    "abstract": "Background: Extracting the stages that structure Machine Learning (ML) pipelines from source code is key for gaining a deeper understanding of data science practices. However, the diversity caused by the constant evolution of the ML ecosystem (e.g., algorithms, libraries, datasets) makes this task challenging. Existing approaches either depend on non-scalable, manual labeling, or on ML classifiers that do not properly support the diversity of the domain. These limitations highlight the need for more flexible and reliable solutions.   Objective: We evaluate whether Small Language Models (SLMs) can leverage their code understanding and classification abilities to address these limitations, and subsequently how they can advance our understanding of data science practices.   Method: We conduct a confirmatory study based on two reference works selected for their relevance regarding current state-of-the-art's limitations. First, we compare several SLMs using Cochran's Q test. The best-performing model is then evaluated against the reference studies using two distinct McNemar's tests. We further analyze how variations in taxonomy definitions affect performance through an additional Cochran's Q test. Finally, a goodness-of-fit analysis is conducted using Pearson's chi-squared tests to compare our insights on data science practices with those from prior studies.",
    "arxiv_url": "https://arxiv.org/abs/2601.03988",
    "authors": [
      "Nicolas Lacroix",
      "Mireille Blay-Fornarino",
      "Sébastien Mosser",
      "Frederic Precioso"
    ],
    "first_author": "Nicolas Lacroix",
    "category": [
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Understanding",
    "tags": [
      "Pipeline Extraction",
      "Reverse Engineering",
      "Jupyter Notebook Analysis",
      "Small Language Models",
      "Instruction-level Classification",
      "Taxonomy Sensitivity",
      "Model Comparison",
      "Goodness-of-fit Analysis"
    ],
    "summary": "本文通过对比多款开源小型语言模型，评估其在从 Jupyter 笔记本代码中反向工程提取机器学习流水线结构（阶段）上的准确性，分析分类法措辞对结果的影响并检验这些结果是否改变对数据科学家实践的既有理解。",
    "quality": "High",
    "conference": "SANER (IEEE International Conference on Software Analysis, Evolution and Reengineering) 2026",
    "pdf_url": "https://arxiv.org/pdf/2601.03988v1",
    "published": "2026-01-07",
    "update_time": "2026-01-07",
    "download_time": "2026-01-09 01:56:28"
  },
  {
    "id": "2601.03878",
    "title": "Understanding Specification-Driven Code Generation with LLMs: An Empirical Study Design",
    "abstract": "Large Language Models (LLMs) are increasingly integrated into software development workflows, yet their behavior in structured, specification-driven processes remains poorly understood. This paper presents an empirical study design using CURRANTE, a Visual Studio Code extension that enables a human-in-the-loop workflow for LLM-assisted code generation. The tool guides developers through three sequential stages--Specification, Tests, and Function--allowing them to define requirements, generate and refine test suites, and produce functions that satisfy those tests. Participants will solve medium-difficulty problems from the LiveCodeBench dataset, while the tool records fine-grained interaction logs, effectiveness metrics (e.g., pass rate, all-pass completion), efficiency indicators (e.g., time-to-pass), and iteration behaviors. The study aims to analyze how human intervention in specification and test refinement influences the quality and dynamics of LLM-generated code. The results will provide empirical insights into the design of next-generation development environments that align human reasoning with model-driven code generation.",
    "arxiv_url": "https://arxiv.org/abs/2601.03878",
    "authors": [
      "Giovanni Rosa",
      "David Moreno-Lumbreras",
      "Gregorio Robles",
      "Jesús M. González-Barahona"
    ],
    "first_author": "Giovanni Rosa",
    "category": [
      "Empirical",
      "Technical"
    ],
    "field": "Software Testing",
    "task": "Test Generation",
    "tags": [
      "Specification-Driven Development",
      "Test-Driven Development",
      "Human-LLM Interaction",
      "IDE Plugin",
      "Test Suite Generation",
      "Test Refinement",
      "Interaction Logging",
      "Effectiveness Metrics",
      "Time-to-Pass",
      "Registered Study Protocol"
    ],
    "summary": "本文提出并描述了一项基于 CURRANTE VSCode 插件的注册报告型实证研究设计，以评估在人类参与的规范（通过测试用例）驱动 TDD 工作流下，LLM 辅助代码生成的有效性、效率与迭代行为。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.03878v1",
    "published": "2026-01-07",
    "update_time": "2026-01-07",
    "download_time": "2026-01-09 01:56:58"
  },
  {
    "id": "2601.04126",
    "title": "InfiniteWeb: Scalable Web Environment Synthesis for GUI Agent Training",
    "abstract": "GUI agents that interact with graphical interfaces on behalf of users represent a promising direction for practical AI assistants. However, training such agents is hindered by the scarcity of suitable environments. We present InfiniteWeb, a system that automatically generates functional web environments at scale for GUI agent training. While LLMs perform well on generating a single webpage, building a realistic and functional website with many interconnected pages faces challenges. We address these challenges through unified specification, task-centric test-driven development, and a combination of website seed with reference design image to ensure diversity. Our system also generates verifiable task evaluators enabling dense reward signals for reinforcement learning. Experiments show that InfiniteWeb surpasses commercial coding agents at realistic website construction, and GUI agents trained on our generated environments achieve significant performance improvements on OSWorld and Online-Mind2Web, demonstrating the effectiveness of proposed system.",
    "arxiv_url": "https://arxiv.org/abs/2601.04126",
    "authors": [
      "Ziyun Zhang",
      "Zezhou Wang",
      "Xiaoyi Zhang",
      "Zongyu Guo",
      "Jiahao Li",
      "Bin Li",
      "Yan Lu"
    ],
    "first_author": "Ziyun Zhang",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Requirements & Design",
    "task": "Specification & Validation",
    "tags": [
      "Environment Synthesis",
      "Unified Specification",
      "Task-Centric TDD",
      "Evaluator Generation",
      "Design-Guided Frontend",
      "Visual-Functional Diversity",
      "Dense Reward Signals",
      "GUI Agent Training"
    ],
    "summary": "本文提出INFINITEWEB，一个通过从任务导出统一数据与接口、采用面向任务的测试驱动开发以及设计图引导的前端生成，自动大规模合成功能性网页环境并生成可验证评估器以支持基于强化学习的GUI代理训练的系统。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.04126v1",
    "published": "2026-01-07",
    "update_time": "2026-01-07",
    "download_time": "2026-01-09 01:58:15"
  },
  {
    "id": "2601.03731",
    "title": "From Laboratory to Real-World Applications: Benchmarking Agentic Code Reasoning at the Repository Level",
    "abstract": "As large language models (LLMs) evolve into autonomous agents, evaluating repository-level reasoning, the ability to maintain logical consistency across massive, real-world, interdependent file systems, has become critical. Current benchmarks typically fluctuate between isolated code snippets and black-box evaluations. We present RepoReason, a white-box diagnostic benchmark centered on abductive assertion verification. To eliminate memorization while preserving authentic logical depth, we implement an execution-driven mutation framework that utilizes the environment as a semantic oracle to regenerate ground-truth states. Furthermore, we establish a fine-grained diagnostic system using dynamic program slicing, quantifying reasoning via three orthogonal metrics: $ESV$ (reading load), $MCL$ (simulation depth), and $DFI$ (integration width). Comprehensive evaluations of frontier models (e.g., Claude-4.5-Sonnet, DeepSeek-v3.1-Terminus) reveal a prevalent aggregation deficit, where integration width serves as the primary cognitive bottleneck. Our findings provide granular white-box insights for optimizing the next generation of agentic software engineering.",
    "arxiv_url": "https://arxiv.org/abs/2601.03731",
    "authors": [
      "Jia Li",
      "Yuxin Su",
      "Michael R. Lyu"
    ],
    "first_author": "Jia Li",
    "category": [
      "Benchmark",
      "Technical",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Repository-Level Reasoning",
      "Abductive Verification",
      "Execution-Driven Mutation",
      "Deterministic Value Protocol",
      "Dynamic Program Slicing",
      "Reading Load (ESV)",
      "Simulation Depth (MCL)",
      "Integration Width (DFI)",
      "White-box Diagnostic",
      "Aggregation Deficit",
      "Long-chain State Tracking",
      "Cross-file Integration"
    ],
    "summary": "本文提出了REPOREASON——一个以掩码断言验证为核心、结合执行驱动变异和动态程序切片的仓库级白盒基准，用于客观评估并细粒度诊断LLM代理在跨文件信息整合与长链状态追踪方面的推理能力，揭示了模型在整合宽度上的严重瓶颈（Aggregation Deficit）。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.03731v1",
    "published": "2026-01-07",
    "update_time": "2026-01-07",
    "download_time": "2026-01-09 02:08:24"
  },
  {
    "id": "2601.04886",
    "title": "Analyzing Message-Code Inconsistency in AI Coding Agent-Authored Pull Requests",
    "abstract": "Pull request (PR) descriptions generated by AI coding agents are the primary channel for communicating code changes to human reviewers. However, the alignment between these messages and the actual changes remains unexplored, raising concerns about the trustworthiness of AI agents. To fill this gap, we analyzed 23,247 agentic PRs across five agents using PR message-code inconsistency (PR-MCI). We contributed 974 manually annotated PRs, found 406 PRs (1.7%) exhibited high PR-MCI, and identified eight PR-MCI types, revealing that descriptions claiming unimplemented changes was the most common issue (45.4%). Statistical tests confirmed that high-MCI PRs had 51.7% lower acceptance rates (28.3% vs. 80.0%) and took 3.5x longer to merge (55.8 vs. 16.0 hours). Our findings suggest that unreliable PR descriptions undermine trust in AI agents, highlighting the need for PR-MCI verification mechanisms and improved PR generation to enable trustworthy human-AI collaboration.",
    "arxiv_url": "https://arxiv.org/abs/2601.04886",
    "authors": [
      "Jingzhi Gong",
      "Giovanni Pinna",
      "Yixin Bian",
      "Jie M. Zhang"
    ],
    "first_author": "Jingzhi Gong",
    "category": [
      "Empirical",
      "Benchmark"
    ],
    "field": "Maintenance",
    "task": "Code Review",
    "tags": [
      "Message-Code Inconsistency",
      "PR Description Fidelity",
      "Human-LLM Interaction",
      "Hallucination",
      "Annotated Dataset",
      "Taxonomy",
      "Acceptance Rate",
      "Merge Delay"
    ],
    "summary": "本文通过分析23,247个由AI代理提交的PR并发布974条人工标注，构建并量化了PR消息-代码不一致（PR-MCI）的分类与流行率，发现高不一致PR显著降低接受率并延长合并时间，强调需改进PR生成与校验以提升人机协作可信度。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.04886v1",
    "published": "2026-01-08",
    "update_time": "2026-01-08",
    "download_time": "2026-01-10 01:51:55"
  },
  {
    "id": "2601.04556",
    "title": "4D-ARE: Bridging the Attribution Gap in LLM Agent Requirements Engineering",
    "abstract": "We deployed an LLM agent with ReAct reasoning and full data access. It executed flawlessly, yet when asked \"Why is completion rate 80%?\", it returned metrics instead of causal explanation. The agent knew how to reason but we had not specified what to reason about. This reflects a gap: runtime reasoning frameworks (ReAct, Chain-of-Thought) have transformed LLM agents, but design-time specification--determining what domain knowledge agents need--remains under-explored. We propose 4D-ARE (4-Dimensional Attribution-Driven Agent Requirements Engineering), a preliminary methodology for specifying attribution-driven agents. The core insight: decision-makers seek attribution, not answers. Attribution concerns organize into four dimensions (Results -> Process -> Support -> Long-term), motivated by Pearl's causal hierarchy. The framework operationalizes through five layers producing artifacts that compile directly to system prompts. We demonstrate the methodology through an industrial pilot deployment in financial services. 4D-ARE addresses what agents should reason about, complementing runtime frameworks that address how. We hypothesize systematic specification amplifies the power of these foundational advances. This paper presents a methodological proposal with preliminary industrial validation; rigorous empirical evaluation is planned for future work.",
    "arxiv_url": "https://arxiv.org/abs/2601.04556",
    "authors": [
      "Bo Yu",
      "Lei Zhao"
    ],
    "first_author": "Bo Yu",
    "category": [
      "Technical"
    ],
    "field": "Requirements & Design",
    "task": "Specification & Validation",
    "tags": [
      "Causal Attribution",
      "Agent Requirements",
      "Design-Time Specification",
      "Elicitation Methods",
      "Prompt Compilation",
      "Boundary Constraints",
      "Decision Support"
    ],
    "summary": "本文提出4D-ARE——一种基于因果归因的LLM代理需求工程方法，通过四个归因维度与五层规范架构将领域知识编译为系统提示，并在金融服务的工业试点中展示了初步验证。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.04556v1",
    "published": "2026-01-08",
    "update_time": "2026-01-08",
    "download_time": "2026-01-10 01:52:22"
  },
  {
    "id": "2601.05242",
    "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
    "abstract": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
    "arxiv_url": "https://arxiv.org/abs/2601.05242",
    "authors": [
      "Shih-Yang Liu",
      "Xin Dong",
      "Ximing Lu",
      "Shizhe Diao",
      "Peter Belcak",
      "Mingjie Liu",
      "Min-Hung Chen",
      "Hongxu Yin",
      "Yu-Chiang Frank Wang",
      "Kwang-Ting Cheng",
      "Yejin Choi",
      "Jan Kautz",
      "Pavlo Molchanov"
    ],
    "first_author": "Shih-Yang Liu",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Reinforcement Learning & Alignment",
    "task": "Multi-reward Policy Optimization",
    "tags": [
      "Multi-objective RL",
      "Reward Decoupling",
      "Group-wise Normalization",
      "Advantage Normalization",
      "Reward Collapse Analysis",
      "Training Stability",
      "Convergence Improvement",
      "Tool Calling",
      "Math Reasoning",
      "Code Reasoning"
    ],
    "summary": "本文提出GDPO，一种在多奖励强化学习中对每个奖励进行组内独立归一化并随后进行批次优势归一化的策略优化方法，以避免GRPO在多奖励设置下导致的奖励信号塌缩并显著提升训练稳定性与收敛性，在工具调用、数学与代码推理任务上均优于GRPO。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.05242v1",
    "published": "2026-01-08",
    "update_time": "2026-01-08",
    "download_time": "2026-01-10 01:52:49"
  },
  {
    "id": "2601.05214",
    "title": "Internal Representations as Indicators of Hallucinations in Agent Tool Selection",
    "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in tool calling and tool usage, but suffer from hallucinations where they choose incorrect tools, provide malformed parameters and exhibit 'tool bypass' behavior by performing simulations and generating outputs instead of invoking specialized tools or external systems. This undermines the reliability of LLM based agents in production systems as it leads to inconsistent results, and bypasses security and audit controls. Such hallucinations in agent tool selection require early detection and error handling. Unlike existing hallucination detection methods that require multiple forward passes or external validation, we present a computationally efficient framework that detects tool-calling hallucinations in real-time by leveraging LLMs' internal representations during the same forward pass used for generation. We evaluate this approach on reasoning tasks across multiple domains, demonstrating strong detection performance (up to 86.4\\% accuracy) while maintaining real-time inference capabilities with minimal computational overhead, particularly excelling at detecting parameter-level hallucinations and inappropriate tool selections, critical for reliable agent deployment.",
    "arxiv_url": "https://arxiv.org/abs/2601.05214",
    "authors": [
      "Kait Healy",
      "Bharathi Srinivasan",
      "Visakh Madathil",
      "Jing Wu"
    ],
    "first_author": "Kait Healy",
    "category": [
      "Technical"
    ],
    "field": "Agent Systems & Reliability",
    "task": "Tool-Calling Hallucination Detection",
    "tags": [
      "Hallucination",
      "Internal Representations",
      "Real-time Detection",
      "Unsupervised Masking",
      "Tool Bypass",
      "Parameter Error Detection",
      "Function Selection",
      "Lightweight Classifier",
      "Inference Efficiency",
      "Agent Reliability"
    ],
    "summary": "本文提出一种利用LLM最后一层内部表征进行实时、无监督的工具调用幻觉检测方法，通过对真实工具调用掩码重预测生成训练数据并训练轻量分类器，从而在多领域推理任务中高效识别不当工具选择和参数级错误，且推理开销极小。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.05214v1",
    "published": "2026-01-08",
    "update_time": "2026-01-08",
    "download_time": "2026-01-10 01:53:36"
  },
  {
    "id": "2601.04540",
    "title": "AdaptEval: A Benchmark for Evaluating Large Language Models on Code Snippet Adaptation",
    "abstract": "Recent advancements in large language models (LLMs) have automated various software engineering tasks, with benchmarks emerging to evaluate their capabilities. However, for adaptation, a critical activity during code reuse, there is no benchmark to assess LLMs' performance, leaving their practical utility in this area unclear. To fill this gap, we propose AdaptEval, a benchmark designed to evaluate LLMs on code snippet adaptation. Unlike existing benchmarks, AdaptEval incorporates the following three distinctive features: First, Practical Context. Tasks in AdaptEval are derived from developers' practices, preserving rich contextual information from Stack Overflow and GitHub communities. Second, Multi-granularity Annotation. Each task is annotated with requirements at both task and adaptation levels, supporting the evaluation of LLMs across diverse adaptation scenarios. Third, Fine-grained Evaluation. AdaptEval includes a two-tier testing framework combining adaptation-level and function-level tests, which enables evaluating LLMs' performance across various individual adaptations. Based on AdaptEval, we conduct the first empirical study to evaluate six instruction-tuned LLMs and especially three reasoning LLMs on code snippet adaptation. Experimental results demonstrate that AdaptEval enables the assessment of LLMs' adaptation capabilities from various perspectives. It also provides critical insights into their current limitations, particularly their struggle to follow explicit instructions. We hope AdaptEval can facilitate further investigation and enhancement of LLMs' capabilities in code snippet adaptation, supporting their real-world applications.",
    "arxiv_url": "https://arxiv.org/abs/2601.04540",
    "authors": [
      "Tanghaoran Zhang",
      "Xinjun Mao",
      "Shangwen Wang",
      "Yuxin Zhao",
      "Yao Lu",
      "Jin Zhang",
      "Zhang Zhang",
      "Kang Yang",
      "Yue Yu"
    ],
    "first_author": "Tanghaoran Zhang",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Editing",
    "tags": [
      "Code Snippet Adaptation",
      "Context-aware Adaptation",
      "Multi-Granularity Annotation",
      "Two-tier Evaluation",
      "Function-level Testing",
      "Adaptation-level Testing",
      "Instruction-following",
      "Reasoning LLMs"
    ],
    "summary": "本文提出AdaptEval基准，收集自真实的Stack Overflow与GitHub适配实例，提供多粒度注释与两层（适配级与函数级）测试框架，并基于此评测多款指令调优与推理型LLM以揭示其在代码片段适配任务中的能力与局限。",
    "quality": "High",
    "conference": "ASE (IEEE/ACM International Conference on Automated Software Engineering) 2025",
    "pdf_url": "https://arxiv.org/pdf/2601.04540v1",
    "published": "2026-01-08",
    "update_time": "2026-01-08",
    "download_time": "2026-01-11 02:04:27"
  },
  {
    "id": "2601.04526",
    "title": "Advancing Language Models for Code-related Tasks",
    "abstract": "Recent advances in language models (LMs) have driven significant progress in various software engineering tasks. However, existing LMs still struggle with complex programming scenarios due to limitations in data quality, model architecture, and reasoning capability. This research systematically addresses these challenges through three complementary directions: (1) improving code data quality with a code difference-guided adversarial augmentation technique (CODA) and a code denoising technique (CodeDenoise); (2) enhancing model architecture via syntax-guided code LMs (LEAM and LEAM++); and (3) advancing model reasoning with a prompting technique (muFiX) and an agent-based technique (Specine). These techniques aim to promote the practical adoption of LMs in software development and further advance intelligent software engineering.",
    "arxiv_url": "https://arxiv.org/abs/2601.04526",
    "authors": [
      "Zhao Tian"
    ],
    "first_author": "Zhao Tian",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Adversarial Augmentation",
      "Input Denoising",
      "Syntax-Guided Generation",
      "AST Representation",
      "Specification Alignment",
      "Test-Guided Prompting",
      "Agent-Based Alignment",
      "Robustness",
      "Mutation Testing"
    ],
    "summary": "本文提出一套系统性方法（CODA、CodeDenoise、LEAM/LEAM++、μFiX 与 Specine），通过改进代码数据质量、模型语法感知架构与推理/对齐机制，提升代码语言模型在生成与鲁棒性上的性能。",
    "quality": "High",
    "conference": "ICSE 2026",
    "pdf_url": "https://arxiv.org/pdf/2601.04526v1",
    "published": "2026-01-08",
    "update_time": "2026-01-08",
    "download_time": "2026-01-11 02:04:42"
  },
  {
    "id": "2601.05187",
    "title": "SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning",
    "abstract": "Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.",
    "arxiv_url": "https://arxiv.org/abs/2601.05187",
    "authors": [
      "Yanchang Liang",
      "Xiaowei Zhao"
    ],
    "first_author": "Yanchang Liang",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Requirements & Design",
    "task": "Specification & Validation",
    "tags": [
      "Graphical Modeling",
      "Simulink Integration",
      "Plan-Execute Agent",
      "Compact Model Representation",
      "Reflection-Guided RL",
      "Sparse-Reward Optimization",
      "Abstract-Reconstruct Augmentation",
      "In-Process Simulation",
      "On-Premise Training",
      "Benchmarking"
    ],
    "summary": "本文提出SimuAgent——一个面向Simulink的基于LLM的建模与仿真助手，采用紧凑的Python字典表示、两阶段计划-执行框架、引入Reflection-GRPO强化学习与Abstract–Reconstruct自监督增强，并发布包含5300任务的SimuBench基准，在低成本本地硬件上实现高效且隐私保护的工业级建模自动化。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.05187v1",
    "published": "2026-01-08",
    "update_time": "2026-01-08",
    "download_time": "2026-01-11 02:05:38"
  },
  {
    "id": "2601.05106",
    "title": "Token-Level LLM Collaboration via FusionRoute",
    "abstract": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
    "arxiv_url": "https://arxiv.org/abs/2601.05106",
    "authors": [
      "Nuoya Xiong",
      "Yuhang Zhou",
      "Hanqing Zeng",
      "Zhaorun Chen",
      "Furong Huang",
      "Shuchao Bi",
      "Lizhu Zhang",
      "Zhuokai Zhao"
    ],
    "first_author": "Nuoya Xiong",
    "category": [
      "Technical"
    ],
    "field": "Multi-LLM Collaboration",
    "task": "Token-level Routing & Logit Fusion",
    "tags": [
      "Token-level Routing",
      "Logit Fusion",
      "Complementary Generator",
      "Lightweight Router",
      "Mixture-of-Experts",
      "Router Post-training",
      "Theoretical Guarantees",
      "Robustness",
      "Efficiency",
      "Multi-Domain Coordination"
    ],
    "summary": "本文提出FusionRoute，一种在每步解码时由轻量级路由器进行专家选择并通过对数几率相加提供补充生成信号的逐标记多LLM协作框架，理论上拓展了策略类并在数学推理、代码生成和指令跟随等多领域上实验证明了其鲁棒性与效率优势。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.05106v1",
    "published": "2026-01-08",
    "update_time": "2026-01-08",
    "download_time": "2026-01-11 02:06:46"
  },
  {
    "id": "2601.05827",
    "title": "SSR: Safeguarding Staking Rewards by Defining and Detecting Logical Defects in DeFi Staking",
    "abstract": "Decentralized Finance (DeFi) staking is one of the most prominent applications within the DeFi ecosystem, where DeFi projects enable users to stake tokens on the platform and reward participants with additional tokens. However, logical defects in DeFi staking could enable attackers to claim unwarranted rewards by manipulating reward amounts, repeatedly claiming rewards, or engaging in other malicious actions. To mitigate these threats, we conducted the first study focused on defining and detecting logical defects in DeFi staking. Through the analysis of 64 security incidents and 144 audit reports, we identified six distinct types of logical defects, each accompanied by detailed descriptions and code examples. Building on this empirical research, we developed SSR (Safeguarding Staking Reward), a static analysis tool designed to detect logical defects in DeFi staking contracts. SSR utilizes a large language model (LLM) to extract fundamental information about staking logic and constructs a DeFi staking model. It then identifies logical defects by analyzing the model and the associated semantic features. We constructed a ground truth dataset based on known security incidents and audit reports to evaluate the effectiveness of SSR. The results indicate that SSR achieves an overall precision of 92.31%, a recall of 87.92%, and an F1-score of 88.85%. Additionally, to assess the prevalence of logical defects in real-world smart contracts, we compiled a large-scale dataset of 15,992 DeFi staking contracts. SSR detected that 3,557 (22.24%) of these contracts contained at least one logical defect.",
    "arxiv_url": "https://arxiv.org/abs/2601.05827",
    "authors": [
      "Zewei Lin",
      "Jiachi Chen",
      "Jingwen Zhang",
      "Zexu Wang",
      "Yuming Feng",
      "Weizhe Zhang",
      "Zibin Zheng"
    ],
    "first_author": "Zewei Lin",
    "category": [
      "Empirical",
      "Technical",
      "Benchmark"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "DeFi Staking",
      "Logical Defect Classification",
      "LLM-guided Extraction",
      "Static Analysis",
      "Reward Manipulation",
      "State Update Omission",
      "Unauthorized Asset Access",
      "Single Pool Reliance",
      "Empirical Measurement",
      "Dataset Release"
    ],
    "summary": "本文通过分析64起安全事件和144份审计报告，归纳出面向DeFi质押的六类逻辑缺陷，并提出SSR——一种结合LLM信息抽取与静态分析的检测工具，在人工构建的基准和1.6万余合约的大规模测评中表现出高精度与召回率并开放了数据与代码。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.05827v1",
    "published": "2026-01-09",
    "update_time": "2026-01-09",
    "download_time": "2026-01-12 02:01:09"
  },
  {
    "id": "2601.05777",
    "title": "EET: Experience-Driven Early Termination for Cost-Efficient Software Engineering Agents",
    "abstract": "Software engineering (SE) agents powered by large language models are increasingly adopted in practice, yet they often incur substantial monetary cost. We introduce EET, an experience-driven early termination approach that reduces the cost of SE agents while preserving task performance. EET extracts structured experience from prior issue-resolution executions and leverages it to guide early termination during patch generation and selection, reducing unproductive iterations. We evaluate EET on the SWE-bench Verified benchmark across three representative SE agents. EET consistently reduces total cost by 19%-55% (32% on average), with negligible loss in resolution rate (at most 0.2%). These efficiency gains are achieved, on average, by identifying early-termination opportunities for 11% of issues and reducing API calls, input tokens, and output tokens by 21%, 30%, and 25%, respectively. We release the code, prompts, and data at https://github.com/EffiSEAgent/EET.",
    "arxiv_url": "https://arxiv.org/abs/2601.05777",
    "authors": [
      "Yaoqi Guo",
      "Ying Xiao",
      "Jie M. Zhang",
      "Mark Harman",
      "Yiling Lou",
      "Yang Liu",
      "Zhenpeng Chen"
    ],
    "first_author": "Yaoqi Guo",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Editing",
    "tags": [
      "Program Repair",
      "Code Agents",
      "Early Stopping",
      "Experience Retrieval",
      "Execution Trajectory Abstraction",
      "Cost Efficiency",
      "Patch Selection"
    ],
    "summary": "本文提出EET，一种将历史执行轨迹提炼为结构化经验并在补丁生成与选择阶段进行检索驱动的早停机制，能在几乎不损失修复率的情况下显著降低软件工程代理的成本与令牌使用。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.05777v1",
    "published": "2026-01-09",
    "update_time": "2026-01-09",
    "download_time": "2026-01-12 02:01:26"
  },
  {
    "id": "2601.04996",
    "title": "AlgBench: To What Extent Do Large Reasoning Models Understand Algorithms?",
    "abstract": "Reasoning ability has become a central focus in the advancement of Large Reasoning Models (LRMs). Although notable progress has been achieved on several reasoning benchmarks such as MATH500 and LiveCodeBench, existing benchmarks for algorithmic reasoning remain limited, failing to answer a critical question: Do LRMs truly master algorithmic reasoning? To answer this question, we propose AlgBench, an expert-curated benchmark that evaluates LRMs under an algorithm-centric paradigm.   AlgBench consists of over 3,000 original problems spanning 27 algorithms, constructed by ACM algorithmic experts and organized under a comprehensive taxonomy, including Euclidean-structured, non-Euclidean-structured, non-optimized, local-optimized, global-optimized, and heuristic-optimized categories. Empirical evaluations on leading LRMs (e.g., Gemini-3-Pro, DeepSeek-v3.2-Speciale and GPT-o3) reveal substantial performance heterogeneity: while models perform well on non-optimized tasks (up to 92%), accuracy drops sharply to around 49% on globally optimized algorithms such as dynamic programming. Further analysis uncovers \\textbf{strategic over-shifts}, wherein models prematurely abandon correct algorithmic designs due to necessary low-entropy tokens. These findings expose fundamental limitations of problem-centric reinforcement learning and highlight the necessity of an algorithm-centric training paradigm for robust algorithmic reasoning.",
    "arxiv_url": "https://arxiv.org/abs/2601.04996",
    "authors": [
      "Henan Sun",
      "Kaichi Yu",
      "Yuyao Wang",
      "Bowen Liu",
      "Xunkai Li",
      "Rong-Hua Li",
      "Nuo Chen",
      "Jia Li"
    ],
    "first_author": "Henan Sun",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Algorithm-centric Benchmark",
      "Algorithm Taxonomy",
      "Contamination Control",
      "Expert-curated Problems",
      "Performance Heterogeneity",
      "Dynamic Programming Weakness",
      "Strategic Over-shift",
      "Global Optimization"
    ],
    "summary": "本文提出AlgBench——一个由算法竞赛/ACM专家手工构建的、包含3000+原始题目和新算法分类的算法推理基准，并在多款大型推理模型上实证评估，揭示模型在全局优化类算法（如动态规划）上的显著弱点、性能异质性以及因低熵必要token导致的策略性过早放弃问题。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.04996v1",
    "published": "2026-01-08",
    "update_time": "2026-01-08",
    "download_time": "2026-01-12 02:05:06"
  },
  {
    "id": "2601.04920",
    "title": "Conversational AI for Rapid Scientific Prototyping: A Case Study on ESA's ELOPE Competition",
    "abstract": "Large language models (LLMs) are increasingly used as coding partners, yet their role in accelerating scientific discovery remains underexplored. This paper presents a case study of using ChatGPT for rapid prototyping in ESA's ELOPE (Event-based Lunar OPtical flow Egomotion estimation) competition. The competition required participants to process event camera data to estimate lunar lander trajectories. Despite joining late, we achieved second place with a score of 0.01282, highlighting the potential of human-AI collaboration in competitive scientific settings. ChatGPT contributed not only executable code but also algorithmic reasoning, data handling routines, and methodological suggestions, such as using fixed number of events instead of fixed time spans for windowing. At the same time, we observed limitations: the model often introduced unnecessary structural changes, gets confused by intermediate discussions about alternative ideas, occasionally produced critical errors and forgets important aspects in longer scientific discussions. By analyzing these strengths and shortcomings, we show how conversational AI can both accelerate development and support conceptual insight in scientific research. We argue that structured integration of LLMs into the scientific workflow can enhance rapid prototyping by proposing best practices for AI-assisted scientific work.",
    "arxiv_url": "https://arxiv.org/abs/2601.04920",
    "authors": [
      "Nils Einecke"
    ],
    "first_author": "Nils Einecke",
    "category": [
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Prompting",
    "tags": [
      "Human-LLM Interaction",
      "Interactive Coding",
      "Scientific Prototyping",
      "Algorithmic Reasoning",
      "Event-based Vision",
      "Egomotion Estimation",
      "Hallucination",
      "Context Drift",
      "Best Practices"
    ],
    "summary": "本文以在ESA ELOPE竞赛中与ChatGPT协作为案例，探讨对话式AI在快速科学原型开发中的作用、优势（如算法建议与数据处理代码）与局限（如结构性改动、记忆衰减与关键错误），并提出整合LLM入科研流程的实践建议。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.04920v1",
    "published": "2026-01-08",
    "update_time": "2026-01-08",
    "download_time": "2026-01-12 02:05:57"
  },
  {
    "id": "2601.05772",
    "title": "StriderSPD: Structure-Guided Joint Representation Learning for Binary Security Patch Detection",
    "abstract": "Vulnerabilities severely threaten software systems, making the timely application of security patches crucial for mitigating attacks. However, software vendors often silently patch vulnerabilities with limited disclosure, where Security Patch Detection (SPD) comes to protect software assets. Recently, most SPD studies have targeted Open-Source Software (OSS), yet a large portion of real-world software is closed-source, where patches are distributed as binaries without accessible source code. The limited binary SPD approaches often lift binaries to abstraction levels, i.e., assembly code or pseudo-code. However, assembly code is register-based instructions conveying limited semantics, while pseudo-code lacks parser-compatible grammar to extract structure, both hindering accurate vulnerability-fix representation learning. In addition, previous studies often obtain training and testing data from the same project for evaluation, which fails to reflect closed-source conditions. To alleviate the above challenges, we propose \\textbf{\\textit{StriderSPD}}, a \\underline{Str}ucture-gu\\underline{ide}d joint \\underline{r}epresentation \\underline{SPD} framework of binary code that integrates a graph branch into a large language model (LLM), leveraging structural information to guide the LLM in identifying security patches. Our novel design of the adapters in the graph branch effectively aligns the representations between assembly code and pseudo-code at the LLM's token level. We further present a two-stage training strategy to address the optimization imbalance caused by the large parameter disparity between StriderSPD's two branches, which enables proper branch fitting. To enable more realistic evaluation, we construct a binary SPD benchmark that is disjoint from prior datasets in both projects and domains and extensively evaluate StriderSPD on this benchmark.",
    "arxiv_url": "https://arxiv.org/abs/2601.05772",
    "authors": [
      "Qingyuan Li",
      "Chenchen Yu",
      "Chuanyi Li",
      "Xin-Cheng Wen",
      "Cheryl Lee",
      "Cuiyun Gao",
      "Bin Luo"
    ],
    "first_author": "Qingyuan Li",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "Binary Patch Classification",
      "Graph-guided LLM",
      "CFG GNN",
      "Adapter-mediated Alignment",
      "Two-stage Training",
      "Cross-project Benchmark",
      "Pseudo-code Semantics",
      "Inference Efficiency",
      "Cross-model Generalizability",
      "Security and Vulnerabilities"
    ],
    "summary": "本文提出 StriderSPD，将基于控制流图的图分支与大模型分支通过三路适配器在 token 级别对齐，并采用两阶段训练与跨项目跨域的二进制补丁基准，实现了显著提升的安全补丁检测效果与高效推理性能。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.05772v1",
    "published": "2026-01-09",
    "update_time": "2026-01-09",
    "download_time": "2026-01-13 01:52:31"
  },
  {
    "id": "2601.05752",
    "title": "AutoMonitor-Bench: Evaluating the Reliability of LLM-Based Misbehavior Monitor",
    "abstract": "We introduce AutoMonitor-Bench, the first benchmark designed to systematically evaluate the reliability of LLM-based misbehavior monitors across diverse tasks and failure modes. AutoMonitor-Bench consists of 3,010 carefully annotated test samples spanning question answering, code generation, and reasoning, with paired misbehavior and benign instances. We evaluate monitors using two complementary metrics: Miss Rate (MR) and False Alarm Rate (FAR), capturing failures to detect misbehavior and oversensitivity to benign behavior, respectively. Evaluating 12 proprietary and 10 open-source LLMs, we observe substantial variability in monitoring performance and a consistent trade-off between MR and FAR, revealing an inherent safety-utility tension. To further explore the limits of monitor reliability, we construct a large-scale training corpus of 153,581 samples and fine-tune Qwen3-4B-Instruction to investigate whether training on known, relatively easy-to-construct misbehavior datasets improves monitoring performance on unseen and more implicit misbehaviors. Our results highlight the challenges of reliable, scalable misbehavior monitoring and motivate future work on task-aware designing and training strategies for LLM-based monitors.",
    "arxiv_url": "https://arxiv.org/abs/2601.05752",
    "authors": [
      "Shu Yang",
      "Jingyu Hu",
      "Tong Li",
      "Hanqi Yan",
      "Wenxuan Wang",
      "Di Wang"
    ],
    "first_author": "Shu Yang",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Model Safety & Monitoring",
    "task": "Misbehavior Detection",
    "tags": [
      "Misbehavior Monitoring",
      "Miss Rate",
      "False Alarm Rate",
      "Paired Instances",
      "Cross-model Evaluation",
      "Monitor Fine-Tuning",
      "Generalization Gap",
      "Specification Gaming",
      "Sycophancy",
      "Safety Violations"
    ],
    "summary": "本文提出了AutoMonitor-Bench——一个含3,010个（误行为/良性）配对样本的基准，用以系统评估基于LLM的误行为监控器，采用Miss Rate与False Alarm Rate衡量，在12款专有与10款开源模型上进行大规模评测并探讨了通过153K样本微调提升监控器泛化能力的局限性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.05752v1",
    "published": "2026-01-09",
    "update_time": "2026-01-09",
    "download_time": "2026-01-13 01:52:54"
  },
  {
    "id": "2601.05755",
    "title": "VIGIL: Defending LLM Agents Against Tool Stream Injection via Verify-Before-Commit",
    "abstract": "LLM agents operating in open environments face escalating risks from indirect prompt injection, particularly within the tool stream where manipulated metadata and runtime feedback hijack execution flow. Existing defenses encounter a critical dilemma as advanced models prioritize injected rules due to strict alignment while static protection mechanisms sever the feedback loop required for adaptive reasoning. To reconcile this conflict, we propose \\textbf{VIGIL}, a framework that shifts the paradigm from restrictive isolation to a verify-before-commit protocol. By facilitating speculative hypothesis generation and enforcing safety through intent-grounded verification, \\textbf{VIGIL} preserves reasoning flexibility while ensuring robust control. We further introduce \\textbf{SIREN}, a benchmark comprising 959 tool stream injection cases designed to simulate pervasive threats characterized by dynamic dependencies. Extensive experiments demonstrate that \\textbf{VIGIL} outperforms state-of-the-art dynamic defenses by reducing the attack success rate by over 22\\% while more than doubling the utility under attack compared to static baselines, thereby achieving an optimal balance between security and utility. Code is available at https://anonymous.4open.science/r/VIGIL-378B/.",
    "arxiv_url": "https://arxiv.org/abs/2601.05755",
    "authors": [
      "Junda Lin",
      "Zhaomeng Zhou",
      "Zhi Zheng",
      "Shuochen Liu",
      "Tong Xu",
      "Yong Chen",
      "Enhong Chen"
    ],
    "first_author": "Junda Lin",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Agent Security",
    "task": "Tool Stream Injection Defense",
    "tags": [
      "Tool Stream Injection",
      "Verify-Before-Commit",
      "Intent-Grounded Verification",
      "Speculative Reasoning",
      "Runtime Verification",
      "Perception Sanitization",
      "Adaptive Backtracking",
      "Dynamic Replanning",
      "Agent Robustness Evaluation",
      "Alignment-Driven Vulnerability"
    ],
    "summary": "本文提出VIGIL——一种在执行前验证的LLM代理防御框架，通过意图约束、感知清洗、推测性回溯与运行时验证来抵御工具流注入，并构建了覆盖多向量注入的评测基准以验证其在降低攻击成功率同时保持实用性的效果。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.05755v1",
    "published": "2026-01-09",
    "update_time": "2026-01-09",
    "download_time": "2026-01-13 01:55:29"
  },
  {
    "id": "2601.05587",
    "title": "HogVul: Black-box Adversarial Code Generation Framework Against LM-based Vulnerability Detectors",
    "abstract": "Recent advances in software vulnerability detection have been driven by Language Model (LM)-based approaches. However, these models remain vulnerable to adversarial attacks that exploit lexical and syntax perturbations, allowing critical flaws to evade detection. Existing black-box attacks on LM-based vulnerability detectors primarily rely on isolated perturbation strategies, limiting their ability to efficiently explore the adversarial code space for optimal perturbations. To bridge this gap, we propose HogVul, a black-box adversarial code generation framework that integrates both lexical and syntax perturbations under a unified dual-channel optimization strategy driven by Particle Swarm Optimization (PSO). By systematically coordinating two-level perturbations, HogVul effectively expands the search space for adversarial examples, enhancing the attack efficacy. Extensive experiments on four benchmark datasets demonstrate that HogVul achieves an average attack success rate improvement of 26.05\\% over state-of-the-art baseline methods. These findings highlight the potential of hybrid optimization strategies in exposing model vulnerabilities.",
    "arxiv_url": "https://arxiv.org/abs/2601.05587",
    "authors": [
      "Jingxiao Yang",
      "Ping He",
      "Tianyu Du",
      "Sun Bing",
      "Xuhong Zhang"
    ],
    "first_author": "Jingxiao Yang",
    "category": [
      "Technical"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "Adversarial Code Generation",
      "Black-box Attack",
      "Lexical Perturbation",
      "Syntax Perturbation",
      "Particle Swarm Optimization",
      "Dual-channel Optimization",
      "Semantic-aware Initialization",
      "Dead-code Injection",
      "Robustness Evaluation"
    ],
    "summary": "本文提出 HogVul，一种基于粒子群优化的黑盒对抗代码生成框架，通过联合词法与语法级扰动并在双通道协同优化下生成可编译的对抗样本，从而显著提升对基于语言模型的漏洞检测器的绕过成功率并验证了方法的有效性与稳健性。",
    "quality": "High",
    "conference": "AAAI Conference on Artificial Intelligence 2026",
    "pdf_url": "https://arxiv.org/pdf/2601.05587v1",
    "published": "2026-01-09",
    "update_time": "2026-01-09",
    "download_time": "2026-01-13 01:57:38"
  },
  {
    "id": "2601.07786",
    "title": "\"TODO: Fix the Mess Gemini Created\": Towards Understanding GenAI-Induced Self-Admitted Technical Debt",
    "abstract": "As large language models (LLMs) such as ChatGPT, Copilot, Claude, and Gemini become integrated into software development workflows, developers increasingly leave traces of AI involvement in their code comments. Among these, some comments explicitly acknowledge both the use of generative AI and the presence of technical shortcomings. Analyzing 6,540 LLM-referencing code comments from public Python and JavaScript-based GitHub repositories (November 2022-July 2025), we identified 81 that also self-admit technical debt(SATD). Developers most often describe postponed testing, incomplete adaptation, and limited understanding of AI-generated code, suggesting that AI assistance affects both when and why technical debt emerges. We term GenAI-Induced Self-admitted Technical debt (GIST) as a proposed conceptual lens to describe recurring cases where developers incorporate AI-generated code while explicitly expressing uncertainty about its behavior or correctness.",
    "arxiv_url": "https://arxiv.org/abs/2601.07786",
    "authors": [
      "Abdullah Al Mujahid",
      "Mia Mohammad Imran"
    ],
    "first_author": "Abdullah Al Mujahid",
    "category": [
      "Empirical",
      "Benchmark"
    ],
    "field": "Requirements & Design",
    "task": "Analysis",
    "tags": [
      "GenAI-Induced Technical Debt",
      "Self-Admitted Technical Debt",
      "Code Comment Mining",
      "AI Attribution",
      "Requirement Debt",
      "Test Debt",
      "Design Debt",
      "Open Coding"
    ],
    "summary": "本文通过对公共GitHub中显式提及LLM的代码注释进行手工标注与分析，提出并实证化了“生成式AI引发的自我承认技术债（GIST）”概念，揭示了AI参与代码时债务类型的分布变化及开发者对AI角色的归因方式。",
    "quality": "Middle",
    "conference": "9th International Conference on Technical Debt (TechDebt 2026) 2026",
    "pdf_url": "https://arxiv.org/pdf/2601.07786v1",
    "published": "2026-01-12",
    "update_time": "2026-01-12",
    "download_time": "2026-01-14 01:59:42"
  },
  {
    "id": "2601.07602",
    "title": "OODEval: Evaluating Large Language Models on Object-Oriented Design",
    "abstract": "Recent advances in large language models (LLMs) have driven extensive evaluations in software engineering. however, most prior work concentrates on code-level tasks, leaving software design capabilities underexplored. To fill this gap, we conduct a comprehensive empirical study evaluating 29 LLMs on object-oriented design (OOD) tasks. Owing to the lack of standardized benchmarks and metrics, we introduce OODEval, a manually constructed benchmark comprising 50 OOD tasks of varying difficulty, and OODEval-Human, the first human-rated OOD benchmark, which includes 940 undergraduate-submitted class diagrams evaluated by instructors. We further propose CLUE (Class Likeness Unified Evaluation), a unified metric set that assesses both global correctness and fine-grained design quality in class diagram generation. Using these benchmarks and metrics, we investigate five research questions: overall correctness, comparison with humans, model dimension analysis, task feature analysis, and bad case analysis. The results indicate that while LLMs achieve high syntactic accuracy, they exhibit substantial semantic deficiencies, particularly in method and relationship generation. Among the evaluated models, Qwen3-Coder-30B achieves the best overall performance, rivaling DeepSeek-R1 and GPT-4o, while Gemma3-4B-IT outperforms GPT-4o-Mini despite its smaller parameter scale. Although top-performing LLMs nearly match the average performance of undergraduates, they remain significantly below the level of the best human designers. Further analysis shows that parameter scale, code specialization, and instruction tuning strongly influence performance, whereas increased design complexity and lower requirement readability degrade it. Bad case analysis reveals common failure modes, including keyword misuse, missing classes or relationships, and omitted methods.",
    "arxiv_url": "https://arxiv.org/abs/2601.07602",
    "authors": [
      "Bingxu Xiao",
      "Yunwei Dong",
      "Yiqi Tang",
      "Manqing Zhang",
      "Yifan Zhou",
      "Chunyan Ma",
      "Yepang Liu"
    ],
    "first_author": "Bingxu Xiao",
    "category": [
      "Benchmark",
      "Empirical",
      "Technical"
    ],
    "field": "Requirements & Design",
    "task": "Specification & Validation",
    "tags": [
      "Software Modeling",
      "Class Diagram Evaluation",
      "Human-Rated Benchmark",
      "CLUE Metric",
      "Requirement-to-Design",
      "Method Generation",
      "Relationship Generation",
      "Semantic Correctness",
      "Syntactic Correctness",
      "Failure Modes",
      "Model Scaling Analysis",
      "Instruction Tuning Effects"
    ],
    "summary": "本文提出了面向面向对象设计的基准OODEval及人工评分数据集OODEval-Human，并设计了CLUE评估指标，基于此对29个大模型进行系统实证评估，揭示模型在语法上较强但在方法与关系等语义层面存在明显不足。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.07602v1",
    "published": "2026-01-12",
    "update_time": "2026-01-12",
    "download_time": "2026-01-14 02:00:01"
  },
  {
    "id": "2601.07790",
    "title": "Benchmarking Small Language Models and Small Reasoning Language Models on System Log Severity Classification",
    "abstract": "System logs are crucial for monitoring and diagnosing modern computing infrastructure, but their scale and complexity require reliable and efficient automated interpretation. Since severity levels are predefined metadata in system log messages, having a model merely classify them offers limited standalone practical value, revealing little about its underlying ability to interpret system logs. We argue that severity classification is more informative when treated as a benchmark for probing runtime log comprehension rather than as an end task. Using real-world journalctl data from Linux production servers, we evaluate nine small language models (SLMs) and small reasoning language models (SRLMs) under zero-shot, few-shot, and retrieval-augmented generation (RAG) prompting. The results reveal strong stratification. Qwen3-4B achieves the highest accuracy at 95.64% with RAG, while Gemma3-1B improves from 20.25% under few-shot prompting to 85.28% with RAG. Notably, the tiny Qwen3-0.6B reaches 88.12% accuracy despite weak performance without retrieval. In contrast, several SRLMs, including Qwen3-1.7B and DeepSeek-R1-Distill-Qwen-1.5B, degrade substantially when paired with RAG. Efficiency measurements further separate models: most Gemma and Llama variants complete inference in under 1.2 seconds per log, whereas Phi-4-Mini-Reasoning exceeds 228 seconds per log while achieving <10% accuracy. These findings suggest that (1) architectural design, (2) training objectives, and (3) the ability to integrate retrieved context under strict output constraints jointly determine performance. By emphasizing small, deployable models, this benchmark aligns with real-time requirements of digital twin (DT) systems and shows that severity classification serves as a lens for evaluating model competence and real-time deployability, with implications for root cause analysis (RCA) and broader DT integration.",
    "arxiv_url": "https://arxiv.org/abs/2601.07790",
    "authors": [
      "Yahya Masri",
      "Emily Ma",
      "Zifu Wang",
      "Joseph Rogers",
      "Chaowei Yang"
    ],
    "first_author": "Yahya Masri",
    "category": [
      "Empirical",
      "Benchmark"
    ],
    "field": "AIOps",
    "task": "Log Parsing",
    "tags": [
      "Severity Classification",
      "Retrieval-Augmented Reasoning",
      "Small Model Evaluation",
      "Inference Efficiency",
      "Model Stratification",
      "Grounded Log Understanding",
      "Zero/Few-shot Robustness",
      "Digital Twin Integration"
    ],
    "summary": "该论文使用真实生产服务器日志，以零/少样本和检索增强提示评估多款小型与小型推理语言模型在日志严重性分类上的准确性与推理效率，揭示结构、训练目标与检索集成能力对性能与实时可部署性的影响。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.07790v1",
    "published": "2026-01-12",
    "update_time": "2026-01-12",
    "download_time": "2026-01-14 02:00:41"
  },
  {
    "id": "2601.07782",
    "title": "Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning",
    "abstract": "LLM agents operating over massive, dynamic tool libraries rely on effective retrieval, yet standard single-shot dense retrievers struggle with complex requests. These failures primarily stem from the disconnect between abstract user goals and technical documentation, and the limited capacity of fixed-size embeddings to model combinatorial tool compositions. To address these challenges, we propose TOOLQP, a lightweight framework that models retrieval as iterative query planning. Instead of single-shot matching, TOOLQP decomposes instructions into sub-tasks and dynamically generates queries to interact with the retriever, effectively bridging the semantic gap by targeting the specific sub-tasks required for composition. We train TOOLQP using synthetic query trajectories followed by optimization via Reinforcement Learning with Verifiable Rewards (RLVR). Experiments demonstrate that TOOLQP achieves state-of-the-art performance, exhibiting superior zero-shot generalization, robustness across diverse retrievers, and significant improvements in downstream agentic execution.",
    "arxiv_url": "https://arxiv.org/abs/2601.07782",
    "authors": [
      "Wei Fang",
      "James Glass"
    ],
    "first_author": "Wei Fang",
    "category": [
      "Technical"
    ],
    "field": "Agent Tooling",
    "task": "Tool Retrieval (Query Planning)",
    "tags": [
      "Query Planning",
      "Iterative Retrieval",
      "Tool Composition",
      "Semantic Gap Bridging",
      "Retriever-Agnostic",
      "Reinforcement Learning for Retrieval",
      "Environment Interaction",
      "Downstream Agent Grounding",
      "Zero-shot Generalization"
    ],
    "summary": "本文提出TOOLQP，一种将工具检索建模为迭代查询规划的轻量框架，通过将复杂请求分解为子任务、动态生成子查询并结合合成轨迹与可验证奖励的强化学习优化，从而在大规模动态工具库中显著提升检索准确性和下游代理执行效果。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.07782v1",
    "published": "2026-01-12",
    "update_time": "2026-01-12",
    "download_time": "2026-01-14 02:01:01"
  },
  {
    "id": "2601.08806",
    "title": "APEX-SWE",
    "abstract": "We introduce the AI Productivity Index for Software Engineering (APEX-SWE), a benchmark for assessing whether frontier AI models can execute economically valuable software engineering work. Unlike existing evaluations that focus on narrow, well-defined tasks, APEX-SWE assesses two novel task types that reflect real-world software engineering work: (1) Integration tasks (n=100), which require constructing end-to-end systems across heterogeneous cloud primitives, business applications, and infrastructure-as-code services, and (2) Observability tasks (n=100), which require debugging production failures using telemetry signals such as logs and dashboards, as well as unstructured context. We evaluated eight frontier models on APEX-SWE. Gemini 3 Pro (Thinking = High) performs best, with a Pass@1 score of 25\\%. Our analysis shows that strong performance is primarily driven by epistemic reasoning, defined as the ability to distinguish between assumptions and verified facts, combined with agency to resolve uncertainty prior to acting. We open-source the APEX-SWE evaluation harness and a dev set (n=50).",
    "arxiv_url": "https://arxiv.org/abs/2601.08806",
    "authors": [
      "Abhi Kottamasu",
      "Akul Datta",
      "Aakash Barthwal",
      "Chirag Mahapatra",
      "Ajay Arun",
      "Adarsh Hiremath",
      "Brendan Foody",
      "Bertie Vidgen"
    ],
    "first_author": "Abhi Kottamasu",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Repository-Level Coding",
    "tags": [
      "Repository-Level Coding",
      "Code Agents",
      "Production Debugging",
      "Observability",
      "End-to-End Integration",
      "Infrastructure-as-Code",
      "Telemetry Analysis",
      "Epistemic Reasoning"
    ],
    "summary": "APEX–SWE提出了一个包含100个集成任务和100个可观测性任务的基准，用于评估前沿AI模型在真实生产级软件工程（跨云原语、业务应用、基础设施即代码与生产故障排查）中的实际执行能力，并开源了开发集与评估工具，发现顶级模型Pass@1约为25%，成功往往依赖于区分假设与已验证事实的认识型推理及在行动前化解不确定性的能力。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.08806v1",
    "published": "2026-01-13",
    "update_time": "2026-01-13",
    "download_time": "2026-01-15 01:56:27"
  },
  {
    "id": "2601.08773",
    "title": "Reliable Graph-RAG for Codebases: AST-Derived Graphs vs LLM-Extracted Knowledge Graphs",
    "abstract": "Retrieval-Augmented Generation for software engineering often relies on vector similarity search, which captures topical similarity but can fail on multi-hop architectural reasoning such as controller to service to repository chains, interface-driven wiring, and inheritance. This paper benchmarks three retrieval pipelines on Java codebases (Shopizer, with additional runs on ThingsBoard and OpenMRS Core): (A) vector-only No-Graph RAG, (B) an LLM-generated knowledge graph RAG (LLM-KB), and (C) a deterministic AST-derived knowledge graph RAG (DKB) built with Tree-sitter and bidirectional traversal.   Using 15 architecture and code-tracing queries per repository, we measure indexing time, query latency, corpus coverage, cost, and answer correctness. DKB builds its graph in seconds, while LLM-KB requires much longer graph generation. LLM-KB also shows indexing incompleteness: on Shopizer, 377 files are skipped or missed, reducing embedded chunk coverage and graph size compared to DKB. End-to-end cost is modest for DKB relative to the vector-only baseline but much higher for LLM-KB, especially as repository scale increases. Query latency is similar for No-Graph and DKB, while LLM-KB is slower and more variable. On the Shopizer question suite, DKB achieves the highest correctness, LLM-KB is close behind, and the vector-only baseline performs worst on upstream architectural queries and has the highest hallucination risk. Overall, deterministic AST-derived graphs provide more reliable coverage and multi-hop grounding than LLM-extracted graphs at substantially lower indexing cost.",
    "arxiv_url": "https://arxiv.org/abs/2601.08773",
    "authors": [
      "Manideep Reddy Chinthareddy"
    ],
    "first_author": "Manideep Reddy Chinthareddy",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Retrieval",
    "tags": [
      "AST-derived Graph",
      "LLM-extracted Graph",
      "Graph-RAG",
      "Multi-hop Reasoning",
      "Repository-Level Retrieval",
      "Indexing Coverage",
      "Cost Analysis",
      "Deterministic Indexing",
      "Tree-sitter Parsing",
      "Hallucination"
    ],
    "summary": "本文在多个 Java 代码库上对比了向量检索、LLM 生成知识图谱和基于 AST 的确定性知识图谱三种 RAG 管线，结果表明基于 AST 的 DKB 在索引覆盖率、答案正确性与成本-延迟权衡上更优且更可复现，而 LLM 生成的图谱存在提取不完整、随机性高和成本显著更大的问题。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.08773v1",
    "published": "2026-01-13",
    "update_time": "2026-01-13",
    "download_time": "2026-01-15 01:56:54"
  },
  {
    "id": "2601.08778",
    "title": "Pervasive Annotation Errors Break Text-to-SQL Benchmarks and Leaderboards",
    "abstract": "Researchers have proposed numerous text-to-SQL techniques to streamline data analytics and accelerate the development of database-driven applications. To compare these techniques and select the best one for deployment, the community depends on public benchmarks and their leaderboards. Since these benchmarks heavily rely on human annotations during question construction and answer evaluation, the validity of the annotations is crucial.   In this paper, we conduct an empirical study that (i) benchmarks annotation error rates for two widely used text-to-SQL benchmarks, BIRD and Spider 2.0-Snow, and (ii) corrects a subset of the BIRD development (Dev) set to measure the impact of annotation errors on text-to-SQL agent performance and leaderboard rankings. Through expert analysis, we show that BIRD Mini-Dev and Spider 2.0-Snow have error rates of 52.8% and 62.8%, respectively. We re-evaluate all 16 open-source agents from the BIRD leaderboard on both the original and the corrected BIRD Dev subsets. We show that performance changes range from -7% to 31% (in relative terms) and rank changes range from $-9$ to $+9$ positions. We further assess whether these impacts generalize to the full BIRD Dev set. We find that the rankings of agents on the uncorrected subset correlate strongly with those on the full Dev set (Spearman's $r_s$=0.85, $p$=3.26e-5), whereas they correlate weakly with those on the corrected subset (Spearman's $r_s$=0.32, $p$=0.23). These findings show that annotation errors can significantly distort reported performance and rankings, potentially misguiding research directions or deployment choices. Our code and data are available at https://github.com/uiuc-kang-lab/text_to_sql_benchmarks.",
    "arxiv_url": "https://arxiv.org/abs/2601.08778",
    "authors": [
      "Tengjun Jin",
      "Yoojin Choi",
      "Yuxuan Zhu",
      "Daniel Kang"
    ],
    "first_author": "Tengjun Jin",
    "category": [
      "Empirical",
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Translation",
    "tags": [
      "Annotation Errors",
      "SQL Verification",
      "Agent-based Review",
      "Annotation Pipeline",
      "Diagnostic Reporting",
      "Human-in-the-loop",
      "Leaderboard Reliability",
      "Ranking Sensitivity",
      "Schema-aware Validation",
      "Execution-based Verification"
    ],
    "summary": "本文通过开发基于代理的审查工具并对 BIRD 与 Spider 2.0-Snow 进行人工验证，发现两大 text-to-SQL 基准分别存在 52.8% 和 62.8% 的标注错误，进而显著扭曲模型性能与排行榜，并提出 SAR-Agent 与 SAPAR 来检测与修正这些注释错误。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.08778v1",
    "published": "2026-01-13",
    "update_time": "2026-01-13",
    "download_time": "2026-01-15 02:06:24"
  },
  {
    "id": "2601.08777",
    "title": "Asymptotic Universal Alignment: A New Alignment Framework via Test-Time Scaling",
    "abstract": "Aligning large language models (LLMs) to serve users with heterogeneous and potentially conflicting preferences is a central challenge for personalized and trustworthy AI. We formalize an ideal notion of universal alignment through test-time scaling: for each prompt, the model produces $k\\ge 1$ candidate responses and a user selects their preferred one. We introduce $(k,f(k))$-robust alignment, which requires the $k$-output model to have win rate $f(k)$ against any other single-output model, and asymptotic universal alignment (U-alignment), which requires $f(k)\\to 1$ as $k\\to\\infty$. Our main result characterizes the optimal convergence rate: there exists a family of single-output policies whose $k$-sample product policies achieve U-alignment at rate $f(k)=\\frac{k}{k+1}$, and no method can achieve a faster rate in general.   We show that popular post-training methods, including Nash learning from human feedback (NLHF), can fundamentally underutilize the benefits of test-time scaling. Even though NLHF is optimal for $k=1$, sampling from the resulting (often deterministic) policy cannot guarantee win rates above $\\tfrac{1}{2}$ except for an arbitrarily small slack. This stems from a lack of output diversity: existing alignment methods can collapse to a single majority-preferred response, making additional samples redundant. In contrast, our approach preserves output diversity and achieves the optimal test-time scaling rate. In particular, we propose a family of symmetric multi-player alignment games and prove that any symmetric Nash equilibrium policy of the $(k+1)$-player alignment game achieves the optimal $(k,\\frac{k}{k+1})$-robust alignment. Finally, we provide theoretical convergence guarantees for self-play learning dynamics in these games and extend the framework to opponents that also generate multiple responses.",
    "arxiv_url": "https://arxiv.org/abs/2601.08777",
    "authors": [
      "Yang Cai",
      "Weiqiang Zheng"
    ],
    "first_author": "Yang Cai",
    "category": [
      "Technical"
    ],
    "field": "Model Alignment & Personalization",
    "task": "Asymptotic Universal Alignment (Test-Time Scaling)",
    "tags": [
      "Test-time Scaling",
      "Asymptotic Universal Alignment",
      "Robust Alignment",
      "Output Diversity",
      "Alignment Games",
      "Nash Equilibrium",
      "Self-Play Convergence",
      "Condorcet Analysis"
    ],
    "summary": "本文通过在测试时生成多候选回复的框架形式化了渐近普适对齐（U‑alignment），证明存在单输出策略家族通过测试时扩展以最优速率k/(k+1)趋近普适对齐，提出对称多玩家对齐博弈以保持输出多样性并给出自我博弈的收敛性保证，同时指出现有方法在测试时扩展下会因输出塌缩而丧失潜在优势。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.08777v1",
    "published": "2026-01-13",
    "update_time": "2026-01-13",
    "download_time": "2026-01-15 02:09:17"
  }
]