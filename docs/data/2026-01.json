[
  {
    "id": "2601.00635",
    "title": "SEMODS: A Validated Dataset of Open-Source Software Engineering Models",
    "abstract": "Integrating Artificial Intelligence into Software Engineering (SE) requires having a curated collection of models suited to SE tasks. With millions of models hosted on Hugging Face (HF) and new ones continuously being created, it is infeasible to identify SE models without a dedicated catalogue. To address this gap, we present SEMODS: an SE-focused dataset of 3,427 models extracted from HF, combining automated collection with rigorous validation through manual annotation and large language model assistance. Our dataset links models to SE tasks and activities from the software development lifecycle, offering a standardized representation of their evaluation results, and supporting multiple applications such as data analysis, model discovery, benchmarking, and model adaptation.",
    "arxiv_url": "https://arxiv.org/abs/2601.00635",
    "authors": [
      "Alexandra González",
      "Xavier Franch",
      "Silverio Martínez-Fernández"
    ],
    "first_author": "Alexandra González",
    "category": [
      "Benchmark"
    ],
    "field": "Model Repositories & Catalogues",
    "task": "SE Model Cataloguing",
    "tags": [
      "Model Catalog",
      "SE Task Taxonomy",
      "Benchmark Harmonization",
      "LLM-assisted Annotation",
      "Model Card Normalization",
      "Automated Dataset Maintenance",
      "Repository Metadata Schema",
      "Model Discovery"
    ],
    "summary": "本文提出SEMODS，一个从Hugging Face系统化收集并通过人工和大模型辅助验证的面向软件工程的模型数据集（3,427个模型），提供SE任务映射、标准化评测表示和自动化更新管道以支持模型发现与基准测试。",
    "quality": "High",
    "conference": "FORGE (ACM International Conference on AI Foundation Models and Software Engineering) 2026",
    "pdf_url": "https://arxiv.org/pdf/2601.00635v1",
    "published": "2026-01-02",
    "update_time": "2026-01-02",
    "download_time": "2026-01-05 02:05:13"
  },
  {
    "id": "2601.00497",
    "title": "STELLAR: A Search-Based Testing Framework for Large Language Model Applications",
    "abstract": "Large Language Model (LLM)-based applications are increasingly deployed across various domains, including customer service, education, and mobility. However, these systems are prone to inaccurate, fictitious, or harmful responses, and their vast, high-dimensional input space makes systematic testing particularly challenging. To address this, we present STELLAR, an automated search-based testing framework for LLM-based applications that systematically uncovers text inputs leading to inappropriate system responses. Our framework models test generation as an optimization problem and discretizes the input space into stylistic, content-related, and perturbation features. Unlike prior work that focuses on prompt optimization or coverage heuristics, our work employs evolutionary optimization to dynamically explore feature combinations that are more likely to expose failures. We evaluate STELLAR on three LLM-based conversational question-answering systems. The first focuses on safety, benchmarking both public and proprietary LLMs against malicious or unsafe prompts. The second and third target navigation, using an open-source and an industrial retrieval-augmented system for in-vehicle venue recommendations. Overall, STELLAR exposes up to 4.3 times (average 2.5 times) more failures than the existing baseline approaches.",
    "arxiv_url": "https://arxiv.org/abs/2601.00497",
    "authors": [
      "Lev Sorokin",
      "Ivan Vasilev",
      "Ken E. Friedl",
      "Andrea Stocco"
    ],
    "first_author": "Lev Sorokin",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Software Testing",
    "task": "Test Generation",
    "tags": [
      "Search-based Testing",
      "Evolutionary Optimization",
      "Feature Discretization",
      "Retrieval-Augmented Generation",
      "Robustness Testing",
      "Safety Testing",
      "Conversational QA",
      "Perturbation Testing"
    ],
    "summary": "本文提出STELLAR，一种将自然语言输入离散为风格、内容和扰动特征并通过进化搜索生成失败诱发测试用例的自动化搜索式测试框架，用以发现LLM应用（如车载对话系统）中的不当或错误响应并在多项用例中显著优于基线方法。",
    "quality": "High",
    "conference": "International Conference on Software Analysis, Evolution and Reengineering (SANER) 2026",
    "pdf_url": "https://arxiv.org/pdf/2601.00497v1",
    "published": "2026-01-01",
    "update_time": "2026-01-01",
    "download_time": "2026-01-05 02:06:23"
  },
  {
    "id": "2601.00753",
    "title": "Early-Stage Prediction of Review Effort in AI-Generated Pull Requests",
    "abstract": "As autonomous AI agents transition from code completion tools to full-fledged teammates capable of opening pull requests (PRs) at scale, software maintainers face a new challenge: not just reviewing code, but managing complex interaction loops with non-human contributors. This paradigm shift raises a critical question: can we predict which agent-generated PRs will consume excessive review effort before any human interaction begins?   Analyzing 33,707 agent-authored PRs from the AIDev dataset across 2,807 repositories, we uncover a striking two-regime behavioral pattern that fundamentally distinguishes autonomous agents from human developers. The first regime, representing 28.3 percent of all PRs, consists of instant merges (less than 1 minute), reflecting success on narrow automation tasks. The second regime involves iterative review cycles where agents frequently stall or abandon refinement (ghosting).   We propose a Circuit Breaker triage model that predicts high-review-effort PRs (top 20 percent) at creation time using only static structural features. A LightGBM model achieves AUC 0.957 on a temporal split, while semantic text features (TF-IDF, CodeBERT) provide negligible predictive value. At a 20 percent review budget, the model intercepts 69 percent of total review effort, enabling zero-latency governance.   Our findings challenge prevailing assumptions in AI-assisted code review: review burden is dictated by what agents touch, not what they say, highlighting the need for structural governance mechanisms in human-AI collaboration.",
    "arxiv_url": "https://arxiv.org/abs/2601.00753",
    "authors": [
      "Dao Sy Duy Minh",
      "Huynh Trung Kiet",
      "Tran Chi Nguyen",
      "Nguyen Lam Phu Quy",
      "Phu Hoa Pham",
      "Nguyen Dinh Ha Duong",
      "Truong Bao Tran"
    ],
    "first_author": "Dao Sy Duy Minh",
    "category": [
      "Empirical",
      "Technical"
    ],
    "field": "Maintenance",
    "task": "Code Review",
    "tags": [
      "Ghosting",
      "Review Triage",
      "Effort Prediction",
      "Creation-time Features",
      "Patch Complexity",
      "Structural Signals",
      "Agent-authored PRs",
      "Circuit Breaker",
      "Zero-latency Triage",
      "Model Interpretability"
    ],
    "summary": "本文基于33,707个由AI代理提交的PR，发现即时合并与迭代失败两种行为模式，并提出基于创建时结构性特征的“断路器”LightGBM模型（AUC≈0.957）用于零延迟预测高审查成本的PR以辅助维护者分流。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.00753v1",
    "published": "2026-01-02",
    "update_time": "2026-01-02",
    "download_time": "2026-01-06 01:54:54"
  },
  {
    "id": "2601.00482",
    "title": "Multi-Agent Coordinated Rename Refactoring",
    "abstract": "The primary value of AI agents in software development lies in their ability to extend the developer's capacity for reasoning and action, not to supplant human involvement. To showcase how to use agents working in tandem with developers, we designed a novel approach for carrying out coordinated renaming. Coordinated renaming, where a single rename refactoring triggers refactorings in multiple, related identifiers, is a frequent yet challenging task. Developers must manually propagate these rename refactorings across numerous files and contexts, a process that is both tedious and highly error-prone. State-of-the-art heuristic-based approaches produce an overwhelming number of false positives, while vanilla Large Language Models (LLMs) provide incomplete suggestions due to their limited context and inability to interact with refactoring tools. This leaves developers with incomplete refactorings or burdens them with filtering too many false positives. Coordinated renaming is exactly the kind of repetitive task that agents can significantly reduce the developers' burden while keeping them in the driver's seat.   We designed, implemented, and evaluated the first multi-agent framework that automates coordinated renaming. It operates on a key insight: a developer's initial refactoring is a clue to infer the scope of related refactorings. Our Scope Inference Agent first transforms this clue into an explicit, natural-language Declared Scope. The Planned Execution Agent then uses this as a strict plan to identify program elements that should undergo refactoring and safely executes the changes by invoking the IDE's own trusted refactoring APIs. Finally, the Replication Agent uses it to guide the project-wide search. We first conducted a formative study on the practice of coordinated renaming in 609K commits in 100 open-source projects and surveyed 205 developers ...",
    "arxiv_url": "https://arxiv.org/abs/2601.00482",
    "authors": [
      "Abhiram Bellur",
      "Mohammed Raihan Ullah",
      "Fraol Batole",
      "Mohit Kansara",
      "Masaharu Morimoto",
      "Kai Ishikawa",
      "Haifeng Chen",
      "Yaroslav Zharov",
      "Timofey Bryksin",
      "Tien N. Nguyen",
      "Hridesh Rajan",
      "Danny Dig"
    ],
    "first_author": "Abhiram Bellur",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Maintenance",
    "task": "Refactoring",
    "tags": [
      "Code Agents",
      "Coordinated Rename",
      "Scope Inference",
      "IDE Refactoring API",
      "Program Slicing",
      "Semantic Search",
      "Developer-in-the-loop",
      "Episodic Memory",
      "Repository-level Propagation",
      "Pull Request Automation"
    ],
    "summary": "本文提出CoRenameAgent——一个在IDE中运行的多智能体框架，通过范围推断、规划执行和复制传播三类代理结合开发者反馈与IDE可信重构API，自动、可靠地执行跨仓库的协同重命名，并在两个基准上显著优于现有方法且其自动生成的PR被社区接受。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.00482v1",
    "published": "2026-01-01",
    "update_time": "2026-01-01",
    "download_time": "2026-01-06 01:55:34"
  },
  {
    "id": "2601.00743",
    "title": "An Agentic Framework for Neuro-Symbolic Programming",
    "abstract": "Integrating symbolic constraints into deep learning models could make them more robust, interpretable, and data-efficient. Still, it remains a time-consuming and challenging task. Existing frameworks like DomiKnowS help this integration by providing a high-level declarative programming interface, but they still assume the user is proficient with the library's specific syntax. We propose AgenticDomiKnowS (ADS) to eliminate this dependency. ADS translates free-form task descriptions into a complete DomiKnowS program using an agentic workflow that creates and tests each DomiKnowS component separately. The workflow supports optional human-in-the-loop intervention, enabling users familiar with DomiKnowS to refine intermediate outputs. We show how ADS enables experienced DomiKnowS users and non-users to rapidly construct neuro-symbolic programs, reducing development time from hours to 10-15 minutes.",
    "arxiv_url": "https://arxiv.org/abs/2601.00743",
    "authors": [
      "Aliakbar Nafar",
      "Chetan Chigurupati",
      "Danial Kamali",
      "Hamid Karimian",
      "Parisa Kordjamshidi"
    ],
    "first_author": "Aliakbar Nafar",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Editing",
    "tags": [
      "Code Agents",
      "Code RAG",
      "Neuro-Symbolic",
      "Domain-Specific Language",
      "Human-LLM Interaction",
      "Interactive Coding",
      "Self-Refinement",
      "VLM Integration"
    ],
    "summary": "本文提出AgenticDomiKnowS，一个将自然语言任务描述通过检索-生成-执行-审查的代理式工作流转化为完整DomiKnowS神经符号程序并支持人机交互与自我修正的系统，能将开发时间从数小时缩短到10–15分钟。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.00743v1",
    "published": "2026-01-02",
    "update_time": "2026-01-02",
    "download_time": "2026-01-06 01:56:06"
  },
  {
    "id": "2601.00559",
    "title": "Cracking IoT Security: Can LLMs Outsmart Static Analysis Tools?",
    "abstract": "Smart home IoT platforms such as openHAB rely on Trigger Action Condition (TAC) rules to automate device behavior, but the interplay among these rules can give rise to interaction threats, unintended or unsafe behaviors emerging from implicit dependencies, conflicting triggers, or overlapping conditions. Identifying these threats requires semantic understanding and structural reasoning that traditionally depend on symbolic, constraint-driven static analysis. This work presents the first comprehensive evaluation of Large Language Models (LLMs) across a multi-category interaction threat taxonomy, assessing their performance on both the original openHAB (oHC/IoTB) dataset and a structurally challenging Mutation dataset designed to test robustness under rule transformations. We benchmark Llama 3.1 8B, Llama 70B, GPT-4o, Gemini-2.5-Pro, and DeepSeek-R1 across zero-, one-, and two-shot settings, comparing their results against oHIT's manually validated ground truth. Our findings show that while LLMs exhibit promising semantic understanding, particularly on action- and condition-related threats, their accuracy degrades significantly for threats requiring cross-rule structural reasoning, especially under mutated rule forms. Model performance varies widely across threat categories and prompt settings, with no model providing consistent reliability. In contrast, the symbolic reasoning baseline maintains stable detection across both datasets, unaffected by rule rewrites or structural perturbations. These results underscore that LLMs alone are not yet dependable for safety critical interaction-threat detection in IoT environments. We discuss the implications for tool design and highlight the potential of hybrid architectures that combine symbolic analysis with LLM-based semantic interpretation to reduce false positives while maintaining structural rigor.",
    "arxiv_url": "https://arxiv.org/abs/2601.00559",
    "authors": [
      "Jason Quantrill",
      "Noura Khajehnouri",
      "Zihan Guo",
      "Manar H. Alalfi"
    ],
    "first_author": "Jason Quantrill",
    "category": [
      "Empirical",
      "Benchmark"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "Rule Interaction Threats",
      "IoT Security",
      "Trigger-Action-Condition",
      "Mutation Testing",
      "Symbolic-LLM Hybrid",
      "Cross-rule Reasoning",
      "Structural Robustness",
      "Prompt Robustness",
      "False Positive Reduction"
    ],
    "summary": "本文评估多种大语言模型在openHAB触发-条件-动作规则交互威胁检测上的表现，并通过一套结构变异数据集与符号静态分析工具对比，发现LLM在语义理解方面较好但在跨规则结构推理与变形规则下显著退化，提出结合符号分析与LLM的混合方案以提高检测可靠性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.00559v1",
    "published": "2026-01-02",
    "update_time": "2026-01-02",
    "download_time": "2026-01-06 01:57:35"
  },
  {
    "id": "2601.02971",
    "title": "Few-shot learning for security bug report identification",
    "abstract": "Security bug reports require prompt identification to minimize the window of vulnerability in software systems. Traditional machine learning (ML) techniques for classifying bug reports to identify security bug reports rely heavily on large amounts of labeled data. However, datasets for security bug reports are often scarce in practice, leading to poor model performance and limited applicability in real-world settings. In this study, we propose a few-shot learning-based technique to effectively identify security bug reports using limited labeled data. We employ SetFit, a state-of-the-art few-shot learning framework that combines sentence transformers with contrastive learning and parameter-efficient fine-tuning. The model is trained on a small labeled dataset of bug reports and is evaluated on its ability to classify these reports as either security-related or non-security-related. Our approach achieves an AUC of 0.865, at best, outperforming traditional ML techniques (baselines) for all of the evaluated datasets. This highlights the potential of SetFit to effectively identify security bug reports. SetFit-based few-shot learning offers a promising alternative to traditional ML techniques to identify security bug reports. The approach enables efficient model development with minimal annotation effort, making it highly suitable for scenarios where labeled data is scarce.",
    "arxiv_url": "https://arxiv.org/abs/2601.02971",
    "authors": [
      "Muhammad Laiq"
    ],
    "first_author": "Muhammad Laiq",
    "category": [
      "Empirical"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "Issue Classification",
      "Few-shot Learning",
      "Contrastive Learning",
      "Parameter-efficient Fine-tuning",
      "Class Imbalance",
      "Security Bug Identification",
      "Empirical Comparison"
    ],
    "summary": "本文在四个开源项目的数据集上实证评估了基于少样本学习的SetFit方法用于安全缺陷报告识别，实验表明在有限标注下可达到最高AUC 0.865并优于传统机器学习基线。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.02971v1",
    "published": "2026-01-06",
    "update_time": "2026-01-06",
    "download_time": "2026-01-08 01:55:02"
  },
  {
    "id": "2601.02868",
    "title": "CodeMEM: AST-Guided Adaptive Memory for Repository-Level Iterative Code Generation",
    "abstract": "Large language models (LLMs) substantially enhance developer productivity in repository-level code generation through interactive collaboration. However, as interactions progress, repository context must be continuously preserved and updated to integrate newly validated information. Meanwhile, the expanding session history increases cognitive burden, often leading to forgetting and the reintroduction of previously resolved errors. Existing memory management approaches show promise but remain limited by natural language-centric representations. To overcome these limitations, we propose CodeMEM, an AST-guided dynamic memory management system tailored for repository-level iterative code generation. Specifically, CodeMEM introduces the Code Context Memory component that dynamically maintains and updates repository context through AST-guided LLM operations, along with the Code Session Memory that constructs a code-centric representation of interaction history and explicitly detects and mitigates forgetting through AST-based analysis. Experimental results on the instruction-following benchmark CodeIF-Bench and the code generation benchmark CoderEval demonstrate that CodeMEM achieves state-of-the-art performance, improving instruction following by 12.2% for the current turn and 11.5% for the session level, and reducing interaction rounds by 2-3, while maintaining competitive inference latency and token efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2601.02868",
    "authors": [
      "Peiding Wang",
      "Li Zhang",
      "Fang Liu",
      "Chongyang Tao",
      "Yinghao Zhu"
    ],
    "first_author": "Peiding Wang",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Retrieval",
    "tags": [
      "Repository-Level Coding",
      "AST-guided memory",
      "Code Context Memory",
      "Code Session Memory",
      "Forgetting Detection",
      "Code-change Analysis",
      "Interaction Efficiency"
    ],
    "summary": "本文提出CODEMEM，一种基于AST引导的动态代码记忆管理系统，通过维护代码上下文记忆与会话记忆并基于AST分析检测与缓解遗忘，从而在仓库级多轮代码生成中显著提升指令遵循率并减少交互轮次。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.02868v1",
    "published": "2026-01-06",
    "update_time": "2026-01-06",
    "download_time": "2026-01-08 01:55:19"
  },
  {
    "id": "2601.02941",
    "title": "SastBench: A Benchmark for Testing Agentic SAST Triage",
    "abstract": "SAST (Static Application Security Testing) tools are among the most widely used techniques in defensive cybersecurity, employed by commercial and non-commercial organizations to identify potential vulnerabilities in software. Despite their great utility, they generate numerous false positives, requiring costly manual filtering (aka triage). While LLM-powered agents show promise for automating cybersecurity tasks, existing benchmarks fail to emulate real-world SAST finding distributions. We introduce SastBench, a benchmark for evaluating SAST triage agents that combines real CVEs as true positives with filtered SAST tool findings as approximate false positives. SastBench features an agent-agnostic design. We evaluate different agents on the benchmark and present a comparative analysis of their performance, provide a detailed analysis of the dataset, and discuss the implications for future development.",
    "arxiv_url": "https://arxiv.org/abs/2601.02941",
    "authors": [
      "Jake Feiglin",
      "Guy Dar"
    ],
    "first_author": "Jake Feiglin",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "SAST Triage",
      "Agentic Benchmark",
      "Realistic False Positives",
      "CVE-based Ground Truth",
      "Agent Evaluation",
      "Security-Oriented Prompting",
      "Language Diversity",
      "Agent-Agnostic Evaluation"
    ],
    "summary": "该论文提出并开源了SASTBENCH，一个以真实CVE作为真阳性并以过滤后的SAST工具报警作为近似假阳性构建的面向自动化SAST告警三分类（triage）的基准，并基于该基准对多种LLM驱动代理进行了比较评估与数据分析。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.02941v1",
    "published": "2026-01-06",
    "update_time": "2026-01-06",
    "download_time": "2026-01-08 01:59:25"
  },
  {
    "id": "2601.02736",
    "title": "Hypothesize-Then-Verify: Speculative Root Cause Analysis for Microservices with Pathwise Parallelism",
    "abstract": "Microservice systems have become the backbone of cloud-native enterprise applications due to their resource elasticity, loosely coupled architecture, and lightweight deployment. Yet, the intrinsic complexity and dynamic runtime interactions of such systems inevitably give rise to anomalies. Ensuring system reliability therefore hinges on effective root cause analysis (RCA), which entails not only localizing the source of anomalies but also characterizing the underlying failures in a timely and interpretable manner. Recent advances in intelligent RCA techniques, particularly those powered by large language models (LLMs), have demonstrated promising capabilities, as LLMs reduce reliance on handcrafted features while offering cross-platform adaptability, task generalization, and flexibility. However, existing LLM-based methods still suffer from two critical limitations: (a) limited exploration diversity, which undermines accuracy, and (b) heavy dependence on large-scale LLMs, which results in slow inference. To overcome these challenges, we propose SpecRCA, a speculative root cause analysis framework for microservices that adopts a \\textit{hypothesize-then-verify} paradigm. SpecRCA first leverages a hypothesis drafting module to rapidly generate candidate root causes, and then employs a parallel root cause verifier to efficiently validate them. Preliminary experiments on the AIOps 2022 dataset demonstrate that SpecRCA achieves superior accuracy and efficiency compared to existing approaches, highlighting its potential as a practical solution for scalable and interpretable RCA in complex microservice environments.",
    "arxiv_url": "https://arxiv.org/abs/2601.02736",
    "authors": [
      "Lingzhe Zhang",
      "Tong Jia",
      "Yunpeng Zhai",
      "Leyi Pan",
      "Chiming Duan",
      "Minghua He",
      "Pei Xiao",
      "Ying Li"
    ],
    "first_author": "Lingzhe Zhang",
    "category": [
      "Technical"
    ],
    "field": "AIOps",
    "task": "Root Cause Analysis",
    "tags": [
      "Speculative Verification",
      "Hypothesize-then-Verify",
      "Pathwise Parallelism",
      "Modality Fusion",
      "Service Topology",
      "Trace Analysis",
      "Granger Causality",
      "Anomalous Metric Detection",
      "Parallel Verification",
      "Diagnosis Synthesis",
      "Efficiency",
      "Interpretability"
    ],
    "summary": "本文提出 SpecRCA——一种面向微服务的假设-验证根因分析框架，通过轻量草拟模型生成多样化候选假设并并行用小型微调 LLM 验证，结合指标/链路/日志的拓扑融合与综合生成诊断报告，在 AIOps 2022 数据集上显著提升定位准确性并降低推理延迟。",
    "quality": "High",
    "conference": "ICSE",
    "pdf_url": "https://arxiv.org/pdf/2601.02736v1",
    "published": "2026-01-06",
    "update_time": "2026-01-06",
    "download_time": "2026-01-08 02:01:34"
  },
  {
    "id": "2601.03988",
    "title": "Using Small Language Models to Reverse-Engineer Machine Learning Pipelines Structures",
    "abstract": "Background: Extracting the stages that structure Machine Learning (ML) pipelines from source code is key for gaining a deeper understanding of data science practices. However, the diversity caused by the constant evolution of the ML ecosystem (e.g., algorithms, libraries, datasets) makes this task challenging. Existing approaches either depend on non-scalable, manual labeling, or on ML classifiers that do not properly support the diversity of the domain. These limitations highlight the need for more flexible and reliable solutions.   Objective: We evaluate whether Small Language Models (SLMs) can leverage their code understanding and classification abilities to address these limitations, and subsequently how they can advance our understanding of data science practices.   Method: We conduct a confirmatory study based on two reference works selected for their relevance regarding current state-of-the-art's limitations. First, we compare several SLMs using Cochran's Q test. The best-performing model is then evaluated against the reference studies using two distinct McNemar's tests. We further analyze how variations in taxonomy definitions affect performance through an additional Cochran's Q test. Finally, a goodness-of-fit analysis is conducted using Pearson's chi-squared tests to compare our insights on data science practices with those from prior studies.",
    "arxiv_url": "https://arxiv.org/abs/2601.03988",
    "authors": [
      "Nicolas Lacroix",
      "Mireille Blay-Fornarino",
      "Sébastien Mosser",
      "Frederic Precioso"
    ],
    "first_author": "Nicolas Lacroix",
    "category": [
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Understanding",
    "tags": [
      "Pipeline Extraction",
      "Reverse Engineering",
      "Jupyter Notebook Analysis",
      "Small Language Models",
      "Instruction-level Classification",
      "Taxonomy Sensitivity",
      "Model Comparison",
      "Goodness-of-fit Analysis"
    ],
    "summary": "本文通过对比多款开源小型语言模型，评估其在从 Jupyter 笔记本代码中反向工程提取机器学习流水线结构（阶段）上的准确性，分析分类法措辞对结果的影响并检验这些结果是否改变对数据科学家实践的既有理解。",
    "quality": "High",
    "conference": "SANER (IEEE International Conference on Software Analysis, Evolution and Reengineering) 2026",
    "pdf_url": "https://arxiv.org/pdf/2601.03988v1",
    "published": "2026-01-07",
    "update_time": "2026-01-07",
    "download_time": "2026-01-09 01:56:28"
  },
  {
    "id": "2601.03878",
    "title": "Understanding Specification-Driven Code Generation with LLMs: An Empirical Study Design",
    "abstract": "Large Language Models (LLMs) are increasingly integrated into software development workflows, yet their behavior in structured, specification-driven processes remains poorly understood. This paper presents an empirical study design using CURRANTE, a Visual Studio Code extension that enables a human-in-the-loop workflow for LLM-assisted code generation. The tool guides developers through three sequential stages--Specification, Tests, and Function--allowing them to define requirements, generate and refine test suites, and produce functions that satisfy those tests. Participants will solve medium-difficulty problems from the LiveCodeBench dataset, while the tool records fine-grained interaction logs, effectiveness metrics (e.g., pass rate, all-pass completion), efficiency indicators (e.g., time-to-pass), and iteration behaviors. The study aims to analyze how human intervention in specification and test refinement influences the quality and dynamics of LLM-generated code. The results will provide empirical insights into the design of next-generation development environments that align human reasoning with model-driven code generation.",
    "arxiv_url": "https://arxiv.org/abs/2601.03878",
    "authors": [
      "Giovanni Rosa",
      "David Moreno-Lumbreras",
      "Gregorio Robles",
      "Jesús M. González-Barahona"
    ],
    "first_author": "Giovanni Rosa",
    "category": [
      "Empirical",
      "Technical"
    ],
    "field": "Software Testing",
    "task": "Test Generation",
    "tags": [
      "Specification-Driven Development",
      "Test-Driven Development",
      "Human-LLM Interaction",
      "IDE Plugin",
      "Test Suite Generation",
      "Test Refinement",
      "Interaction Logging",
      "Effectiveness Metrics",
      "Time-to-Pass",
      "Registered Study Protocol"
    ],
    "summary": "本文提出并描述了一项基于 CURRANTE VSCode 插件的注册报告型实证研究设计，以评估在人类参与的规范（通过测试用例）驱动 TDD 工作流下，LLM 辅助代码生成的有效性、效率与迭代行为。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.03878v1",
    "published": "2026-01-07",
    "update_time": "2026-01-07",
    "download_time": "2026-01-09 01:56:58"
  },
  {
    "id": "2601.04126",
    "title": "InfiniteWeb: Scalable Web Environment Synthesis for GUI Agent Training",
    "abstract": "GUI agents that interact with graphical interfaces on behalf of users represent a promising direction for practical AI assistants. However, training such agents is hindered by the scarcity of suitable environments. We present InfiniteWeb, a system that automatically generates functional web environments at scale for GUI agent training. While LLMs perform well on generating a single webpage, building a realistic and functional website with many interconnected pages faces challenges. We address these challenges through unified specification, task-centric test-driven development, and a combination of website seed with reference design image to ensure diversity. Our system also generates verifiable task evaluators enabling dense reward signals for reinforcement learning. Experiments show that InfiniteWeb surpasses commercial coding agents at realistic website construction, and GUI agents trained on our generated environments achieve significant performance improvements on OSWorld and Online-Mind2Web, demonstrating the effectiveness of proposed system.",
    "arxiv_url": "https://arxiv.org/abs/2601.04126",
    "authors": [
      "Ziyun Zhang",
      "Zezhou Wang",
      "Xiaoyi Zhang",
      "Zongyu Guo",
      "Jiahao Li",
      "Bin Li",
      "Yan Lu"
    ],
    "first_author": "Ziyun Zhang",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Requirements & Design",
    "task": "Specification & Validation",
    "tags": [
      "Environment Synthesis",
      "Unified Specification",
      "Task-Centric TDD",
      "Evaluator Generation",
      "Design-Guided Frontend",
      "Visual-Functional Diversity",
      "Dense Reward Signals",
      "GUI Agent Training"
    ],
    "summary": "本文提出INFINITEWEB，一个通过从任务导出统一数据与接口、采用面向任务的测试驱动开发以及设计图引导的前端生成，自动大规模合成功能性网页环境并生成可验证评估器以支持基于强化学习的GUI代理训练的系统。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.04126v1",
    "published": "2026-01-07",
    "update_time": "2026-01-07",
    "download_time": "2026-01-09 01:58:15"
  },
  {
    "id": "2601.03731",
    "title": "From Laboratory to Real-World Applications: Benchmarking Agentic Code Reasoning at the Repository Level",
    "abstract": "As large language models (LLMs) evolve into autonomous agents, evaluating repository-level reasoning, the ability to maintain logical consistency across massive, real-world, interdependent file systems, has become critical. Current benchmarks typically fluctuate between isolated code snippets and black-box evaluations. We present RepoReason, a white-box diagnostic benchmark centered on abductive assertion verification. To eliminate memorization while preserving authentic logical depth, we implement an execution-driven mutation framework that utilizes the environment as a semantic oracle to regenerate ground-truth states. Furthermore, we establish a fine-grained diagnostic system using dynamic program slicing, quantifying reasoning via three orthogonal metrics: $ESV$ (reading load), $MCL$ (simulation depth), and $DFI$ (integration width). Comprehensive evaluations of frontier models (e.g., Claude-4.5-Sonnet, DeepSeek-v3.1-Terminus) reveal a prevalent aggregation deficit, where integration width serves as the primary cognitive bottleneck. Our findings provide granular white-box insights for optimizing the next generation of agentic software engineering.",
    "arxiv_url": "https://arxiv.org/abs/2601.03731",
    "authors": [
      "Jia Li",
      "Yuxin Su",
      "Michael R. Lyu"
    ],
    "first_author": "Jia Li",
    "category": [
      "Benchmark",
      "Technical",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Repository-Level Reasoning",
      "Abductive Verification",
      "Execution-Driven Mutation",
      "Deterministic Value Protocol",
      "Dynamic Program Slicing",
      "Reading Load (ESV)",
      "Simulation Depth (MCL)",
      "Integration Width (DFI)",
      "White-box Diagnostic",
      "Aggregation Deficit",
      "Long-chain State Tracking",
      "Cross-file Integration"
    ],
    "summary": "本文提出了REPOREASON——一个以掩码断言验证为核心、结合执行驱动变异和动态程序切片的仓库级白盒基准，用于客观评估并细粒度诊断LLM代理在跨文件信息整合与长链状态追踪方面的推理能力，揭示了模型在整合宽度上的严重瓶颈（Aggregation Deficit）。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.03731v1",
    "published": "2026-01-07",
    "update_time": "2026-01-07",
    "download_time": "2026-01-09 02:08:24"
  },
  {
    "id": "2601.04886",
    "title": "Analyzing Message-Code Inconsistency in AI Coding Agent-Authored Pull Requests",
    "abstract": "Pull request (PR) descriptions generated by AI coding agents are the primary channel for communicating code changes to human reviewers. However, the alignment between these messages and the actual changes remains unexplored, raising concerns about the trustworthiness of AI agents. To fill this gap, we analyzed 23,247 agentic PRs across five agents using PR message-code inconsistency (PR-MCI). We contributed 974 manually annotated PRs, found 406 PRs (1.7%) exhibited high PR-MCI, and identified eight PR-MCI types, revealing that descriptions claiming unimplemented changes was the most common issue (45.4%). Statistical tests confirmed that high-MCI PRs had 51.7% lower acceptance rates (28.3% vs. 80.0%) and took 3.5x longer to merge (55.8 vs. 16.0 hours). Our findings suggest that unreliable PR descriptions undermine trust in AI agents, highlighting the need for PR-MCI verification mechanisms and improved PR generation to enable trustworthy human-AI collaboration.",
    "arxiv_url": "https://arxiv.org/abs/2601.04886",
    "authors": [
      "Jingzhi Gong",
      "Giovanni Pinna",
      "Yixin Bian",
      "Jie M. Zhang"
    ],
    "first_author": "Jingzhi Gong",
    "category": [
      "Empirical",
      "Benchmark"
    ],
    "field": "Maintenance",
    "task": "Code Review",
    "tags": [
      "Message-Code Inconsistency",
      "PR Description Fidelity",
      "Human-LLM Interaction",
      "Hallucination",
      "Annotated Dataset",
      "Taxonomy",
      "Acceptance Rate",
      "Merge Delay"
    ],
    "summary": "本文通过分析23,247个由AI代理提交的PR并发布974条人工标注，构建并量化了PR消息-代码不一致（PR-MCI）的分类与流行率，发现高不一致PR显著降低接受率并延长合并时间，强调需改进PR生成与校验以提升人机协作可信度。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.04886v1",
    "published": "2026-01-08",
    "update_time": "2026-01-08",
    "download_time": "2026-01-10 01:51:55"
  },
  {
    "id": "2601.04556",
    "title": "4D-ARE: Bridging the Attribution Gap in LLM Agent Requirements Engineering",
    "abstract": "We deployed an LLM agent with ReAct reasoning and full data access. It executed flawlessly, yet when asked \"Why is completion rate 80%?\", it returned metrics instead of causal explanation. The agent knew how to reason but we had not specified what to reason about. This reflects a gap: runtime reasoning frameworks (ReAct, Chain-of-Thought) have transformed LLM agents, but design-time specification--determining what domain knowledge agents need--remains under-explored. We propose 4D-ARE (4-Dimensional Attribution-Driven Agent Requirements Engineering), a preliminary methodology for specifying attribution-driven agents. The core insight: decision-makers seek attribution, not answers. Attribution concerns organize into four dimensions (Results -> Process -> Support -> Long-term), motivated by Pearl's causal hierarchy. The framework operationalizes through five layers producing artifacts that compile directly to system prompts. We demonstrate the methodology through an industrial pilot deployment in financial services. 4D-ARE addresses what agents should reason about, complementing runtime frameworks that address how. We hypothesize systematic specification amplifies the power of these foundational advances. This paper presents a methodological proposal with preliminary industrial validation; rigorous empirical evaluation is planned for future work.",
    "arxiv_url": "https://arxiv.org/abs/2601.04556",
    "authors": [
      "Bo Yu",
      "Lei Zhao"
    ],
    "first_author": "Bo Yu",
    "category": [
      "Technical"
    ],
    "field": "Requirements & Design",
    "task": "Specification & Validation",
    "tags": [
      "Causal Attribution",
      "Agent Requirements",
      "Design-Time Specification",
      "Elicitation Methods",
      "Prompt Compilation",
      "Boundary Constraints",
      "Decision Support"
    ],
    "summary": "本文提出4D-ARE——一种基于因果归因的LLM代理需求工程方法，通过四个归因维度与五层规范架构将领域知识编译为系统提示，并在金融服务的工业试点中展示了初步验证。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.04556v1",
    "published": "2026-01-08",
    "update_time": "2026-01-08",
    "download_time": "2026-01-10 01:52:22"
  },
  {
    "id": "2601.05242",
    "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
    "abstract": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
    "arxiv_url": "https://arxiv.org/abs/2601.05242",
    "authors": [
      "Shih-Yang Liu",
      "Xin Dong",
      "Ximing Lu",
      "Shizhe Diao",
      "Peter Belcak",
      "Mingjie Liu",
      "Min-Hung Chen",
      "Hongxu Yin",
      "Yu-Chiang Frank Wang",
      "Kwang-Ting Cheng",
      "Yejin Choi",
      "Jan Kautz",
      "Pavlo Molchanov"
    ],
    "first_author": "Shih-Yang Liu",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Reinforcement Learning & Alignment",
    "task": "Multi-reward Policy Optimization",
    "tags": [
      "Multi-objective RL",
      "Reward Decoupling",
      "Group-wise Normalization",
      "Advantage Normalization",
      "Reward Collapse Analysis",
      "Training Stability",
      "Convergence Improvement",
      "Tool Calling",
      "Math Reasoning",
      "Code Reasoning"
    ],
    "summary": "本文提出GDPO，一种在多奖励强化学习中对每个奖励进行组内独立归一化并随后进行批次优势归一化的策略优化方法，以避免GRPO在多奖励设置下导致的奖励信号塌缩并显著提升训练稳定性与收敛性，在工具调用、数学与代码推理任务上均优于GRPO。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.05242v1",
    "published": "2026-01-08",
    "update_time": "2026-01-08",
    "download_time": "2026-01-10 01:52:49"
  },
  {
    "id": "2601.05214",
    "title": "Internal Representations as Indicators of Hallucinations in Agent Tool Selection",
    "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in tool calling and tool usage, but suffer from hallucinations where they choose incorrect tools, provide malformed parameters and exhibit 'tool bypass' behavior by performing simulations and generating outputs instead of invoking specialized tools or external systems. This undermines the reliability of LLM based agents in production systems as it leads to inconsistent results, and bypasses security and audit controls. Such hallucinations in agent tool selection require early detection and error handling. Unlike existing hallucination detection methods that require multiple forward passes or external validation, we present a computationally efficient framework that detects tool-calling hallucinations in real-time by leveraging LLMs' internal representations during the same forward pass used for generation. We evaluate this approach on reasoning tasks across multiple domains, demonstrating strong detection performance (up to 86.4\\% accuracy) while maintaining real-time inference capabilities with minimal computational overhead, particularly excelling at detecting parameter-level hallucinations and inappropriate tool selections, critical for reliable agent deployment.",
    "arxiv_url": "https://arxiv.org/abs/2601.05214",
    "authors": [
      "Kait Healy",
      "Bharathi Srinivasan",
      "Visakh Madathil",
      "Jing Wu"
    ],
    "first_author": "Kait Healy",
    "category": [
      "Technical"
    ],
    "field": "Agent Systems & Reliability",
    "task": "Tool-Calling Hallucination Detection",
    "tags": [
      "Hallucination",
      "Internal Representations",
      "Real-time Detection",
      "Unsupervised Masking",
      "Tool Bypass",
      "Parameter Error Detection",
      "Function Selection",
      "Lightweight Classifier",
      "Inference Efficiency",
      "Agent Reliability"
    ],
    "summary": "本文提出一种利用LLM最后一层内部表征进行实时、无监督的工具调用幻觉检测方法，通过对真实工具调用掩码重预测生成训练数据并训练轻量分类器，从而在多领域推理任务中高效识别不当工具选择和参数级错误，且推理开销极小。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.05214v1",
    "published": "2026-01-08",
    "update_time": "2026-01-08",
    "download_time": "2026-01-10 01:53:36"
  },
  {
    "id": "2601.04540",
    "title": "AdaptEval: A Benchmark for Evaluating Large Language Models on Code Snippet Adaptation",
    "abstract": "Recent advancements in large language models (LLMs) have automated various software engineering tasks, with benchmarks emerging to evaluate their capabilities. However, for adaptation, a critical activity during code reuse, there is no benchmark to assess LLMs' performance, leaving their practical utility in this area unclear. To fill this gap, we propose AdaptEval, a benchmark designed to evaluate LLMs on code snippet adaptation. Unlike existing benchmarks, AdaptEval incorporates the following three distinctive features: First, Practical Context. Tasks in AdaptEval are derived from developers' practices, preserving rich contextual information from Stack Overflow and GitHub communities. Second, Multi-granularity Annotation. Each task is annotated with requirements at both task and adaptation levels, supporting the evaluation of LLMs across diverse adaptation scenarios. Third, Fine-grained Evaluation. AdaptEval includes a two-tier testing framework combining adaptation-level and function-level tests, which enables evaluating LLMs' performance across various individual adaptations. Based on AdaptEval, we conduct the first empirical study to evaluate six instruction-tuned LLMs and especially three reasoning LLMs on code snippet adaptation. Experimental results demonstrate that AdaptEval enables the assessment of LLMs' adaptation capabilities from various perspectives. It also provides critical insights into their current limitations, particularly their struggle to follow explicit instructions. We hope AdaptEval can facilitate further investigation and enhancement of LLMs' capabilities in code snippet adaptation, supporting their real-world applications.",
    "arxiv_url": "https://arxiv.org/abs/2601.04540",
    "authors": [
      "Tanghaoran Zhang",
      "Xinjun Mao",
      "Shangwen Wang",
      "Yuxin Zhao",
      "Yao Lu",
      "Jin Zhang",
      "Zhang Zhang",
      "Kang Yang",
      "Yue Yu"
    ],
    "first_author": "Tanghaoran Zhang",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Editing",
    "tags": [
      "Code Snippet Adaptation",
      "Context-aware Adaptation",
      "Multi-Granularity Annotation",
      "Two-tier Evaluation",
      "Function-level Testing",
      "Adaptation-level Testing",
      "Instruction-following",
      "Reasoning LLMs"
    ],
    "summary": "本文提出AdaptEval基准，收集自真实的Stack Overflow与GitHub适配实例，提供多粒度注释与两层（适配级与函数级）测试框架，并基于此评测多款指令调优与推理型LLM以揭示其在代码片段适配任务中的能力与局限。",
    "quality": "High",
    "conference": "ASE (IEEE/ACM International Conference on Automated Software Engineering) 2025",
    "pdf_url": "https://arxiv.org/pdf/2601.04540v1",
    "published": "2026-01-08",
    "update_time": "2026-01-08",
    "download_time": "2026-01-11 02:04:27"
  },
  {
    "id": "2601.04526",
    "title": "Advancing Language Models for Code-related Tasks",
    "abstract": "Recent advances in language models (LMs) have driven significant progress in various software engineering tasks. However, existing LMs still struggle with complex programming scenarios due to limitations in data quality, model architecture, and reasoning capability. This research systematically addresses these challenges through three complementary directions: (1) improving code data quality with a code difference-guided adversarial augmentation technique (CODA) and a code denoising technique (CodeDenoise); (2) enhancing model architecture via syntax-guided code LMs (LEAM and LEAM++); and (3) advancing model reasoning with a prompting technique (muFiX) and an agent-based technique (Specine). These techniques aim to promote the practical adoption of LMs in software development and further advance intelligent software engineering.",
    "arxiv_url": "https://arxiv.org/abs/2601.04526",
    "authors": [
      "Zhao Tian"
    ],
    "first_author": "Zhao Tian",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Adversarial Augmentation",
      "Input Denoising",
      "Syntax-Guided Generation",
      "AST Representation",
      "Specification Alignment",
      "Test-Guided Prompting",
      "Agent-Based Alignment",
      "Robustness",
      "Mutation Testing"
    ],
    "summary": "本文提出一套系统性方法（CODA、CodeDenoise、LEAM/LEAM++、μFiX 与 Specine），通过改进代码数据质量、模型语法感知架构与推理/对齐机制，提升代码语言模型在生成与鲁棒性上的性能。",
    "quality": "High",
    "conference": "ICSE 2026",
    "pdf_url": "https://arxiv.org/pdf/2601.04526v1",
    "published": "2026-01-08",
    "update_time": "2026-01-08",
    "download_time": "2026-01-11 02:04:42"
  },
  {
    "id": "2601.05187",
    "title": "SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning",
    "abstract": "Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.",
    "arxiv_url": "https://arxiv.org/abs/2601.05187",
    "authors": [
      "Yanchang Liang",
      "Xiaowei Zhao"
    ],
    "first_author": "Yanchang Liang",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Requirements & Design",
    "task": "Specification & Validation",
    "tags": [
      "Graphical Modeling",
      "Simulink Integration",
      "Plan-Execute Agent",
      "Compact Model Representation",
      "Reflection-Guided RL",
      "Sparse-Reward Optimization",
      "Abstract-Reconstruct Augmentation",
      "In-Process Simulation",
      "On-Premise Training",
      "Benchmarking"
    ],
    "summary": "本文提出SimuAgent——一个面向Simulink的基于LLM的建模与仿真助手，采用紧凑的Python字典表示、两阶段计划-执行框架、引入Reflection-GRPO强化学习与Abstract–Reconstruct自监督增强，并发布包含5300任务的SimuBench基准，在低成本本地硬件上实现高效且隐私保护的工业级建模自动化。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.05187v1",
    "published": "2026-01-08",
    "update_time": "2026-01-08",
    "download_time": "2026-01-11 02:05:38"
  },
  {
    "id": "2601.05106",
    "title": "Token-Level LLM Collaboration via FusionRoute",
    "abstract": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
    "arxiv_url": "https://arxiv.org/abs/2601.05106",
    "authors": [
      "Nuoya Xiong",
      "Yuhang Zhou",
      "Hanqing Zeng",
      "Zhaorun Chen",
      "Furong Huang",
      "Shuchao Bi",
      "Lizhu Zhang",
      "Zhuokai Zhao"
    ],
    "first_author": "Nuoya Xiong",
    "category": [
      "Technical"
    ],
    "field": "Multi-LLM Collaboration",
    "task": "Token-level Routing & Logit Fusion",
    "tags": [
      "Token-level Routing",
      "Logit Fusion",
      "Complementary Generator",
      "Lightweight Router",
      "Mixture-of-Experts",
      "Router Post-training",
      "Theoretical Guarantees",
      "Robustness",
      "Efficiency",
      "Multi-Domain Coordination"
    ],
    "summary": "本文提出FusionRoute，一种在每步解码时由轻量级路由器进行专家选择并通过对数几率相加提供补充生成信号的逐标记多LLM协作框架，理论上拓展了策略类并在数学推理、代码生成和指令跟随等多领域上实验证明了其鲁棒性与效率优势。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.05106v1",
    "published": "2026-01-08",
    "update_time": "2026-01-08",
    "download_time": "2026-01-11 02:06:46"
  },
  {
    "id": "2601.05827",
    "title": "SSR: Safeguarding Staking Rewards by Defining and Detecting Logical Defects in DeFi Staking",
    "abstract": "Decentralized Finance (DeFi) staking is one of the most prominent applications within the DeFi ecosystem, where DeFi projects enable users to stake tokens on the platform and reward participants with additional tokens. However, logical defects in DeFi staking could enable attackers to claim unwarranted rewards by manipulating reward amounts, repeatedly claiming rewards, or engaging in other malicious actions. To mitigate these threats, we conducted the first study focused on defining and detecting logical defects in DeFi staking. Through the analysis of 64 security incidents and 144 audit reports, we identified six distinct types of logical defects, each accompanied by detailed descriptions and code examples. Building on this empirical research, we developed SSR (Safeguarding Staking Reward), a static analysis tool designed to detect logical defects in DeFi staking contracts. SSR utilizes a large language model (LLM) to extract fundamental information about staking logic and constructs a DeFi staking model. It then identifies logical defects by analyzing the model and the associated semantic features. We constructed a ground truth dataset based on known security incidents and audit reports to evaluate the effectiveness of SSR. The results indicate that SSR achieves an overall precision of 92.31%, a recall of 87.92%, and an F1-score of 88.85%. Additionally, to assess the prevalence of logical defects in real-world smart contracts, we compiled a large-scale dataset of 15,992 DeFi staking contracts. SSR detected that 3,557 (22.24%) of these contracts contained at least one logical defect.",
    "arxiv_url": "https://arxiv.org/abs/2601.05827",
    "authors": [
      "Zewei Lin",
      "Jiachi Chen",
      "Jingwen Zhang",
      "Zexu Wang",
      "Yuming Feng",
      "Weizhe Zhang",
      "Zibin Zheng"
    ],
    "first_author": "Zewei Lin",
    "category": [
      "Empirical",
      "Technical",
      "Benchmark"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "DeFi Staking",
      "Logical Defect Classification",
      "LLM-guided Extraction",
      "Static Analysis",
      "Reward Manipulation",
      "State Update Omission",
      "Unauthorized Asset Access",
      "Single Pool Reliance",
      "Empirical Measurement",
      "Dataset Release"
    ],
    "summary": "本文通过分析64起安全事件和144份审计报告，归纳出面向DeFi质押的六类逻辑缺陷，并提出SSR——一种结合LLM信息抽取与静态分析的检测工具，在人工构建的基准和1.6万余合约的大规模测评中表现出高精度与召回率并开放了数据与代码。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.05827v1",
    "published": "2026-01-09",
    "update_time": "2026-01-09",
    "download_time": "2026-01-12 02:01:09"
  },
  {
    "id": "2601.05777",
    "title": "EET: Experience-Driven Early Termination for Cost-Efficient Software Engineering Agents",
    "abstract": "Software engineering (SE) agents powered by large language models are increasingly adopted in practice, yet they often incur substantial monetary cost. We introduce EET, an experience-driven early termination approach that reduces the cost of SE agents while preserving task performance. EET extracts structured experience from prior issue-resolution executions and leverages it to guide early termination during patch generation and selection, reducing unproductive iterations. We evaluate EET on the SWE-bench Verified benchmark across three representative SE agents. EET consistently reduces total cost by 19%-55% (32% on average), with negligible loss in resolution rate (at most 0.2%). These efficiency gains are achieved, on average, by identifying early-termination opportunities for 11% of issues and reducing API calls, input tokens, and output tokens by 21%, 30%, and 25%, respectively. We release the code, prompts, and data at https://github.com/EffiSEAgent/EET.",
    "arxiv_url": "https://arxiv.org/abs/2601.05777",
    "authors": [
      "Yaoqi Guo",
      "Ying Xiao",
      "Jie M. Zhang",
      "Mark Harman",
      "Yiling Lou",
      "Yang Liu",
      "Zhenpeng Chen"
    ],
    "first_author": "Yaoqi Guo",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Editing",
    "tags": [
      "Program Repair",
      "Code Agents",
      "Early Stopping",
      "Experience Retrieval",
      "Execution Trajectory Abstraction",
      "Cost Efficiency",
      "Patch Selection"
    ],
    "summary": "本文提出EET，一种将历史执行轨迹提炼为结构化经验并在补丁生成与选择阶段进行检索驱动的早停机制，能在几乎不损失修复率的情况下显著降低软件工程代理的成本与令牌使用。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.05777v1",
    "published": "2026-01-09",
    "update_time": "2026-01-09",
    "download_time": "2026-01-12 02:01:26"
  },
  {
    "id": "2601.04996",
    "title": "AlgBench: To What Extent Do Large Reasoning Models Understand Algorithms?",
    "abstract": "Reasoning ability has become a central focus in the advancement of Large Reasoning Models (LRMs). Although notable progress has been achieved on several reasoning benchmarks such as MATH500 and LiveCodeBench, existing benchmarks for algorithmic reasoning remain limited, failing to answer a critical question: Do LRMs truly master algorithmic reasoning? To answer this question, we propose AlgBench, an expert-curated benchmark that evaluates LRMs under an algorithm-centric paradigm.   AlgBench consists of over 3,000 original problems spanning 27 algorithms, constructed by ACM algorithmic experts and organized under a comprehensive taxonomy, including Euclidean-structured, non-Euclidean-structured, non-optimized, local-optimized, global-optimized, and heuristic-optimized categories. Empirical evaluations on leading LRMs (e.g., Gemini-3-Pro, DeepSeek-v3.2-Speciale and GPT-o3) reveal substantial performance heterogeneity: while models perform well on non-optimized tasks (up to 92%), accuracy drops sharply to around 49% on globally optimized algorithms such as dynamic programming. Further analysis uncovers \\textbf{strategic over-shifts}, wherein models prematurely abandon correct algorithmic designs due to necessary low-entropy tokens. These findings expose fundamental limitations of problem-centric reinforcement learning and highlight the necessity of an algorithm-centric training paradigm for robust algorithmic reasoning.",
    "arxiv_url": "https://arxiv.org/abs/2601.04996",
    "authors": [
      "Henan Sun",
      "Kaichi Yu",
      "Yuyao Wang",
      "Bowen Liu",
      "Xunkai Li",
      "Rong-Hua Li",
      "Nuo Chen",
      "Jia Li"
    ],
    "first_author": "Henan Sun",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Algorithm-centric Benchmark",
      "Algorithm Taxonomy",
      "Contamination Control",
      "Expert-curated Problems",
      "Performance Heterogeneity",
      "Dynamic Programming Weakness",
      "Strategic Over-shift",
      "Global Optimization"
    ],
    "summary": "本文提出AlgBench——一个由算法竞赛/ACM专家手工构建的、包含3000+原始题目和新算法分类的算法推理基准，并在多款大型推理模型上实证评估，揭示模型在全局优化类算法（如动态规划）上的显著弱点、性能异质性以及因低熵必要token导致的策略性过早放弃问题。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.04996v1",
    "published": "2026-01-08",
    "update_time": "2026-01-08",
    "download_time": "2026-01-12 02:05:06"
  },
  {
    "id": "2601.04920",
    "title": "Conversational AI for Rapid Scientific Prototyping: A Case Study on ESA's ELOPE Competition",
    "abstract": "Large language models (LLMs) are increasingly used as coding partners, yet their role in accelerating scientific discovery remains underexplored. This paper presents a case study of using ChatGPT for rapid prototyping in ESA's ELOPE (Event-based Lunar OPtical flow Egomotion estimation) competition. The competition required participants to process event camera data to estimate lunar lander trajectories. Despite joining late, we achieved second place with a score of 0.01282, highlighting the potential of human-AI collaboration in competitive scientific settings. ChatGPT contributed not only executable code but also algorithmic reasoning, data handling routines, and methodological suggestions, such as using fixed number of events instead of fixed time spans for windowing. At the same time, we observed limitations: the model often introduced unnecessary structural changes, gets confused by intermediate discussions about alternative ideas, occasionally produced critical errors and forgets important aspects in longer scientific discussions. By analyzing these strengths and shortcomings, we show how conversational AI can both accelerate development and support conceptual insight in scientific research. We argue that structured integration of LLMs into the scientific workflow can enhance rapid prototyping by proposing best practices for AI-assisted scientific work.",
    "arxiv_url": "https://arxiv.org/abs/2601.04920",
    "authors": [
      "Nils Einecke"
    ],
    "first_author": "Nils Einecke",
    "category": [
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Prompting",
    "tags": [
      "Human-LLM Interaction",
      "Interactive Coding",
      "Scientific Prototyping",
      "Algorithmic Reasoning",
      "Event-based Vision",
      "Egomotion Estimation",
      "Hallucination",
      "Context Drift",
      "Best Practices"
    ],
    "summary": "本文以在ESA ELOPE竞赛中与ChatGPT协作为案例，探讨对话式AI在快速科学原型开发中的作用、优势（如算法建议与数据处理代码）与局限（如结构性改动、记忆衰减与关键错误），并提出整合LLM入科研流程的实践建议。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.04920v1",
    "published": "2026-01-08",
    "update_time": "2026-01-08",
    "download_time": "2026-01-12 02:05:57"
  },
  {
    "id": "2601.05772",
    "title": "StriderSPD: Structure-Guided Joint Representation Learning for Binary Security Patch Detection",
    "abstract": "Vulnerabilities severely threaten software systems, making the timely application of security patches crucial for mitigating attacks. However, software vendors often silently patch vulnerabilities with limited disclosure, where Security Patch Detection (SPD) comes to protect software assets. Recently, most SPD studies have targeted Open-Source Software (OSS), yet a large portion of real-world software is closed-source, where patches are distributed as binaries without accessible source code. The limited binary SPD approaches often lift binaries to abstraction levels, i.e., assembly code or pseudo-code. However, assembly code is register-based instructions conveying limited semantics, while pseudo-code lacks parser-compatible grammar to extract structure, both hindering accurate vulnerability-fix representation learning. In addition, previous studies often obtain training and testing data from the same project for evaluation, which fails to reflect closed-source conditions. To alleviate the above challenges, we propose \\textbf{\\textit{StriderSPD}}, a \\underline{Str}ucture-gu\\underline{ide}d joint \\underline{r}epresentation \\underline{SPD} framework of binary code that integrates a graph branch into a large language model (LLM), leveraging structural information to guide the LLM in identifying security patches. Our novel design of the adapters in the graph branch effectively aligns the representations between assembly code and pseudo-code at the LLM's token level. We further present a two-stage training strategy to address the optimization imbalance caused by the large parameter disparity between StriderSPD's two branches, which enables proper branch fitting. To enable more realistic evaluation, we construct a binary SPD benchmark that is disjoint from prior datasets in both projects and domains and extensively evaluate StriderSPD on this benchmark.",
    "arxiv_url": "https://arxiv.org/abs/2601.05772",
    "authors": [
      "Qingyuan Li",
      "Chenchen Yu",
      "Chuanyi Li",
      "Xin-Cheng Wen",
      "Cheryl Lee",
      "Cuiyun Gao",
      "Bin Luo"
    ],
    "first_author": "Qingyuan Li",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "Binary Patch Classification",
      "Graph-guided LLM",
      "CFG GNN",
      "Adapter-mediated Alignment",
      "Two-stage Training",
      "Cross-project Benchmark",
      "Pseudo-code Semantics",
      "Inference Efficiency",
      "Cross-model Generalizability",
      "Security and Vulnerabilities"
    ],
    "summary": "本文提出 StriderSPD，将基于控制流图的图分支与大模型分支通过三路适配器在 token 级别对齐，并采用两阶段训练与跨项目跨域的二进制补丁基准，实现了显著提升的安全补丁检测效果与高效推理性能。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.05772v1",
    "published": "2026-01-09",
    "update_time": "2026-01-09",
    "download_time": "2026-01-13 01:52:31"
  },
  {
    "id": "2601.05752",
    "title": "AutoMonitor-Bench: Evaluating the Reliability of LLM-Based Misbehavior Monitor",
    "abstract": "We introduce AutoMonitor-Bench, the first benchmark designed to systematically evaluate the reliability of LLM-based misbehavior monitors across diverse tasks and failure modes. AutoMonitor-Bench consists of 3,010 carefully annotated test samples spanning question answering, code generation, and reasoning, with paired misbehavior and benign instances. We evaluate monitors using two complementary metrics: Miss Rate (MR) and False Alarm Rate (FAR), capturing failures to detect misbehavior and oversensitivity to benign behavior, respectively. Evaluating 12 proprietary and 10 open-source LLMs, we observe substantial variability in monitoring performance and a consistent trade-off between MR and FAR, revealing an inherent safety-utility tension. To further explore the limits of monitor reliability, we construct a large-scale training corpus of 153,581 samples and fine-tune Qwen3-4B-Instruction to investigate whether training on known, relatively easy-to-construct misbehavior datasets improves monitoring performance on unseen and more implicit misbehaviors. Our results highlight the challenges of reliable, scalable misbehavior monitoring and motivate future work on task-aware designing and training strategies for LLM-based monitors.",
    "arxiv_url": "https://arxiv.org/abs/2601.05752",
    "authors": [
      "Shu Yang",
      "Jingyu Hu",
      "Tong Li",
      "Hanqi Yan",
      "Wenxuan Wang",
      "Di Wang"
    ],
    "first_author": "Shu Yang",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Model Safety & Monitoring",
    "task": "Misbehavior Detection",
    "tags": [
      "Misbehavior Monitoring",
      "Miss Rate",
      "False Alarm Rate",
      "Paired Instances",
      "Cross-model Evaluation",
      "Monitor Fine-Tuning",
      "Generalization Gap",
      "Specification Gaming",
      "Sycophancy",
      "Safety Violations"
    ],
    "summary": "本文提出了AutoMonitor-Bench——一个含3,010个（误行为/良性）配对样本的基准，用以系统评估基于LLM的误行为监控器，采用Miss Rate与False Alarm Rate衡量，在12款专有与10款开源模型上进行大规模评测并探讨了通过153K样本微调提升监控器泛化能力的局限性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.05752v1",
    "published": "2026-01-09",
    "update_time": "2026-01-09",
    "download_time": "2026-01-13 01:52:54"
  },
  {
    "id": "2601.05755",
    "title": "VIGIL: Defending LLM Agents Against Tool Stream Injection via Verify-Before-Commit",
    "abstract": "LLM agents operating in open environments face escalating risks from indirect prompt injection, particularly within the tool stream where manipulated metadata and runtime feedback hijack execution flow. Existing defenses encounter a critical dilemma as advanced models prioritize injected rules due to strict alignment while static protection mechanisms sever the feedback loop required for adaptive reasoning. To reconcile this conflict, we propose \\textbf{VIGIL}, a framework that shifts the paradigm from restrictive isolation to a verify-before-commit protocol. By facilitating speculative hypothesis generation and enforcing safety through intent-grounded verification, \\textbf{VIGIL} preserves reasoning flexibility while ensuring robust control. We further introduce \\textbf{SIREN}, a benchmark comprising 959 tool stream injection cases designed to simulate pervasive threats characterized by dynamic dependencies. Extensive experiments demonstrate that \\textbf{VIGIL} outperforms state-of-the-art dynamic defenses by reducing the attack success rate by over 22\\% while more than doubling the utility under attack compared to static baselines, thereby achieving an optimal balance between security and utility. Code is available at https://anonymous.4open.science/r/VIGIL-378B/.",
    "arxiv_url": "https://arxiv.org/abs/2601.05755",
    "authors": [
      "Junda Lin",
      "Zhaomeng Zhou",
      "Zhi Zheng",
      "Shuochen Liu",
      "Tong Xu",
      "Yong Chen",
      "Enhong Chen"
    ],
    "first_author": "Junda Lin",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Agent Security",
    "task": "Tool Stream Injection Defense",
    "tags": [
      "Tool Stream Injection",
      "Verify-Before-Commit",
      "Intent-Grounded Verification",
      "Speculative Reasoning",
      "Runtime Verification",
      "Perception Sanitization",
      "Adaptive Backtracking",
      "Dynamic Replanning",
      "Agent Robustness Evaluation",
      "Alignment-Driven Vulnerability"
    ],
    "summary": "本文提出VIGIL——一种在执行前验证的LLM代理防御框架，通过意图约束、感知清洗、推测性回溯与运行时验证来抵御工具流注入，并构建了覆盖多向量注入的评测基准以验证其在降低攻击成功率同时保持实用性的效果。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.05755v1",
    "published": "2026-01-09",
    "update_time": "2026-01-09",
    "download_time": "2026-01-13 01:55:29"
  },
  {
    "id": "2601.05587",
    "title": "HogVul: Black-box Adversarial Code Generation Framework Against LM-based Vulnerability Detectors",
    "abstract": "Recent advances in software vulnerability detection have been driven by Language Model (LM)-based approaches. However, these models remain vulnerable to adversarial attacks that exploit lexical and syntax perturbations, allowing critical flaws to evade detection. Existing black-box attacks on LM-based vulnerability detectors primarily rely on isolated perturbation strategies, limiting their ability to efficiently explore the adversarial code space for optimal perturbations. To bridge this gap, we propose HogVul, a black-box adversarial code generation framework that integrates both lexical and syntax perturbations under a unified dual-channel optimization strategy driven by Particle Swarm Optimization (PSO). By systematically coordinating two-level perturbations, HogVul effectively expands the search space for adversarial examples, enhancing the attack efficacy. Extensive experiments on four benchmark datasets demonstrate that HogVul achieves an average attack success rate improvement of 26.05\\% over state-of-the-art baseline methods. These findings highlight the potential of hybrid optimization strategies in exposing model vulnerabilities.",
    "arxiv_url": "https://arxiv.org/abs/2601.05587",
    "authors": [
      "Jingxiao Yang",
      "Ping He",
      "Tianyu Du",
      "Sun Bing",
      "Xuhong Zhang"
    ],
    "first_author": "Jingxiao Yang",
    "category": [
      "Technical"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "Adversarial Code Generation",
      "Black-box Attack",
      "Lexical Perturbation",
      "Syntax Perturbation",
      "Particle Swarm Optimization",
      "Dual-channel Optimization",
      "Semantic-aware Initialization",
      "Dead-code Injection",
      "Robustness Evaluation"
    ],
    "summary": "本文提出 HogVul，一种基于粒子群优化的黑盒对抗代码生成框架，通过联合词法与语法级扰动并在双通道协同优化下生成可编译的对抗样本，从而显著提升对基于语言模型的漏洞检测器的绕过成功率并验证了方法的有效性与稳健性。",
    "quality": "High",
    "conference": "AAAI Conference on Artificial Intelligence 2026",
    "pdf_url": "https://arxiv.org/pdf/2601.05587v1",
    "published": "2026-01-09",
    "update_time": "2026-01-09",
    "download_time": "2026-01-13 01:57:38"
  },
  {
    "id": "2601.07786",
    "title": "\"TODO: Fix the Mess Gemini Created\": Towards Understanding GenAI-Induced Self-Admitted Technical Debt",
    "abstract": "As large language models (LLMs) such as ChatGPT, Copilot, Claude, and Gemini become integrated into software development workflows, developers increasingly leave traces of AI involvement in their code comments. Among these, some comments explicitly acknowledge both the use of generative AI and the presence of technical shortcomings. Analyzing 6,540 LLM-referencing code comments from public Python and JavaScript-based GitHub repositories (November 2022-July 2025), we identified 81 that also self-admit technical debt(SATD). Developers most often describe postponed testing, incomplete adaptation, and limited understanding of AI-generated code, suggesting that AI assistance affects both when and why technical debt emerges. We term GenAI-Induced Self-admitted Technical debt (GIST) as a proposed conceptual lens to describe recurring cases where developers incorporate AI-generated code while explicitly expressing uncertainty about its behavior or correctness.",
    "arxiv_url": "https://arxiv.org/abs/2601.07786",
    "authors": [
      "Abdullah Al Mujahid",
      "Mia Mohammad Imran"
    ],
    "first_author": "Abdullah Al Mujahid",
    "category": [
      "Empirical",
      "Benchmark"
    ],
    "field": "Requirements & Design",
    "task": "Analysis",
    "tags": [
      "GenAI-Induced Technical Debt",
      "Self-Admitted Technical Debt",
      "Code Comment Mining",
      "AI Attribution",
      "Requirement Debt",
      "Test Debt",
      "Design Debt",
      "Open Coding"
    ],
    "summary": "本文通过对公共GitHub中显式提及LLM的代码注释进行手工标注与分析，提出并实证化了“生成式AI引发的自我承认技术债（GIST）”概念，揭示了AI参与代码时债务类型的分布变化及开发者对AI角色的归因方式。",
    "quality": "Middle",
    "conference": "9th International Conference on Technical Debt (TechDebt 2026) 2026",
    "pdf_url": "https://arxiv.org/pdf/2601.07786v1",
    "published": "2026-01-12",
    "update_time": "2026-01-12",
    "download_time": "2026-01-14 01:59:42"
  },
  {
    "id": "2601.07602",
    "title": "OODEval: Evaluating Large Language Models on Object-Oriented Design",
    "abstract": "Recent advances in large language models (LLMs) have driven extensive evaluations in software engineering. however, most prior work concentrates on code-level tasks, leaving software design capabilities underexplored. To fill this gap, we conduct a comprehensive empirical study evaluating 29 LLMs on object-oriented design (OOD) tasks. Owing to the lack of standardized benchmarks and metrics, we introduce OODEval, a manually constructed benchmark comprising 50 OOD tasks of varying difficulty, and OODEval-Human, the first human-rated OOD benchmark, which includes 940 undergraduate-submitted class diagrams evaluated by instructors. We further propose CLUE (Class Likeness Unified Evaluation), a unified metric set that assesses both global correctness and fine-grained design quality in class diagram generation. Using these benchmarks and metrics, we investigate five research questions: overall correctness, comparison with humans, model dimension analysis, task feature analysis, and bad case analysis. The results indicate that while LLMs achieve high syntactic accuracy, they exhibit substantial semantic deficiencies, particularly in method and relationship generation. Among the evaluated models, Qwen3-Coder-30B achieves the best overall performance, rivaling DeepSeek-R1 and GPT-4o, while Gemma3-4B-IT outperforms GPT-4o-Mini despite its smaller parameter scale. Although top-performing LLMs nearly match the average performance of undergraduates, they remain significantly below the level of the best human designers. Further analysis shows that parameter scale, code specialization, and instruction tuning strongly influence performance, whereas increased design complexity and lower requirement readability degrade it. Bad case analysis reveals common failure modes, including keyword misuse, missing classes or relationships, and omitted methods.",
    "arxiv_url": "https://arxiv.org/abs/2601.07602",
    "authors": [
      "Bingxu Xiao",
      "Yunwei Dong",
      "Yiqi Tang",
      "Manqing Zhang",
      "Yifan Zhou",
      "Chunyan Ma",
      "Yepang Liu"
    ],
    "first_author": "Bingxu Xiao",
    "category": [
      "Benchmark",
      "Empirical",
      "Technical"
    ],
    "field": "Requirements & Design",
    "task": "Specification & Validation",
    "tags": [
      "Software Modeling",
      "Class Diagram Evaluation",
      "Human-Rated Benchmark",
      "CLUE Metric",
      "Requirement-to-Design",
      "Method Generation",
      "Relationship Generation",
      "Semantic Correctness",
      "Syntactic Correctness",
      "Failure Modes",
      "Model Scaling Analysis",
      "Instruction Tuning Effects"
    ],
    "summary": "本文提出了面向面向对象设计的基准OODEval及人工评分数据集OODEval-Human，并设计了CLUE评估指标，基于此对29个大模型进行系统实证评估，揭示模型在语法上较强但在方法与关系等语义层面存在明显不足。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.07602v1",
    "published": "2026-01-12",
    "update_time": "2026-01-12",
    "download_time": "2026-01-14 02:00:01"
  },
  {
    "id": "2601.07790",
    "title": "Benchmarking Small Language Models and Small Reasoning Language Models on System Log Severity Classification",
    "abstract": "System logs are crucial for monitoring and diagnosing modern computing infrastructure, but their scale and complexity require reliable and efficient automated interpretation. Since severity levels are predefined metadata in system log messages, having a model merely classify them offers limited standalone practical value, revealing little about its underlying ability to interpret system logs. We argue that severity classification is more informative when treated as a benchmark for probing runtime log comprehension rather than as an end task. Using real-world journalctl data from Linux production servers, we evaluate nine small language models (SLMs) and small reasoning language models (SRLMs) under zero-shot, few-shot, and retrieval-augmented generation (RAG) prompting. The results reveal strong stratification. Qwen3-4B achieves the highest accuracy at 95.64% with RAG, while Gemma3-1B improves from 20.25% under few-shot prompting to 85.28% with RAG. Notably, the tiny Qwen3-0.6B reaches 88.12% accuracy despite weak performance without retrieval. In contrast, several SRLMs, including Qwen3-1.7B and DeepSeek-R1-Distill-Qwen-1.5B, degrade substantially when paired with RAG. Efficiency measurements further separate models: most Gemma and Llama variants complete inference in under 1.2 seconds per log, whereas Phi-4-Mini-Reasoning exceeds 228 seconds per log while achieving <10% accuracy. These findings suggest that (1) architectural design, (2) training objectives, and (3) the ability to integrate retrieved context under strict output constraints jointly determine performance. By emphasizing small, deployable models, this benchmark aligns with real-time requirements of digital twin (DT) systems and shows that severity classification serves as a lens for evaluating model competence and real-time deployability, with implications for root cause analysis (RCA) and broader DT integration.",
    "arxiv_url": "https://arxiv.org/abs/2601.07790",
    "authors": [
      "Yahya Masri",
      "Emily Ma",
      "Zifu Wang",
      "Joseph Rogers",
      "Chaowei Yang"
    ],
    "first_author": "Yahya Masri",
    "category": [
      "Empirical",
      "Benchmark"
    ],
    "field": "AIOps",
    "task": "Log Parsing",
    "tags": [
      "Severity Classification",
      "Retrieval-Augmented Reasoning",
      "Small Model Evaluation",
      "Inference Efficiency",
      "Model Stratification",
      "Grounded Log Understanding",
      "Zero/Few-shot Robustness",
      "Digital Twin Integration"
    ],
    "summary": "该论文使用真实生产服务器日志，以零/少样本和检索增强提示评估多款小型与小型推理语言模型在日志严重性分类上的准确性与推理效率，揭示结构、训练目标与检索集成能力对性能与实时可部署性的影响。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.07790v1",
    "published": "2026-01-12",
    "update_time": "2026-01-12",
    "download_time": "2026-01-14 02:00:41"
  },
  {
    "id": "2601.07782",
    "title": "Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning",
    "abstract": "LLM agents operating over massive, dynamic tool libraries rely on effective retrieval, yet standard single-shot dense retrievers struggle with complex requests. These failures primarily stem from the disconnect between abstract user goals and technical documentation, and the limited capacity of fixed-size embeddings to model combinatorial tool compositions. To address these challenges, we propose TOOLQP, a lightweight framework that models retrieval as iterative query planning. Instead of single-shot matching, TOOLQP decomposes instructions into sub-tasks and dynamically generates queries to interact with the retriever, effectively bridging the semantic gap by targeting the specific sub-tasks required for composition. We train TOOLQP using synthetic query trajectories followed by optimization via Reinforcement Learning with Verifiable Rewards (RLVR). Experiments demonstrate that TOOLQP achieves state-of-the-art performance, exhibiting superior zero-shot generalization, robustness across diverse retrievers, and significant improvements in downstream agentic execution.",
    "arxiv_url": "https://arxiv.org/abs/2601.07782",
    "authors": [
      "Wei Fang",
      "James Glass"
    ],
    "first_author": "Wei Fang",
    "category": [
      "Technical"
    ],
    "field": "Agent Tooling",
    "task": "Tool Retrieval (Query Planning)",
    "tags": [
      "Query Planning",
      "Iterative Retrieval",
      "Tool Composition",
      "Semantic Gap Bridging",
      "Retriever-Agnostic",
      "Reinforcement Learning for Retrieval",
      "Environment Interaction",
      "Downstream Agent Grounding",
      "Zero-shot Generalization"
    ],
    "summary": "本文提出TOOLQP，一种将工具检索建模为迭代查询规划的轻量框架，通过将复杂请求分解为子任务、动态生成子查询并结合合成轨迹与可验证奖励的强化学习优化，从而在大规模动态工具库中显著提升检索准确性和下游代理执行效果。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.07782v1",
    "published": "2026-01-12",
    "update_time": "2026-01-12",
    "download_time": "2026-01-14 02:01:01"
  },
  {
    "id": "2601.08806",
    "title": "APEX-SWE",
    "abstract": "We introduce the AI Productivity Index for Software Engineering (APEX-SWE), a benchmark for assessing whether frontier AI models can execute economically valuable software engineering work. Unlike existing evaluations that focus on narrow, well-defined tasks, APEX-SWE assesses two novel task types that reflect real-world software engineering work: (1) Integration tasks (n=100), which require constructing end-to-end systems across heterogeneous cloud primitives, business applications, and infrastructure-as-code services, and (2) Observability tasks (n=100), which require debugging production failures using telemetry signals such as logs and dashboards, as well as unstructured context. We evaluated eight frontier models on APEX-SWE. Gemini 3 Pro (Thinking = High) performs best, with a Pass@1 score of 25\\%. Our analysis shows that strong performance is primarily driven by epistemic reasoning, defined as the ability to distinguish between assumptions and verified facts, combined with agency to resolve uncertainty prior to acting. We open-source the APEX-SWE evaluation harness and a dev set (n=50).",
    "arxiv_url": "https://arxiv.org/abs/2601.08806",
    "authors": [
      "Abhi Kottamasu",
      "Akul Datta",
      "Aakash Barthwal",
      "Chirag Mahapatra",
      "Ajay Arun",
      "Adarsh Hiremath",
      "Brendan Foody",
      "Bertie Vidgen"
    ],
    "first_author": "Abhi Kottamasu",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Repository-Level Coding",
    "tags": [
      "Repository-Level Coding",
      "Code Agents",
      "Production Debugging",
      "Observability",
      "End-to-End Integration",
      "Infrastructure-as-Code",
      "Telemetry Analysis",
      "Epistemic Reasoning"
    ],
    "summary": "APEX–SWE提出了一个包含100个集成任务和100个可观测性任务的基准，用于评估前沿AI模型在真实生产级软件工程（跨云原语、业务应用、基础设施即代码与生产故障排查）中的实际执行能力，并开源了开发集与评估工具，发现顶级模型Pass@1约为25%，成功往往依赖于区分假设与已验证事实的认识型推理及在行动前化解不确定性的能力。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.08806v1",
    "published": "2026-01-13",
    "update_time": "2026-01-13",
    "download_time": "2026-01-15 01:56:27"
  },
  {
    "id": "2601.08773",
    "title": "Reliable Graph-RAG for Codebases: AST-Derived Graphs vs LLM-Extracted Knowledge Graphs",
    "abstract": "Retrieval-Augmented Generation for software engineering often relies on vector similarity search, which captures topical similarity but can fail on multi-hop architectural reasoning such as controller to service to repository chains, interface-driven wiring, and inheritance. This paper benchmarks three retrieval pipelines on Java codebases (Shopizer, with additional runs on ThingsBoard and OpenMRS Core): (A) vector-only No-Graph RAG, (B) an LLM-generated knowledge graph RAG (LLM-KB), and (C) a deterministic AST-derived knowledge graph RAG (DKB) built with Tree-sitter and bidirectional traversal.   Using 15 architecture and code-tracing queries per repository, we measure indexing time, query latency, corpus coverage, cost, and answer correctness. DKB builds its graph in seconds, while LLM-KB requires much longer graph generation. LLM-KB also shows indexing incompleteness: on Shopizer, 377 files are skipped or missed, reducing embedded chunk coverage and graph size compared to DKB. End-to-end cost is modest for DKB relative to the vector-only baseline but much higher for LLM-KB, especially as repository scale increases. Query latency is similar for No-Graph and DKB, while LLM-KB is slower and more variable. On the Shopizer question suite, DKB achieves the highest correctness, LLM-KB is close behind, and the vector-only baseline performs worst on upstream architectural queries and has the highest hallucination risk. Overall, deterministic AST-derived graphs provide more reliable coverage and multi-hop grounding than LLM-extracted graphs at substantially lower indexing cost.",
    "arxiv_url": "https://arxiv.org/abs/2601.08773",
    "authors": [
      "Manideep Reddy Chinthareddy"
    ],
    "first_author": "Manideep Reddy Chinthareddy",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Retrieval",
    "tags": [
      "AST-derived Graph",
      "LLM-extracted Graph",
      "Graph-RAG",
      "Multi-hop Reasoning",
      "Repository-Level Retrieval",
      "Indexing Coverage",
      "Cost Analysis",
      "Deterministic Indexing",
      "Tree-sitter Parsing",
      "Hallucination"
    ],
    "summary": "本文在多个 Java 代码库上对比了向量检索、LLM 生成知识图谱和基于 AST 的确定性知识图谱三种 RAG 管线，结果表明基于 AST 的 DKB 在索引覆盖率、答案正确性与成本-延迟权衡上更优且更可复现，而 LLM 生成的图谱存在提取不完整、随机性高和成本显著更大的问题。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.08773v1",
    "published": "2026-01-13",
    "update_time": "2026-01-13",
    "download_time": "2026-01-15 01:56:54"
  },
  {
    "id": "2601.08778",
    "title": "Pervasive Annotation Errors Break Text-to-SQL Benchmarks and Leaderboards",
    "abstract": "Researchers have proposed numerous text-to-SQL techniques to streamline data analytics and accelerate the development of database-driven applications. To compare these techniques and select the best one for deployment, the community depends on public benchmarks and their leaderboards. Since these benchmarks heavily rely on human annotations during question construction and answer evaluation, the validity of the annotations is crucial.   In this paper, we conduct an empirical study that (i) benchmarks annotation error rates for two widely used text-to-SQL benchmarks, BIRD and Spider 2.0-Snow, and (ii) corrects a subset of the BIRD development (Dev) set to measure the impact of annotation errors on text-to-SQL agent performance and leaderboard rankings. Through expert analysis, we show that BIRD Mini-Dev and Spider 2.0-Snow have error rates of 52.8% and 62.8%, respectively. We re-evaluate all 16 open-source agents from the BIRD leaderboard on both the original and the corrected BIRD Dev subsets. We show that performance changes range from -7% to 31% (in relative terms) and rank changes range from $-9$ to $+9$ positions. We further assess whether these impacts generalize to the full BIRD Dev set. We find that the rankings of agents on the uncorrected subset correlate strongly with those on the full Dev set (Spearman's $r_s$=0.85, $p$=3.26e-5), whereas they correlate weakly with those on the corrected subset (Spearman's $r_s$=0.32, $p$=0.23). These findings show that annotation errors can significantly distort reported performance and rankings, potentially misguiding research directions or deployment choices. Our code and data are available at https://github.com/uiuc-kang-lab/text_to_sql_benchmarks.",
    "arxiv_url": "https://arxiv.org/abs/2601.08778",
    "authors": [
      "Tengjun Jin",
      "Yoojin Choi",
      "Yuxuan Zhu",
      "Daniel Kang"
    ],
    "first_author": "Tengjun Jin",
    "category": [
      "Empirical",
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Translation",
    "tags": [
      "Annotation Errors",
      "SQL Verification",
      "Agent-based Review",
      "Annotation Pipeline",
      "Diagnostic Reporting",
      "Human-in-the-loop",
      "Leaderboard Reliability",
      "Ranking Sensitivity",
      "Schema-aware Validation",
      "Execution-based Verification"
    ],
    "summary": "本文通过开发基于代理的审查工具并对 BIRD 与 Spider 2.0-Snow 进行人工验证，发现两大 text-to-SQL 基准分别存在 52.8% 和 62.8% 的标注错误，进而显著扭曲模型性能与排行榜，并提出 SAR-Agent 与 SAPAR 来检测与修正这些注释错误。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.08778v1",
    "published": "2026-01-13",
    "update_time": "2026-01-13",
    "download_time": "2026-01-15 02:06:24"
  },
  {
    "id": "2601.08777",
    "title": "Asymptotic Universal Alignment: A New Alignment Framework via Test-Time Scaling",
    "abstract": "Aligning large language models (LLMs) to serve users with heterogeneous and potentially conflicting preferences is a central challenge for personalized and trustworthy AI. We formalize an ideal notion of universal alignment through test-time scaling: for each prompt, the model produces $k\\ge 1$ candidate responses and a user selects their preferred one. We introduce $(k,f(k))$-robust alignment, which requires the $k$-output model to have win rate $f(k)$ against any other single-output model, and asymptotic universal alignment (U-alignment), which requires $f(k)\\to 1$ as $k\\to\\infty$. Our main result characterizes the optimal convergence rate: there exists a family of single-output policies whose $k$-sample product policies achieve U-alignment at rate $f(k)=\\frac{k}{k+1}$, and no method can achieve a faster rate in general.   We show that popular post-training methods, including Nash learning from human feedback (NLHF), can fundamentally underutilize the benefits of test-time scaling. Even though NLHF is optimal for $k=1$, sampling from the resulting (often deterministic) policy cannot guarantee win rates above $\\tfrac{1}{2}$ except for an arbitrarily small slack. This stems from a lack of output diversity: existing alignment methods can collapse to a single majority-preferred response, making additional samples redundant. In contrast, our approach preserves output diversity and achieves the optimal test-time scaling rate. In particular, we propose a family of symmetric multi-player alignment games and prove that any symmetric Nash equilibrium policy of the $(k+1)$-player alignment game achieves the optimal $(k,\\frac{k}{k+1})$-robust alignment. Finally, we provide theoretical convergence guarantees for self-play learning dynamics in these games and extend the framework to opponents that also generate multiple responses.",
    "arxiv_url": "https://arxiv.org/abs/2601.08777",
    "authors": [
      "Yang Cai",
      "Weiqiang Zheng"
    ],
    "first_author": "Yang Cai",
    "category": [
      "Technical"
    ],
    "field": "Model Alignment & Personalization",
    "task": "Asymptotic Universal Alignment (Test-Time Scaling)",
    "tags": [
      "Test-time Scaling",
      "Asymptotic Universal Alignment",
      "Robust Alignment",
      "Output Diversity",
      "Alignment Games",
      "Nash Equilibrium",
      "Self-Play Convergence",
      "Condorcet Analysis"
    ],
    "summary": "本文通过在测试时生成多候选回复的框架形式化了渐近普适对齐（U‑alignment），证明存在单输出策略家族通过测试时扩展以最优速率k/(k+1)趋近普适对齐，提出对称多玩家对齐博弈以保持输出多样性并给出自我博弈的收敛性保证，同时指出现有方法在测试时扩展下会因输出塌缩而丧失潜在优势。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.08777v1",
    "published": "2026-01-13",
    "update_time": "2026-01-13",
    "download_time": "2026-01-15 02:09:17"
  },
  {
    "id": "2601.09703",
    "title": "ShortCoder: Knowledge-Augmented Syntax Optimization for Token-Efficient Code Generation",
    "abstract": "Code generation tasks aim to automate the conversion of user requirements into executable code, significantly reducing manual development efforts and enhancing software productivity. The emergence of large language models (LLMs) has significantly advanced code generation, though their efficiency is still impacted by certain inherent architectural constraints. Each token generation necessitates a complete inference pass, requiring persistent retention of contextual information in memory and escalating resource consumption. While existing research prioritizes inference-phase optimizations such as prompt compression and model quantization, the generation phase remains underexplored. To tackle these challenges, we propose a knowledge-infused framework named ShortCoder, which optimizes code generation efficiency while preserving semantic equivalence and readability. In particular, we introduce: (1) ten syntax-level simplification rules for Python, derived from AST-preserving transformations, achieving 18.1% token reduction without functional compromise; (2) a hybrid data synthesis pipeline integrating rule-based rewriting with LLM-guided refinement, producing ShorterCodeBench, a corpus of validated tuples of original code and simplified code with semantic consistency; (3) a fine-tuning strategy that injects conciseness awareness into the base LLMs. Extensive experimental results demonstrate that ShortCoder consistently outperforms state-of-the-art methods on HumanEval, achieving an improvement of 18.1%-37.8% in generation efficiency over previous methods while ensuring the performance of code generation.",
    "arxiv_url": "https://arxiv.org/abs/2601.09703",
    "authors": [
      "Sicong Liu",
      "Yanxian Huang",
      "Mingwei Liu",
      "Jiachi Chen",
      "Ensheng Shi",
      "Yuchi Ma",
      "Hongyu Zhang",
      "Yin Zhang",
      "Yanlin Wang"
    ],
    "first_author": "Sicong Liu",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Performance Optimization",
    "tags": [
      "Token Efficiency",
      "Syntax Simplification",
      "AST-preserving Transformation",
      "Rule-based Rewriting",
      "LLM-guided Refinement",
      "Conciseness-aware Fine-tuning",
      "Data Synthesis",
      "Generation Latency Reduction",
      "Readability Preservation"
    ],
    "summary": "ShortCoder提出一种知识增强的Python语法简化与微调框架，基于10条AST保持的简化规则和混合数据合成（规则重写+LLM精化）生成简洁代码对并注入简洁性意识至模型，从而在保持语义正确性的前提下显著减少生成token数并提升代码生成效率。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.09703v1",
    "published": "2026-01-14",
    "update_time": "2026-01-14",
    "download_time": "2026-01-16 01:56:29"
  },
  {
    "id": "2601.09695",
    "title": "How well LLM-based test generation techniques perform with newer LLM versions?",
    "abstract": "The rapid evolution of Large Language Models (LLMs) has strongly impacted software engineering, leading to a growing number of studies on automated unit test generation. However, the standalone use of LLMs without post-processing has proven insufficient, often producing tests that fail to compile or achieve high coverage. Several techniques have been proposed to address these issues, reporting improvements in test compilation and coverage. While important, LLM-based test generation techniques have been evaluated against relatively weak baselines (for todays' standards), i.e., old LLM versions and relatively weak prompts, which may exacerbate the performance contribution of the approaches. In other words, stronger (newer) LLMs may obviate any advantage these techniques bring. We investigate this issue by replicating four state-of-the-art LLM-based test generation tools, HITS, SymPrompt, TestSpark, and CoverUp that include engineering components aimed at guiding the test generation process through compilation and execution feedback, and evaluate their relative effectiveness and efficiency over a plain LLM test generation method. We integrate current LLM versions in all approaches and run an experiment on 393 classes and 3,657 methods. Our results show that the plain LLM approach can outperform previous state-of-the-art approaches in all test effectiveness metrics we used: line coverage (by 17.72%), branch coverage (by 19.80%) and mutation score (by 20.92%), and it does so at a comparable cost (LLM queries). We also observe that the granularity at which the plain LLM is applied has a significant impact on the cost. We therefore propose targeting first the program classes, where test generation is more efficient, and then the uncovered methods to reduce the number of LLM requests. This strategy achieves comparable (slightly higher) effectiveness while requiring about 20% fewer LLM requests.",
    "arxiv_url": "https://arxiv.org/abs/2601.09695",
    "authors": [
      "Michael Konstantinou",
      "Renzo Degiovanni",
      "Mike Papadakis"
    ],
    "first_author": "Michael Konstantinou",
    "category": [
      "Empirical"
    ],
    "field": "Software Testing",
    "task": "Test Generation",
    "tags": [
      "Mutation Testing",
      "Class-level Generation",
      "Method-level Generation",
      "Hybrid Granularity",
      "LLM Cost Analysis",
      "Compilation Failures",
      "Coverage Evaluation",
      "Replication Study",
      "LLM Version Comparison"
    ],
    "summary": "本文复现并比较了四种基于LLM的单元测试生成工具与简单的零样本LLM提示在新一代LLM（如gpt-4o-mini、Llama 3.3、DeepSeek V3）上的效果与成本，结果表明Plain-LLM在覆盖率和变异得分上超越了这些方法，并提出先按类再针对未覆盖方法的混合粒度策略可在保持效果的同时节省约20%请求成本。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.09695v1",
    "published": "2026-01-14",
    "update_time": "2026-01-14",
    "download_time": "2026-01-16 01:56:52"
  },
  {
    "id": "2601.09282",
    "title": "Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing",
    "abstract": "Cluster workload allocation often requires complex configurations, creating a usability gap. This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. The system employs a Large Language Model (LLM) integrated via a Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. A prototype featuring a cluster state cache and an intent analyzer (using AWS Bedrock) was developed. Empirical evaluation demonstrated high LLM parsing accuracy (>95% Subset Accuracy on an evaluation ground-truth dataset) for top-tier models like Amazon Nova Pro/Premier and Mistral Pixtral Large, significantly outperforming a baseline engine. Scheduling quality tests across six scenarios showed the prototype achieved superior or equivalent placement compared to standard Kubernetes configurations, particularly excelling in complex and quantitative scenarios and handling conflicting soft preferences. The results validate using LLMs for accessible scheduling but highlight limitations like synchronous LLM latency, suggesting asynchronous processing for production readiness. This work confirms the viability of semantic soft affinity for simplifying workload orchestration.",
    "arxiv_url": "https://arxiv.org/abs/2601.09282",
    "authors": [
      "Leszek Sliwko",
      "Jolanta Mizeria-Pietraszko"
    ],
    "first_author": "Leszek Sliwko",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Cluster Management & Scheduling",
    "task": "Semantic Soft-Affinity Scheduling",
    "tags": [
      "Semantic Soft-Affinity",
      "Natural-Language Intent Parsing",
      "Kubernetes Scheduler Extender",
      "Cluster State Cache",
      "Score Scalarization",
      "Placement Quality Evaluation",
      "Synchronous LLM Latency"
    ],
    "summary": "本文提出并实现了一个基于大语言模型的语义意图驱动Kubernetes调度原型，将自然语言的分配提示解析为软亲和性偏好以优化节点放置，并在评估中表现出高解析准确率和优越或相当的调度质量，同时指出同步LLM延迟为部署限制。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.09282v1",
    "published": "2026-01-14",
    "update_time": "2026-01-14",
    "download_time": "2026-01-16 02:02:14"
  },
  {
    "id": "2601.09097",
    "title": "Programming over Thinking: Efficient and Robust Multi-Constraint Planning",
    "abstract": "Multi-constraint planning involves identifying, evaluating, and refining candidate plans while satisfying multiple, potentially conflicting constraints. Existing large language model (LLM) approaches face fundamental limitations in this domain. Pure reasoning paradigms, which rely on long natural language chains, are prone to inconsistency, error accumulation, and prohibitive cost as constraints compound. Conversely, LLMs combined with coding- or solver-based strategies lack flexibility: they often generate problem-specific code from scratch or depend on fixed solvers, failing to capture generalizable logic across diverse problems. To address these challenges, we introduce the Scalable COde Planning Engine (SCOPE), a framework that disentangles query-specific reasoning from generic code execution. By separating reasoning from execution, SCOPE produces solver functions that are consistent, deterministic, and reusable across queries while requiring only minimal changes to input parameters. SCOPE achieves state-of-the-art performance while lowering cost and latency. For example, with GPT-4o, it reaches 93.1% success on TravelPlanner, a 61.6% gain over the best baseline (CoT) while cutting inference cost by 1.4x and time by ~4.67x. Code is available at https://github.com/DerrickGXD/SCOPE.",
    "arxiv_url": "https://arxiv.org/abs/2601.09097",
    "authors": [
      "Derrick Goh Xin Deik",
      "Quanyu Long",
      "Zhengyuan Liu",
      "Nancy F. Chen",
      "Wenya Wang"
    ],
    "first_author": "Derrick Goh Xin Deik",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Code Agents",
      "Reusable Solver",
      "Problem Formalization",
      "Parameter Extraction",
      "Structured Representation",
      "Combination Enumeration",
      "Output Reflection",
      "Deterministic Execution",
      "Efficiency",
      "Robustness"
    ],
    "summary": "本文提出SCOPE，一种将查询特定的结构化问题形式化与通用可重用求解器生成分离的多代理框架，能从单个示例自动抽取参数化表示并生成确定性、可复用的求解函数，从而在多约束规划任务上实现更高的鲁棒性与更低的成本与延迟。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.09097v1",
    "published": "2026-01-14",
    "update_time": "2026-01-14",
    "download_time": "2026-01-16 02:05:26"
  },
  {
    "id": "2601.10496",
    "title": "Model See, Model Do? Exposure-Aware Evaluation of Bug-vs-Fix Preference in Code LLMs",
    "abstract": "Large language models are increasingly used for code generation and debugging, but their outputs can still contain bugs, that originate from training data. Distinguishing whether an LLM prefers correct code, or a familiar incorrect version might be influenced by what it's been exposed to during training. We introduce an exposure-aware evaluation framework that quantifies how prior exposure to buggy versus fixed code influences a model's preference. Using the ManySStuBs4J benchmark, we apply Data Portraits for membership testing on the Stack-V2 corpus to estimate whether each buggy and fixed variant was seen during training. We then stratify examples by exposure and compare model preference using code completion as well as multiple likelihood-based scoring metrics We find that most examples (67%) have neither variant in the training data, and when only one is present, fixes are more frequently present than bugs. In model generations, models reproduce buggy lines far more often than fixes, with bug-exposed examples amplifying this tendency and fix-exposed examples showing only marginal improvement. In likelihood scoring, minimum and maximum token-probability metrics consistently prefer the fixed code across all conditions, indicating a stable bias toward correct fixes. In contrast, metrics like the Gini coefficient reverse preference when only the buggy variant was seen. Our results indicate that exposure can skew bug-fix evaluations and highlight the risk that LLMs may propagate memorised errors in practice.",
    "arxiv_url": "https://arxiv.org/abs/2601.10496",
    "authors": [
      "Ali Al-Kaswan",
      "Claudio Spiess",
      "Prem Devanbu",
      "Arie van Deursen",
      "Maliheh Izadi"
    ],
    "first_author": "Ali Al-Kaswan",
    "category": [
      "Empirical",
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Completion",
    "tags": [
      "Exposure-aware evaluation",
      "Membership inference",
      "Bug propagation",
      "Memorisation",
      "Likelihood-based scoring",
      "Generation matching",
      "Metric sensitivity",
      "Model bias"
    ],
    "summary": "本文提出一种面向训练数据曝光的评估框架，基于成员资格推断判断SStuBs中的bug/修复是否出现在训练语料，进而通过似然评分与生成匹配分析曝光如何影响代码LLM对bug与修复的偏好，发现生成端更易复现bug且曝光会放大此现象，而某些似然指标则稳定偏好修复。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.10496v1",
    "published": "2026-01-15",
    "update_time": "2026-01-15",
    "download_time": "2026-01-17 01:50:45"
  },
  {
    "id": "2601.10258",
    "title": "Evolving with AI: A Longitudinal Analysis of Developer Logs",
    "abstract": "AI-powered coding assistants are rapidly becoming fixtures in professional IDEs, yet their sustained influence on everyday development remains poorly understood. Prior research has focused on short-term use or self-reported perceptions, leaving open questions about how sustained AI use reshapes actual daily coding practices in the long term. We address this gap with a mixed-method study of AI adoption in IDEs, combining longitudinal two-year fine-grained telemetry from 800 developers with a survey of 62 professionals. We analyze five dimensions of workflow change: productivity, code quality, code editing, code reuse, and context switching. Telemetry reveals that AI users produce substantially more code but also delete significantly more. Meanwhile, survey respondents report productivity gains and perceive minimal changes in other dimensions. Our results offer empirical insights into the silent restructuring of software workflows and provide implications for designing future AI-augmented tooling.",
    "arxiv_url": "https://arxiv.org/abs/2601.10258",
    "authors": [
      "Agnia Sergeyuk",
      "Eric Huang",
      "Dariia Karaeva",
      "Anastasiia Serova",
      "Yaroslav Golubev",
      "Iftekhar Ahmed"
    ],
    "first_author": "Agnia Sergeyuk",
    "category": [
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Editing",
    "tags": [
      "Longitudinal Telemetry",
      "IDE Interaction",
      "Productivity",
      "Code Quality",
      "Code Reuse",
      "Context Switching",
      "Human-LLM Interaction",
      "Survey Triangulation",
      "Workflow Fragmentation",
      "External Code Copying"
    ],
    "summary": "本文通过对800名开发者两年IDE遥测日志与62份问卷的混合方法纵向分析，发现使用AI助手的开发者产出更多代码但删除也更多、更频繁复制外部代码且切换上下文更频繁，而受访者自我感知主要表现为生产力提升。",
    "quality": "High",
    "conference": "ICSE (International Conference on Software Engineering) 2026",
    "pdf_url": "https://arxiv.org/pdf/2601.10258v1",
    "published": "2026-01-15",
    "update_time": "2026-01-15",
    "download_time": "2026-01-17 01:51:19"
  },
  {
    "id": "2601.10402",
    "title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering",
    "abstract": "The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable blueprint for AI capable of autonomous exploration beyond human-precedent complexities.",
    "arxiv_url": "https://arxiv.org/abs/2601.10402",
    "authors": [
      "Xinyu Zhu",
      "Yuzhu Cai",
      "Zexi Liu",
      "Bingyang Zheng",
      "Cheng Wang",
      "Rui Ye",
      "Jiaao Chen",
      "Hanrui Wang",
      "Wei-Chen Wang",
      "Yuzhi Zhang",
      "Linfeng Zhang",
      "Weinan E",
      "Di Jin",
      "Siheng Chen"
    ],
    "first_author": "Xinyu Zhu",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Agents",
    "tags": [
      "Ultra-Long-Horizon",
      "Hierarchical Memory",
      "Context Management",
      "Agentic Science",
      "Cognitive Caching",
      "Experiment Planning",
      "Long-Running Automation",
      "Benchmark Evaluation"
    ],
    "summary": "论文提出面向超长周期自主科研的智能体ML‑Master 2.0，通过层级化认知缓存（HCC）将短期经验提炼为稳定知识与跨任务“智慧”，在OpenAI的MLE‑Bench上24小时预算下取得SOTA表现，显著提升机器学习工程任务的长期探索与策略一致性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.10402v1",
    "published": "2026-01-15",
    "update_time": "2026-01-15",
    "download_time": "2026-01-17 01:55:22"
  },
  {
    "id": "2601.10343",
    "title": "OctoBench: Benchmarking Scaffold-Aware Instruction Following in Repository-Grounded Agentic Coding",
    "abstract": "Modern coding scaffolds turn LLMs into capable software agents, but their ability to follow scaffold-specified instructions remains under-examined, especially when constraints are heterogeneous and persist across interactions. To fill this gap, we introduce OctoBench, which benchmarks scaffold-aware instruction following in repository-grounded agentic coding. OctoBench includes 34 environments and 217 tasks instantiated under three scaffold types, and is paired with 7,098 objective checklist items. To disentangle solving the task from following the rules, we provide an automated observation-and-scoring toolkit that captures full trajectories and performs fine-grained checks. Experiments on eight representative models reveal a systematic gap between task-solving and scaffold-aware compliance, underscoring the need for training and evaluation that explicitly targets heterogeneous instruction following. We release the benchmark to support reproducible benchmarking and to accelerate the development of more scaffold-aware coding agents.",
    "arxiv_url": "https://arxiv.org/abs/2601.10343",
    "authors": [
      "Deming Ding",
      "Shichun Liu",
      "Enhui Yang",
      "Jiahang Lin",
      "Ziying Chen",
      "Shihan Dou",
      "Honglin Guo",
      "Weiyu Cheng",
      "Pengyu Zhao",
      "Chengjun Xiao",
      "Qunhong Zeng",
      "Qi Zhang",
      "Xuanjing Huang",
      "Qidi Xu",
      "Tao Gui"
    ],
    "first_author": "Deming Ding",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Code Agents",
      "Repository-Level Coding",
      "Scaffold Compliance",
      "Checklist-based Evaluation",
      "LLM-as-a-Judge",
      "Observation Harness",
      "Instruction Prioritization",
      "Cross-Scaffold Robustness",
      "Conflict Testing"
    ],
    "summary": "本文提出OCTOBENCH——一个面向仓库驱动的代理化编程脚手架的指令遵循基准，包含34个环境、217个任务与7098个可判定检查项，并配套轨迹记录与自动化打分工具以评估模型在异构、持久约束与冲突情形下的合规性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.10343v1",
    "published": "2026-01-15",
    "update_time": "2026-01-15",
    "download_time": "2026-01-17 01:56:20"
  },
  {
    "id": "2601.10338",
    "title": "Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale",
    "abstract": "The rise of AI agent frameworks has introduced agent skills, modular packages containing instructions and executable code that dynamically extend agent capabilities. While this architecture enables powerful customization, skills execute with implicit trust and minimal vetting, creating a significant yet uncharacterized attack surface. We conduct the first large-scale empirical security analysis of this emerging ecosystem, collecting 42,447 skills from two major marketplaces and systematically analyzing 31,132 using SkillScan, a multi-stage detection framework integrating static analysis with LLM-based semantic classification. Our findings reveal pervasive security risks: 26.1% of skills contain at least one vulnerability, spanning 14 distinct patterns across four categories: prompt injection, data exfiltration, privilege escalation, and supply chain risks. Data exfiltration (13.3%) and privilege escalation (11.8%) are most prevalent, while 5.2% of skills exhibit high-severity patterns strongly suggesting malicious intent. We find that skills bundling executable scripts are 2.12x more likely to contain vulnerabilities than instruction-only skills (OR=2.12, p<0.001). Our contributions include: (1) a grounded vulnerability taxonomy derived from 8,126 vulnerable skills, (2) a validated detection methodology achieving 86.7% precision and 82.5% recall, and (3) an open dataset and detection toolkit to support future research. These results demonstrate an urgent need for capability-based permission systems and mandatory security vetting before this attack vector is further exploited.",
    "arxiv_url": "https://arxiv.org/abs/2601.10338",
    "authors": [
      "Yi Liu",
      "Weizhe Wang",
      "Ruitao Feng",
      "Yao Zhang",
      "Guangquan Xu",
      "Gelei Deng",
      "Yuekang Li",
      "Leo Zhang"
    ],
    "first_author": "Yi Liu",
    "category": [
      "Empirical",
      "Benchmark"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "Agent Skills",
      "Supply Chain Risks",
      "Prompt Injection",
      "Data Exfiltration",
      "Privilege Escalation",
      "Static Analysis",
      "Semantic Classification",
      "Detection Framework",
      "Security Taxonomy",
      "Open Dataset"
    ],
    "summary": "本文对两大市场收集的3万余个AI代理技能进行大规模实证安全分析，提出结合静态分析与LLM语义分类的SkillScan框架，揭示四大类14种广泛存在的漏洞并公开数据与工具以促进防护研究。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.10338v1",
    "published": "2026-01-15",
    "update_time": "2026-01-15",
    "download_time": "2026-01-18 02:03:20"
  },
  {
    "id": "2601.10253",
    "title": "Developer Interaction Patterns with Proactive AI: A Five-Day Field Study",
    "abstract": "Current in-IDE AI coding tools typically rely on time-consuming manual prompting and context management, whereas proactive alternatives that anticipate developer needs without explicit invocation remain underexplored. Understanding when humans are receptive to such proactive AI assistance during their daily work remains an open question in human-AI interaction research. We address this gap through a field study of proactive AI assistance in professional developer workflows. We present a five-day in-the-wild study with 15 developers who interacted with a proactive feature of an AI assistant integrated into a production-grade IDE that offers code quality suggestions based on in-IDE developer activity. We examined 229 AI interventions across 5,732 interaction points to understand how proactive suggestions are received across workflow stages, how developers experience them, and their perceived impact. Our findings reveal systematic patterns in human receptivity to proactive suggestions: interventions at workflow boundaries (e.g., post-commit) achieved 52% engagement rates, while mid-task interventions (e.g., on declined edit) were dismissed 62% of the time. Notably, well-timed proactive suggestions required significantly less interpretation time than reactive suggestions (45.4s versus 101.4s, W = 109.00, r = 0.533, p = 0.0016), indicating enhanced cognitive alignment. This study provides actionable implications for designing proactive coding assistants, including how to time interventions, align them with developer context, and strike a balance between AI agency and user control in production IDEs.",
    "arxiv_url": "https://arxiv.org/abs/2601.10253",
    "authors": [
      "Nadine Kuo",
      "Agnia Sergeyuk",
      "Valerie Chen",
      "Maliheh Izadi"
    ],
    "first_author": "Nadine Kuo",
    "category": [
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Human-LLM Interaction",
    "tags": [
      "Proactive Assistance",
      "In-IDE Integration",
      "Timing of Interventions",
      "Workflow Boundaries",
      "User Control",
      "Cognitive Load",
      "Code Quality Suggestions",
      "Field Study"
    ],
    "summary": "论文通过将主动式代码质量建议集成到生产级IDE并进行为期五天的实地研究，揭示开发者对LLM主动干预的时机依赖型接受模式（如提交后更易接受），并提出改进主动助理设计的实践启示。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.10253v1",
    "published": "2026-01-15",
    "update_time": "2026-01-15",
    "download_time": "2026-01-18 02:03:36"
  },
  {
    "id": "2601.10681",
    "title": "Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems",
    "abstract": "Large language model (LLM) contexts are typically constructed using retrieval-augmented generation (RAG), which involves ranking and selecting the top-k passages. The approach causes fragmentation in information graphs in document structures, over-retrieval, and duplication of content alongside insufficient query context, including 2nd and 3rd order facets. In this paper, a structure-informed and diversity-constrained context bubble construction framework is proposed that assembles coherent, citable bundles of spans under a strict token budget. The method preserves and exploits inherent document structure by organising multi-granular spans (e.g., sections and rows) and using task-conditioned structural priors to guide retrieval. Starting from high-relevance anchor spans, a context bubble is constructed through constrained selection that balances query relevance, marginal coverage, and redundancy penalties. It will explicitly constrain diversity and budget, producing compact and informative context sets, unlike top-k retrieval. Moreover, a full retrieval is emitted that traces the scoring and selection choices of the records, thus providing auditability and deterministic tuning. Experiments on enterprise documents demonstrate the efficiency of context bubble as it significantly reduces redundant context, is better able to cover secondary facets and has a better answer quality and citation faithfulness within a limited context window. Ablation studies demonstrate that both structural priors as well as diversity constraint selection are necessary; removing either component results in a decline in coverage and an increase in redundant or incomplete context.",
    "arxiv_url": "https://arxiv.org/abs/2601.10681",
    "authors": [
      "Amir Khurshid",
      "Abhishek Sehgal"
    ],
    "first_author": "Amir Khurshid",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Retrieval",
    "tags": [
      "Context Window Optimization",
      "Structure-Aware Retrieval",
      "Diversity-Constrained Selection",
      "MMR-style Selection",
      "Auditability",
      "Token Budgeting",
      "Enterprise Documents",
      "Citation Faithfulness"
    ],
    "summary": "该论文提出一种结构感知与多样性约束的“上下文气泡”构造方法，在严格token预算下从企业文档中选择多粒度片段以兼顾相关性、覆盖与去冗余，并通过可审计的选择流程显著提升RAG的答案质量与引用可信度。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.10681v1",
    "published": "2026-01-15",
    "update_time": "2026-01-15",
    "download_time": "2026-01-18 02:04:16"
  },
  {
    "id": "2601.10498",
    "title": "Projected Microbatch Accumulation yields reference-free proximal policy updates for reinforcement learning",
    "abstract": "This note introduces Projected Microbatch Accumulation (PROMA), a proximal policy update method for large language model fine-tuning. PROMA accumulates policy gradients across microbatches by projecting out sequence-wise gradient components before microbatch aggregation. The projection is applied layer-wise during the backward pass, enabling efficient implementation without additional forward or backward passes. Empirically, PROMA enforces tighter control of local KL divergence than GRPO, resulting in more stable policy learning. Unlike PPO and GRPO, PROMA achieves proximal updates without inducing entropy collapse and does not rely on a reference policy or likelihood-ratio clipping.",
    "arxiv_url": "https://arxiv.org/abs/2601.10498",
    "authors": [
      "Nilin Abrahamsen"
    ],
    "first_author": "Nilin Abrahamsen",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Alignment",
    "tags": [
      "RLHF Optimization",
      "Proximal Updates",
      "Reference-Free RL",
      "Gradient Projection",
      "KL Control",
      "Entropy Preservation",
      "Microbatch Accumulation",
      "Stability"
    ],
    "summary": "本文提出PROMA方法，通过在反向传播中对微批次累积梯度进行序列梯度正交投影，实现无需参考策略与比值截断的近端策略更新，从而在LLM强化学习微调中更稳健地控制KL并保持更高熵与平滑更新。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.10498v1",
    "published": "2026-01-15",
    "update_time": "2026-01-15",
    "download_time": "2026-01-18 02:05:45"
  },
  {
    "id": "2601.11362",
    "title": "RITA: A Tool for Automated Requirements Classification and Specification from Online User Feedback",
    "abstract": "Context and motivation. Online user feedback is a valuable resource for requirements engineering, but its volume and noise make analysis difficult. Existing tools support individual feedback analysis tasks, but their capabilities are rarely integrated into end-to-end support. Problem. The lack of end-to-end integration limits the practical adoption of existing RE tools and makes it difficult to assess their real-world usefulness. Solution. To address this challenge, we present RITA, a tool that integrates lightweight open-source large language models into a unified workflow for feedback-driven RE. RITA supports automated request classification, non-functional requirement identification, and natural-language requirements specification generation from online feedback via a user-friendly interface, and integrates with Jira for seamless transfer of requirements specifications to development tools. Results and conclusions. RITA exploits previously evaluated LLM-based RE techniques to efficiently transform raw user feedback into requirements artefacts, helping bridge the gap between research and practice. A demonstration is available at: https://youtu.be/8meCLpwQWV8.",
    "arxiv_url": "https://arxiv.org/abs/2601.11362",
    "authors": [
      "Manjeshwar Aniruddh Mallya",
      "Alessio Ferrari",
      "Mohammad Amin Zadenoori",
      "Jacek Dąbrowski"
    ],
    "first_author": "Manjeshwar Aniruddh Mallya",
    "category": [
      "Technical"
    ],
    "field": "Requirements & Design",
    "task": "Specification & Validation",
    "tags": [
      "Requirements Classification",
      "Non-Functional Requirements",
      "User Feedback Mining",
      "Requirements Specification",
      "User Stories",
      "Jira Integration",
      "Prompt Engineering",
      "Lightweight LLMs",
      "End-to-End Tool"
    ],
    "summary": "本文介绍了RITA工具，将轻量开源LLM整合到端到端流程中，从在线用户反馈自动进行请求分类、非功能需求识别与需求规格/用户故事生成，并支持一键同步至Jira。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.11362v1",
    "published": "2026-01-16",
    "update_time": "2026-01-16",
    "download_time": "2026-01-19 02:01:11"
  },
  {
    "id": "2601.11077",
    "title": "ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development",
    "abstract": "The evolution of Large Language Models (LLMs) into autonomous agents has expanded the scope of AI coding from localized code generation to complex, repository-level, and execution-driven problem solving. However, current benchmarks predominantly evaluate code logic in static contexts, neglecting the dynamic, full-process requirements of real-world engineering, particularly in backend development which demands rigorous environment configuration and service deployment. To address this gap, we introduce ABC-Bench, a benchmark explicitly designed to evaluate agentic backend coding within a realistic, executable workflow. Using a scalable automated pipeline, we curated 224 practical tasks spanning 8 languages and 19 frameworks from open-source repositories. Distinct from previous evaluations, ABC-Bench require the agents to manage the entire development lifecycle from repository exploration to instantiating containerized services and pass the external end-to-end API tests. Our extensive evaluation reveals that even state-of-the-art models struggle to deliver reliable performance on these holistic tasks, highlighting a substantial disparity between current model capabilities and the demands of practical backend engineering. Our code is available at https://github.com/OpenMOSS/ABC-Bench.",
    "arxiv_url": "https://arxiv.org/abs/2601.11077",
    "authors": [
      "Jie Yang",
      "Honglin Guo",
      "Li Ji",
      "Jiazheng Zhou",
      "Rui Zheng",
      "Zhikai Lei",
      "Shuo Zhang",
      "Zhiheng Xi",
      "Shichun Liu",
      "Yuxin Wang",
      "Bo Wang",
      "Yining Zheng",
      "Tao Gui",
      "Xipeng Qiu"
    ],
    "first_author": "Jie Yang",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Repository-Level Coding",
    "tags": [
      "Repository-Level Coding",
      "Code Agents",
      "Containerization",
      "Environment Configuration",
      "End-to-End API Testing",
      "Task Generation Pipeline",
      "Deployment Robustness",
      "Backend Framework Diversity"
    ],
    "summary": "本文提出ABC-Bench，一个由224个可执行后端全生命周期任务构成的基准与自动化构建流水线，要求代理完成仓库探索、代码修改、环境配置与容器化部署并通过端到端API测试，评估显示环境配置与部署为当前模型的主要瓶颈。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.11077v1",
    "published": "2026-01-16",
    "update_time": "2026-01-16",
    "download_time": "2026-01-19 02:01:37"
  },
  {
    "id": "2601.10398",
    "title": "LatentRefusal: Latent-Signal Refusal for Unanswerable Text-to-SQL Queries",
    "abstract": "In LLM-based text-to-SQL systems, unanswerable and underspecified user queries may generate not only incorrect text but also executable programs that yield misleading results or violate safety constraints, posing a major barrier to safe deployment. Existing refusal strategies for such queries either rely on output-level instruction following, which is brittle due to model hallucinations, or estimate output uncertainty, which adds complexity and overhead. To address this challenge, we formalize safe refusal in text-to-SQL systems as an answerability-gating problem and propose LatentRefusal, a latent-signal refusal mechanism that predicts query answerability from intermediate hidden activations of a large language model. We introduce the Tri-Residual Gated Encoder, a lightweight probing architecture, to suppress schema noise and amplify sparse, localized cues of question-schema mismatch that indicate unanswerability. Extensive empirical evaluations across diverse ambiguous and unanswerable settings, together with ablation studies and interpretability analyses, demonstrate the effectiveness of the proposed approach and show that LatentRefusal provides an attachable and efficient safety layer for text-to-SQL systems. Across four benchmarks, LatentRefusal improves average F1 to 88.5 percent on both backbones while adding approximately 2 milliseconds of probe overhead.",
    "arxiv_url": "https://arxiv.org/abs/2601.10398",
    "authors": [
      "Xuancheng Ren",
      "Shijing Hu",
      "Zhihui Lu",
      "Jiangqi Huang",
      "Qiang Duan"
    ],
    "first_author": "Xuancheng Ren",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Alignment",
    "tags": [
      "Pre-generation refusal",
      "Latent-signal probing",
      "Gated encoder probe",
      "Question-schema mismatch",
      "Schema-noise suppression",
      "Single-pass gating",
      "Execution-free safety",
      "Low-latency"
    ],
    "summary": "本文提出LatentRefusal，通过在冻结的大型模型中对中间隐藏态进行轻量探针检测并放大问题与数据库模式不匹配的稀疏信号，从而在生成或执行任何SQL之前以单次前向、低延迟且无执行的方式判断Text-to-SQL查询是否可回答并进行安全拒绝。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.10398v1",
    "published": "2026-01-15",
    "update_time": "2026-01-15",
    "download_time": "2026-01-19 02:07:00"
  },
  {
    "id": "2601.10011",
    "title": "Memo-SQL: Structured Decomposition and Experience-Driven Self-Correction for Training-Free NL2SQL",
    "abstract": "Existing NL2SQL systems face two critical limitations: (1) they rely on in-context learning with only correct examples, overlooking the rich signal in historical error-fix pairs that could guide more robust self-correction; and (2) test-time scaling approaches often decompose questions arbitrarily, producing near-identical SQL candidates across runs and diminishing ensemble gains. Moreover, these methods suffer from a stark accuracy-efficiency trade-off: high performance demands excessive computation, while fast variants compromise quality. We present Memo-SQL, a training-free framework that addresses these issues through two simple ideas: structured decomposition and experience-aware self-correction. Instead of leaving decomposition to chance, we apply three clear strategies, entity-wise, hierarchical, and atomic sequential, to encourage diverse reasoning. For correction, we build a dynamic memory of both successful queries and historical error-fix pairs, and use retrieval-augmented prompting to bring relevant examples into context at inference time, no fine-tuning or external APIs required. On BIRD, Memo-SQL achieves 68.5% execution accuracy, setting a new state of the art among open, zero-fine-tuning methods, while using over 10 times fewer resources than prior TTS approaches.",
    "arxiv_url": "https://arxiv.org/abs/2601.10011",
    "authors": [
      "Zerui Yang",
      "Weichuan Wang",
      "Yanwei Xu",
      "Linqi Song",
      "Yudai Matsuda",
      "Wei Han",
      "Bo Bai"
    ],
    "first_author": "Zerui Yang",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Translation",
    "tags": [
      "Structured Decomposition",
      "Error-Correction Memory",
      "Retrieval-Augmented ICL",
      "Experience-Guided Self-Correction",
      "ReAct+Reflection",
      "Test-Time Scaling",
      "Ensemble Diversity",
      "Execution-Guided Repair",
      "NL2SQL"
    ],
    "summary": "本文提出Memo-SQL，一种无需训练的NL2SQL框架，通过实体/层次/原子三种结构化分解策略结合基于历史错误—修正对的检索增强即时自我纠正（ReAct+Reflection），在显著降低计算开销的同时提升执行准确率。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.10011v1",
    "published": "2026-01-15",
    "update_time": "2026-01-15",
    "download_time": "2026-01-19 02:15:31"
  },
  {
    "id": "2601.10942",
    "title": "Change And Cover: Last-Mile, Pull Request-Based Regression Test Augmentation",
    "abstract": "Software is in constant evolution, with developers frequently submitting pull requests (PRs) to introduce new features or fix bugs. Testing PRs is critical to maintaining software quality. Yet, even in projects with extensive test suites, some PR-modified lines remain untested, leaving a \"last-mile\" regression test gap. Existing test generators typically aim to improve overall coverage, but do not specifically target the uncovered lines in PRs. We present Change And Cover (ChaCo), an LLM-based test augmentation technique that addresses this gap. It makes three contributions: (i) ChaCo considers the PR-specific patch coverage, offering developers augmented tests for code just when it is on the developers' mind. (ii) We identify providing suitable test context as a crucial challenge for an LLM to generate useful tests, and present two techniques to extract relevant test content, such as existing test functions, fixtures, and data generators. (iii) To make augmented tests acceptable for developers, ChaCo carefully integrates them into the existing test suite, e.g., by matching the test's structure and style with the existing tests, and generates a summary of the test addition for developer review. We evaluate ChaCo on 145 PRs from three popular and complex open-source projects - SciPy, Qiskit, and Pandas. The approach successfully helps 30% of PRs achieve full patch coverage, at the cost of $0.11, showing its effectiveness and practicality. Human reviewers find the tests to be worth adding (4.53/5.0), well integrated (4.2/5.0), and relevant to the PR (4.7/5.0). Ablations show test context is crucial for context-aware test generation, leading to 2x coverage. We submitted 12 tests, of which 8 have already been merged, and two previously unknown bugs were exposed and fixed. We envision our approach to be integrated into CI workflows, automating the last mile of regression test augmentation.",
    "arxiv_url": "https://arxiv.org/abs/2601.10942",
    "authors": [
      "Zitong Zhou",
      "Matteo Paltenghi",
      "Miryung Kim",
      "Michael Pradel"
    ],
    "first_author": "Zitong Zhou",
    "category": [
      "Technical"
    ],
    "field": "Software Testing",
    "task": "Test Generation",
    "tags": [
      "PR-based testing",
      "Patch coverage",
      "Context-aware generation",
      "Test suite integration",
      "Fixture reuse",
      "Runtime feedback",
      "Developer acceptability"
    ],
    "summary": "本文提出ChaCo，一种基于大语言模型的PR特定回归测试增强方法，通过提取PR与现有测试的上下文、迭代运行反馈并将生成的测试按项目风格整合到测试套件中，从而有效弥补补丁覆盖率的“最后一公里”缺口并被维护者接受。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.10942v1",
    "published": "2026-01-16",
    "update_time": "2026-01-16",
    "download_time": "2026-01-20 01:56:44"
  },
  {
    "id": "2601.10865",
    "title": "Multi-Agent Taint Specification Extraction for Vulnerability Detection",
    "abstract": "Static Application Security Testing (SAST) tools using taint analysis are widely viewed as providing higher-quality vulnerability detection results compared to traditional pattern-based approaches. However, performing static taint analysis for JavaScript poses two major challenges. First, JavaScript's dynamic features complicate data flow extraction required for taint tracking. Second, npm's large library ecosystem makes it difficult to identify relevant sources/sinks and establish taint propagation across dependencies. In this paper, we present SemTaint, a multi-agent system that strategically combines the semantic understanding of Large Language Models (LLMs) with traditional static program analysis to extract taint specifications, including sources, sinks, call edges, and library flow summaries tailored to each package. Conceptually, SemTaint uses static program analysis to calculate a call graph and defers to an LLM to resolve call edges that cannot be resolved statically. Further, it uses the LLM to classify sources and sinks for a given CWE. The resulting taint specification is then provided to a SAST tool, which performs vulnerability analysis. We integrate SemTaint with CodeQL, a state-of-the-art SAST tool, and demonstrate its effectiveness by detecting 106 of 162 vulnerabilities previously undetectable by CodeQL. Furthermore, we find 4 novel vulnerabilities in 4 popular npm packages. In doing so, we demonstrate that LLMs can practically enhance existing static program analysis algorithms, combining the strengths of both symbolic reasoning and semantic understanding for improved vulnerability detection.",
    "arxiv_url": "https://arxiv.org/abs/2601.10865",
    "authors": [
      "Jonah Ghebremichael",
      "Saastha Vasan",
      "Saad Ullah",
      "Greg Tystahl",
      "David Adei",
      "Christopher Kruegel",
      "Giovanni Vigna",
      "William Enck",
      "Alexandros Kapravelos"
    ],
    "first_author": "Jonah Ghebremichael",
    "category": [
      "Technical"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "Taint Specification Extraction",
      "Call Graph Repair",
      "Taint-Informed Callee Resolution",
      "Flow Summaries",
      "Demand-Driven Dependency Modeling",
      "Multi-Agent Reasoning",
      "LLM-Augmented Static Analysis",
      "CodeQL Integration"
    ],
    "summary": "SemTaint 提出一种静态驱动、LLM 增强的多代理系统，通过有选择地修复调用图、识别源/汇并生成库流摘要，为 CodeQL 提供可复用的污点规范，从而显著提升对 JavaScript/npm 包中漏洞的静态检测能力并发现若干新漏洞。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.10865v1",
    "published": "2026-01-15",
    "update_time": "2026-01-15",
    "download_time": "2026-01-20 01:57:15"
  },
  {
    "id": "2601.11124",
    "title": "Learn Before Represent: Bridging Generative and Contrastive Learning for Domain-Specific LLM Embeddings",
    "abstract": "Large Language Models (LLMs) adapted via contrastive learning excel in general representation learning but struggle in vertical domains like chemistry and law, primarily due to a lack of domain-specific knowledge. This work identifies a core bottleneck: the prevailing ``LLM+CL'' paradigm focuses on semantic alignment but cannot perform knowledge acquisition, leading to failures on specialized terminology. To bridge this gap, we propose Learn Before Represent (LBR), a novel two-stage framework. LBR first injects domain knowledge via an Information Bottleneck-Constrained Generative Learning stage, preserving the LLM's causal attention to maximize knowledge acquisition while compressing semantics. It then performs Generative-Refined Contrastive Learning on the compressed representations for alignment. This approach maintains architectural consistency and resolves the objective conflict between generative and contrastive learning. Extensive experiments on medical, chemistry, and code retrieval tasks show that LBR significantly outperforms strong baselines. Our work establishes a new paradigm for building accurate and robust representations in vertical domains.",
    "arxiv_url": "https://arxiv.org/abs/2601.11124",
    "authors": [
      "Xiaoyu Liang",
      "Yuchen Peng",
      "Jiale Luo",
      "Wenhao Wang",
      "Haoji Hu",
      "Xincheng Zhou"
    ],
    "first_author": "Xiaoyu Liang",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Representation Learning",
    "tags": [
      "Information Bottleneck",
      "Contrastive Learning",
      "Generative Learning",
      "Knowledge Injection",
      "Causal Attention",
      "Representation Collapse Mitigation",
      "Vertical Domain Adaptation",
      "Code Retrieval"
    ],
    "summary": "论文提出两阶段的“先学再表征”框架，通过信息瓶颈约束的生成学习先注入领域知识、再进行对比学习对齐表示，在保持自回归结构一致性的同时缓解生成与对比目标冲突，显著提升医疗、化学与代码检索等垂直领域的嵌入效果。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.11124v1",
    "published": "2026-01-16",
    "update_time": "2026-01-16",
    "download_time": "2026-01-20 02:03:42"
  },
  {
    "id": "2601.10955",
    "title": "Beyond Max Tokens: Stealthy Resource Amplification via Tool Calling Chains in LLM Agents",
    "abstract": "The agent-tool communication loop is a critical attack surface in modern Large Language Model (LLM) agents. Existing Denial-of-Service (DoS) attacks, primarily triggered via user prompts or injected retrieval-augmented generation (RAG) context, are ineffective for this new paradigm. They are fundamentally single-turn and often lack a task-oriented approach, making them conspicuous in goal-oriented workflows and unable to exploit the compounding costs of multi-turn agent-tool interactions. We introduce a stealthy, multi-turn economic DoS attack that operates at the tool layer under the guise of a correctly completed task. Our method adjusts text-visible fields and a template-governed return policy in a benign, Model Context Protocol (MCP)-compatible tool server, optimizing these edits with a Monte Carlo Tree Search (MCTS) optimizer. These adjustments leave function signatures unchanged and preserve the final payload, steering the agent into prolonged, verbose tool-calling sequences using text-only notices. This compounds costs across turns, escaping single-turn caps while keeping the final answer correct to evade validation. Across six LLMs on the ToolBench and BFCL benchmarks, our attack expands tasks into trajectories exceeding 60,000 tokens, inflates costs by up to 658x, and raises energy by 100-560x. It drives GPU KV cache occupancy from <1% to 35-74% and cuts co-running throughput by approximately 50%. Because the server remains protocol-compatible and task outcomes are correct, conventional checks fail. These results elevate the agent-tool interface to a first-class security frontier, demanding a paradigm shift from validating final answers to monitoring the economic and computational cost of the entire agentic process.",
    "arxiv_url": "https://arxiv.org/abs/2601.10955",
    "authors": [
      "Kaiyu Zhou",
      "Yongsen Zheng",
      "Yicheng He",
      "Meng Xue",
      "Xueluan Gong",
      "Yuji Wang",
      "Kwok-Yan Lam"
    ],
    "first_author": "Kaiyu Zhou",
    "category": [
      "Empirical",
      "Technical"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "Agent Security",
      "Tool Calling",
      "Economic DoS",
      "MCP Compatibility",
      "Monte Carlo Tree Search",
      "Stealth Attack",
      "Resource Exhaustion",
      "KV Cache Pressure"
    ],
    "summary": "论文提出一种在MCP工具层通过文本可见字段与模板化返回策略触发的多轮隐蔽经济型DoS攻击，并用MCTS自动优化以在保持任务正确性的同时极大放大代理工具调用链的算力与成本开销，实证表明在多LLM与工具基准上可显著膨胀token与能耗。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.10955v1",
    "published": "2026-01-16",
    "update_time": "2026-01-16",
    "download_time": "2026-01-20 02:06:09"
  },
  {
    "id": "2601.10773",
    "title": "LogicLens: Leveraging Semantic Code Graph to explore Multi Repository large systems",
    "abstract": "Understanding large software systems is a challenging task, especially when code is distributed across multiple repositories and microservices. Developers often need to reason not only about the structure of the code, but also about its domain logic and runtime behaviors, which are typically implicit and scattered. We introduce LogicLens, a reactive conversational agent that assists developers in exploring complex software systems through a semantic multi-repository graph. This graph is built in a preprocessing step by combining syntactic code analysis, via AST parsing and repository traversal, with semantic enrichment using Large Language Models (LLMs). The resulting graph captures both structural elements, such as files, classes, and functions, as well as functional abstractions like domain entities, operations, and workflows. Once the graph is constructed, LogicLens enables developers to interact with it via natural language, dynamically retrieving relevant subgraphs and answering technical or functional queries. We present the architecture of the system, discuss emergent behaviors, and evaluate its effectiveness on real-world multi-repository scenarios. We demonstrate emergent capabilities including impact analysis and symptom-based debugging that arise naturally from the semantic graph structure.",
    "arxiv_url": "https://arxiv.org/abs/2601.10773",
    "authors": [
      "Niko Usai",
      "Dario Montagnini",
      "Kristian Ilianov Iliev",
      "Raffaele Camanzo"
    ],
    "first_author": "Niko Usai",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Understanding",
    "tags": [
      "Software Knowledge Graph",
      "Cross-Repository Reasoning",
      "Graph RAG",
      "ReAct Agent",
      "Semantic Enrichment",
      "AST-based Extraction",
      "Conversational Code Exploration",
      "Impact Analysis",
      "Symptom-based Debugging"
    ],
    "summary": "本文提出LogicLens，一种将AST解析构建结构图并用LLM进行语义增强以生成跨多仓库语义知识图的交互式对话代理，支持基于图的检索、影响分析与故障定位以帮助开发者理解大型分布式系统。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.10773v1",
    "published": "2026-01-15",
    "update_time": "2026-01-15",
    "download_time": "2026-01-21 02:00:12"
  },
  {
    "id": "2601.10220",
    "title": "Agentic Pipelines in Embedded Software Engineering: Emerging Practices and Challenges",
    "abstract": "A new transformation is underway in software engineering, driven by the rapid adoption of generative AI in development workflows. Similar to how version control systems once automated manual coordination, AI tools are now beginning to automate many aspects of programming. For embedded software engineering organizations, however, this marks their first experience integrating AI into safety-critical and resource-constrained environments. The strict demands for determinism, reliability, and traceability pose unique challenges for adopting generative technologies.   In this paper, we present findings from a qualitative study with ten senior experts from four companies who are evaluating generative AI-augmented development for embedded software. Through semi-structured focus group interviews and structured brainstorming sessions, we identified eleven emerging practices and fourteen challenges related to the orchestration, responsible governance, and sustainable adoption of generative AI tools. Our results show how embedded software engineering teams are rethinking workflows, roles, and toolchains to enable a sustainable transition toward agentic pipelines and generative AI-augmented development.",
    "arxiv_url": "https://arxiv.org/abs/2601.10220",
    "authors": [
      "Simin Sun",
      "Miroslaw Staron"
    ],
    "first_author": "Simin Sun",
    "category": [
      "Empirical"
    ],
    "field": "Embedded SE & AI Governance",
    "task": "Generative AI Integration & Governance",
    "tags": [
      "Agentic Pipelines",
      "Human-LLM Interaction",
      "Determinism",
      "Traceability",
      "Compiler-in-the-loop",
      "AI Governance",
      "Toolchain Integration",
      "Certification"
    ],
    "summary": "本文通过对来自四家公司十位嵌入式软件专家的焦点小组访谈与结构化头脑风暴，识别了将生成式 AI 融入嵌入式软件工程以保持确定性、可靠性与可追溯性时出现的11项新兴实践与14项挑战，并讨论了相关治理与工作流重构的影响。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.10220v1",
    "published": "2026-01-15",
    "update_time": "2026-01-15",
    "download_time": "2026-01-21 02:01:15"
  },
  {
    "id": "2601.10820",
    "title": "Towards Reliable ML Feature Engineering via Planning in Constrained-Topology of LLM Agents",
    "abstract": "Recent advances in code generation models have unlocked unprecedented opportunities for automating feature engineering, yet their adoption in real-world ML teams remains constrained by critical challenges: (i) the scarcity of datasets capturing the iterative and complex coding processes of production-level feature engineering, (ii) limited integration and personalization of widely used coding agents, such as CoPilot and Devin, with a team's unique tools, codebases, workflows, and practices, and (iii) suboptimal human-AI collaboration due to poorly timed or insufficient feedback. We address these challenges with a planner-guided, constrained-topology multi-agent framework that generates code for repositories in a multi-step fashion. The LLM-powered planner leverages a team's environment, represented as a graph, to orchestrate calls to available agents, generate context-aware prompts, and use downstream failures to retroactively correct upstream artifacts. It can request human intervention at critical steps, ensuring generated code is reliable, maintainable, and aligned with team expectations. On a novel in-house dataset, our approach achieves 38% and 150% improvement in the evaluation metric over manually crafted and unplanned workflows respectively. In practice, when building features for recommendation models serving over 120 million users, our approach has delivered real-world impact by reducing feature engineering cycles from three weeks to a single day.",
    "arxiv_url": "https://arxiv.org/abs/2601.10820",
    "authors": [
      "Himanshu Thakur",
      "Anusha Kamath",
      "Anurag Muthyala",
      "Dhwani Sanmukhani",
      "Smruthi Mukund",
      "Jay Katukuri"
    ],
    "first_author": "Himanshu Thakur",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Repository-Level Coding",
    "tags": [
      "Repository-Level Coding",
      "Code Agents",
      "Planner-Guided Orchestration",
      "Constrained Topology",
      "Human-LLM Interaction",
      "PySpark Feature Engineering",
      "Failure-driven Repair",
      "Robustness"
    ],
    "summary": "本文提出一种基于规划器的受约束拓扑多智能体框架，用于仓库级别的PySpark特征工程，通过动态协调现有代理、利用下游失败进行回溯修复并支持人机交互来提升代码生成的可靠性与可维护性，并发布了对应的基准数据集。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.10820v1",
    "published": "2026-01-15",
    "update_time": "2026-01-15",
    "download_time": "2026-01-21 02:11:56"
  },
  {
    "id": "2601.11960",
    "title": "R$^2$PO: Decoupling Training Trajectories from Inference Responses for LLM Reasoning",
    "abstract": "Reinforcement learning has become a central paradigm for improving LLM reasoning. However, existing methods use a single policy to produce both inference responses and training optimization trajectories. The objective conflict between generating stable inference responses and diverse training trajectories leads to insufficient exploration, which harms reasoning capability. In this paper, to address the problem, we propose R$^2$PO (Residual Rollout Policy Optimization), which introduces a lightweight Residual Rollout-Head atop the policy to decouple training trajectories from inference responses, enabling controlled trajectory diversification during training while keeping inference generation stable. Experiments across multiple benchmarks show that our method consistently outperforms baselines, achieving average accuracy gains of 3.1% on MATH-500 and 2.4% on APPS, while also reducing formatting errors and mitigating length bias for stable optimization. Our code is publicly available at https://github.com/RRPO-ARR/Code.",
    "arxiv_url": "https://arxiv.org/abs/2601.11960",
    "authors": [
      "Jingchu Wang",
      "Bingbing Xu",
      "Yige Yuan",
      "Bin Xie",
      "Xiaoqian Sun",
      "Huawei Shen"
    ],
    "first_author": "Jingchu Wang",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Residual Rollout Head",
      "Decoupled Exploration",
      "Trajectory Diversification",
      "Group Inverse-Frequency Reward",
      "Training Stability",
      "Length-Bias Mitigation",
      "Formatting Error Reduction",
      "Inference-Discardable Module",
      "Low Overhead"
    ],
    "summary": "本文提出R2PO，通过在冻结主干上加入轻量级残差Rollout-Head将训练轨迹与推理响应解耦，实现可控的轨迹多样化以增强RL微调的探索性，从而在数学与代码推理任务上提升性能并改善训练稳定性与格式/长度偏差问题，推理时可丢弃该模块以保持生成稳定性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.11960v1",
    "published": "2026-01-17",
    "update_time": "2026-01-17",
    "download_time": "2026-01-21 02:15:11"
  }
]