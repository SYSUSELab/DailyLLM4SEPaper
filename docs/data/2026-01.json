[
  {
    "id": "2601.00635",
    "title": "SEMODS: A Validated Dataset of Open-Source Software Engineering Models",
    "abstract": "Integrating Artificial Intelligence into Software Engineering (SE) requires having a curated collection of models suited to SE tasks. With millions of models hosted on Hugging Face (HF) and new ones continuously being created, it is infeasible to identify SE models without a dedicated catalogue. To address this gap, we present SEMODS: an SE-focused dataset of 3,427 models extracted from HF, combining automated collection with rigorous validation through manual annotation and large language model assistance. Our dataset links models to SE tasks and activities from the software development lifecycle, offering a standardized representation of their evaluation results, and supporting multiple applications such as data analysis, model discovery, benchmarking, and model adaptation.",
    "arxiv_url": "https://arxiv.org/abs/2601.00635",
    "authors": [
      "Alexandra González",
      "Xavier Franch",
      "Silverio Martínez-Fernández"
    ],
    "first_author": "Alexandra González",
    "category": [
      "Benchmark"
    ],
    "field": "Model Repositories & Catalogues",
    "task": "SE Model Cataloguing",
    "tags": [
      "Model Catalog",
      "SE Task Taxonomy",
      "Benchmark Harmonization",
      "LLM-assisted Annotation",
      "Model Card Normalization",
      "Automated Dataset Maintenance",
      "Repository Metadata Schema",
      "Model Discovery"
    ],
    "summary": "本文提出SEMODS，一个从Hugging Face系统化收集并通过人工和大模型辅助验证的面向软件工程的模型数据集（3,427个模型），提供SE任务映射、标准化评测表示和自动化更新管道以支持模型发现与基准测试。",
    "quality": "High",
    "conference": "FORGE (ACM International Conference on AI Foundation Models and Software Engineering) 2026",
    "pdf_url": "https://arxiv.org/pdf/2601.00635v1",
    "published": "2026-01-02",
    "update_time": "2026-01-02",
    "download_time": "2026-01-05 02:05:13"
  },
  {
    "id": "2601.00497",
    "title": "STELLAR: A Search-Based Testing Framework for Large Language Model Applications",
    "abstract": "Large Language Model (LLM)-based applications are increasingly deployed across various domains, including customer service, education, and mobility. However, these systems are prone to inaccurate, fictitious, or harmful responses, and their vast, high-dimensional input space makes systematic testing particularly challenging. To address this, we present STELLAR, an automated search-based testing framework for LLM-based applications that systematically uncovers text inputs leading to inappropriate system responses. Our framework models test generation as an optimization problem and discretizes the input space into stylistic, content-related, and perturbation features. Unlike prior work that focuses on prompt optimization or coverage heuristics, our work employs evolutionary optimization to dynamically explore feature combinations that are more likely to expose failures. We evaluate STELLAR on three LLM-based conversational question-answering systems. The first focuses on safety, benchmarking both public and proprietary LLMs against malicious or unsafe prompts. The second and third target navigation, using an open-source and an industrial retrieval-augmented system for in-vehicle venue recommendations. Overall, STELLAR exposes up to 4.3 times (average 2.5 times) more failures than the existing baseline approaches.",
    "arxiv_url": "https://arxiv.org/abs/2601.00497",
    "authors": [
      "Lev Sorokin",
      "Ivan Vasilev",
      "Ken E. Friedl",
      "Andrea Stocco"
    ],
    "first_author": "Lev Sorokin",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Software Testing",
    "task": "Test Generation",
    "tags": [
      "Search-based Testing",
      "Evolutionary Optimization",
      "Feature Discretization",
      "Retrieval-Augmented Generation",
      "Robustness Testing",
      "Safety Testing",
      "Conversational QA",
      "Perturbation Testing"
    ],
    "summary": "本文提出STELLAR，一种将自然语言输入离散为风格、内容和扰动特征并通过进化搜索生成失败诱发测试用例的自动化搜索式测试框架，用以发现LLM应用（如车载对话系统）中的不当或错误响应并在多项用例中显著优于基线方法。",
    "quality": "High",
    "conference": "International Conference on Software Analysis, Evolution and Reengineering (SANER) 2026",
    "pdf_url": "https://arxiv.org/pdf/2601.00497v1",
    "published": "2026-01-01",
    "update_time": "2026-01-01",
    "download_time": "2026-01-05 02:06:23"
  },
  {
    "id": "2601.00753",
    "title": "Early-Stage Prediction of Review Effort in AI-Generated Pull Requests",
    "abstract": "As autonomous AI agents transition from code completion tools to full-fledged teammates capable of opening pull requests (PRs) at scale, software maintainers face a new challenge: not just reviewing code, but managing complex interaction loops with non-human contributors. This paradigm shift raises a critical question: can we predict which agent-generated PRs will consume excessive review effort before any human interaction begins?   Analyzing 33,707 agent-authored PRs from the AIDev dataset across 2,807 repositories, we uncover a striking two-regime behavioral pattern that fundamentally distinguishes autonomous agents from human developers. The first regime, representing 28.3 percent of all PRs, consists of instant merges (less than 1 minute), reflecting success on narrow automation tasks. The second regime involves iterative review cycles where agents frequently stall or abandon refinement (ghosting).   We propose a Circuit Breaker triage model that predicts high-review-effort PRs (top 20 percent) at creation time using only static structural features. A LightGBM model achieves AUC 0.957 on a temporal split, while semantic text features (TF-IDF, CodeBERT) provide negligible predictive value. At a 20 percent review budget, the model intercepts 69 percent of total review effort, enabling zero-latency governance.   Our findings challenge prevailing assumptions in AI-assisted code review: review burden is dictated by what agents touch, not what they say, highlighting the need for structural governance mechanisms in human-AI collaboration.",
    "arxiv_url": "https://arxiv.org/abs/2601.00753",
    "authors": [
      "Dao Sy Duy Minh",
      "Huynh Trung Kiet",
      "Tran Chi Nguyen",
      "Nguyen Lam Phu Quy",
      "Phu Hoa Pham",
      "Nguyen Dinh Ha Duong",
      "Truong Bao Tran"
    ],
    "first_author": "Dao Sy Duy Minh",
    "category": [
      "Empirical",
      "Technical"
    ],
    "field": "Maintenance",
    "task": "Code Review",
    "tags": [
      "Ghosting",
      "Review Triage",
      "Effort Prediction",
      "Creation-time Features",
      "Patch Complexity",
      "Structural Signals",
      "Agent-authored PRs",
      "Circuit Breaker",
      "Zero-latency Triage",
      "Model Interpretability"
    ],
    "summary": "本文基于33,707个由AI代理提交的PR，发现即时合并与迭代失败两种行为模式，并提出基于创建时结构性特征的“断路器”LightGBM模型（AUC≈0.957）用于零延迟预测高审查成本的PR以辅助维护者分流。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.00753v1",
    "published": "2026-01-02",
    "update_time": "2026-01-02",
    "download_time": "2026-01-06 01:54:54"
  },
  {
    "id": "2601.00482",
    "title": "Multi-Agent Coordinated Rename Refactoring",
    "abstract": "The primary value of AI agents in software development lies in their ability to extend the developer's capacity for reasoning and action, not to supplant human involvement. To showcase how to use agents working in tandem with developers, we designed a novel approach for carrying out coordinated renaming. Coordinated renaming, where a single rename refactoring triggers refactorings in multiple, related identifiers, is a frequent yet challenging task. Developers must manually propagate these rename refactorings across numerous files and contexts, a process that is both tedious and highly error-prone. State-of-the-art heuristic-based approaches produce an overwhelming number of false positives, while vanilla Large Language Models (LLMs) provide incomplete suggestions due to their limited context and inability to interact with refactoring tools. This leaves developers with incomplete refactorings or burdens them with filtering too many false positives. Coordinated renaming is exactly the kind of repetitive task that agents can significantly reduce the developers' burden while keeping them in the driver's seat.   We designed, implemented, and evaluated the first multi-agent framework that automates coordinated renaming. It operates on a key insight: a developer's initial refactoring is a clue to infer the scope of related refactorings. Our Scope Inference Agent first transforms this clue into an explicit, natural-language Declared Scope. The Planned Execution Agent then uses this as a strict plan to identify program elements that should undergo refactoring and safely executes the changes by invoking the IDE's own trusted refactoring APIs. Finally, the Replication Agent uses it to guide the project-wide search. We first conducted a formative study on the practice of coordinated renaming in 609K commits in 100 open-source projects and surveyed 205 developers ...",
    "arxiv_url": "https://arxiv.org/abs/2601.00482",
    "authors": [
      "Abhiram Bellur",
      "Mohammed Raihan Ullah",
      "Fraol Batole",
      "Mohit Kansara",
      "Masaharu Morimoto",
      "Kai Ishikawa",
      "Haifeng Chen",
      "Yaroslav Zharov",
      "Timofey Bryksin",
      "Tien N. Nguyen",
      "Hridesh Rajan",
      "Danny Dig"
    ],
    "first_author": "Abhiram Bellur",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Maintenance",
    "task": "Refactoring",
    "tags": [
      "Code Agents",
      "Coordinated Rename",
      "Scope Inference",
      "IDE Refactoring API",
      "Program Slicing",
      "Semantic Search",
      "Developer-in-the-loop",
      "Episodic Memory",
      "Repository-level Propagation",
      "Pull Request Automation"
    ],
    "summary": "本文提出CoRenameAgent——一个在IDE中运行的多智能体框架，通过范围推断、规划执行和复制传播三类代理结合开发者反馈与IDE可信重构API，自动、可靠地执行跨仓库的协同重命名，并在两个基准上显著优于现有方法且其自动生成的PR被社区接受。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.00482v1",
    "published": "2026-01-01",
    "update_time": "2026-01-01",
    "download_time": "2026-01-06 01:55:34"
  },
  {
    "id": "2601.00743",
    "title": "An Agentic Framework for Neuro-Symbolic Programming",
    "abstract": "Integrating symbolic constraints into deep learning models could make them more robust, interpretable, and data-efficient. Still, it remains a time-consuming and challenging task. Existing frameworks like DomiKnowS help this integration by providing a high-level declarative programming interface, but they still assume the user is proficient with the library's specific syntax. We propose AgenticDomiKnowS (ADS) to eliminate this dependency. ADS translates free-form task descriptions into a complete DomiKnowS program using an agentic workflow that creates and tests each DomiKnowS component separately. The workflow supports optional human-in-the-loop intervention, enabling users familiar with DomiKnowS to refine intermediate outputs. We show how ADS enables experienced DomiKnowS users and non-users to rapidly construct neuro-symbolic programs, reducing development time from hours to 10-15 minutes.",
    "arxiv_url": "https://arxiv.org/abs/2601.00743",
    "authors": [
      "Aliakbar Nafar",
      "Chetan Chigurupati",
      "Danial Kamali",
      "Hamid Karimian",
      "Parisa Kordjamshidi"
    ],
    "first_author": "Aliakbar Nafar",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Editing",
    "tags": [
      "Code Agents",
      "Code RAG",
      "Neuro-Symbolic",
      "Domain-Specific Language",
      "Human-LLM Interaction",
      "Interactive Coding",
      "Self-Refinement",
      "VLM Integration"
    ],
    "summary": "本文提出AgenticDomiKnowS，一个将自然语言任务描述通过检索-生成-执行-审查的代理式工作流转化为完整DomiKnowS神经符号程序并支持人机交互与自我修正的系统，能将开发时间从数小时缩短到10–15分钟。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.00743v1",
    "published": "2026-01-02",
    "update_time": "2026-01-02",
    "download_time": "2026-01-06 01:56:06"
  },
  {
    "id": "2601.00559",
    "title": "Cracking IoT Security: Can LLMs Outsmart Static Analysis Tools?",
    "abstract": "Smart home IoT platforms such as openHAB rely on Trigger Action Condition (TAC) rules to automate device behavior, but the interplay among these rules can give rise to interaction threats, unintended or unsafe behaviors emerging from implicit dependencies, conflicting triggers, or overlapping conditions. Identifying these threats requires semantic understanding and structural reasoning that traditionally depend on symbolic, constraint-driven static analysis. This work presents the first comprehensive evaluation of Large Language Models (LLMs) across a multi-category interaction threat taxonomy, assessing their performance on both the original openHAB (oHC/IoTB) dataset and a structurally challenging Mutation dataset designed to test robustness under rule transformations. We benchmark Llama 3.1 8B, Llama 70B, GPT-4o, Gemini-2.5-Pro, and DeepSeek-R1 across zero-, one-, and two-shot settings, comparing their results against oHIT's manually validated ground truth. Our findings show that while LLMs exhibit promising semantic understanding, particularly on action- and condition-related threats, their accuracy degrades significantly for threats requiring cross-rule structural reasoning, especially under mutated rule forms. Model performance varies widely across threat categories and prompt settings, with no model providing consistent reliability. In contrast, the symbolic reasoning baseline maintains stable detection across both datasets, unaffected by rule rewrites or structural perturbations. These results underscore that LLMs alone are not yet dependable for safety critical interaction-threat detection in IoT environments. We discuss the implications for tool design and highlight the potential of hybrid architectures that combine symbolic analysis with LLM-based semantic interpretation to reduce false positives while maintaining structural rigor.",
    "arxiv_url": "https://arxiv.org/abs/2601.00559",
    "authors": [
      "Jason Quantrill",
      "Noura Khajehnouri",
      "Zihan Guo",
      "Manar H. Alalfi"
    ],
    "first_author": "Jason Quantrill",
    "category": [
      "Empirical",
      "Benchmark"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "Rule Interaction Threats",
      "IoT Security",
      "Trigger-Action-Condition",
      "Mutation Testing",
      "Symbolic-LLM Hybrid",
      "Cross-rule Reasoning",
      "Structural Robustness",
      "Prompt Robustness",
      "False Positive Reduction"
    ],
    "summary": "本文评估多种大语言模型在openHAB触发-条件-动作规则交互威胁检测上的表现，并通过一套结构变异数据集与符号静态分析工具对比，发现LLM在语义理解方面较好但在跨规则结构推理与变形规则下显著退化，提出结合符号分析与LLM的混合方案以提高检测可靠性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.00559v1",
    "published": "2026-01-02",
    "update_time": "2026-01-02",
    "download_time": "2026-01-06 01:57:35"
  },
  {
    "id": "2601.02971",
    "title": "Few-shot learning for security bug report identification",
    "abstract": "Security bug reports require prompt identification to minimize the window of vulnerability in software systems. Traditional machine learning (ML) techniques for classifying bug reports to identify security bug reports rely heavily on large amounts of labeled data. However, datasets for security bug reports are often scarce in practice, leading to poor model performance and limited applicability in real-world settings. In this study, we propose a few-shot learning-based technique to effectively identify security bug reports using limited labeled data. We employ SetFit, a state-of-the-art few-shot learning framework that combines sentence transformers with contrastive learning and parameter-efficient fine-tuning. The model is trained on a small labeled dataset of bug reports and is evaluated on its ability to classify these reports as either security-related or non-security-related. Our approach achieves an AUC of 0.865, at best, outperforming traditional ML techniques (baselines) for all of the evaluated datasets. This highlights the potential of SetFit to effectively identify security bug reports. SetFit-based few-shot learning offers a promising alternative to traditional ML techniques to identify security bug reports. The approach enables efficient model development with minimal annotation effort, making it highly suitable for scenarios where labeled data is scarce.",
    "arxiv_url": "https://arxiv.org/abs/2601.02971",
    "authors": [
      "Muhammad Laiq"
    ],
    "first_author": "Muhammad Laiq",
    "category": [
      "Empirical"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "Issue Classification",
      "Few-shot Learning",
      "Contrastive Learning",
      "Parameter-efficient Fine-tuning",
      "Class Imbalance",
      "Security Bug Identification",
      "Empirical Comparison"
    ],
    "summary": "本文在四个开源项目的数据集上实证评估了基于少样本学习的SetFit方法用于安全缺陷报告识别，实验表明在有限标注下可达到最高AUC 0.865并优于传统机器学习基线。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.02971v1",
    "published": "2026-01-06",
    "update_time": "2026-01-06",
    "download_time": "2026-01-08 01:55:02"
  },
  {
    "id": "2601.02868",
    "title": "CodeMEM: AST-Guided Adaptive Memory for Repository-Level Iterative Code Generation",
    "abstract": "Large language models (LLMs) substantially enhance developer productivity in repository-level code generation through interactive collaboration. However, as interactions progress, repository context must be continuously preserved and updated to integrate newly validated information. Meanwhile, the expanding session history increases cognitive burden, often leading to forgetting and the reintroduction of previously resolved errors. Existing memory management approaches show promise but remain limited by natural language-centric representations. To overcome these limitations, we propose CodeMEM, an AST-guided dynamic memory management system tailored for repository-level iterative code generation. Specifically, CodeMEM introduces the Code Context Memory component that dynamically maintains and updates repository context through AST-guided LLM operations, along with the Code Session Memory that constructs a code-centric representation of interaction history and explicitly detects and mitigates forgetting through AST-based analysis. Experimental results on the instruction-following benchmark CodeIF-Bench and the code generation benchmark CoderEval demonstrate that CodeMEM achieves state-of-the-art performance, improving instruction following by 12.2% for the current turn and 11.5% for the session level, and reducing interaction rounds by 2-3, while maintaining competitive inference latency and token efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2601.02868",
    "authors": [
      "Peiding Wang",
      "Li Zhang",
      "Fang Liu",
      "Chongyang Tao",
      "Yinghao Zhu"
    ],
    "first_author": "Peiding Wang",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Retrieval",
    "tags": [
      "Repository-Level Coding",
      "AST-guided memory",
      "Code Context Memory",
      "Code Session Memory",
      "Forgetting Detection",
      "Code-change Analysis",
      "Interaction Efficiency"
    ],
    "summary": "本文提出CODEMEM，一种基于AST引导的动态代码记忆管理系统，通过维护代码上下文记忆与会话记忆并基于AST分析检测与缓解遗忘，从而在仓库级多轮代码生成中显著提升指令遵循率并减少交互轮次。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.02868v1",
    "published": "2026-01-06",
    "update_time": "2026-01-06",
    "download_time": "2026-01-08 01:55:19"
  },
  {
    "id": "2601.02941",
    "title": "SastBench: A Benchmark for Testing Agentic SAST Triage",
    "abstract": "SAST (Static Application Security Testing) tools are among the most widely used techniques in defensive cybersecurity, employed by commercial and non-commercial organizations to identify potential vulnerabilities in software. Despite their great utility, they generate numerous false positives, requiring costly manual filtering (aka triage). While LLM-powered agents show promise for automating cybersecurity tasks, existing benchmarks fail to emulate real-world SAST finding distributions. We introduce SastBench, a benchmark for evaluating SAST triage agents that combines real CVEs as true positives with filtered SAST tool findings as approximate false positives. SastBench features an agent-agnostic design. We evaluate different agents on the benchmark and present a comparative analysis of their performance, provide a detailed analysis of the dataset, and discuss the implications for future development.",
    "arxiv_url": "https://arxiv.org/abs/2601.02941",
    "authors": [
      "Jake Feiglin",
      "Guy Dar"
    ],
    "first_author": "Jake Feiglin",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "SAST Triage",
      "Agentic Benchmark",
      "Realistic False Positives",
      "CVE-based Ground Truth",
      "Agent Evaluation",
      "Security-Oriented Prompting",
      "Language Diversity",
      "Agent-Agnostic Evaluation"
    ],
    "summary": "该论文提出并开源了SASTBENCH，一个以真实CVE作为真阳性并以过滤后的SAST工具报警作为近似假阳性构建的面向自动化SAST告警三分类（triage）的基准，并基于该基准对多种LLM驱动代理进行了比较评估与数据分析。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.02941v1",
    "published": "2026-01-06",
    "update_time": "2026-01-06",
    "download_time": "2026-01-08 01:59:25"
  },
  {
    "id": "2601.02736",
    "title": "Hypothesize-Then-Verify: Speculative Root Cause Analysis for Microservices with Pathwise Parallelism",
    "abstract": "Microservice systems have become the backbone of cloud-native enterprise applications due to their resource elasticity, loosely coupled architecture, and lightweight deployment. Yet, the intrinsic complexity and dynamic runtime interactions of such systems inevitably give rise to anomalies. Ensuring system reliability therefore hinges on effective root cause analysis (RCA), which entails not only localizing the source of anomalies but also characterizing the underlying failures in a timely and interpretable manner. Recent advances in intelligent RCA techniques, particularly those powered by large language models (LLMs), have demonstrated promising capabilities, as LLMs reduce reliance on handcrafted features while offering cross-platform adaptability, task generalization, and flexibility. However, existing LLM-based methods still suffer from two critical limitations: (a) limited exploration diversity, which undermines accuracy, and (b) heavy dependence on large-scale LLMs, which results in slow inference. To overcome these challenges, we propose SpecRCA, a speculative root cause analysis framework for microservices that adopts a \\textit{hypothesize-then-verify} paradigm. SpecRCA first leverages a hypothesis drafting module to rapidly generate candidate root causes, and then employs a parallel root cause verifier to efficiently validate them. Preliminary experiments on the AIOps 2022 dataset demonstrate that SpecRCA achieves superior accuracy and efficiency compared to existing approaches, highlighting its potential as a practical solution for scalable and interpretable RCA in complex microservice environments.",
    "arxiv_url": "https://arxiv.org/abs/2601.02736",
    "authors": [
      "Lingzhe Zhang",
      "Tong Jia",
      "Yunpeng Zhai",
      "Leyi Pan",
      "Chiming Duan",
      "Minghua He",
      "Pei Xiao",
      "Ying Li"
    ],
    "first_author": "Lingzhe Zhang",
    "category": [
      "Technical"
    ],
    "field": "AIOps",
    "task": "Root Cause Analysis",
    "tags": [
      "Speculative Verification",
      "Hypothesize-then-Verify",
      "Pathwise Parallelism",
      "Modality Fusion",
      "Service Topology",
      "Trace Analysis",
      "Granger Causality",
      "Anomalous Metric Detection",
      "Parallel Verification",
      "Diagnosis Synthesis",
      "Efficiency",
      "Interpretability"
    ],
    "summary": "本文提出 SpecRCA——一种面向微服务的假设-验证根因分析框架，通过轻量草拟模型生成多样化候选假设并并行用小型微调 LLM 验证，结合指标/链路/日志的拓扑融合与综合生成诊断报告，在 AIOps 2022 数据集上显著提升定位准确性并降低推理延迟。",
    "quality": "High",
    "conference": "ICSE",
    "pdf_url": "https://arxiv.org/pdf/2601.02736v1",
    "published": "2026-01-06",
    "update_time": "2026-01-06",
    "download_time": "2026-01-08 02:01:34"
  },
  {
    "id": "2601.03988",
    "title": "Using Small Language Models to Reverse-Engineer Machine Learning Pipelines Structures",
    "abstract": "Background: Extracting the stages that structure Machine Learning (ML) pipelines from source code is key for gaining a deeper understanding of data science practices. However, the diversity caused by the constant evolution of the ML ecosystem (e.g., algorithms, libraries, datasets) makes this task challenging. Existing approaches either depend on non-scalable, manual labeling, or on ML classifiers that do not properly support the diversity of the domain. These limitations highlight the need for more flexible and reliable solutions.   Objective: We evaluate whether Small Language Models (SLMs) can leverage their code understanding and classification abilities to address these limitations, and subsequently how they can advance our understanding of data science practices.   Method: We conduct a confirmatory study based on two reference works selected for their relevance regarding current state-of-the-art's limitations. First, we compare several SLMs using Cochran's Q test. The best-performing model is then evaluated against the reference studies using two distinct McNemar's tests. We further analyze how variations in taxonomy definitions affect performance through an additional Cochran's Q test. Finally, a goodness-of-fit analysis is conducted using Pearson's chi-squared tests to compare our insights on data science practices with those from prior studies.",
    "arxiv_url": "https://arxiv.org/abs/2601.03988",
    "authors": [
      "Nicolas Lacroix",
      "Mireille Blay-Fornarino",
      "Sébastien Mosser",
      "Frederic Precioso"
    ],
    "first_author": "Nicolas Lacroix",
    "category": [
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Understanding",
    "tags": [
      "Pipeline Extraction",
      "Reverse Engineering",
      "Jupyter Notebook Analysis",
      "Small Language Models",
      "Instruction-level Classification",
      "Taxonomy Sensitivity",
      "Model Comparison",
      "Goodness-of-fit Analysis"
    ],
    "summary": "本文通过对比多款开源小型语言模型，评估其在从 Jupyter 笔记本代码中反向工程提取机器学习流水线结构（阶段）上的准确性，分析分类法措辞对结果的影响并检验这些结果是否改变对数据科学家实践的既有理解。",
    "quality": "High",
    "conference": "SANER (IEEE International Conference on Software Analysis, Evolution and Reengineering) 2026",
    "pdf_url": "https://arxiv.org/pdf/2601.03988v1",
    "published": "2026-01-07",
    "update_time": "2026-01-07",
    "download_time": "2026-01-09 01:56:28"
  },
  {
    "id": "2601.03878",
    "title": "Understanding Specification-Driven Code Generation with LLMs: An Empirical Study Design",
    "abstract": "Large Language Models (LLMs) are increasingly integrated into software development workflows, yet their behavior in structured, specification-driven processes remains poorly understood. This paper presents an empirical study design using CURRANTE, a Visual Studio Code extension that enables a human-in-the-loop workflow for LLM-assisted code generation. The tool guides developers through three sequential stages--Specification, Tests, and Function--allowing them to define requirements, generate and refine test suites, and produce functions that satisfy those tests. Participants will solve medium-difficulty problems from the LiveCodeBench dataset, while the tool records fine-grained interaction logs, effectiveness metrics (e.g., pass rate, all-pass completion), efficiency indicators (e.g., time-to-pass), and iteration behaviors. The study aims to analyze how human intervention in specification and test refinement influences the quality and dynamics of LLM-generated code. The results will provide empirical insights into the design of next-generation development environments that align human reasoning with model-driven code generation.",
    "arxiv_url": "https://arxiv.org/abs/2601.03878",
    "authors": [
      "Giovanni Rosa",
      "David Moreno-Lumbreras",
      "Gregorio Robles",
      "Jesús M. González-Barahona"
    ],
    "first_author": "Giovanni Rosa",
    "category": [
      "Empirical",
      "Technical"
    ],
    "field": "Software Testing",
    "task": "Test Generation",
    "tags": [
      "Specification-Driven Development",
      "Test-Driven Development",
      "Human-LLM Interaction",
      "IDE Plugin",
      "Test Suite Generation",
      "Test Refinement",
      "Interaction Logging",
      "Effectiveness Metrics",
      "Time-to-Pass",
      "Registered Study Protocol"
    ],
    "summary": "本文提出并描述了一项基于 CURRANTE VSCode 插件的注册报告型实证研究设计，以评估在人类参与的规范（通过测试用例）驱动 TDD 工作流下，LLM 辅助代码生成的有效性、效率与迭代行为。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.03878v1",
    "published": "2026-01-07",
    "update_time": "2026-01-07",
    "download_time": "2026-01-09 01:56:58"
  },
  {
    "id": "2601.04126",
    "title": "InfiniteWeb: Scalable Web Environment Synthesis for GUI Agent Training",
    "abstract": "GUI agents that interact with graphical interfaces on behalf of users represent a promising direction for practical AI assistants. However, training such agents is hindered by the scarcity of suitable environments. We present InfiniteWeb, a system that automatically generates functional web environments at scale for GUI agent training. While LLMs perform well on generating a single webpage, building a realistic and functional website with many interconnected pages faces challenges. We address these challenges through unified specification, task-centric test-driven development, and a combination of website seed with reference design image to ensure diversity. Our system also generates verifiable task evaluators enabling dense reward signals for reinforcement learning. Experiments show that InfiniteWeb surpasses commercial coding agents at realistic website construction, and GUI agents trained on our generated environments achieve significant performance improvements on OSWorld and Online-Mind2Web, demonstrating the effectiveness of proposed system.",
    "arxiv_url": "https://arxiv.org/abs/2601.04126",
    "authors": [
      "Ziyun Zhang",
      "Zezhou Wang",
      "Xiaoyi Zhang",
      "Zongyu Guo",
      "Jiahao Li",
      "Bin Li",
      "Yan Lu"
    ],
    "first_author": "Ziyun Zhang",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Requirements & Design",
    "task": "Specification & Validation",
    "tags": [
      "Environment Synthesis",
      "Unified Specification",
      "Task-Centric TDD",
      "Evaluator Generation",
      "Design-Guided Frontend",
      "Visual-Functional Diversity",
      "Dense Reward Signals",
      "GUI Agent Training"
    ],
    "summary": "本文提出INFINITEWEB，一个通过从任务导出统一数据与接口、采用面向任务的测试驱动开发以及设计图引导的前端生成，自动大规模合成功能性网页环境并生成可验证评估器以支持基于强化学习的GUI代理训练的系统。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.04126v1",
    "published": "2026-01-07",
    "update_time": "2026-01-07",
    "download_time": "2026-01-09 01:58:15"
  },
  {
    "id": "2601.03731",
    "title": "From Laboratory to Real-World Applications: Benchmarking Agentic Code Reasoning at the Repository Level",
    "abstract": "As large language models (LLMs) evolve into autonomous agents, evaluating repository-level reasoning, the ability to maintain logical consistency across massive, real-world, interdependent file systems, has become critical. Current benchmarks typically fluctuate between isolated code snippets and black-box evaluations. We present RepoReason, a white-box diagnostic benchmark centered on abductive assertion verification. To eliminate memorization while preserving authentic logical depth, we implement an execution-driven mutation framework that utilizes the environment as a semantic oracle to regenerate ground-truth states. Furthermore, we establish a fine-grained diagnostic system using dynamic program slicing, quantifying reasoning via three orthogonal metrics: $ESV$ (reading load), $MCL$ (simulation depth), and $DFI$ (integration width). Comprehensive evaluations of frontier models (e.g., Claude-4.5-Sonnet, DeepSeek-v3.1-Terminus) reveal a prevalent aggregation deficit, where integration width serves as the primary cognitive bottleneck. Our findings provide granular white-box insights for optimizing the next generation of agentic software engineering.",
    "arxiv_url": "https://arxiv.org/abs/2601.03731",
    "authors": [
      "Jia Li",
      "Yuxin Su",
      "Michael R. Lyu"
    ],
    "first_author": "Jia Li",
    "category": [
      "Benchmark",
      "Technical",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Repository-Level Reasoning",
      "Abductive Verification",
      "Execution-Driven Mutation",
      "Deterministic Value Protocol",
      "Dynamic Program Slicing",
      "Reading Load (ESV)",
      "Simulation Depth (MCL)",
      "Integration Width (DFI)",
      "White-box Diagnostic",
      "Aggregation Deficit",
      "Long-chain State Tracking",
      "Cross-file Integration"
    ],
    "summary": "本文提出了REPOREASON——一个以掩码断言验证为核心、结合执行驱动变异和动态程序切片的仓库级白盒基准，用于客观评估并细粒度诊断LLM代理在跨文件信息整合与长链状态追踪方面的推理能力，揭示了模型在整合宽度上的严重瓶颈（Aggregation Deficit）。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.03731v1",
    "published": "2026-01-07",
    "update_time": "2026-01-07",
    "download_time": "2026-01-09 02:08:24"
  }
]