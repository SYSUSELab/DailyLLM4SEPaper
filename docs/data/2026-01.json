[
  {
    "id": "2601.00635",
    "title": "SEMODS: A Validated Dataset of Open-Source Software Engineering Models",
    "abstract": "Integrating Artificial Intelligence into Software Engineering (SE) requires having a curated collection of models suited to SE tasks. With millions of models hosted on Hugging Face (HF) and new ones continuously being created, it is infeasible to identify SE models without a dedicated catalogue. To address this gap, we present SEMODS: an SE-focused dataset of 3,427 models extracted from HF, combining automated collection with rigorous validation through manual annotation and large language model assistance. Our dataset links models to SE tasks and activities from the software development lifecycle, offering a standardized representation of their evaluation results, and supporting multiple applications such as data analysis, model discovery, benchmarking, and model adaptation.",
    "arxiv_url": "https://arxiv.org/abs/2601.00635",
    "authors": [
      "Alexandra González",
      "Xavier Franch",
      "Silverio Martínez-Fernández"
    ],
    "first_author": "Alexandra González",
    "category": [
      "Benchmark"
    ],
    "field": "Model Repositories & Catalogues",
    "task": "SE Model Cataloguing",
    "tags": [
      "Model Catalog",
      "SE Task Taxonomy",
      "Benchmark Harmonization",
      "LLM-assisted Annotation",
      "Model Card Normalization",
      "Automated Dataset Maintenance",
      "Repository Metadata Schema",
      "Model Discovery"
    ],
    "summary": "本文提出SEMODS，一个从Hugging Face系统化收集并通过人工和大模型辅助验证的面向软件工程的模型数据集（3,427个模型），提供SE任务映射、标准化评测表示和自动化更新管道以支持模型发现与基准测试。",
    "quality": "High",
    "conference": "FORGE (ACM International Conference on AI Foundation Models and Software Engineering) 2026",
    "pdf_url": "https://arxiv.org/pdf/2601.00635v1",
    "published": "2026-01-02",
    "update_time": "2026-01-02",
    "download_time": "2026-01-05 02:05:13"
  },
  {
    "id": "2601.00497",
    "title": "STELLAR: A Search-Based Testing Framework for Large Language Model Applications",
    "abstract": "Large Language Model (LLM)-based applications are increasingly deployed across various domains, including customer service, education, and mobility. However, these systems are prone to inaccurate, fictitious, or harmful responses, and their vast, high-dimensional input space makes systematic testing particularly challenging. To address this, we present STELLAR, an automated search-based testing framework for LLM-based applications that systematically uncovers text inputs leading to inappropriate system responses. Our framework models test generation as an optimization problem and discretizes the input space into stylistic, content-related, and perturbation features. Unlike prior work that focuses on prompt optimization or coverage heuristics, our work employs evolutionary optimization to dynamically explore feature combinations that are more likely to expose failures. We evaluate STELLAR on three LLM-based conversational question-answering systems. The first focuses on safety, benchmarking both public and proprietary LLMs against malicious or unsafe prompts. The second and third target navigation, using an open-source and an industrial retrieval-augmented system for in-vehicle venue recommendations. Overall, STELLAR exposes up to 4.3 times (average 2.5 times) more failures than the existing baseline approaches.",
    "arxiv_url": "https://arxiv.org/abs/2601.00497",
    "authors": [
      "Lev Sorokin",
      "Ivan Vasilev",
      "Ken E. Friedl",
      "Andrea Stocco"
    ],
    "first_author": "Lev Sorokin",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Software Testing",
    "task": "Test Generation",
    "tags": [
      "Search-based Testing",
      "Evolutionary Optimization",
      "Feature Discretization",
      "Retrieval-Augmented Generation",
      "Robustness Testing",
      "Safety Testing",
      "Conversational QA",
      "Perturbation Testing"
    ],
    "summary": "本文提出STELLAR，一种将自然语言输入离散为风格、内容和扰动特征并通过进化搜索生成失败诱发测试用例的自动化搜索式测试框架，用以发现LLM应用（如车载对话系统）中的不当或错误响应并在多项用例中显著优于基线方法。",
    "quality": "High",
    "conference": "International Conference on Software Analysis, Evolution and Reengineering (SANER) 2026",
    "pdf_url": "https://arxiv.org/pdf/2601.00497v1",
    "published": "2026-01-01",
    "update_time": "2026-01-01",
    "download_time": "2026-01-05 02:06:23"
  },
  {
    "id": "2601.00753",
    "title": "Early-Stage Prediction of Review Effort in AI-Generated Pull Requests",
    "abstract": "As autonomous AI agents transition from code completion tools to full-fledged teammates capable of opening pull requests (PRs) at scale, software maintainers face a new challenge: not just reviewing code, but managing complex interaction loops with non-human contributors. This paradigm shift raises a critical question: can we predict which agent-generated PRs will consume excessive review effort before any human interaction begins?   Analyzing 33,707 agent-authored PRs from the AIDev dataset across 2,807 repositories, we uncover a striking two-regime behavioral pattern that fundamentally distinguishes autonomous agents from human developers. The first regime, representing 28.3 percent of all PRs, consists of instant merges (less than 1 minute), reflecting success on narrow automation tasks. The second regime involves iterative review cycles where agents frequently stall or abandon refinement (ghosting).   We propose a Circuit Breaker triage model that predicts high-review-effort PRs (top 20 percent) at creation time using only static structural features. A LightGBM model achieves AUC 0.957 on a temporal split, while semantic text features (TF-IDF, CodeBERT) provide negligible predictive value. At a 20 percent review budget, the model intercepts 69 percent of total review effort, enabling zero-latency governance.   Our findings challenge prevailing assumptions in AI-assisted code review: review burden is dictated by what agents touch, not what they say, highlighting the need for structural governance mechanisms in human-AI collaboration.",
    "arxiv_url": "https://arxiv.org/abs/2601.00753",
    "authors": [
      "Dao Sy Duy Minh",
      "Huynh Trung Kiet",
      "Tran Chi Nguyen",
      "Nguyen Lam Phu Quy",
      "Phu Hoa Pham",
      "Nguyen Dinh Ha Duong",
      "Truong Bao Tran"
    ],
    "first_author": "Dao Sy Duy Minh",
    "category": [
      "Empirical",
      "Technical"
    ],
    "field": "Maintenance",
    "task": "Code Review",
    "tags": [
      "Ghosting",
      "Review Triage",
      "Effort Prediction",
      "Creation-time Features",
      "Patch Complexity",
      "Structural Signals",
      "Agent-authored PRs",
      "Circuit Breaker",
      "Zero-latency Triage",
      "Model Interpretability"
    ],
    "summary": "本文基于33,707个由AI代理提交的PR，发现即时合并与迭代失败两种行为模式，并提出基于创建时结构性特征的“断路器”LightGBM模型（AUC≈0.957）用于零延迟预测高审查成本的PR以辅助维护者分流。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.00753v1",
    "published": "2026-01-02",
    "update_time": "2026-01-02",
    "download_time": "2026-01-06 01:54:54"
  },
  {
    "id": "2601.00482",
    "title": "Multi-Agent Coordinated Rename Refactoring",
    "abstract": "The primary value of AI agents in software development lies in their ability to extend the developer's capacity for reasoning and action, not to supplant human involvement. To showcase how to use agents working in tandem with developers, we designed a novel approach for carrying out coordinated renaming. Coordinated renaming, where a single rename refactoring triggers refactorings in multiple, related identifiers, is a frequent yet challenging task. Developers must manually propagate these rename refactorings across numerous files and contexts, a process that is both tedious and highly error-prone. State-of-the-art heuristic-based approaches produce an overwhelming number of false positives, while vanilla Large Language Models (LLMs) provide incomplete suggestions due to their limited context and inability to interact with refactoring tools. This leaves developers with incomplete refactorings or burdens them with filtering too many false positives. Coordinated renaming is exactly the kind of repetitive task that agents can significantly reduce the developers' burden while keeping them in the driver's seat.   We designed, implemented, and evaluated the first multi-agent framework that automates coordinated renaming. It operates on a key insight: a developer's initial refactoring is a clue to infer the scope of related refactorings. Our Scope Inference Agent first transforms this clue into an explicit, natural-language Declared Scope. The Planned Execution Agent then uses this as a strict plan to identify program elements that should undergo refactoring and safely executes the changes by invoking the IDE's own trusted refactoring APIs. Finally, the Replication Agent uses it to guide the project-wide search. We first conducted a formative study on the practice of coordinated renaming in 609K commits in 100 open-source projects and surveyed 205 developers ...",
    "arxiv_url": "https://arxiv.org/abs/2601.00482",
    "authors": [
      "Abhiram Bellur",
      "Mohammed Raihan Ullah",
      "Fraol Batole",
      "Mohit Kansara",
      "Masaharu Morimoto",
      "Kai Ishikawa",
      "Haifeng Chen",
      "Yaroslav Zharov",
      "Timofey Bryksin",
      "Tien N. Nguyen",
      "Hridesh Rajan",
      "Danny Dig"
    ],
    "first_author": "Abhiram Bellur",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Maintenance",
    "task": "Refactoring",
    "tags": [
      "Code Agents",
      "Coordinated Rename",
      "Scope Inference",
      "IDE Refactoring API",
      "Program Slicing",
      "Semantic Search",
      "Developer-in-the-loop",
      "Episodic Memory",
      "Repository-level Propagation",
      "Pull Request Automation"
    ],
    "summary": "本文提出CoRenameAgent——一个在IDE中运行的多智能体框架，通过范围推断、规划执行和复制传播三类代理结合开发者反馈与IDE可信重构API，自动、可靠地执行跨仓库的协同重命名，并在两个基准上显著优于现有方法且其自动生成的PR被社区接受。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.00482v1",
    "published": "2026-01-01",
    "update_time": "2026-01-01",
    "download_time": "2026-01-06 01:55:34"
  },
  {
    "id": "2601.00743",
    "title": "An Agentic Framework for Neuro-Symbolic Programming",
    "abstract": "Integrating symbolic constraints into deep learning models could make them more robust, interpretable, and data-efficient. Still, it remains a time-consuming and challenging task. Existing frameworks like DomiKnowS help this integration by providing a high-level declarative programming interface, but they still assume the user is proficient with the library's specific syntax. We propose AgenticDomiKnowS (ADS) to eliminate this dependency. ADS translates free-form task descriptions into a complete DomiKnowS program using an agentic workflow that creates and tests each DomiKnowS component separately. The workflow supports optional human-in-the-loop intervention, enabling users familiar with DomiKnowS to refine intermediate outputs. We show how ADS enables experienced DomiKnowS users and non-users to rapidly construct neuro-symbolic programs, reducing development time from hours to 10-15 minutes.",
    "arxiv_url": "https://arxiv.org/abs/2601.00743",
    "authors": [
      "Aliakbar Nafar",
      "Chetan Chigurupati",
      "Danial Kamali",
      "Hamid Karimian",
      "Parisa Kordjamshidi"
    ],
    "first_author": "Aliakbar Nafar",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Editing",
    "tags": [
      "Code Agents",
      "Code RAG",
      "Neuro-Symbolic",
      "Domain-Specific Language",
      "Human-LLM Interaction",
      "Interactive Coding",
      "Self-Refinement",
      "VLM Integration"
    ],
    "summary": "本文提出AgenticDomiKnowS，一个将自然语言任务描述通过检索-生成-执行-审查的代理式工作流转化为完整DomiKnowS神经符号程序并支持人机交互与自我修正的系统，能将开发时间从数小时缩短到10–15分钟。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.00743v1",
    "published": "2026-01-02",
    "update_time": "2026-01-02",
    "download_time": "2026-01-06 01:56:06"
  },
  {
    "id": "2601.00559",
    "title": "Cracking IoT Security: Can LLMs Outsmart Static Analysis Tools?",
    "abstract": "Smart home IoT platforms such as openHAB rely on Trigger Action Condition (TAC) rules to automate device behavior, but the interplay among these rules can give rise to interaction threats, unintended or unsafe behaviors emerging from implicit dependencies, conflicting triggers, or overlapping conditions. Identifying these threats requires semantic understanding and structural reasoning that traditionally depend on symbolic, constraint-driven static analysis. This work presents the first comprehensive evaluation of Large Language Models (LLMs) across a multi-category interaction threat taxonomy, assessing their performance on both the original openHAB (oHC/IoTB) dataset and a structurally challenging Mutation dataset designed to test robustness under rule transformations. We benchmark Llama 3.1 8B, Llama 70B, GPT-4o, Gemini-2.5-Pro, and DeepSeek-R1 across zero-, one-, and two-shot settings, comparing their results against oHIT's manually validated ground truth. Our findings show that while LLMs exhibit promising semantic understanding, particularly on action- and condition-related threats, their accuracy degrades significantly for threats requiring cross-rule structural reasoning, especially under mutated rule forms. Model performance varies widely across threat categories and prompt settings, with no model providing consistent reliability. In contrast, the symbolic reasoning baseline maintains stable detection across both datasets, unaffected by rule rewrites or structural perturbations. These results underscore that LLMs alone are not yet dependable for safety critical interaction-threat detection in IoT environments. We discuss the implications for tool design and highlight the potential of hybrid architectures that combine symbolic analysis with LLM-based semantic interpretation to reduce false positives while maintaining structural rigor.",
    "arxiv_url": "https://arxiv.org/abs/2601.00559",
    "authors": [
      "Jason Quantrill",
      "Noura Khajehnouri",
      "Zihan Guo",
      "Manar H. Alalfi"
    ],
    "first_author": "Jason Quantrill",
    "category": [
      "Empirical",
      "Benchmark"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "Rule Interaction Threats",
      "IoT Security",
      "Trigger-Action-Condition",
      "Mutation Testing",
      "Symbolic-LLM Hybrid",
      "Cross-rule Reasoning",
      "Structural Robustness",
      "Prompt Robustness",
      "False Positive Reduction"
    ],
    "summary": "本文评估多种大语言模型在openHAB触发-条件-动作规则交互威胁检测上的表现，并通过一套结构变异数据集与符号静态分析工具对比，发现LLM在语义理解方面较好但在跨规则结构推理与变形规则下显著退化，提出结合符号分析与LLM的混合方案以提高检测可靠性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2601.00559v1",
    "published": "2026-01-02",
    "update_time": "2026-01-02",
    "download_time": "2026-01-06 01:57:35"
  }
]