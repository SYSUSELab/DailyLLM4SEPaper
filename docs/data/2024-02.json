[
  {
    "id": "2402.19173",
    "title": "StarCoder 2 and The Stack v2: The Next Generation",
    "abstract": "The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data.",
    "arxiv_url": "https://arxiv.org/abs/2402.19173",
    "authors": [
      "Anton Lozhkov",
      "Raymond Li",
      "Loubna Ben Allal",
      "Federico Cassano",
      "Joel Lamy-Poirier",
      "Nouamane Tazi",
      "Ao Tang",
      "Dmytro Pykhtar",
      "Jiawei Liu",
      "Yuxiang Wei",
      "Tianyang Liu",
      "Max Tian",
      "Denis Kocetkov",
      "Arthur Zucker",
      "Younes Belkada",
      "Zijian Wang",
      "Qian Liu",
      "Dmitry Abulkhanov",
      "Indraneil Paul",
      "Zhuang Li",
      "Wen-Ding Li",
      "Megan Risdal",
      "Jia Li",
      "Jian Zhu",
      "Terry Yue Zhuo",
      "Evgenii Zheltonozhskii",
      "Nii Osae Osae Dade",
      "Wenhao Yu",
      "Lucas Krauß",
      "Naman Jain",
      "Yixuan Su",
      "Xuanli He",
      "Manan Dey",
      "Edoardo Abati",
      "Yekun Chai",
      "Niklas Muennighoff",
      "Xiangru Tang",
      "Muhtasham Oblokulov",
      "Christopher Akiki",
      "Marc Marone",
      "Chenghao Mou",
      "Mayank Mishra",
      "Alex Gu",
      "Binyuan Hui",
      "Tri Dao",
      "Armel Zebaze",
      "Olivier Dehaene",
      "Nicolas Patry",
      "Canwen Xu",
      "Julian McAuley",
      "Han Hu",
      "Torsten Scholak",
      "Sebastien Paquet",
      "Jennifer Robinson",
      "Carolyn Jane Anderson",
      "Nicolas Chapados",
      "Mostofa Patwary",
      "Nima Tajbakhsh",
      "Yacine Jernite",
      "Carlos Muñoz Ferrandis",
      "Lingming Zhang",
      "Sean Hughes",
      "Thomas Wolf",
      "Arjun Guha",
      "Leandro von Werra",
      "Harm de Vries"
    ],
    "first_author": "Anton Lozhkov",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Pre-Training",
    "tags": [
      "software-archive-based corpus",
      "multilingual source-code corpus",
      "license-aware filtering",
      "deduplication and provenance (SWHIDs)",
      "PII redaction and malicious-code removal",
      "notebook and pull-request curation",
      "two-stage context window training (4k→16k)",
      "training with trillions of tokens (overcompute vs Chinchilla)",
      "open-weight release with transparent data provenance",
      "opt-out/governance tooling",
      "comprehensive cross-benchmark evaluation"
    ],
    "summary": "BigCode项目基于Software Heritage构建了大规模多语言代码语料并发布了新版数据集与可公开权重的Code LLM，通过许可证感知去重、PII/恶意代码处理和两阶段（4k→16k）训练在多项代码基准上进行了全面评估并公开了数据可溯源标识。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2402.19173v1",
    "published": "2024-02-29",
    "update_time": "2024-02-29",
    "download_time": "2025-12-16 10:40:22"
  },
  {
    "id": "2402.16906",
    "title": "Debug like a Human: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step",
    "abstract": "Large language models (LLMs) are leading significant progress in code generation. Beyond one-pass code generation, recent works further integrate unit tests and program verifiers into LLMs to iteratively refine the generated programs. However, these works consider the generated programs as an indivisible entity, which falls short for LLMs in debugging the programs, especially when the programs contain complex logic flows and data operations. In contrast, when human developers debug programs, they typically set breakpoints and selectively examine runtime execution information. The execution flow and the intermediate variables play a crucial role in the debugging process, yet they are underutilized in the existing literature on code generation. In this study, we introduce Large Language Model Debugger (LDB), a novel debugging framework that enables LLMs to refine their generated programs with the runtime execution information. Specifically, LDB segments the programs into basic blocks and tracks the values of intermediate variables after each block throughout the runtime execution. This allows LLMs to concentrate on simpler code units within the overall execution flow, verify their correctness against the task description block by block, and efficiently pinpoint any potential errors. Experiments demonstrate that LDB consistently enhances the baseline performance by up to 9.8% across the HumanEval, MBPP, and TransCoder benchmarks, archiving new state-of-the-art performance in code debugging for various LLM selections.",
    "arxiv_url": "https://arxiv.org/abs/2402.16906",
    "authors": [
      "Li Zhong",
      "Zilong Wang",
      "Jingbo Shang"
    ],
    "first_author": "Li Zhong",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Editing",
    "tags": [
      "Runtime Execution Traces",
      "Basic Block Segmentation",
      "Control-Flow Graph Profiling",
      "Intermediate Variable Inspection",
      "Block-wise Correctness Verification",
      "Iterative Regeneration",
      "Visible Test Case Guided Debugging"
    ],
    "summary": "本文提出LDB，一种将运行时执行信息（基于控制流图的基本块与中间变量值）引入大语言模型的调试框架，按块逐步验证并迭代修正生成的程序，显著提升多项代码生成基准的性能。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2402.16906v6",
    "published": "2024-02-25",
    "update_time": "2024-06-06",
    "download_time": "2025-12-16 13:30:15"
  }
]