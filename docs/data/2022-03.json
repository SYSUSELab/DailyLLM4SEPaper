[
  {
    "id": "2203.13474",
    "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis",
    "abstract": "Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multi-turn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: https://github.com/salesforce/CodeGen.",
    "arxiv_url": "https://arxiv.org/abs/2203.13474",
    "authors": [
      "Erik Nijkamp",
      "Bo Pang",
      "Hiroaki Hayashi",
      "Lifu Tu",
      "Huan Wang",
      "Yingbo Zhou",
      "Silvio Savarese",
      "Caiming Xiong"
    ],
    "first_author": "Erik Nijkamp",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Prompting",
    "tags": [
      "Multi-turn program synthesis",
      "Progressive natural-language specification",
      "Subproblem factorization/decomposition",
      "Emergent zero-shot code generation",
      "Scaling study across model sizes",
      "Sequential curriculum training on mixed code corpora",
      "Multi-step synthesis evaluation benchmark",
      "Open-source training library and checkpoints"
    ],
    "summary": "本文提出并开源了一系列大规模自回归代码模型与训练库，并构建首个多轮程序合成基准（MTPB），实验证明将问题分解为多轮自然语言子任务能显著提升代码生成性能。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2203.13474v5",
    "published": "2022-03-25",
    "update_time": "2023-02-27",
    "download_time": "2025-12-16 10:45:05"
  }
]