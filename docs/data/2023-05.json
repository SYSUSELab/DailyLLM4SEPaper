[
  {
    "id": "2305.06161",
    "title": "StarCoder: may the source be with you!",
    "abstract": "The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.",
    "arxiv_url": "https://arxiv.org/abs/2305.06161",
    "authors": [
      "Raymond Li",
      "Loubna Ben Allal",
      "Yangtian Zi",
      "Niklas Muennighoff",
      "Denis Kocetkov",
      "Chenghao Mou",
      "Marc Marone",
      "Christopher Akiki",
      "Jia Li",
      "Jenny Chim",
      "Qian Liu",
      "Evgenii Zheltonozhskii",
      "Terry Yue Zhuo",
      "Thomas Wang",
      "Olivier Dehaene",
      "Mishig Davaadorj",
      "Joel Lamy-Poirier",
      "João Monteiro",
      "Oleh Shliazhko",
      "Nicolas Gontier",
      "Nicholas Meade",
      "Armel Zebaze",
      "Ming-Ho Yee",
      "Logesh Kumar Umapathi",
      "Jian Zhu",
      "Benjamin Lipkin",
      "Muhtasham Oblokulov",
      "Zhiruo Wang",
      "Rudra Murthy",
      "Jason Stillerman",
      "Siva Sankalp Patel",
      "Dmitry Abulkhanov",
      "Marco Zocca",
      "Manan Dey",
      "Zhihan Zhang",
      "Nour Fahmy",
      "Urvashi Bhattacharyya",
      "Wenhao Yu",
      "Swayam Singh",
      "Sasha Luccioni",
      "Paulo Villegas",
      "Maxim Kunakov",
      "Fedor Zhdanov",
      "Manuel Romero",
      "Tony Lee",
      "Nadav Timor",
      "Jennifer Ding",
      "Claire Schlesinger",
      "Hailey Schoelkopf",
      "Jan Ebert",
      "Tri Dao",
      "Mayank Mishra",
      "Alex Gu",
      "Jennifer Robinson",
      "Carolyn Jane Anderson",
      "Brendan Dolan-Gavitt",
      "Danish Contractor",
      "Siva Reddy",
      "Daniel Fried",
      "Dzmitry Bahdanau",
      "Yacine Jernite",
      "Carlos Muñoz Ferrandis",
      "Sean Hughes",
      "Thomas Wolf",
      "Arjun Guha",
      "Leandro von Werra",
      "Harm de Vries"
    ],
    "first_author": "Raymond Li",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Pre-Training",
    "tags": [
      "8K-context code modeling",
      "Fill-in-the-Middle infilling",
      "Multi-Query Attention for fast large-batch inference",
      "PII detection and redaction pipeline with annotated dataset",
      "Attribution tracing via lightweight membership check + BM25 search",
      "Open-source responsible model release under OpenRAIL-M",
      "Python fine-tuning for specialized performance",
      "Comprehensive multi-benchmark evaluation",
      "Data governance and opt-out tooling for training corpora"
    ],
    "summary": "本文介绍了BigCode社区开源的15.5B参数代码模型StarCoder及其基座StarCoderBase，具备8K上下文、代码填充能力和基于多查询注意力的快速大批量推理，并辅以PII去识别数据集、归属追踪工具及以更可商用的OpenRAIL-M许可进行的安全开放发布。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2305.06161v2",
    "published": "2023-05-09",
    "update_time": "2023-12-13",
    "download_time": "2025-12-16 10:37:26"
  },
  {
    "id": "2305.07922",
    "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation",
    "abstract": "Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations in terms of architecture and pretraining tasks. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks. The former paradigm is limited by inflexibility in applications while in the latter, the model is treated as a single system for all tasks, leading to suboptimal performance on a subset of tasks. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some downstream tasks and hence result in substantial performance degrade. To address these limitations, we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks. Such flexibility is enabled by our proposed mixture of pretraining objectives to mitigate the pretrain-finetune discrepancy. These objectives cover span denoising, contrastive learning, text-code matching, and causal LM pretraining tasks, on both unimodal and bimodal multilingual code corpora. Furthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs without training from scratch to efficiently scale up our models, and explore instruction-tuning to align with natural language instructions. We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art (SoTA) model performance on various code-related tasks, such as code generation and completion, math programming, and text-to-code retrieval tasks. Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA results on HumanEval code generation task against other open code LLMs.",
    "arxiv_url": "https://arxiv.org/abs/2305.07922",
    "authors": [
      "Yue Wang",
      "Hung Le",
      "Akhilesh Deepak Gotmare",
      "Nghi D. Q. Bui",
      "Junnan Li",
      "Steven C. H. Hoi"
    ],
    "first_author": "Yue Wang",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Pre-Training",
    "tags": [
      "Modular encoder-decoder design",
      "Mixture of pretraining objectives",
      "Span denoising and causal LM combination",
      "Text-code contrastive learning",
      "Text-code matching for cross-modal alignment",
      "Shallow-encoder / deep-frozen-decoder initialization",
      "Instruction tuning for code",
      "Retrieval-augmented generation"
    ],
    "summary": "本文提出CodeT5+—一种可灵活组合模块的编码器-解码器代码大模型，采用跨度去噪、对比学习、文本-代码匹配与因果语言建模等混合预训练目标，并通过冻结大型解码器加浅层编码器的初始化与指令调优，在多种代码理解与生成基准上取得领先性能。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2305.07922v2",
    "published": "2023-05-13",
    "update_time": "2023-05-20",
    "download_time": "2025-12-16 10:42:08"
  },
  {
    "id": "2305.02309",
    "title": "CodeGen2: Lessons for Training LLMs on Programming and Natural Languages",
    "abstract": "Large language models (LLMs) have demonstrated remarkable abilities in representation learning for program synthesis and understanding tasks. The quality of the learned representations appears to be dictated by the neural scaling laws as a function of the number of model parameters and observations, while imposing upper bounds on the model performance by the amount of available data and compute, which is costly.   In this study, we attempt to render the training of LLMs for program synthesis more efficient by unifying four key components: (1) model architectures, (2) learning methods, (3) infill sampling, and, (4) data distributions. Specifically, for the model architecture, we attempt to unify encoder and decoder-based models into a single prefix-LM. For learning methods, (i) causal language modeling, (ii) span corruption, (iii) infilling are unified into a simple learning algorithm. For infill sampling, we explore the claim of a \"free lunch\" hypothesis. For data distributions, the effect of a mixture distribution and multi-epoch training of programming and natural languages on model performance is explored.   We conduct a comprehensive series of empirical experiments on 1B LLMs, for which failures and successes of this exploration are distilled into five lessons. We will provide a final recipe for training and release CodeGen2 models in size 1B, 3.7B, 7B, and, 16B parameters, along with the training framework as open-source: https://github.com/salesforce/CodeGen.",
    "arxiv_url": "https://arxiv.org/abs/2305.02309",
    "authors": [
      "Erik Nijkamp",
      "Hiroaki Hayashi",
      "Caiming Xiong",
      "Silvio Savarese",
      "Yingbo Zhou"
    ],
    "first_author": "Erik Nijkamp",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Pre-Training",
    "tags": [
      "Prefix-LM unification",
      "mixture objective (causal + span corruption)",
      "infill / fill-in-the-middle sampling",
      "code and natural language data mixture",
      "multi-epoch pretraining",
      "extensive ablation study of training choices",
      "training recipe and engineering",
      "open-source model & training release"
    ],
    "summary": "本文通过对Prefix-LM架构、因果语言建模与span损坏混合目标、infill采样策略以及代码与自然语言混合多轮训练的系统消融实验，总结出训练CodeGen2系列模型的经验并开源训练配方与模型。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2305.02309v2",
    "published": "2023-05-03",
    "update_time": "2023-07-11",
    "download_time": "2025-12-16 10:42:46"
  },
  {
    "id": "2305.01210",
    "title": "Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation",
    "abstract": "Program synthesis has been long studied with recent approaches focused on directly using the power of Large Language Models (LLMs) to generate code. Programming benchmarks, with curated synthesis problems and test-cases, are used to measure the performance of various LLMs on code synthesis. However, these test-cases can be limited in both quantity and quality for fully assessing the functional correctness of the generated code. Such limitation in the existing benchmarks begs the following question: In the era of LLMs, is the code generated really correct? To answer this, we propose EvalPlus -- a code synthesis evaluation framework to rigorously benchmark the functional correctness of LLM-synthesized code. EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies. While EvalPlus is general, we extend the test-cases of the popular HumanEval benchmark by 80x to build HumanEval+. Our extensive evaluation across 26 popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs, reducing the pass@k by up-to 19.3-28.9%. We also surprisingly found that test insufficiency can lead to mis-ranking. For example, both WizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+, while none of them could on HumanEval. Our work not only indicates that prior popular code synthesis evaluation results do not accurately reflect the true performance of LLMs for code synthesis, but also opens up a new direction to improve such programming benchmarks through automated testing. We have open-sourced our tools, enhanced datasets as well as all LLM-generated code at https://github.com/evalplus/evalplus to facilitate and accelerate future LLM-for-code research.",
    "arxiv_url": "https://arxiv.org/abs/2305.01210",
    "authors": [
      "Jiawei Liu",
      "Chunqiu Steven Xia",
      "Yuyao Wang",
      "Lingming Zhang"
    ],
    "first_author": "Jiawei Liu",
    "category": [
      "Benchmark",
      "Technical",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Completion",
    "tags": [
      "Automated test-input generation",
      "LLM-seeded seed input generation",
      "Type-aware mutation testing",
      "Differential testing against reference implementation",
      "Test-suite reduction",
      "Greedy set-cover selection for tests",
      "Benchmark augmentation for evaluation",
      "Detection of evaluation-induced model misranking",
      "Ground-truth oracle validation"
    ],
    "summary": "本文提出EvalPlus，通过LLM引导的种子输入与类型感知变异自动生成并精简大量测试用例以扩充程序合成基准，从而更严格地评估LLM生成代码的功能正确性并揭示现有基准高估模型性能与可能导致的模型错排。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2305.01210v3",
    "published": "2023-05-02",
    "update_time": "2023-10-30",
    "download_time": "2025-12-16 13:57:55"
  },
  {
    "id": "2305.18584",
    "title": "Coeditor: Leveraging Contextual Changes for Multi-round Code Auto-editing",
    "abstract": "Developers often dedicate significant time to maintaining and refactoring existing code. However, most prior work on generative models for code focuses solely on creating new code, overlooking the distinctive needs of editing existing code. In this work, we explore a multi-round code auto-editing setting, aiming to predict edits to a code region based on recent changes within the same codebase. Our model, Coeditor, is a fine-tuned language model specifically designed for code editing tasks. We represent code changes using a line diff format and employ static analysis to form large customized model contexts, ensuring the availability of appropriate information for prediction. We collect a code editing dataset from the commit histories of 1650 open-source Python projects for training and evaluation. In a simplified single-round, single-edit task, Coeditor significantly outperforms GPT-3.5 and SOTA open-source code completion models (bringing exact-match accuracy from 34.7 up to 60.4), demonstrating the benefits of incorporating editing history for code completion. In a multi-round, multi-edit setting, we observe substantial gains by iteratively conditioning on additional user edits. We have open-sourced our code, data, and model weights to encourage future research and have released a VSCode extension powered by our model for interactive IDE usage.",
    "arxiv_url": "https://arxiv.org/abs/2305.18584",
    "authors": [
      "Jiayi Wei",
      "Greg Durrett",
      "Isil Dillig"
    ],
    "first_author": "Jiayi Wei",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Editing",
    "tags": [
      "Multi-round code auto-editing",
      "Repo-level diff conditioning",
      "Line-based diff representation",
      "Masked span infilling for edits",
      "Static analysis for context retrieval",
      "Block-sparse attention for long contexts",
      "Commit-history-derived training data",
      "Interactive IDE integration"
    ],
    "summary": "本文提出了面向多轮仓库级代码自动编辑的新任务与评测，设计了基于行差异编码、静态分析和块稀疏注意力的编辑模型，并构建了来自提交历史的新数据集以验证其在编辑自动化上的显著提升。",
    "quality": "High",
    "conference": "International Conference on Learning Representations (ICLR 2024)",
    "pdf_url": "https://arxiv.org/pdf/2305.18584v2",
    "published": "2023-05-29",
    "update_time": "2024-04-28",
    "download_time": "2025-12-16 13:58:36"
  },
  {
    "id": "2305.05959",
    "title": "Survey of Code Search Based on Deep Learning",
    "abstract": "Code writing is repetitive and predictable, inspiring us to develop various code intelligence techniques. This survey focuses on code search, that is, to retrieve code that matches a given query by effectively capturing the semantic similarity between the query and code. Deep learning, being able to extract complex semantics information, has achieved great success in this field. Recently, various deep learning methods, such as graph neural networks and pretraining models, have been applied to code search with significant progress. Deep learning is now the leading paradigm for code search. In this survey, we provide a comprehensive overview of deep learning-based code search. We review the existing deep learning-based code search framework which maps query/code to vectors and measures their similarity. Furthermore, we propose a new taxonomy to illustrate the state-of-the-art deep learning-based code search in a three-steps process: query semantics modeling, code semantics modeling, and matching modeling which involves the deep learning model training. Finally, we suggest potential avenues for future research in this promising field.",
    "arxiv_url": "https://arxiv.org/abs/2305.05959",
    "authors": [
      "Yutao Xie",
      "Jiayi Lin",
      "Hande Dong",
      "Lei Zhang",
      "Zhonghai Wu"
    ],
    "first_author": "Yutao Xie",
    "category": [
      "Survey"
    ],
    "field": "Coding Assistant",
    "task": "Code Retrieval",
    "tags": [
      "NL-to-code retrieval",
      "Query intent modeling",
      "Code semantic representation",
      "Graph-based code representations",
      "Transformer-based code encoders",
      "Contrastive and ranking training objectives",
      "Evaluation metrics and benchmarking",
      "Training / pretraining strategies for retrieval"
    ],
    "summary": "本文综述了基于深度学习的代码检索研究，提出了查询语义建模、代码语义建模与匹配建模的三步分类法，并总结了现有方法、评估指标与未来研究方向。",
    "quality": "High",
    "conference": "ACM Transactions on Software Engineering and Methodology 2023",
    "pdf_url": "https://arxiv.org/pdf/2305.05959v2",
    "published": "2023-05-10",
    "update_time": "2023-12-13",
    "download_time": "2025-12-11 17:43:30"
  },
  {
    "id": "2305.17145",
    "title": "Type Prediction With Program Decomposition and Fill-in-the-Type Training",
    "abstract": "TypeScript and Python are two programming languages that support optional type annotations, which are useful but tedious to introduce and maintain. This has motivated automated type prediction: given an untyped program, produce a well-typed output program. Large language models (LLMs) are promising for type prediction, but there are challenges: fill-in-the-middle performs poorly, programs may not fit into the context window, generated types may not type check, and it is difficult to measure how well-typed the output program is. We address these challenges by building OpenTau, a search-based approach for type prediction that leverages large language models. We propose a new metric for type prediction quality, give a tree-based program decomposition that searches a space of generated types, and present fill-in-the-type fine-tuning for LLMs. We evaluate our work with a new dataset for TypeScript type prediction, and show that 47.4% of files type check (14.5% absolute improvement) with an overall rate of 3.3 type errors per file. All code, data, and models are available at: https://github.com/GammaTauAI/opentau.",
    "arxiv_url": "https://arxiv.org/abs/2305.17145",
    "authors": [
      "Federico Cassano",
      "Ming-Ho Yee",
      "Noah Shinn",
      "Arjun Guha",
      "Steven Holtzen"
    ],
    "first_author": "Federico Cassano",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Editing",
    "tags": [
      "Tree-based program decomposition",
      "Fill-in-the-type fine-tuning",
      "Typedness evaluation metric",
      "Search over type candidates",
      "Type-checking guided ranking",
      "Local type inference",
      "TypeScript gradual migration"
    ],
    "summary": "本文提出OPENTAU，通过将程序递归分解为树状代码块、基于搜索的候选类型生成与一种名为fill-in-the-type的微调方法，并辅以新的typedness评估和TypeScript数据集，有效提升了自动类型注入的可检类型率与类型精确度。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2305.17145v1",
    "published": "2023-05-25",
    "update_time": "2023-05-25",
    "download_time": "2025-12-11 17:53:38"
  },
  {
    "id": "2305.03111",
    "title": "Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs",
    "abstract": "Text-to-SQL parsing, which aims at converting natural language instructions into executable SQLs, has gained increasing attention in recent years. In particular, Codex and ChatGPT have shown impressive results in this task. However, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on database schema with few rows of database contents leaving the gap between academic study and real-world applications. To mitigate this gap, we present Bird, a big benchmark for large-scale database grounded in text-to-SQL tasks, containing 12,751 pairs of text-to-SQL data and 95 databases with a total size of 33.4 GB, spanning 37 professional domains. Our emphasis on database values highlights the new challenges of dirty database contents, external knowledge between NL questions and database contents, and SQL efficiency, particularly in the context of massive databases. To solve these problems, text-to-SQL models must feature database value comprehension in addition to semantic parsing. The experimental results demonstrate the significance of database values in generating accurate text-to-SQLs for big databases. Furthermore, even the most effective text-to-SQL models, i.e. ChatGPT, only achieves 40.08% in execution accuracy, which is still far from the human result of 92.96%, proving that challenges still stand. Besides, we also provide an efficiency analysis to offer insights into generating text-to-efficient-SQLs that are beneficial to industries. We believe that BIRD will contribute to advancing real-world applications of text-to-SQL research. The leaderboard and source code are available: https://bird-bench.github.io/.",
    "arxiv_url": "https://arxiv.org/abs/2305.03111",
    "authors": [
      "Jinyang Li",
      "Binyuan Hui",
      "Ge Qu",
      "Jiaxi Yang",
      "Binhua Li",
      "Bowen Li",
      "Bailin Wang",
      "Bowen Qin",
      "Rongyu Cao",
      "Ruiying Geng",
      "Nan Huo",
      "Xuanhe Zhou",
      "Chenhao Ma",
      "Guoliang Li",
      "Kevin C. C. Chang",
      "Fei Huang",
      "Reynold Cheng",
      "Yongbin Li"
    ],
    "first_author": "Jinyang Li",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Text-to-SQL & Databases",
    "task": "Text-to-SQL Benchmark (Large-scale, value-grounded, efficiency-aware)",
    "tags": [
      "Large-scale database values",
      "Noisy / dirty value handling",
      "External knowledge grounding",
      "SQL execution efficiency",
      "Valid Efficiency Score (VES)",
      "Crowdsourced NL–SQL annotation",
      "Double-blind annotation workflow",
      "Domain-diverse databases",
      "Hidden test set for leakage avoidance",
      "FT vs ICL evaluation (T5 vs LLMs)",
      "Execution-based accuracy evaluation"
    ],
    "summary": "该论文提出了BIRD基准，包含95个真实大规模数据库（33.4GB）与12,751对文本到SQL样本，强调数据库值的噪声处理、外部知识推理与查询效率并提出VES指标及多模型基线评测。",
    "quality": "High",
    "conference": "NeurIPS 2023",
    "pdf_url": "https://arxiv.org/pdf/2305.03111v3",
    "published": "2023-05-04",
    "update_time": "2023-11-15",
    "download_time": "2025-12-12 22:21:49"
  }
]