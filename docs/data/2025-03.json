[
  {
    "id": "2503.01449",
    "title": "Benchmarking Large Language Models for Multi-Language Software Vulnerability Detection",
    "abstract": "Recent advancements in generative AI have led to the widespread adoption of large language models (LLMs) in software engineering, addressing numerous long-standing challenges. However, a comprehensive study examining the capabilities of LLMs in software vulnerability detection (SVD), a crucial aspect of software security, is currently lacking. Existing research primarily focuses on evaluating LLMs using C/C++ datasets. It typically explores only one or two strategies among prompt engineering, instruction tuning, and sequence classification fine-tuning for open-source LLMs. Consequently, there is a significant knowledge gap regarding the effectiveness of diverse LLMs in detecting vulnerabilities across various programming languages. To address this knowledge gap, we present a comprehensive empirical study evaluating the performance of LLMs on the SVD task. We have compiled a comprehensive dataset comprising 8,260 vulnerable functions in Python, 7,505 in Java, and 28,983 in JavaScript. We assess five open-source LLMs using multiple approaches, including prompt engineering, instruction tuning, and sequence classification fine-tuning. These LLMs are benchmarked against five fine-tuned small language models and two open-source static application security testing tools. Furthermore, we explore two avenues to improve LLM performance on SVD: a) Data perspective: Retraining models using downsampled balanced datasets. b) Model perspective: Investigating ensemble learning methods that combine predictions from multiple LLMs. Our comprehensive experiments demonstrate that SVD remains a challenging task for LLMs. This study provides a thorough understanding of the role of LLMs in SVD and offers practical insights for future advancements in leveraging generative AI to enhance software security practices.",
    "arxiv_url": "https://arxiv.org/abs/2503.01449",
    "authors": [
      "Ting Zhang",
      "Chengran Yang",
      "Yindu Su",
      "Martin Weyssow",
      "Hung Nguyen",
      "Tan Bui",
      "Hong Jin Kang",
      "Yikun Li",
      "Eng Lieh Ouh",
      "Lwin Khin Shar",
      "David Lo"
    ],
    "first_author": "Ting Zhang",
    "category": [
      "Empirical",
      "Benchmark"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "Function-level multi-language vulnerability dataset",
      "Python/Java/JavaScript vulnerable functions",
      "Prompt engineering vs instruction tuning vs sequence-classification fine-tuning",
      "Comparison with small language models and SAST tools",
      "Ensemble methods for combining LLM predictions",
      "Class imbalance mitigation via downsampling",
      "Empirical benchmarking of open-source LLMs for SVD",
      "Cross-language vulnerability detection evaluation"
    ],
    "summary": "本文构建了包含 Python、Java 和 JavaScript 函数级漏洞的综合数据集，并系统评估多种开源 LLM（通过提示工程、指令微调和序列分类微调）在漏洞检测任务中的效果，比较了小模型和静态分析工具，同时探索了下采样平衡数据与模型集成以提升性能的策略。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2503.01449v1",
    "published": "2025-03-03",
    "update_time": "2025-03-03",
    "download_time": "2025-12-16 14:32:02"
  },
  {
    "id": "2503.06680",
    "title": "FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation for Feature Implementation",
    "abstract": "Implementing new features in repository-level codebases is a crucial application of code generation models. However, current benchmarks lack a dedicated evaluation framework for this capability. To fill this gap, we introduce FEA-Bench, a benchmark designed to assess the ability of large language models (LLMs) to perform incremental development within code repositories. We collect pull requests from 83 GitHub repositories and use rule-based and intent-based filtering to construct task instances focused on new feature development. Each task instance containing code changes is paired with relevant unit test files to ensure that the solution can be verified. The feature implementation requires LLMs to simultaneously possess code completion capabilities for new components and code editing abilities for other relevant parts in the code repository, providing a more comprehensive evaluation method of LLMs' automated software engineering capabilities. Experimental results show that LLMs perform significantly worse in the FEA-Bench, highlighting considerable challenges in such repository-level incremental code development.",
    "arxiv_url": "https://arxiv.org/abs/2503.06680",
    "authors": [
      "Wei Li",
      "Xin Zhang",
      "Zhongxin Guo",
      "Shaoguang Mao",
      "Wen Luo",
      "Guangyue Peng",
      "Yangyu Huang",
      "Houfeng Wang",
      "Scarlett Li"
    ],
    "first_author": "Wei Li",
    "category": [
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Editing",
    "tags": [
      "Repository-level incremental development",
      "Feature implementation from pull requests",
      "Unit-test-based execution validation",
      "Cross-file edits and new-file addition",
      "Automated PR-to-task collection pipeline",
      "Large-patch long-form code generation"
    ],
    "summary": "本文提出FEA-Bench——一个从GitHub pull request构建的基准，用以评估LLM在仓库级别实现新功能（包括新增组件与跨文件编辑）能力，并以单元测试执行结果作为验证，实验证明当前模型在此任务上表现较差。",
    "quality": "High",
    "conference": "ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2503.06680v2",
    "published": "2025-03-09",
    "update_time": "2025-06-19",
    "download_time": "2025-12-11 16:44:20"
  },
  {
    "id": "2503.06689",
    "title": "DependEval: Benchmarking LLMs for Repository Dependency Understanding",
    "abstract": "While large language models (LLMs) have shown considerable promise in code generation, real-world software development demands advanced repository-level reasoning. This includes understanding dependencies, project structures, and managing multi-file changes. However, the ability of LLMs to effectively comprehend and handle complex code repositories has yet to be fully explored. To address challenges, we introduce a hierarchical benchmark designed to evaluate repository dependency understanding (DependEval). Benchmark is based on 15,576 repositories collected from real-world websites. It evaluates models on three core tasks: Dependency Recognition, Repository Construction, and Multi-file Editing, across 8 programming languages from actual code repositories. Our evaluation of over 25 LLMs reveals substantial performance gaps and provides valuable insights into repository-level code understanding.",
    "arxiv_url": "https://arxiv.org/abs/2503.06689",
    "authors": [
      "Junjia Du",
      "Yadi Liu",
      "Hongcheng Guo",
      "Jiawei Wang",
      "Haojian Huang",
      "Yunyi Ni",
      "Zhoujun Li"
    ],
    "first_author": "Junjia Du",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Repository dependency resolution",
      "Cross-file dependency parsing",
      "Repository structure generation from requirements",
      "Coordinated multi-file editing",
      "Hierarchical evaluation metrics",
      "Static import-based dependency extraction",
      "Call-chain extraction",
      "Multilingual repository benchmark"
    ],
    "summary": "本文提出DependEval，一个覆盖8种编程语言、基于15,576个真实仓库的分层基准，用于评估LLM在依赖识别、仓库构建和多文件编辑等仓库级代码理解任务上的能力，并在25+模型上给出细粒度性能分析。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2503.06689v1",
    "published": "2025-03-09",
    "update_time": "2025-03-09",
    "download_time": "2025-12-11 16:45:03"
  },
  {
    "id": "2503.07010",
    "title": "ProjectEval: A Benchmark for Programming Agents Automated Evaluation on Project-Level Code Generation",
    "abstract": "Recently, LLM agents have made rapid progress in improving their programming capabilities. However, existing benchmarks lack the ability to automatically evaluate from users' perspective, and also lack the explainability of the results of LLM agents' code generation capabilities. Thus, we introduce ProjectEval, a new benchmark for LLM agents project-level code generation's automated evaluation by simulating user interaction. ProjectEval is constructed by LLM with human reviewing. It has three different level inputs of natural languages or code skeletons. ProjectEval can evaluate the generated projects by user interaction simulation for execution, and by code similarity through existing objective indicators. Through ProjectEval, we find that systematic engineering project code, overall understanding of the project and comprehensive analysis capability are the keys for LLM agents to achieve practical projects. Our findings and benchmark provide valuable insights for developing more effective programming agents that can be deployed in future real-world production.",
    "arxiv_url": "https://arxiv.org/abs/2503.07010",
    "authors": [
      "Kaiyuan Liu",
      "Youcheng Pan",
      "Yang Xiang",
      "Daojing He",
      "Jing Li",
      "Yexing Du",
      "Tianrun Gao"
    ],
    "first_author": "Kaiyuan Liu",
    "category": [
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Project-level code generation benchmark",
      "User interaction simulation",
      "Execution-based automated testing",
      "Three-level input (NL / checklist / skeleton)",
      "Parameter-description alignment",
      "Semi-automated construction (LLM + human review)",
      "Web and console project evaluation",
      "Explainable evaluation metrics"
    ],
    "summary": "本文提出了ProjectEval，一个通过模拟用户交互并结合三种输入级别与执行型测试套件，对编程代理项目级代码生成进行自动化且可解释评估的基准。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2503.07010v2",
    "published": "2025-03-10",
    "update_time": "2025-05-31",
    "download_time": "2025-12-11 16:45:49"
  },
  {
    "id": "2503.07358",
    "title": "RepoST: Scalable Repository-Level Coding Environment Construction with Sandbox Testing",
    "abstract": "We present RepoST, a scalable method to construct environments that provide execution feedback for repository-level code generation for both training and evaluation. Unlike existing works that aim to build entire repositories for execution, which is challenging for both human and LLMs, we provide execution feedback with sandbox testing, which isolates a given target function and its dependencies to a separate script for testing. Sandbox testing reduces the complexity of external dependencies and enables constructing environments at a large scale. We use our method to construct RepoST-Train, a large-scale train set with 7,415 functions from 832 repositories. Training with the execution feedback provided by RepoST-Train leads to a performance gain of 5.5% Pass@1 on HumanEval and 3.5% Pass@1 on RepoEval. We also build an evaluation dataset, RepoST-Eval, and benchmark 12 code generation models.",
    "arxiv_url": "https://arxiv.org/abs/2503.07358",
    "authors": [
      "Yiqing Xie",
      "Alex Xie",
      "Divyanshu Sheth",
      "Pengfei Liu",
      "Daniel Fried",
      "Carolyn Rose"
    ],
    "first_author": "Yiqing Xie",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Completion",
    "tags": [
      "Sandbox Testing",
      "Repository-level Environment Construction",
      "Function-level Sandboxing",
      "Automated Test Generation",
      "Equivalence Testing",
      "Mocking External APIs and Files",
      "AST-based Functionality Verification",
      "Iterative Debugging and Filtering",
      "Executable Evaluation Scripts",
      "Large-scale Train/Eval Dataset Construction"
    ],
    "summary": "本文提出REPOST，通过将目标函数及其局部依赖沙箱化并由LLM生成测试与模拟资源，自动构建可执行的仓库级代码生成训练与评估环境，并发布大规模的训练集与评测集以提升模型性能。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2503.07358v1",
    "published": "2025-03-10",
    "update_time": "2025-03-10",
    "download_time": "2025-12-11 16:46:22"
  },
  {
    "id": "2503.09433",
    "title": "CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards CWE Detection",
    "abstract": "Identifying vulnerabilities in source code is crucial, especially in critical software components. Existing methods such as static analysis, dynamic analysis, formal verification, and recently Large Language Models are widely used to detect security flaws. This paper introduces CASTLE (CWE Automated Security Testing and Low-Level Evaluation), a benchmarking framework for evaluating the vulnerability detection capabilities of different methods. We assess 13 static analysis tools, 10 LLMs, and 2 formal verification tools using a hand-crafted dataset of 250 micro-benchmark programs covering 25 common CWEs. We propose the CASTLE Score, a novel evaluation metric to ensure fair comparison. Our results reveal key differences: ESBMC (a formal verification tool) minimizes false positives but struggles with vulnerabilities beyond model checking, such as weak cryptography or SQL injection. Static analyzers suffer from high false positives, increasing manual validation efforts for developers. LLMs perform exceptionally well in the CASTLE dataset when identifying vulnerabilities in small code snippets. However, their accuracy declines, and hallucinations increase as the code size grows. These results suggest that LLMs could play a pivotal role in future security solutions, particularly within code completion frameworks, where they can provide real-time guidance to prevent vulnerabilities. The dataset is accessible at https://github.com/CASTLE-Benchmark.",
    "arxiv_url": "https://arxiv.org/abs/2503.09433",
    "authors": [
      "Richard A. Dubniczky",
      "Krisztofer Zoltán Horvát",
      "Tamás Bisztray",
      "Mohamed Amine Ferrag",
      "Lucas C. Cordeiro",
      "Norbert Tihanyi"
    ],
    "first_author": "Richard A. Dubniczky",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "CWE micro-benchmarks",
      "compilable C snippets",
      "line-level vulnerability labeling",
      "benchmark scoring metric",
      "static analyzer false-positive analysis",
      "LLM detection scalability",
      "formal verification coverage"
    ],
    "summary": "本文构建了一个包含250个可编译C微基准、覆盖25类CWE的漏洞检测基准并提出评估度量，通过比较静态分析器、形式验证工具与多款LLM揭示各方法在误报率、覆盖范围与随代码规模扩展时性能衰减的差异。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2503.09433v2",
    "published": "2025-03-12",
    "update_time": "2025-03-31",
    "download_time": "2025-12-11 17:19:57"
  },
  {
    "id": "2503.22388",
    "title": "Why Stop at One Error? Benchmarking LLMs as Data Science Code Debuggers for Multi-Hop and Multi-Bug Errors",
    "abstract": "LLMs are transforming software development, yet current code generation and code repair benchmarks mainly assess syntactic and functional correctness in simple, single-error cases. LLMs' capabilities to autonomously find and fix runtime logical errors in complex data science code remain largely unexplored. To address this gap, we introduce DSDBench: the Data Science Debugging Benchmark, the first benchmark for systematic evaluation of LLMs on multi-hop error tracing and multi-bug detection in data science code debugging. DSDBench adapts datasets from existing data science task benchmarks, such as DABench and MatPlotBench, featuring realistic data science debugging tasks with automatically synthesized multi-hop, multi-bug code snippets. DSDBench includes 1,117 annotated samples with 741 cause-effect error pairs and runtime error messages. Evaluations of state-of-the-art LLMs on DSDBench show significant performance gaps, highlighting challenges in debugging logical runtime errors in data science code. DSDBench offers a crucial resource to evaluate and improve LLMs' debugging and reasoning capabilities, enabling more reliable AI-assisted data science in the future. DSDBench is publicly available at github.com/KevinCL16/DSDBench.",
    "arxiv_url": "https://arxiv.org/abs/2503.22388",
    "authors": [
      "Zhiyu Yang",
      "Shuo Wang",
      "Yukun Yan",
      "Yang Deng"
    ],
    "first_author": "Zhiyu Yang",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Multi-hop Error Tracing",
      "Multi-Bug Detection",
      "Runtime Error Injection",
      "Cause-Effect Line Annotation",
      "Automated Error Injection Pipeline",
      "Library API Misuse",
      "Interactive Notebook Debugging",
      "Data Science Workflow Failures"
    ],
    "summary": "本文提出并发布了DSDBench——一个针对数据科学脚本的基准数据集与自动化注入/对齐流水线，用以评估LLM在多跳与多错误运行时调试（定位根因并关联错误触发行）方面的能力，并通过大规模实证揭示了现有模型在此任务上的显著不足。",
    "quality": "High",
    "conference": "EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2503.22388v3",
    "published": "2025-03-28",
    "update_time": "2025-09-16",
    "download_time": "2025-12-11 17:20:27"
  },
  {
    "id": "2503.04359",
    "title": "LONGCODEU: Benchmarking Long-Context Language Models on Long Code Understanding",
    "abstract": "Current advanced long-context language models offer great potential for real-world software engineering applications. However, progress in this critical domain remains hampered by a fundamental limitation: the absence of a rigorous evaluation framework for long code understanding. To gap this obstacle, we propose a long code understanding benchmark LONGCODEU from four aspects (8 tasks) to evaluate LCLMs' long code understanding ability required for practical applications, including code unit perception, intra-code unit understanding, inter-code unit relation understanding, and long code documentation understanding. We evaluate 9 popular LCLMs on LONGCODEU (i.e., 6 general models and 3 code models). Our experimental results reveal key limitations in current LCLMs' capabilities for long code understanding. Particularly, the performance of LCLMs drops dramatically when the long code length is greater than 32K, falling far short of their claimed 128K-1M context windows. In the four aspects, inter-code unit relation understanding is the most challenging for LCLMs. Our study provides valuable insights for optimizing LCLMs and driving advancements in software engineering.",
    "arxiv_url": "https://arxiv.org/abs/2503.04359",
    "authors": [
      "Jia Li",
      "Xuyuan Guo",
      "Lei Li",
      "Kechi Zhang",
      "Ge Li",
      "Jia Li",
      "Zhengwei Tao",
      "Fang Liu",
      "Chongyang Tao",
      "Yuqi Zhu",
      "Zhi Jin"
    ],
    "first_author": "Jia Li",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Understanding",
    "tags": [
      "Long-context evaluation",
      "Repository-level code understanding",
      "Inter-code-unit relation reasoning",
      "Function (code unit) identification",
      "Long documentation comprehension",
      "Temporal filtering to reduce data contamination",
      "Context-length stress testing (up to 128K tokens)"
    ],
    "summary": "该论文提出了LONGCODEU基准，收集真实仓库中的超长代码并从代码单元感知、单位内理解、单位间关系理解和长文档理解四个方面设计8项任务，对多款长上下文模型进行评测以揭示其在超32K上下文长度下的性能瓶颈。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2503.04359v1",
    "published": "2025-03-06",
    "update_time": "2025-03-06",
    "download_time": "2025-12-12 21:33:30"
  },
  {
    "id": "2503.04149",
    "title": "Dynamic Benchmarking of Reasoning Capabilities in Code Large Language Models Under Data Contamination",
    "abstract": "The rapid evolution of code largelanguage models underscores the need for effective and transparent benchmarking of their reasoning capabilities. However, the current benchmarking approach heavily depends on publicly available, human-created datasets. The widespread use of these fixed benchmark datasets makes the benchmarking process to be static and thus particularly susceptible to data contamination, an unavoidable consequence of the extensive data collection processes used to train Code LLMs. Existing approaches that address data contamination often suffer from human effort limitations and imbalanced problem complexity. To tackle these challenges, we propose \\tool, a novel benchmarking suite for evaluating Code LLMs under potential data contamination. Given a seed programming problem, \\tool employs multiple agents to extract and modify the context without altering the core logic, generating semantically equivalent variations. We introduce a dynamic data generation methods and conduct empirical studies on two seed datasets across 21 Code LLMs. Results show that \\tool effectively benchmarks reasoning capabilities under contamination risks while generating diverse problem sets to ensure consistent and reliable evaluations.",
    "arxiv_url": "https://arxiv.org/abs/2503.04149",
    "authors": [
      "Simin Chen",
      "Pranav Pusarla",
      "Baishakhi Ray"
    ],
    "first_author": "Simin Chen",
    "category": [
      "Benchmark",
      "Technical",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Metamorphic Testing",
      "Dynamic Problem Generation",
      "Complexity-Preserving Transformation",
      "Validation Agent",
      "Data Contamination",
      "Semantic Diversification",
      "Benchmark Robustness",
      "LLM-as-Judge"
    ],
    "summary": "本文提出DyCodeEval：一种基于变形测试并使用多Agent LLM自动生成语义多样且保持复杂度不变的动态编程题基准，以在数据污染场景下可靠评估代码LLM的推理能力并通过验证Agent确保题目与测试用例的正确性。",
    "quality": "High",
    "conference": "ICML 2025",
    "pdf_url": "https://arxiv.org/pdf/2503.04149v2",
    "published": "2025-03-06",
    "update_time": "2025-06-03",
    "download_time": "2025-12-17 19:00:33"
  },
  {
    "id": "2503.10452",
    "title": "DynaCode: A Dynamic Complexity-Aware Code Benchmark for Evaluating Large Language Models in Code Generation",
    "abstract": "The rapid advancement of large language models (LLMs) has significantly improved their performance in code generation tasks. However, existing code benchmarks remain static, consisting of fixed datasets with predefined problems. This makes them vulnerable to memorization during training, where LLMs recall specific test cases instead of generalizing to new problems, leading to data contamination and unreliable evaluation results. To address these issues, we introduce DynaCode, a dynamic, complexity-aware benchmark that overcomes the limitations of static datasets. DynaCode evaluates LLMs systematically using a complexity-aware metric, incorporating both code complexity and call-graph structures. DynaCode achieves large-scale diversity, generating up to 189 million unique nested code problems across four distinct levels of code complexity, referred to as units, and 16 types of call graphs. Results on 12 latest LLMs show an average performance drop of 16.8% to 45.7% compared to MBPP+, a static code generation benchmark, with performance progressively decreasing as complexity increases. This demonstrates DynaCode's ability to effectively differentiate LLMs. Additionally, by leveraging call graphs, we gain insights into LLM behavior, particularly their preference for handling subfunction interactions within nested code. Our benchmark and evaluation code are available at https://github.com/HWH-2000/DynaCode.",
    "arxiv_url": "https://arxiv.org/abs/2503.10452",
    "authors": [
      "Wenhao Hu",
      "Jinhao Duan",
      "Chunchen Wei",
      "Li Zhang",
      "Yue Zhang",
      "Kaidi Xu"
    ],
    "first_author": "Wenhao Hu",
    "category": [
      "Benchmark",
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Generation Benchmarking",
    "tags": [
      "Dynamic Benchmark",
      "Complexity-aware Metric",
      "Call-Graph Construction",
      "Cyclomatic Complexity",
      "Data Contamination",
      "Nested Code Generation",
      "Execution-based Evaluation",
      "Error Analysis"
    ],
    "summary": "DynaCode 提出一个基于调用图与复杂度感知的动态代码生成基准，通过自动构造多层嵌套问题生成大规模任务集合，结合静态与动态复杂度度量来更公平地评估并揭示 LLM 在不同代码与调用图复杂度下的弱点，同时缓解数据污染问题。",
    "quality": "High",
    "conference": "ACL",
    "pdf_url": "https://arxiv.org/pdf/2503.10452v2",
    "published": "2025-03-13",
    "update_time": "2025-05-29",
    "download_time": "2025-12-17 19:01:15"
  },
  {
    "id": "2503.15242",
    "title": "BigO(Bench) -- Can LLMs Generate Code with Controlled Time and Space Complexity?",
    "abstract": "We introduce BigO(Bench), a novel coding benchmark designed to evaluate the capabilities of generative language models in understanding and generating code with specified time and space complexities. This benchmark addresses the gap in current evaluations that often overlook the ability of models to comprehend and produce code constrained by computational complexity. BigO(Bench) includes tooling to infer the algorithmic complexity of any Python function from profiling measurements, including human- or LLM-generated solutions. BigO(Bench) also includes of set of 3,105 coding problems and 1,190,250 solutions from Code Contests annotated with inferred (synthetic) time and space complexity labels from the complexity framework, as well as corresponding runtime and memory footprint values for a large set of input sizes. We present results from evaluating multiple state-of-the-art language models on this benchmark, highlighting their strengths and weaknesses in handling complexity requirements. In particular, token-space reasoning models are unrivaled in code generation but not in complexity understanding, hinting that they may not generalize well to tasks for which no reward was given at training time.",
    "arxiv_url": "https://arxiv.org/abs/2503.15242",
    "authors": [
      "Pierre Chambon",
      "Baptiste Roziere",
      "Benoit Sagot",
      "Gabriel Synnaeve"
    ],
    "first_author": "Pierre Chambon",
    "category": [
      "Benchmark",
      "Technical",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Performance Optimization",
    "tags": [
      "Dynamic Complexity Inference",
      "Complexity-conditioned Generation",
      "Complexity Prediction",
      "Runtime Profiling",
      "Memory Footprint Estimation",
      "Curve Fitting",
      "Fuzzing-based Input Scaling",
      "Benchmarking Code Complexity",
      "Sandboxed Execution",
      "LLM Complexity Evaluation"
    ],
    "summary": "本文提出BigO(Bench)，一个包含3,105道题与约1.19M解法并附带动态时间/空间复杂度推断工具的基准，用以评估并比较LLM在满足指定复杂度约束下生成与理解代码的能力。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2503.15242v2",
    "published": "2025-03-19",
    "update_time": "2025-03-20",
    "download_time": "2025-12-17 19:01:47"
  }
]