[
  {
    "id": "2511.17368",
    "title": "Exploring Scientific Debt: Harnessing AI for SATD Identification in Scientific Software",
    "abstract": "Developers often leave behind clues in their code, admitting where it falls short, known as Self-Admitted Technical Debt (SATD). In the world of Scientific Software (SSW), where innovation moves fast and collaboration is key, such debt is not just common but deeply impactful. As research relies on accurate and reproducible results, accumulating SATD can threaten the very foundations of scientific discovery. Yet, despite its significance, the relationship between SATD and SSW remains largely unexplored, leaving a crucial gap in understanding how to manage SATD in this critical domain. This study explores SATD in SSW repositories, comparing SATD in scientific versus general-purpose open-source software and evaluating transformer-based models for SATD identification. We analyzed SATD in 27 scientific and general-purpose repositories across multiple domains and languages. We fine-tuned and compared 10 transformer-based models (100M-7B parameters) on 67,066 labeled code comments. SSW contains 9.25x more Scientific Debt and 4.93x more SATD than general-purpose software due to complex computations, domain constraints, and evolving research needs. Furthermore, our best model outperforms existing ones. This study uncovers how SATD in SSW differs from general software, revealing its impact on quality and scientific validity. By recognizing these challenges, developers and researchers can adopt smarter strategies to manage debt and safeguard the integrity of scientific discovery.",
    "arxiv_url": "https://arxiv.org/abs/2511.17368",
    "authors": [
      "Eric L. Melin",
      "Ahmed Musa Awon",
      "Nasir U. Eisty",
      "Neil A. Ernst",
      "Shurui Zhou"
    ],
    "first_author": "Eric L. Melin",
    "category": [
      "Technical",
      "Benchmark",
      "Empirical"
    ],
    "field": "Maintenance",
    "task": "Code Review",
    "tags": [
      "Scientific Debt",
      "Self-Admitted Technical Debt (SATD) detection",
      "Code comment classification",
      "Transformer fine-tuning for SATD",
      "Data augmentation for labeling",
      "Cross-project evaluation",
      "Domain-specific analysis of scientific software",
      "Large-model (100M–7B) performance comparison"
    ],
    "summary": "本文构建并扩充了针对科学软件中“Scientific Debt”的标注数据集，微调并比较了多种transformer模型用于代码注释中的SATD识别与分类，发现科学软件中SATD和Scientific Debt显著高于通用软件且最佳模型优于现有基线。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.17368v1",
    "published": "2025-11-21",
    "update_time": "2025-11-21",
    "download_time": "2025-12-16 14:02:52"
  },
  {
    "id": "2511.17330",
    "title": "Agentic Program Verification",
    "abstract": "Automatically generated code is gaining traction recently, owing to the prevalence of Large Language Models (LLMs). Further, the AlphaProof initiative has demonstrated the possibility of using AI for general mathematical reasoning. Reasoning about computer programs (software) can be accomplished via general mathematical reasoning; however, it tends to be more structured and richer in contexts. This forms an attractive proposition, since then AI agents can be used to reason about voluminous code that gets generated by AI.   In this work, we present a first LLM agent, AutoRocq, for conducting program verification. Unlike past works, which rely on extensive training of LLMs on proof examples, our agent learns on-the-fly and improves the proof via an iterative refinement loop. The iterative improvement of the proof is achieved by the proof agent communicating with the Rocq (formerly Coq) theorem prover to get additional context and feedback. The final result of the iteration is a proof derivation checked by the Rocq theorem prover. In this way, our proof construction involves autonomous collaboration between the proof agent and the theorem prover. This autonomy facilitates the search for proofs and decision-making in deciding on the structure of the proof tree.   Experimental evaluation on SV-COMP benchmarks and on Linux kernel modules shows promising efficacy in achieving automated program verification. As automation in code generation becomes more widespread, we posit that our proof agent can be potentially integrated with AI coding agents to achieve a generate and validate loop, thus moving closer to the vision of trusted automatic programming.",
    "arxiv_url": "https://arxiv.org/abs/2511.17330",
    "authors": [
      "Haoxin Tu",
      "Huan Zhao",
      "Yahui Song",
      "Mehtab Zafar",
      "Ruijie Meng",
      "Abhik Roychoudhury"
    ],
    "first_author": "Haoxin Tu",
    "category": [
      "Technical"
    ],
    "field": "Formal Verification & Theorem Proving",
    "task": "Agentic Program Proof Generation",
    "tags": [
      "Agentic theorem-prover interaction",
      "On-the-fly iterative proof refinement",
      "Tactic prediction guided by proof trees",
      "Context-aware lemma retrieval",
      "Tree-structured proof representations",
      "Autonomous proof search and decision-making",
      "Generate-and-validate verification loop"
    ],
    "summary": "本文提出了AutoRocq，一种智能体化的自动程序证明系统，通过与交互式定理证明器循环交互、上下文感知的引理检索与树状证明表示实现逐步精炼的策略，在SV-COMP与Linux内核模块上显著提升了自动化验证能力。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.17330v1",
    "published": "2025-11-21",
    "update_time": "2025-11-21",
    "download_time": "2025-12-16 14:03:40"
  },
  {
    "id": "2511.17262",
    "title": "SlsReuse: LLM-Powered Serverless Function Reuse",
    "abstract": "Serverless computing has rapidly emerged as a popular cloud computing paradigm. It enables developers to implement function-level tasks, i.e., serverless functions, without managing infrastructure. While reducing operational overhead, it poses challenges, especially for novice developers. Developing functions from scratch requires adapting to heterogeneous, platform-specific programming styles, making the process time-consuming and error-prone. Function reuse offers a promising solution to address these challenges. However, research on serverless computing lacks a dedicated approach for function recommendation. Existing techniques from traditional contexts remain insufficient due to the semantic gap between task descriptions and heterogeneous function implementations. Advances in large language models (LLMs), pre-trained on large-scale corpora, create opportunities to bridge this gap by aligning developer requirements with function semantics.   This paper presents SlsReuse, the first LLM-powered framework for serverless function reuse. Specifically, SlsReuse first constructs a reusable function repository serving as a foundational knowledge base. Then, it learns unified semantic-enhanced representations of heterogeneous functions through effective prompt engineering with few-shot prompting, capturing implicit code intent, target platforms, programming languages, and cloud services. Finally, given a natural language task query, SlsReuse performs intent-aware discovery combined with a multi-level pruning strategy and similarity matching. We evaluate SlsReuse on a curated dataset of 110 task queries. Built on ChatGPT-4o, one of the most representative LLMs, SlsReuse achieves Recall@10 of 91.20%, exceeding the state-of-the-art baseline by 24.53 percentage points.",
    "arxiv_url": "https://arxiv.org/abs/2511.17262",
    "authors": [
      "Jinfeng Wen",
      "Yuehan Sun"
    ],
    "first_author": "Jinfeng Wen",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Retrieval",
    "tags": [
      "Serverless function retrieval",
      "Semantic code representation",
      "Few-shot prompt engineering",
      "Intent-aware discovery",
      "Multi-level pruning",
      "Cross-platform heterogeneity",
      "Function repository construction",
      "Similarity-based ranking"
    ],
    "summary": "本文提出SlsReuse——一个基于大模型、通过少量示例提示抽取语义增强表示并结合意图感知检索与多级剪枝实现无服务器函数重用的框架，并构建了500个函数的仓库和110条评测查询展示其显著效果。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.17262v1",
    "published": "2025-11-21",
    "update_time": "2025-11-21",
    "download_time": "2025-12-16 14:04:08"
  },
  {
    "id": "2511.17027",
    "title": "ReVul-CoT: Towards Effective Software Vulnerability Assessment with Retrieval-Augmented Generation and Chain-of-Thought Prompting",
    "abstract": "Context: Software Vulnerability Assessment (SVA) plays a vital role in evaluating and ranking vulnerabilities in software systems to ensure their security and reliability. Objective: Although Large Language Models (LLMs) have recently shown remarkable potential in SVA, they still face two major limitations. First, most LLMs are trained on general-purpose corpora and thus lack domain-specific knowledge essential for effective SVA. Second, they tend to rely on shallow pattern matching instead of deep contextual reasoning, making it challenging to fully comprehend complex code semantics and their security implications. Method: To alleviate these limitations, we propose a novel framework ReVul-CoT that integrates Retrieval-Augmented Generation (RAG) with Chain-of-Thought (COT) prompting. In ReVul-CoT, the RAG module dynamically retrieves contextually relevant information from a constructed local knowledge base that consolidates vulnerability data from authoritative sources (such as NVD and CWE), along with corresponding code snippets and descriptive information. Building on DeepSeek-V3.1, CoT prompting guides the LLM to perform step-by-step reasoning over exploitability, impact scope, and related factors Results: We evaluate ReVul-CoT on a dataset of 12,070 vulnerabilities. Experimental results show that ReVul-CoT outperforms state-of-the-art SVA baselines by 16.50%-42.26% in terms of MCC, and outperforms the best baseline by 10.43%, 15.86%, and 16.50% in Accuracy, F1-score, and MCC, respectively. Our ablation studies further validate the contributions of considering dynamic retrieval, knowledge integration, and CoT-based reasoning. Conclusion: Our results demonstrate that combining RAG with CoT prompting significantly enhances LLM-based SVA and points out promising directions for future research.",
    "arxiv_url": "https://arxiv.org/abs/2511.17027",
    "authors": [
      "Zhijie Chen",
      "Xiang Chen",
      "Ziming Li",
      "Jiacheng Xue",
      "Chaoyang Gao"
    ],
    "first_author": "Zhijie Chen",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Assessment (Severity Scoring)",
    "tags": [
      "Retrieval-Augmented Generation (RAG)",
      "Chain-of-Thought (CoT) prompting",
      "Local vulnerability knowledge base",
      "CVSS v3 severity prediction",
      "Code and vulnerability-description fusion",
      "Dynamic retrieval strategy",
      "Exploitability and impact reasoning",
      "Ablation study of framework components"
    ],
    "summary": "本文提出ReVul-CoT框架，将动态检索增强生成与链式思维提示相结合，利用构建的本地漏洞知识库对代码与描述进行逐步推理，从而显著提升基于CVSS v3的软件漏洞严重性评估性能。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.17027v1",
    "published": "2025-11-21",
    "update_time": "2025-11-21",
    "download_time": "2025-12-16 14:04:51"
  },
  {
    "id": "2511.16858",
    "title": "Is the Cure Still Worse Than the Disease? Test Overfitting by LLMs in Automated Program Repair",
    "abstract": "Automated program repair has been shown to be susceptible to generating repaired code that passes on seen tests but fails on a hold-out set of hidden tests. This problem, dubbed test overfitting, has been identified and studied before the rise of large language models. We experimentally study how much test overfitting is still a problem today, using repository-level SWE-bench tasks.",
    "arxiv_url": "https://arxiv.org/abs/2511.16858",
    "authors": [
      "Toufique Ahmed",
      "Jatin Ganhotra",
      "Avraham Shinnar",
      "Martin Hirzel"
    ],
    "first_author": "Toufique Ahmed",
    "category": [
      "Empirical"
    ],
    "field": "Quality Management",
    "task": "Bug Repair",
    "tags": [
      "Test Overfitting",
      "White-box vs Black-box Tests",
      "Repository-level Automated Program Repair",
      "Reproduction Test Generation",
      "Test-based Code Refinement Loop",
      "Reward-guided Patch Selection",
      "Test Exposure/Hiding Mitigation",
      "Limit Study with Revealed Tests"
    ],
    "summary": "本文在仓库级自动程序修复任务上实证量化了基于LLM的方法在使用白盒测试时产生的测试过拟合问题，并评估了基于测试的补丁改进与揭示金标准测试对修复效果的影响。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.16858v1",
    "published": "2025-11-20",
    "update_time": "2025-11-20",
    "download_time": "2025-12-16 14:05:25"
  },
  {
    "id": "2511.16787",
    "title": "NALA_MAINZ at BLP-2025 Task 2: A Multi-agent Approach for Bangla Instruction to Python Code Generation",
    "abstract": "This paper presents JGU Mainz's winning system for the BLP-2025 Shared Task on Code Generation from Bangla Instructions. We propose a multi-agent-based pipeline. First, a code-generation agent produces an initial solution from the input instruction. The candidate program is then executed against the provided unit tests (pytest-style, assert-based). Only the failing cases are forwarded to a debugger agent, which reruns the tests, extracts error traces, and, conditioning on the error messages, the current program, and the relevant test cases, generates a revised solution. Using this approach, our submission achieved first place in the shared task with a $Pass@1$ score of 95.4. We also make our code public.",
    "arxiv_url": "https://arxiv.org/abs/2511.16787",
    "authors": [
      "Hossain Shaikh Saadi",
      "Faria Alam",
      "Mario Sanz-Guerrero",
      "Minh Duc Bui",
      "Manuel Mager",
      "Katharina von der Wense"
    ],
    "first_author": "Hossain Shaikh Saadi",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Prompting",
    "tags": [
      "Multi-agent pipeline",
      "Test-driven debugging",
      "Selective failure-only debugging",
      "Unit-test execution feedback",
      "Error-trace-conditioned repair",
      "Automatic test-case augmentation",
      "Bangla instruction-to-code"
    ],
    "summary": "本文提出了一种面向孟加拉语指令到Python代码生成的多智能体流水线：先由生成代理产出代码并运行单元测试，仅将失败样例与错误追踪交给调试代理进行基于错误信息的修复，从而在BLP-2025共享任务中取得第一（Pass@1=95.4%）。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.16787v1",
    "published": "2025-11-20",
    "update_time": "2025-11-20",
    "download_time": "2025-12-16 14:06:05"
  },
  {
    "id": "2511.16395",
    "title": "CorrectHDL: Agentic HDL Design with LLMs Leveraging High-Level Synthesis as Reference",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable potential in hardware front-end design using hardware description languages (HDLs). However, their inherent tendency toward hallucination often introduces functional errors into the generated HDL designs. To address this issue, we propose the framework CorrectHDL that leverages high-level synthesis (HLS) results as functional references to correct potential errors in LLM-generated HDL designs.The input to the proposed framework is a C/C++ program that specifies the target circuit's functionality. The program is provided to an LLM to directly generate an HDL design, whose syntax errors are repaired using a Retrieval-Augmented Generation (RAG) mechanism. The functional correctness of the LLM-generated circuit is iteratively improved by comparing its simulated behavior with an HLS reference design produced by conventional HLS tools, which ensures the functional correctness of the result but can lead to suboptimal area and power efficiency. Experimental results demonstrate that circuits generated by the proposed framework achieve significantly better area and power efficiency than conventional HLS designs and approach the quality of human-engineered circuits. Meanwhile, the correctness of the resulting HDL implementation is maintained, highlighting the effectiveness and potential of agentic HDL design leveraging the generative capabilities of LLMs and the rigor of traditional correctness-driven IC design flows.",
    "arxiv_url": "https://arxiv.org/abs/2511.16395",
    "authors": [
      "Kangwei Xu",
      "Grace Li Zhang",
      "Ulf Schlichtmann",
      "Bing Li"
    ],
    "first_author": "Kangwei Xu",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Translation",
    "tags": [
      "HLS-guided differential verification",
      "RAG-based HDL syntax repair",
      "C/C++ submodule decomposition",
      "Submodule boundary instrumentation",
      "Backward slicing for debugging",
      "LLM-driven HDL generation",
      "HDL testbench translation",
      "Area and power (PPA) optimization",
      "Toolchain integration (HLS/RTL sim/synthesis)"
    ],
    "summary": "本文提出CorrectHDL，一种以HLS生成的HDL作为功能金标准、结合LLM生成、RAG语法修复、差分验证与子模块切片的自动化HDL设计与调试框架，可在保证功能正确性的同时显著降低面积与功耗并接近人工设计质量。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.16395v1",
    "published": "2025-11-20",
    "update_time": "2025-11-20",
    "download_time": "2025-12-16 14:06:52"
  },
  {
    "id": "2511.16383",
    "title": "An Agent-Based Framework for the Automatic Validation of Mathematical Optimization Models",
    "abstract": "Recently, using Large Language Models (LLMs) to generate optimization models from natural language descriptions has became increasingly popular. However, a major open question is how to validate that the generated models are correct and satisfy the requirements defined in the natural language description. In this work, we propose a novel agent-based method for automatic validation of optimization models that builds upon and extends methods from software testing to address optimization modeling . This method consists of several agents that initially generate a problem-level testing API, then generate tests utilizing this API, and, lastly, generate mutations specific to the optimization model (a well-known software testing technique assessing the fault detection power of the test suite). In this work, we detail this validation framework and show, through experiments, the high quality of validation provided by this agent ensemble in terms of the well-known software testing measure called mutation coverage.",
    "arxiv_url": "https://arxiv.org/abs/2511.16383",
    "authors": [
      "Alexander Zadorojniy",
      "Segev Wasserkrug",
      "Eitan Farchi"
    ],
    "first_author": "Alexander Zadorojniy",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Software Testing",
    "task": "Test Generation",
    "tags": [
      "Agent-based validation",
      "Problem-level testing API",
      "Unit-test generation for optimization models",
      "Optimization-model mutation operators",
      "Auxiliary verifier model generation",
      "Mutation coverage evaluation"
    ],
    "summary": "本文提出一个基于多智能体的自动化验证框架，通过构建问题级测试接口、自动生成面向优化建模的单元测试与变异体（mutation）来对自然语言生成的线性规划模型进行模型不可知的正确性验证，并以变异覆盖率评估其有效性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.16383v1",
    "published": "2025-11-20",
    "update_time": "2025-11-20",
    "download_time": "2025-12-16 14:07:36"
  },
  {
    "id": "2511.16224",
    "title": "Beyond Code Similarity: Benchmarking the Plausibility, Efficiency, and Complexity of LLM-Generated Smart Contracts",
    "abstract": "Smart Contracts are critical components of blockchain ecosystems, with Solidity as the dominant programming language. While LLMs excel at general-purpose code generation, the unique constraints of Smart Contracts, such as gas consumption, security, and determinism, raise open questions about the reliability of LLM-generated Solidity code. Existing studies lack a comprehensive evaluation of these critical functional and non-functional properties. We benchmark four state-of-the-art models under zero-shot and retrieval-augmented generation settings across 500 real-world functions. Our multi-faceted assessment employs code similarity metrics, semantic embeddings, automated test execution, gas profiling, and cognitive and cyclomatic complexity analysis. Results show that while LLMs produce code with high semantic similarity to real contracts, their functional correctness is low: only 20% to 26% of zero-shot generations behave identically to ground-truth implementations under testing. The generated code is consistently simpler, with significantly lower complexity and gas consumption, often due to omitted validation logic. Retrieval-Augmented Generation markedly improves performance, boosting functional correctness by up to 45% and yielding more concise and efficient code. Our findings reveal a significant gap between semantic similarity and functional plausibility in LLM-generated Smart Contracts. We conclude that while RAG is a powerful enhancer, achieving robust, production-ready code generation remains a substantial challenge, necessitating careful expert validation.",
    "arxiv_url": "https://arxiv.org/abs/2511.16224",
    "authors": [
      "Francesco Salzano",
      "Simone Scalabrino",
      "Rocco Oliveto",
      "Remo Pareschi"
    ],
    "first_author": "Francesco Salzano",
    "category": [
      "Empirical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Prompting",
    "tags": [
      "Solidity smart contracts",
      "Gas profiling",
      "Functional plausibility testing",
      "Retrieval-augmented generation",
      "AST structural similarity (Tree Edit Distance)",
      "Automated test execution",
      "Cyclomatic and cognitive complexity",
      "Semantic code embeddings"
    ],
    "summary": "本文在500个真实Solidity函数上比较四种LLM的零样本与检索增强生成，系统评估语义/结构相似度、自动化测试通过率、燃气消耗与代码复杂度，发现语义相似高但功能正确率低，且检索增强显著提升可行性与效率。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.16224v2",
    "published": "2025-11-20",
    "update_time": "2025-11-21",
    "download_time": "2025-12-16 14:08:20"
  },
  {
    "id": "2511.16123",
    "title": "Domain-constrained Synthesis of Inconsistent Key Aspects in Textual Vulnerability Descriptions",
    "abstract": "Textual Vulnerability Descriptions (TVDs) are crucial for security analysts to understand and address software vulnerabilities. However, the key aspect inconsistencies in TVDs from different repositories pose challenges for achieving a comprehensive understanding of vulnerabilities. Existing approaches aim to mitigate inconsistencies by aligning TVDs with external knowledge bases, but they often discard valuable information and fail to synthesize comprehensive representations. In this paper, we propose a domain-constrained LLM-based synthesis framework for unifying key aspects of TVDs. Our framework consists of three stages: 1) Extraction, guided by rule-based templates to ensure all critical details are captured; 2) Self-evaluation, using domain-specific anchor words to assess semantic variability across sources; and 3) Fusion, leveraging information entropy to reconcile inconsistencies and prioritize relevant details. This framework improves synthesis performance, increasing the F1 score for key aspect augmentation from 0.82 to 0.87, while enhancing comprehension and efficiency by over 30\\%. We further develop Digest Labels, a practical tool for visualizing TVDs, which human evaluations show significantly boosts usability.",
    "arxiv_url": "https://arxiv.org/abs/2511.16123",
    "authors": [
      "Linyi Han",
      "Shidong Pan",
      "Zhenchang Xing",
      "Sofonias Yitagesu",
      "Xiaowang Zhang",
      "Zhiyong Feng",
      "Jiamou Sun",
      "Qing Huang"
    ],
    "first_author": "Linyi Han",
    "category": [
      "Technical"
    ],
    "field": "Vulnerability Analysis & Documentation",
    "task": "Textual Vulnerability Description Synthesis",
    "tags": [
      "Domain-constrained synthesis",
      "Rule-based extraction templates",
      "Domain-specific anchor words",
      "Information-entropy based fusion",
      "Digest Labels visualization",
      "Cross-repository TVD integration",
      "Key-aspect reconciliation",
      "Human usability evaluation"
    ],
    "summary": "本文提出一种基于领域约束的合成框架，通过规则化提取、基于领域锚词的自评估和信息熵驱动的融合，统一不同漏洞库中不一致的文本漏洞描述关键要素并设计了可视化的 Digest Labels 系统以提升理解与效率。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.16123v1",
    "published": "2025-11-20",
    "update_time": "2025-11-20",
    "download_time": "2025-12-16 14:09:03"
  },
  {
    "id": "2511.16092",
    "title": "The Future of Development Environments with AI Foundation Models: NII Shonan Meeting 222 Report",
    "abstract": "Generative Artificial Intelligence (GenAI) models are achieving remarkable performance in various tasks, including code generation, testing, code review, and program repair. The ability to increase the level of abstraction away from writing code has the potential to change the Human-AI interaction within the integrated development environment (IDE). To explore the impact of GenAI on IDEs, 33 experts from the Software Engineering, Artificial Intelligence, and Human-Computer Interaction domains gathered to discuss challenges and opportunities at Shonan Meeting 222. This is the report",
    "arxiv_url": "https://arxiv.org/abs/2511.16092",
    "authors": [
      "Xing Hu",
      "Raula Gaikovina Kula",
      "Christoph Treude"
    ],
    "first_author": "Xing Hu",
    "category": [
      "Survey"
    ],
    "field": "Coding Assistant",
    "task": "Code Prompting",
    "tags": [
      "IDE–FM integration",
      "Human–AI interaction in IDEs",
      "Prompt language design for code",
      "Foundation model engineering",
      "AI dataset management for code",
      "FM debugging and validation",
      "Cross-platform FM deployment",
      "AI agent orchestration"
    ],
    "summary": "本文报告总结了33位专家在Shonan会议上关于将AI基础模型整合进开发环境的讨论，涵盖了人机交互、提示语言、模型与数据工程、调试与部署等挑战与机遇。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.16092v1",
    "published": "2025-11-20",
    "update_time": "2025-11-20",
    "download_time": "2025-12-16 14:09:36"
  },
  {
    "id": "2511.16708",
    "title": "Multi-Agent Code Verification via Information Theory",
    "abstract": "LLMs generate buggy code: 29.6% of SWE-bench solved patches fail, 62% of BaxBench solutions have vulnerabilities, and existing tools only catch 65% of bugs with 35% false positives. We built CodeX-Verify, a multi-agent system that uses four specialized agents to detect different types of bugs. We prove mathematically that combining agents with different detection patterns finds more bugs than any single agent when the agents look for different problems, using submodularity of mutual information under conditional independence. Measuring agent correlation of rho = 0.05 to 0.25 confirms they detect different bugs. Testing on 99 code samples with verified labels shows our system catches 76.1% of bugs, matching the best existing method (Meta Prompt Testing: 75%) while running faster and without test execution. We tested all 15 agent combinations and found that using multiple agents improves accuracy by 39.7 percentage points (from 32.8% to 72.4%) compared to single agents, with diminishing returns of +14.9pp, +13.5pp, and +11.2pp for agents 2, 3, and 4, validating our theoretical model. The best two-agent combination (Correctness + Performance) reaches 79.3% accuracy. Testing on 300 real patches from Claude Sonnet 4.5 runs in under 200ms per sample, making this practical for production use.",
    "arxiv_url": "https://arxiv.org/abs/2511.16708",
    "authors": [
      "Shreshth Rajan"
    ],
    "first_author": "Shreshth Rajan",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Code Verification & Analysis",
    "task": "Multi-Agent Code Verification",
    "tags": [
      "Multi-agent static analysis",
      "Information-theoretic submodularity",
      "Mutual information aggregation",
      "Agent specialization (Correctness/Security/Performance/Style)",
      "All-combinations ablation study",
      "Zero-execution verification",
      "Heuristic weight selection",
      "Released annotated verification dataset",
      "Diminishing returns of agents",
      "Low inter-agent correlation measurement"
    ],
    "summary": "本文提出CodeX-Verify，一种由正确性、安全、性能与风格四个专门代理组成的静态多代理代码验证系统，并用互信息子模性质给出理论证明、穷尽性消融实验验证性能提升且公开了99个标注样本的数据集。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.16708v3",
    "published": "2025-11-20",
    "update_time": "2025-12-03",
    "download_time": "2025-12-16 14:10:11"
  },
  {
    "id": "2511.16005",
    "title": "InfCode-C++: Intent-Guided Semantic Retrieval and AST-Structured Search for C++ Issue Resolution",
    "abstract": "Large language model (LLM) agents have recently shown strong performance on repository-level issue resolution, but existing systems are almost exclusively designed for Python and rely heavily on lexical retrieval and shallow code navigation. These approaches transfer poorly to C++ projects, where overloaded identifiers, nested namespaces, template instantiations, and deep control-flow structures make context retrieval and fault localization substantially more difficult. As a result, state-of-the-art Python-oriented agents show a drastic performance drop on the C++ subset of MultiSWE-bench. We introduce INFCODE-C++, the first C++-aware autonomous system for end-to-end issue resolution. The system combines two complementary retrieval mechanisms -- semantic code-intent retrieval and deterministic AST-structured querying -- to construct accurate, language-aware context for repair.These components enable precise localization and robust patch synthesis in large, statically typed C++ repositories. Evaluated on the \\texttt{MultiSWE-bench-CPP} benchmark, INFCODE-C++ achieves a resolution rate of 25.58\\%, outperforming the strongest prior agent by 10.85 percentage points and more than doubling the performance of MSWE-agent. Ablation and behavioral studies further demonstrate the critical role of semantic retrieval, structural analysis, and accurate reproduction in C++ issue resolution. INFCODE-C++ highlights the need for language-aware reasoning in multi-language software agents and establishes a foundation for future research on scalable, LLM-driven repair for complex, statically typed ecosystems.",
    "arxiv_url": "https://arxiv.org/abs/2511.16005",
    "authors": [
      "Qingao Dong",
      "Mengfei Wang",
      "Hengzhi Zhang",
      "Zhichao Li",
      "Yuan Yuan",
      "Mu Li",
      "Xiang Gao",
      "Hailong Sun",
      "Chunming Hu",
      "Weifeng Lv"
    ],
    "first_author": "Qingao Dong",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Quality Management",
    "task": "Bug Repair",
    "tags": [
      "Intent-guided semantic retrieval",
      "AST-structured query engine",
      "C++ overload and namespace disambiguation",
      "Deterministic code navigation tools",
      "Multi-file feature-level context aggregation",
      "Repository-level patch synthesis",
      "Language-aware agent design"
    ],
    "summary": "本文提出InfCode-C++，通过意图驱动的语义检索与基于AST的结构化查询，为大型C++代码库构建精确语义上下文，从而显著提升自动化问题定位与修复效果并开源了系统与评测基准。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.16005v1",
    "published": "2025-11-20",
    "update_time": "2025-11-20",
    "download_time": "2025-12-16 14:11:05"
  },
  {
    "id": "2511.16004",
    "title": "InfCode: Adversarial Iterative Refinement of Tests and Patches for Reliable Software Issue Resolution",
    "abstract": "Large language models have advanced software engineering automation, yet resolving real-world software issues remains difficult because it requires repository-level reasoning, accurate diagnostics, and strong verification signals. Existing agent-based and pipeline-based methods often rely on insufficient tests, which can lead to patches that satisfy verification but fail to fix the underlying defect. We present InfCode, an adversarial multi-agent framework for automated repository-level issue resolution. InfCode iteratively refines both tests and patches through adversarial interaction between a Test Patch Generator and a Code Patch Generator, while a Selector agent identifies the most reliable fix. The framework runs inside a containerized environment that supports realistic repository inspection, modification, and validation. Experiments on SWE-bench Lite and SWE-bench Verified using models such as DeepSeek-V3 and Claude 4.5 Sonnet show that InfCode consistently outperforms strong baselines. It achieves 79.4% performance on SWE-bench Verified, establishing a new state-of-the-art. We have released InfCode as an open-source project at https://github.com/Tokfinity/InfCode.",
    "arxiv_url": "https://arxiv.org/abs/2511.16004",
    "authors": [
      "KeFan Li",
      "Mengfei Wang",
      "Hengzhi Zhang",
      "Zhichao Li",
      "Yuan Yuan",
      "Mu Li",
      "Xiang Gao",
      "Hailong Sun",
      "Chunming Hu",
      "Weifeng Lv"
    ],
    "first_author": "KeFan Li",
    "category": [
      "Technical"
    ],
    "field": "Quality Management",
    "task": "Bug Repair",
    "tags": [
      "Adversarial Test-Patch Iteration",
      "Multi-Agent Repair Framework",
      "Test Strengthening",
      "Patch Selection and Ranking",
      "Containerized Repository Execution",
      "Repository-aware Code Manipulation",
      "Robustness-driven Patch Generation"
    ],
    "summary": "本文提出 InfCode：在容器化仓库环境中通过测试生成器与代码生成器的对抗式迭代来强化测试并改进补丁，辅以选择器挑选最可靠修复，从而提升仓库级别问题修复的可靠性并在基准上取得SOTA结果。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.16004v1",
    "published": "2025-11-20",
    "update_time": "2025-11-20",
    "download_time": "2025-12-16 14:11:48"
  },
  {
    "id": "2511.15817",
    "title": "A Causal Perspective on Measuring, Explaining and Mitigating Smells in LLM-Generated Code",
    "abstract": "Recent advances in large language models (LLMs) have accelerated their adoption in software engineering contexts. However, concerns persist about the structural quality of the code they produce. In particular, LLMs often replicate poor coding practices, introducing code smells (i.e., patterns that hinder readability, maintainability, or design integrity). Although prior research has examined the detection or repair of smells, we still lack a clear understanding of how and when these issues emerge in generated code.   This paper addresses this gap by systematically measuring, explaining and mitigating smell propensity in LLM-generated code. We build on the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating particular smell types, and establish its robustness as a signal of structural quality. Using PSC as an instrument for causal analysis, we identify how generation strategy, model size, model architecture and prompt formulation shape the structural properties of generated code. Our findings show that prompt design and architectural choices play a decisive role in smell propensity and motivate practical mitigation strategies that reduce its occurrence. A user study further demonstrates that PSC helps developers interpret model behavior and assess code quality, providing evidence that smell propensity signals can support human judgement. Taken together, our work lays the groundwork for integrating quality-aware assessments into the evaluation and deployment of LLMs for code.",
    "arxiv_url": "https://arxiv.org/abs/2511.15817",
    "authors": [
      "Alejandro Velasco",
      "Daniel Rodriguez-Cardenas",
      "Dipin Khati",
      "David N. Palacio",
      "Luftar Rahman Alif",
      "Denys Poshyvanyk"
    ],
    "first_author": "Alejandro Velasco",
    "category": [
      "Technical",
      "Benchmark",
      "Empirical"
    ],
    "field": "Quality Management",
    "task": "Code Smell Measurement & Mitigation",
    "tags": [
      "Propensity Smelly Score (PSC)",
      "Causal inference on generation factors",
      "Prompt-based mitigation",
      "Decoding strategy effects",
      "Model architecture impact",
      "Semantic-preserving code transformations (SECT)",
      "Robustness and information-gain analysis",
      "Developer interpretability user study"
    ],
    "summary": "本文从因果视角验证并扩展了PSC度量，用以量化并解释LLM生成代码中的代码异味，评估生成策略、模型规模与架构及提示对异味倾向的影响，提出基于提示的缓解策略并通过用户研究证明其实用性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.15817v3",
    "published": "2025-11-19",
    "update_time": "2025-12-09",
    "download_time": "2025-12-16 14:12:57"
  },
  {
    "id": "2511.15293",
    "title": "A Viable Paradigm of Software Automation: Iterative End-to-End Automated Software Development",
    "abstract": "Software development automation is a long-term goal in software engineering. With the development of artificial intelligence (AI), more and more researchers are exploring approaches to software automation. They view AI systems as tools or assistants in software development, still requiring significant human involvement. Another initiative is ``vibe coding'', where AI systems write and repeatedly revise most (or even all) of the code. We foresee these two development paths will converge towards the same destination: AI systems participate in throughout the software development lifecycle, expanding boundaries of full-stack software development. In this paper, we present a vision of an iterative end-to-end automated software development paradigm AutoSW. It operates in an analyze-plan-implement-deliver loop, where AI systems as human partners become first-class actors, translating human intentions expressed in natural language into executable software. We explore a lightweight prototype across the paradigm and initially execute various representative cases. The results indicate that AutoSW can successfully deliver executable software, providing a feasible direction for truly end-to-end automated software development.",
    "arxiv_url": "https://arxiv.org/abs/2511.15293",
    "authors": [
      "Jia Li",
      "Zhi Jin",
      "Huangzhao Zhang",
      "Kechi Zhang",
      "Jiaru Qian",
      "Tiankuo Zhao"
    ],
    "first_author": "Jia Li",
    "category": [
      "Technical"
    ],
    "field": "Software Automation",
    "task": "End-to-End Automated Software Development (analyze-plan-implement-deliver)",
    "tags": [
      "Orchestral agent system",
      "Iterative analyze-plan-implement-deliver loop",
      "Conversational requirement elicitation",
      "Plan generation with traceability",
      "Automated system design",
      "Task-level implementation orchestration",
      "Automated test generation and vulnerability checking",
      "Deployment and delivery automation",
      "Human-in-the-loop physical-constraint specification",
      "Software evolution and change propagation"
    ],
    "summary": "本文提出了AutoSW——一种以编排式智能体为核心的迭代端到端软件自动化范式，通过分析-规划-实现-交付循环，将自然语言意图自动转化为可部署软件，并提供了轻量原型与示例验证其可行性。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.15293v2",
    "published": "2025-11-19",
    "update_time": "2025-11-23",
    "download_time": "2025-12-16 14:13:35"
  },
  {
    "id": "2511.15757",
    "title": "Rethinking Kernel Program Repair: Benchmarking and Enhancing LLMs with RGym",
    "abstract": "Large Language Models (LLMs) have revolutionized automated program repair (APR) but current benchmarks like SWE-Bench predominantly focus on userspace applications and overlook the complexities of kernel-space debugging and repair. The Linux kernel poses unique challenges due to its monolithic structure, concurrency, and low-level hardware interactions. Prior efforts such as KGym and CrashFixer have highlighted the difficulty of APR in this domain, reporting low success rates or relying on costly and complex pipelines and pricey cloud infrastructure. In this work, we introduce RGym, a lightweight, platform-agnostic APR evaluation framework for the Linux kernel designed to operate on local commodity hardware. Built on RGym, we propose a simple yet effective APR pipeline leveraging specialized localization techniques (e.g., call stacks and blamed commits) to overcome the unrealistic usage of oracles in KGym. We test on a filtered and verified dataset of 143 bugs. Our method achieves up to a 43.36% pass rate with GPT-5 Thinking while maintaining a cost of under $0.20 per bug. We further conduct an ablation study to analyze contributions from our proposed localization strategy, prompt structure, and model choice, and demonstrate that feedback-based retries can significantly enhance success rates.",
    "arxiv_url": "https://arxiv.org/abs/2511.15757",
    "authors": [
      "Kareem Shehada",
      "Yifan Wu",
      "Wyatt D. Feng",
      "Adithya Iyer",
      "Gryphon Kumfert",
      "Yangruibo Ding",
      "Zhiyun Qian"
    ],
    "first_author": "Kareem Shehada",
    "category": [
      "Technical",
      "Benchmark",
      "Empirical"
    ],
    "field": "Quality Management",
    "task": "Bug Repair",
    "tags": [
      "Kernel-space automated repair",
      "KASAN crash bug reproduction",
      "Bug-inducing-commit (BIC) localization",
      "Call-stack-based localization",
      "Function-wise patching",
      "Feedback-driven retry loop",
      "Local Docker+QEMU test harness",
      "Cost-efficient LLM APR evaluation",
      "Ablation of localization and prompt components"
    ],
    "summary": "本文提出了RGym——一个可在本地运行的轻量级Linux内核自动修复评测框架，整理并验证了143个KASAN内核漏洞，并通过基于错误诱发提交与调用栈的现实定位策略、函数级补丁生成与反馈重试流程，使用大模型在低成本下显著提升内核程序修复成功率。",
    "quality": "High",
    "conference": "NeurIPS 2025 Workshop: Evaluating the Evolving LLM Lifecycle: Benchmarks, Emergent Abilities, and Scaling 2025",
    "pdf_url": "https://arxiv.org/pdf/2511.15757v1",
    "published": "2025-11-19",
    "update_time": "2025-11-19",
    "download_time": "2025-12-16 14:14:25"
  },
  {
    "id": "2511.15168",
    "title": "Finetuning LLMs for Automatic Form Interaction on Web-Browser in Selenium Testing Framework",
    "abstract": "Automated web application testing is a critical component of modern software development, with frameworks like Selenium widely adopted for validating functionality through browser automation. Among the essential aspects of such testing is the ability to interact with and validate web forms, a task that requires syntactically correct, executable scripts with high coverage of input fields. Despite its importance, this task remains underexplored in the context of large language models (LLMs), and no public benchmark or dataset exists to evaluate LLMs on form interaction generation systematically. This paper introduces a novel method for training LLMs to generate high-quality test cases in Selenium, specifically targeting form interaction testing. We curate both synthetic and human-annotated datasets for training and evaluation, covering diverse real-world forms and testing scenarios. We define clear metrics for syntax correctness, script executability, and input field coverage. Our empirical study demonstrates that our approach significantly outperforms strong baselines, including GPT-4o and other popular LLMs, across all evaluation metrics. Our work lays the groundwork for future research on LLM-based web testing and provides resources to support ongoing progress in this area.",
    "arxiv_url": "https://arxiv.org/abs/2511.15168",
    "authors": [
      "Nguyen-Khang Le",
      "Hiep Nguyen",
      "Ngoc-Minh Nguyen",
      "Son T. Luu",
      "Trung Vo",
      "Quan Minh Bui",
      "Shoshin Nomura",
      "Le-Minh Nguyen"
    ],
    "first_author": "Nguyen-Khang Le",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Software Testing",
    "task": "Test Generation",
    "tags": [
      "Selenium script generation",
      "Form interaction automation",
      "Synthetic HTML field pool",
      "Human-annotated form scenarios",
      "Prompted scenario-to-code pipeline",
      "Executable-script filtering",
      "Input-field coverage metric"
    ],
    "summary": "本文提出一种基于合成与人工标注数据、通过提示生成并微调模型以自动生成可执行的Selenium表单交互测试脚本的方法，并构建了首个针对表单交互的测试数据集和评估指标，在语法正确性、可执行性与字段覆盖率上显著优于强基线。",
    "quality": "High",
    "conference": "Proceedings of KSE 2025",
    "pdf_url": "https://arxiv.org/pdf/2511.15168v2",
    "published": "2025-11-19",
    "update_time": "2025-11-20",
    "download_time": "2025-12-16 14:15:12"
  },
  {
    "id": "2511.15755",
    "title": "Multi-Agent LLM Orchestration Achieves Deterministic, High-Quality Decision Support for Incident Response",
    "abstract": "Large language models (LLMs) promise to accelerate incident response in production systems, yet single-agent approaches generate vague, unusable recommendations. We present MyAntFarm.ai, a reproducible containerized framework demonstrating that multi-agent orchestration fundamentally transforms LLM-based incident response quality. Through 348 controlled trials comparing single-agent copilot versus multi-agent systems on identical incident scenarios, we find that multi-agent orchestration achieves 100% actionable recommendation rate versus 1.7% for single-agent approaches, an 80 times improvement in action specificity and 140 times improvement in solution correctness. Critically, multi-agent systems exhibit zero quality variance across all trials, enabling production SLA commitments impossible with inconsistent single-agent outputs. Both architectures achieve similar comprehension latency (approx.40s), establishing that the architectural value lies in deterministic quality, not speed. We introduce Decision Quality (DQ), a novel metric capturing validity, specificity, and correctness properties essential for operational deployment that existing LLM metrics do not address. These findings reframe multi-agent orchestration from a performance optimization to a production-readiness requirement for LLM-based incident response. All code, Docker configurations, and trial data are publicly available for reproduction.",
    "arxiv_url": "https://arxiv.org/abs/2511.15755",
    "authors": [
      "Philip Drammeh"
    ],
    "first_author": "Philip Drammeh",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "AIOps",
    "task": "Incident Response / Decision Support",
    "tags": [
      "Multi-agent orchestration",
      "Prompt decomposition",
      "Specialized agent pipeline (diagnosis/planning/risk)",
      "Decision Quality (DQ) metric",
      "Time-to-Usable-Understanding (T2U)",
      "Containerized reproducible evaluation framework",
      "Deterministic output guarantees",
      "Regex-based specificity scoring",
      "Token-overlap correctness scoring",
      "Actionable remediation command generation"
    ],
    "summary": "本文提出并评估了 MyAntFarm.ai：通过将 incident response 分解为诊断、修复计划和风险评估的多代理LLM编排，结合新引入的 Decision Quality 和 T2U 指标，在 348 次可复现仿真实验中实现了对事故响应可执行性、特异性和正确性的显著且确定性的提升。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.15755v1",
    "published": "2025-11-19",
    "update_time": "2025-11-19",
    "download_time": "2025-12-16 14:15:53"
  },
  {
    "id": "2511.19427",
    "title": "Prompt Less, Smile More: MTP with Semantic Engineering in Lieu of Prompt Engineering",
    "abstract": "AI-Integrated programming is emerging as a foundational paradigm for building intelligent systems with large language models (LLMs). Recent approaches such as Meaning Typed Programming (MTP) automate prompt generation by leveraging the semantics already present in code. However, many real-world applications depend on contextual cues, developer intent, and domain-specific reasoning that extend beyond what static code semantics alone can express. To address this limitation, we introduce Semantic Engineering, a lightweight method for enriching program semantics so that LLM-based systems can more accurately reflect developer intent without requiring full manual prompt design. We present Semantic Context Annotations (SemTexts), a language-level mechanism that allows developers to embed natural-language context directly into program constructs. Integrated into the Jac programming language, Semantic Engineering extends MTP to incorporate these enriched semantics during prompt generation. We further introduce a benchmark suite designed to reflect realistic AI-Integrated application scenarios. Our evaluation shows that Semantic Engineering substantially improves prompt fidelity, achieving performance comparable to Prompt Engineering while requiring significantly less developer effort.",
    "arxiv_url": "https://arxiv.org/abs/2511.19427",
    "authors": [
      "Jayanaka L. Dantanarayana",
      "Savini Kashmira",
      "Thakee Nathees",
      "Zichen Zhang",
      "Krisztian Flautner",
      "Lingjia Tang",
      "Jason Mars"
    ],
    "first_author": "Jayanaka L. Dantanarayana",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Prompting",
    "tags": [
      "Semantic Context Annotations",
      "Semantic Engineering",
      "MTP integration",
      "MT-IR enrichment",
      "Compiler pass for semantic binding",
      "Prompt generation from code semantics",
      "Jac language integration",
      "AI-integrated application benchmark",
      "Developer effort vs. prompt fidelity evaluation"
    ],
    "summary": "本文提出Semantic Engineering与SemText语义上下文注释，通过在Jac语言中将自然语言语义绑定到程序构件并扩展MTP的MT-IR，从而自动生成更贴合开发者意图的提示，显著提升AI集成应用的性能并提供现实场景基准评测，同时大幅降低开发者工作量。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.19427v1",
    "published": "2025-11-24",
    "update_time": "2025-11-24",
    "download_time": "2025-12-16 14:16:31"
  },
  {
    "id": "2511.19132",
    "title": "LLMs-Powered Real-Time Fault Injection: An Approach Toward Intelligent Fault Test Cases Generation",
    "abstract": "A well-known testing method for the safety evaluation and real-time validation of automotive software systems (ASSs) is Fault Injection (FI). In accordance with the ISO 26262 standard, the faults are introduced artificially for the purpose of analyzing the safety properties and verifying the safety mechanisms during the development phase. However, the current FI method and tools have a significant limitation in that they require manual identification of FI attributes, including fault type, location and time. The more complex the system, the more expensive, time-consuming and labour-intensive the process. To address the aforementioned challenge, a novel Large Language Models (LLMs)-assisted fault test cases (TCs) generation approach for utilization during real-time FI tests is proposed in this paper. To this end, considering the representativeness and coverage criteria, the applicability of various LLMs to create fault TCs from the functional safety requirements (FSRs) has been investigated. Through the validation results of LLMs, the superiority of the proposed approach utilizing gpt-4o in comparison to other state-of-the-art models has been demonstrated. Specifically, the proposed approach exhibits high performance in terms of FSRs classification and fault TCs generation with F1-score of 88% and 97.5%, respectively. To illustrate the proposed approach, the generated fault TCs were executed in real time on a hardware-in-the-loop system, where a high-fidelity automotive system model served as a case study. This novel approach offers a means of optimizing the real-time testing process, thereby reducing costs while simultaneously enhancing the safety properties of complex safety-critical ASSs.",
    "arxiv_url": "https://arxiv.org/abs/2511.19132",
    "authors": [
      "Mohammad Abboush",
      "Ahmad Hatahet",
      "Andreas Rausch"
    ],
    "first_author": "Mohammad Abboush",
    "category": [
      "Technical"
    ],
    "field": "Software Testing",
    "task": "Test Generation",
    "tags": [
      "Requirements-to-fault-testcase generation",
      "LLM-assisted requirements classification",
      "Real-time hardware-in-the-loop validation",
      "Sensor and actuator fault modeling",
      "Concurrent fault injection",
      "ISO 26262-aligned testing",
      "CAN-layer fault types",
      "JSON fault-vector encoding",
      "Coverage-driven fault selection"
    ],
    "summary": "本文提出一种基于大型语言模型的实时故障注入测试用例生成方法，从功能安全需求自动分类并生成传感器/执行器故障用例，并在高保真硬件在环汽车模型上验证了其有效性与高F1得分。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.19132v1",
    "published": "2025-11-24",
    "update_time": "2025-11-24",
    "download_time": "2025-12-16 14:17:10"
  },
  {
    "id": "2511.19422",
    "title": "SLMFix: Leveraging Small Language Models for Error Fixing with Reinforcement Learning",
    "abstract": "Recent advancements in large language models (LLMs) have shown very impressive capabilities in code generation across many programming languages. However, even state-of-the-art LLMs generate programs that contains syntactic errors and fail to complete the given tasks, especially for low-resource programming languages (LRPLs). In addition, high training cost makes finetuning LLMs unaffordable with constrained computational resources, further undermining the effectiveness of LLMs for code generation. In this work, we propose SLMFix, a novel code generation pipeline that leverages a small language model (SLM) finetuned using reinforcement learning (RL) techniques to fix syntactic errors in LLM-generated programs to improve the quality of LLM-generated programs for domain-specific languages (DSLs). In specific, we applied RL on the SLM for the program repair task using a reward calculated using both a static validator and a static semantic similarity metric. Our experimental results demonstrate the effectiveness and generalizability of our approach across multiple DSLs, achieving more than 95% pass rate on the static validator. Notably, SLMFix brings substantial improvement to the base model and outperforms supervised finetuning approach even for 7B models on a LRPL, showing the potential of our approach as an alternative to traditional finetuning approaches.",
    "arxiv_url": "https://arxiv.org/abs/2511.19422",
    "authors": [
      "David Jiahao Fu",
      "Aryan Gupta",
      "Aaron Councilman",
      "David Grove",
      "Yu-Xiong Wang",
      "Vikram Adve"
    ],
    "first_author": "David Jiahao Fu",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Editing",
    "tags": [
      "SLM post-editing",
      "Reinforcement learning for program repair",
      "AST-based semantic reward",
      "Static validator reward",
      "DSL-focused error fixing",
      "Ansible evaluation corpus",
      "Low-resource language adaptation",
      "Post-hoc fixer pipeline"
    ],
    "summary": "本文提出SLMFix：在不微调大模型的前提下，用强化学习微调的小型语言模型对LLM生成的DSL代码进行静态错误修复，并以静态验证器与基于AST的相似性作为奖励显著提升生成代码质量（在Ansible、Bash、SQL上验证）。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.19422v1",
    "published": "2025-11-24",
    "update_time": "2025-11-24",
    "download_time": "2025-12-16 14:17:53"
  },
  {
    "id": "2511.19130",
    "title": "Can LLMs Recover Program Semantics? A Systematic Evaluation with Symbolic Execution",
    "abstract": "Obfuscation poses a persistent challenge for software engineering tasks such as program comprehension, maintenance, testing, and vulnerability detection. While compiler optimizations and third-party code often introduce transformations that obscure program intent, existing analysis tools and large language models (LLMs) struggle to recover the original semantics. In this work, we investigate whether LLMs, when fine-tuned with symbolic execution artifacts, can effectively deobfuscate programs and restore analyzability. We construct a benchmark by applying four widely studied transformations-control-flow flattening, opaque predicates, arithmetic encoding, and branch encoding-across diverse C programs from TUM Obfuscation Benchmarks, the LLVM test suite, and algorithmic repositories. We then compare three state-of-the-art LLMs under two training configurations: baseline fine-tuning on obfuscated/original code pairs, and enhanced fine-tuning with additional KLEE artifacts such as SMT constraints, path statistics, and test cases. Our evaluation examines syntactic correctness (compilation success), semantic fidelity (behavioral equivalence under symbolic execution), and code quality (readability and structure). Results show that GPT-4.1-mini achieves the strongest deobfuscation overall, and that incorporating KLEE artifacts consistently improves semantic preservation and compilation success across models. These findings highlight deobfuscation as a broader software engineering concern, demonstrating that combining LLMs with symbolic execution can strengthen automated testing, static analysis, and program comprehension in the presence of obfuscation.",
    "arxiv_url": "https://arxiv.org/abs/2511.19130",
    "authors": [
      "Rong Feng",
      "Suman Saha"
    ],
    "first_author": "Rong Feng",
    "category": [
      "Technical",
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Understanding",
    "tags": [
      "Deobfuscation",
      "Symbolic execution artifacts",
      "SMT constraints",
      "Path statistics",
      "Test-case guided fine-tuning",
      "Obfuscation transformations",
      "Semantic equivalence evaluation",
      "Compilation-aware deobfuscation"
    ],
    "summary": "本文构建了一个包含四类混淆变换的去混淆基准，并系统评估了将符号执行产物（如SMT约束、路径统计、测试用例）融入LLM微调以提升生成代码的可编译性与语义保真性的效果，结果表明混合方法显著优于仅用代码的微调。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.19130v1",
    "published": "2025-11-24",
    "update_time": "2025-11-24",
    "download_time": "2025-12-16 14:18:32"
  },
  {
    "id": "2511.20403",
    "title": "LLMs for Automated Unit Test Generation and Assessment in Java: The AgoneTest Framework",
    "abstract": "Unit testing is an essential but resource-intensive step in software development, ensuring individual code units function correctly. This paper introduces AgoneTest, an automated evaluation framework for Large Language Model-generated (LLM) unit tests in Java. AgoneTest does not aim to propose a novel test generation algorithm; rather, it supports researchers and developers in comparing different LLMs and prompting strategies through a standardized end-to-end evaluation pipeline under realistic conditions. We introduce the Classes2Test dataset, which maps Java classes under test to their corresponding test classes, and a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment. Experimental results show that, for the subset of tests that compile, LLM-generated tests can match or exceed human-written tests in terms of coverage and defect detection. Our findings also demonstrate that enhanced prompting strategies contribute to test quality. AgoneTest clarifies the potential of LLMs in software testing and offers insights for future improvements in model design, prompt engineering, and testing practices.",
    "arxiv_url": "https://arxiv.org/abs/2511.20403",
    "authors": [
      "Andrea Lops",
      "Fedelucio Narducci",
      "Azzurra Ragone",
      "Michelantonio Trizio",
      "Claudio Bartolini"
    ],
    "first_author": "Andrea Lops",
    "category": [
      "Technical",
      "Benchmark",
      "Empirical"
    ],
    "field": "Software Testing",
    "task": "Test Generation",
    "tags": [
      "End-to-end evaluation pipeline",
      "Class-level unit test generation",
      "Annotated class-to-test mappings",
      "Automated project setup and build integration",
      "Mutation score integration",
      "Test smells detection",
      "Prompt engineering comparison",
      "Comparative LLM test compilability and coverage"
    ],
    "summary": "本文提出AGONETEST——一个用于评估LLM生成的Java单元测试的端到端自动化框架，并发布了面向类级测试的注释数据集，结合覆盖率、变异测试与测试气味等指标比较不同模型和提示策略的性能。",
    "quality": "High",
    "conference": "IEEE/ACM International Conference on Automated Software Engineering (ASE 2025)",
    "pdf_url": "https://arxiv.org/pdf/2511.20403v2",
    "published": "2025-11-25",
    "update_time": "2025-11-26",
    "download_time": "2025-12-16 14:19:46"
  },
  {
    "id": "2511.21382",
    "title": "Large Language Models for Unit Test Generation: Achievements, Challenges, and the Road Ahead",
    "abstract": "Unit testing is an essential yet laborious technique for verifying software and mitigating regression risks. Although classic automated methods effectively explore program structures, they often lack the semantic information required to produce realistic inputs and assertions. Large Language Models (LLMs) address this limitation by utilizing by leveraging their data-driven knowledge of code semantics and programming patterns. To analyze the state of the art in this domain, we conducted a systematic literature review of 115 publications published between May 2021 and August 2025. We propose a unified taxonomy based on the unit test generation lifecycle that treats LLMs as stochastic generators requiring systematic engineering constraints. This framework analyzes the literature regarding core generative strategies and a set of enhancement techniques ranging from pre-generation context enrichment to post-generation quality assurance. Our analysis reveals that prompt engineering has emerged as the dominant utilization strategy and accounts for 89% of the studies due to its flexibility. We find that iterative validation and repair loops have become the standard mechanism to ensure robust usability and lead to significant improvements in compilation and execution pass rates. However, critical challenges remain regarding the weak fault detection capabilities of generated tests and the lack of standardized evaluation benchmarks. We conclude with a roadmap for future research that emphasizes the progression towards autonomous testing agents and hybrid systems combining LLMs with traditional software engineering tools. This survey provides researchers and practitioners with a comprehensive perspective on converting the potential of LLMs into industrial-grade testing solutions.",
    "arxiv_url": "https://arxiv.org/abs/2511.21382",
    "authors": [
      "Bei Chu",
      "Yang Feng",
      "Kui Liu",
      "Zifan Nan",
      "Zhaoqiang Guo",
      "Baowen Xu"
    ],
    "first_author": "Bei Chu",
    "category": [
      "Survey"
    ],
    "field": "Software Testing",
    "task": "Test Generation",
    "tags": [
      "Prompt engineering for test generation",
      "Context enrichment via program analysis",
      "Iterative validation and repair loops",
      "Mocking and external dependency synthesis",
      "Usability vs. fault-detection tradeoff",
      "Autonomous testing agents",
      "Hybrid LLM–classical tooling integration",
      "Lack of standardized evaluation benchmarks"
    ],
    "summary": "本文对2021–2025年间115篇关于使用大型语言模型生成单元测试的工作进行了系统综述，提出基于测试生成生命周期的统一分类，概述了以提示工程为主的生成策略、上下文增强与后处理修复技术，指出了生成测试可执行性虽有提升但故障检测能力薄弱与评估基准缺失等关键挑战，并给出未来研究路线图。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.21382v1",
    "published": "2025-11-26",
    "update_time": "2025-11-26",
    "download_time": "2025-12-16 14:21:09"
  },
  {
    "id": "2511.21380",
    "title": "Multi-Agent Systems for Dataset Adaptation in Software Engineering: Capabilities, Limitations, and Future Directions",
    "abstract": "Automating the adaptation of software engineering (SE) research artifacts across datasets is essential for scalability and reproducibility, yet it remains largely unstudied. Recent advances in large language model (LLM)-based multi-agent systems, such as GitHub Copilot's agent mode, promise to automate complex development workflows through coordinated reasoning, code generation, and tool interaction. This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks. We evaluate Copilot, backed by GPT-4.1 and Claude Sonnet 4, on adapting SE research artifacts from benchmark repositories including ROCODE and LogHub2.0. Through a five-stage evaluation pipeline (file comprehension, code editing, command generation, validation, and final execution), we measure success rates, analyze failure patterns, and assess prompt-based interventions designed to enhance agent performance. Results show that current systems can identify key files and generate partial adaptations but rarely produce functionally correct implementations. Prompt-level interventions, especially providing execution error messages and reference code, substantially improve structural similarity to ground truth (from 7.25% to 67.14%), highlighting the importance of contextual and feedback-driven guidance. Our findings reveal both the promise and limitations of today's multi-agent LLM systems for dataset adaptation, and suggest concrete directions for building more reliable, self-correcting agents in future SE research.",
    "arxiv_url": "https://arxiv.org/abs/2511.21380",
    "authors": [
      "Jingyi Chen",
      "Xiaoyan Guo",
      "Songqiang Chen",
      "Shing-Chi Cheung",
      "Jiasi Shen"
    ],
    "first_author": "Jingyi Chen",
    "category": [
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Editing",
    "tags": [
      "Multi-agent orchestration",
      "Dataset adaptation automation",
      "Repository-level file comprehension",
      "Automated command generation and execution",
      "Prompt-based intervention and feedback",
      "Failure-mode analysis",
      "Structural-similarity evaluation"
    ],
    "summary": "本文首度通过五阶段评估流水线，实证研究基于多智能体的大语言模型系统在软件工程研究工件的数据集适配任务中的表现、失败模式及提示干预效果，发现系统能生成部分结构正确的改动但很少能直接产出功能正确的实现，且带有执行错误信息与参考代码的提示能显著提升结构相似度。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.21380v1",
    "published": "2025-11-26",
    "update_time": "2025-11-26",
    "download_time": "2025-12-16 14:21:41"
  },
  {
    "id": "2511.21022",
    "title": "Lightweight Model Editing for LLMs to Correct Deprecated API Recommendations",
    "abstract": "Pre-trained or fine-tuned on large code corpora, Large Language Models (LLMs) have demonstrated strong performance in code completion tasks. However, their embedded knowledge is constrained by the timeliness of training data, which often includes code using deprecated APIs. Consequently, LLMs frequently generate deprecated APIs that will no longer be supported in future versions of third-party libraries. While retraining LLMs on updated codebases could refresh their API knowledge, this approach is computationally expensive. Recently, lightweight model editing methods have emerged to efficiently correct specific knowledge in LLMs. However, it remains unclear whether these methods can effectively update deprecated API knowledge and enable edited models to generate up-to-date APIs. To address this gap, we conduct the first systematic study applying 10 state-of-the-art model editing techniques to update deprecated API knowledge in three LLMs: Qwen2.5-Coder, StarCoder2, and DeepSeek-Coder. We introduce EDAPIBench, a dedicated benchmark featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances. Our results show that the parameter-efficient fine-tuning method AdaLoRA achieves the best performance in enabling edited models to generate correct, up-to-date APIs, but falls short in Specificity (i.e., the editing influences untargeted knowledge). To resolve this, we propose AdaLoRA-L, which defines \"Common API Layers\" (layers within the LLMs with high importance across all APIs, storing general knowledge and excluded from editing) and restricts edits exclusively to \"Specific API Layers\" (layers with high importance only for the target API, storing the API-specific knowledge). Experimental results demonstrate that AdaLoRA-L significantly improves Specificity while maintaining comparable performance across other evaluation metrics.",
    "arxiv_url": "https://arxiv.org/abs/2511.21022",
    "authors": [
      "Guancheng Lin",
      "Xiao Yu",
      "Jacky Keung",
      "Xing Hu",
      "Xin Xia",
      "Alex X. Liu"
    ],
    "first_author": "Guancheng Lin",
    "category": [
      "Technical",
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Alignment",
    "tags": [
      "Deprecated API knowledge editing",
      "Automated benchmark construction from API mappings",
      "Parameter-efficient fine-tuning (low-rank adapters)",
      "Gradient-based layer importance scoring",
      "Selective layer editing (Common vs Specific layers)",
      "Specificity-aware editing metrics",
      "Evaluation on real-world GitHub function contexts"
    ],
    "summary": "本文构建了首个用于评估过时API知识编辑的基准并提出了一种基于梯度层重要性限制编辑范围的轻量模型编辑方法，以在保持效果与泛化能力的同时显著提升编辑的特异性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.21022v1",
    "published": "2025-11-26",
    "update_time": "2025-11-26",
    "download_time": "2025-12-16 14:22:33"
  },
  {
    "id": "2511.20933",
    "title": "Hierarchical Evaluation of Software Design Capabilities of Large Language Models of Code",
    "abstract": "Large language models (LLMs) are being increasingly adopted in the software engineering domain, yet the robustness of their grasp on core software design concepts remains unclear. We conduct an empirical study to systematically evaluate their understanding of cohesion (intra-module) and coupling (inter-module). We programmatically generate poorly designed code fragments and test the DeepSeek-R1 model family ($14$B, $32$B, $70$B) under varying levels of guidance, from simple \\textit{Verification} to \\textit{Guided} and \\textit{Open-ended Generation}, while varying contextual noise by injecting distractor elements. While models exhibit a solid baseline understanding of both concepts in ideal conditions, their practical knowledge is fragile and highly asymmetrical. Reasoning about coupling proves brittle; performance collapses in noisy, open-ended scenarios, with F1 scores dropping by over $50\\%$. In contrast, the models' analysis of cohesion is remarkably robust to internal noise in guided tasks, showing little performance degradation. However, this resilience also fails when all guidance is removed. Reasoning-trace analysis confirms these failure modes, revealing \\textit{cognitive shortcutting} for coupling versus a more exhaustive (yet still failing) analysis for cohesion. To summarize, while LLMs can provide reliable assistance for recognizing design flaws, their ability to reason autonomously in noisy, realistic contexts is limited, highlighting the critical need for more scalable and robust program understanding capabilities.",
    "arxiv_url": "https://arxiv.org/abs/2511.20933",
    "authors": [
      "Mootez Saad",
      "Boqi Chen",
      "José Antonio Hernández López",
      "Dániel Varró",
      "Tushar Sharma"
    ],
    "first_author": "Mootez Saad",
    "category": [
      "Empirical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Cohesion evaluation",
      "Coupling evaluation",
      "Synthetic code generation for design flaws",
      "Controlled distractor/noise injection",
      "Hierarchical prompting (verification/guided/open-ended)",
      "Chain-of-Thought trace analysis",
      "Robustness to contextual noise",
      "Design-quality benchmarking"
    ],
    "summary": "本文提出了一个可控的基准与评估框架，通过生成带内聚性与耦合性缺陷的代码并注入干扰，系统实证评估大规模代码模型在不同提示层级下对软件设计原则的识别与推理能力，揭示了对耦合推理的高度脆弱性与对内聚分析的相对鲁棒性及其失败机制。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.20933v1",
    "published": "2025-11-25",
    "update_time": "2025-11-25",
    "download_time": "2025-12-16 14:23:03"
  },
  {
    "id": "2511.21197",
    "title": "Bug Detective and Quality Coach: Developers' Mental Models of AI-Assisted IDE Tools",
    "abstract": "AI-assisted tools support developers in performing cognitively demanding tasks such as bug detection and code readability assessment. Despite the advancements in the technical characteristics of these tools, little is known about how developers mentally model them and how mismatches affect trust, control, and adoption. We conducted six co-design workshops with 58 developers to elicit their mental models about AI-assisted bug detection and readability features. It emerged that developers conceive bug detection tools as \\textit{bug detectives}, which warn users only in case of critical issues, guaranteeing transparency, actionable feedback, and confidence cues. Readability assessment tools, on the other hand, are envisioned as \\textit{quality coaches}, which provide contextual, personalized, and progressive guidance. Trust, in both tasks, depends on the clarity of explanations, timing, and user control. A set of design principles for Human-Centered AI in IDEs has been distilled, aiming to balance disruption with support, conciseness with depth, and automation with human agency.",
    "arxiv_url": "https://arxiv.org/abs/2511.21197",
    "authors": [
      "Paolo Buono",
      "Mary Cerullo",
      "Stefano Cirillo",
      "Giuseppe Desolda",
      "Francesco Greco",
      "Emanuela Guglielmi",
      "Grazia Margarella",
      "Giuseppe Polese",
      "Simone Scalabrino",
      "Cesare Tucci"
    ],
    "first_author": "Paolo Buono",
    "category": [
      "Empirical"
    ],
    "field": "Requirements & Design",
    "task": "Elicitation",
    "tags": [
      "Developer Mental Models",
      "Co-design Workshops",
      "Bug Detective (metaphor)",
      "Quality Coach (metaphor)",
      "Design Principles for Human-Centered IDEs",
      "Explanation Transparency",
      "User Control and Timing",
      "Actionable Contextual Feedback",
      "Personalized Readability Guidance"
    ],
    "summary": "本文通过对58名开发者开展六次共创研讨会，探讨了AI辅助IDE中错误检测与代码可读性评估的心智模型，提出“Bug Detective”和“Quality Coach”两种核心模型并归纳了以透明性、可操作性、时机与用户控制为核心的设计原则。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.21197v1",
    "published": "2025-11-26",
    "update_time": "2025-11-26",
    "download_time": "2025-12-16 14:23:51"
  },
  {
    "id": "2511.19875",
    "title": "CodeFuse-CommitEval: Towards Benchmarking LLM's Power on Commit Message and Code Change Inconsistency Detection",
    "abstract": "Version control relies on commit messages to convey the rationale for code changes, but these messages are often low quality and, more critically, inconsistent with their diffs-known as message-code inconsistency (MCI). MCIs mislead reviewers, hinder maintenance, contaminate research datasets, and may obscure security patches. Yet, no dedicated benchmark exists to evaluate models for MCI detection. We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models (LLMs). Built on the ApacheCM dataset for diversity and quality, we generate seven types of inconsistent messages through rule-guided mutations of originally consistent commits and apply two-fold validation to verify both positive and negative samples. Using this labeled dataset of message-diff pairs, we evaluate six state-of-the-art open-source LLMs under a vanilla setting and with three augmentation strategies: few-shot prompting, chain-of-thought, and extended context. Results show models detect inconsistent commits more reliably than consistent ones (average Recall 85.95%, Precision 80.28%, Specificity 63.8%); gpt-oss-20B performs best overall but uses over twice the tokens of others. Augmentation effects vary: adjacent context helps larger models but adds noise for smaller ones; few-shot improves accuracy and reduces token use, yet increases universally incorrect predictions; chain-of-thought boosts precision and specificity at the cost of recall and higher token consumption. Type-wise analysis reveals higher detectability for component, file-path, and operation inconsistencies, but lower accuracy and higher token cost for intent-level \"purpose\" inconsistencies. CODEFUSE-COMMITEVAL provides a rigorous foundation for measuring, comparing, and advancing MCI detection, highlighting the need for richer context and balanced data to capture high-level semantic gaps.",
    "arxiv_url": "https://arxiv.org/abs/2511.19875",
    "authors": [
      "Qingyu Zhang",
      "Puzhuo Liu",
      "Peng Di",
      "Chenxiong Qian"
    ],
    "first_author": "Qingyu Zhang",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Version Control & Collaboration",
    "task": "Git VCS",
    "tags": [
      "Message-Code Inconsistency Detection",
      "Synthetic Mutation Rules",
      "Commit-Diff Pair Labeling",
      "Two-fold Validation",
      "Prompting Augmentations (few-shot, CoT, extended context)",
      "Context Window Sensitivity",
      "Inconsistency-Type Analysis",
      "Token Consumption Trade-offs"
    ],
    "summary": "本文提出了CodeFuse-CommitEval基准，通过对高质量提交进行七类规则化变异并经双重验证构建标签化的提交消息—代码差异对，进而评估不同开源大模型及多种提示增强策略在检测提交消息与代码不一致（MCI）任务上的性能与类型差异。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.19875v1",
    "published": "2025-11-25",
    "update_time": "2025-11-25",
    "download_time": "2025-12-16 14:24:25"
  },
  {
    "id": "2511.20709",
    "title": "DUALGUAGE: Automated Joint Security-Functionality Benchmarking for Secure Code Generation",
    "abstract": "Large language models (LLMs) and autonomous coding agents are increasingly used to generate software across a wide range of domains. Yet a core requirement remains unmet: ensuring that generated code is secure without compromising its functional correctness. Existing benchmarks and evaluations for secure code generation fall short-many measure only vulnerability reduction, disregard correctness preservation, or evaluate security and functionality on separate datasets, violating the fundamental need for simultaneous joint evaluation. We present DUALGAUGE, the first fully automated benchmarking framework designed to rigorously evaluate the security and correctness of LLM-generated code in unison. Given the lack of datasets enabling joint evaluation of secure code generation, we also present DUALGAUGE-BENCH, a curated benchmark suite of diverse coding tasks, each paired with manually validated test suites for both security and functionality, designed for full coverage of specification requirements. At the core of DUALGAUGE is an agentic program executor, which runs a program against given tests in sandboxed environments, and an LLM-based evaluator, which assesses both correctness and vulnerability behavior against expected outcomes. We rigorously evaluated and ensured the quality of DUALGAUGE-BENCH and the accuracy of DUALGAUGE, and applied DUALGAUGE to benchmarking ten leading LLMs on DUALGAUGE-BENCH across thousands of test scenarios. Our results reveal critical gaps in correct and secure code generation by these LLMs, for which our open-source system and datasets help accelerate progress via reproducible, scalable, and rigorous evaluation.",
    "arxiv_url": "https://arxiv.org/abs/2511.20709",
    "authors": [
      "Abhijeet Pathak",
      "Suvadra Barua",
      "Dinesh Gudimetla",
      "Rupam Patir",
      "Jiawei Guo",
      "Hongxin Hu",
      "Haipeng Cai"
    ],
    "first_author": "Abhijeet Pathak",
    "category": [
      "Technical",
      "Benchmark",
      "Empirical"
    ],
    "field": "Software Testing",
    "task": "Testing automation",
    "tags": [
      "Joint security–functionality benchmarking",
      "Runtime exploitability/security tests",
      "Agentic execution and dependency resolution",
      "LLM-based semantic test evaluator",
      "Coverage-enforced test suites",
      "Human-and-LLM co-creation of tests",
      "Sandboxed program execution"
    ],
    "summary": "本文提出DUALGAUGE及DUALGAUGE-BENCH：一个自动化的基准与系统，通过沙箱执行、覆盖驱动的功能/安全测试及LLM评估器对生成代码进行联合的功能正确性与安全性评估，并在大规模模型上揭示多项安全-功能权衡与失败模式。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.20709v1",
    "published": "2025-11-24",
    "update_time": "2025-11-24",
    "download_time": "2025-12-16 14:25:05"
  },
  {
    "id": "2511.19635",
    "title": "Agint: Agentic Graph Compilation for Software Engineering Agents",
    "abstract": "LLM-based coding agents are increasingly common but still face challenges in context management, latency, reliability, reproducibility, and scalability. We present Agint, an agentic graph compiler, interpreter, and runtime that incrementally and hierarchically converts natural-language instructions into typed, effect-aware code DAGs. Agint introduces explicit type floors (text to data to spec to code) grounded in semantic graph transformations and a hybrid LLM and function-based JIT runtime. This enables dynamic graph refinement, reproducible and optimizable execution, speculative evaluation, and interoperability with existing developer tools. Agint's typed graph bindings improve reliability and allow concurrent composition of concurrent codebases by construction, supporting accelerated development with smaller and faster models, lower latency, efficient context utilization, and higher throughput. Hierarchical compilation allows scalable graph edits, while the graph structure supports reproducibility and efficient parallel generation. Agint provides a composable unix-style toolchain: dagify (DAG compiler), dagent (hybrid JIT runtime), schemagin (schema generator), and datagin (data transformer) for realtime, low-latency code and dataflow creation. Human developers and coding agents refine graphs through the Agint CLI, while non-technical users use Agint Flow GUI for visual editing, conversational refinement, and debugging to promote prototype agentic workflows to production code. This continuous co-creation model allows teams to prototype quickly, refine seamlessly, and deploy reliably, bridging natural language, compiler methods, and developer tooling to enable a new generation of composable, team-centric coding agents at scale.",
    "arxiv_url": "https://arxiv.org/abs/2511.19635",
    "authors": [
      "Abhi Chivukula",
      "Jay Somasundaram",
      "Vijay Somasundaram"
    ],
    "first_author": "Abhi Chivukula",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Prompting",
    "tags": [
      "Agentic Graph Compilation",
      "Typed Intermediate Representations",
      "Effect-aware DAGs",
      "Hybrid LLM/function JIT Runtime",
      "Speculative Parallel Execution",
      "Virtual Shim / Virtual Function Abstraction",
      "Locality-preserving Graph Transformations",
      "Incremental Resolution and Refinement",
      "Human-in-the-loop Visual Editing"
    ],
    "summary": "本文提出Agint，一种将自然语言意图编译为带类型与副作用感知的可执行DAG的编译器、解释器与运行时，通过分层类型地板、混合LLM/函数JIT、并行与推测执行实现低延迟、可复现且可组合的软件工程代理流水线。",
    "quality": "High",
    "conference": "NeurIPS (Deep Learning for Code workshop) 2025",
    "pdf_url": "https://arxiv.org/pdf/2511.19635v1",
    "published": "2025-11-24",
    "update_time": "2025-11-24",
    "download_time": "2025-12-16 14:26:11"
  },
  {
    "id": "2511.23408",
    "title": "Evaluating LLMs for One-Shot Patching of Real and Artificial Vulnerabilities",
    "abstract": "Automated vulnerability patching is crucial for software security, and recent advancements in Large Language Models (LLMs) present promising capabilities for automating this task. However, existing research has primarily assessed LLMs using publicly disclosed vulnerabilities, leaving their effectiveness on related artificial vulnerabilities largely unexplored. In this study, we empirically evaluate the patching effectiveness and complementarity of several prominent LLMs, such as OpenAI's GPT variants, LLaMA, DeepSeek, and Mistral models, using both real and artificial vulnerabilities. Our evaluation employs Proof-of-Vulnerability (PoV) test execution to concretely assess whether LLM-generated source code successfully patches vulnerabilities. Our results reveal that LLMs patch real vulnerabilities more effectively compared to artificial ones. Additionally, our analysis reveals significant variability across LLMs in terms of overlapping (multiple LLMs patching the same vulnerabilities) and complementarity (vulnerabilities patched exclusively by a single LLM), emphasizing the importance of selecting appropriate LLMs for effective vulnerability patching.",
    "arxiv_url": "https://arxiv.org/abs/2511.23408",
    "authors": [
      "Aayush Garg",
      "Zanis Ali Khan",
      "Renzo Degiovanni",
      "Qiang Tang"
    ],
    "first_author": "Aayush Garg",
    "category": [
      "Empirical"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Repair",
    "tags": [
      "One-shot vulnerability patching",
      "Proof-of-Vulnerability test execution",
      "Mutation-derived artificial vulnerabilities",
      "Patch overlap and complementarity analysis",
      "LLM comparative evaluation across architectures",
      "Execution-based patch validation",
      "Java/CWE vulnerability cases",
      "Uniform prompting strategy"
    ],
    "summary": "本文通过对多款大型语言模型在15个真实Java漏洞及其41个人工构造变体上的一次性补丁生成进行实证评估，使用PoV测试执行验证补丁有效性并分析模型间的重叠与互补性，发现模型对真实漏洞的修补率普遍高于人工漏洞且不同模型在可修补漏洞上存在显著差异。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.23408v1",
    "published": "2025-11-28",
    "update_time": "2025-11-28",
    "download_time": "2025-12-16 14:26:58"
  },
  {
    "id": "2511.23321",
    "title": "Chart2Code-MoLA: Efficient Multi-Modal Code Generation via Adaptive Expert Routing",
    "abstract": "Chart-to-code generation is a critical task in automated data visualization, translating complex chart structures into executable programs. While recent Multi-modal Large Language Models (MLLMs) improve chart representation, existing approaches still struggle to achieve cross-type generalization, memory efficiency, and modular design. To address these challenges, this paper proposes C2C-MoLA, a multimodal framework that synergizes Mixture of Experts (MoE) with Low-Rank Adaptation (LoRA). The MoE component uses a complexity-aware routing mechanism with domain-specialized experts and load-balanced sparse gating, dynamically allocating inputs based on learnable structural metrics like element count and chart complexity. LoRA enables parameter-efficient updates for resource-conscious tuning, further supported by a tailored training strategy that aligns routing stability with semantic accuracy. Experiments on Chart2Code-160k show that the proposed model improves generation accuracy by up to 17%, reduces peak GPU memory by 18%, and accelerates convergence by 20%, when compared to standard fine-tuning and LoRA-only baselines, particularly on complex charts. Ablation studies validate optimal designs, such as 8 experts and rank-8 LoRA, and confirm scalability for real-world multimodal code generation.",
    "arxiv_url": "https://arxiv.org/abs/2511.23321",
    "authors": [
      "Yifei Wang",
      "Jacky Keung",
      "Zhenyu Mao",
      "Jingyu Zhang",
      "Yuchen Cao"
    ],
    "first_author": "Yifei Wang",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Translation",
    "tags": [
      "Mixture-of-Experts",
      "Complexity-aware routing",
      "Low-Rank Adaptation",
      "Multimodal chart-to-code generation",
      "Structure-aware sparse gating",
      "Load-balancing regularization",
      "Memory-efficient training",
      "Syntax-and-semantics reconstruction loss",
      "Dual-stream visual encoder"
    ],
    "summary": "本文提出 C2C-MoLA，将可学习的结构复杂度引导的专家混合（MoE）与低秩适配（LoRA）结合，用于高效的多模态图表到可执行代码生成，在Chart2Code-160k上显著提高准确率、降低显存占用并加速收敛。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.23321v1",
    "published": "2025-11-28",
    "update_time": "2025-11-28",
    "download_time": "2025-12-16 14:27:31"
  },
  {
    "id": "2512.01010",
    "title": "Chain of Unit-Physics: A Primitive-Centric Approach to Scientific Code Synthesis",
    "abstract": "Agentic large language models are proposed as autonomous code generators for scientific computing, yet their reliability in high-stakes problems remains unclear. Developing computational scientific software from natural-language queries remains challenging broadly due to (a) sparse representation of domain codes during training and (b) the limited feasibility of RLHF with a small expert community. To address these limitations, this work conceptualizes an inverse approach to code design, embodied in the Chain of Unit-Physics framework: a first-principles (or primitives)-centric, multi-agent system in which human expert knowledge is encoded as unit-physics tests that explicitly constrain code generation. The framework is evaluated on a nontrivial combustion task, used here as a representative benchmark for scientific problem with realistic physical constraints. Closed-weight systems and code-focused agentic variants fail to produce correct end-to-end solvers, despite tool and web access, exhibiting four recurrent error classes: interface (syntax/API) hallucinations, overconfident assumptions, numerical/physical incoherence, and configuration fragility. Open-weight models with chain-of-thought (CoT) decoding reduce interface errors but still yield incorrect solutions. On the benchmark task, the proposed framework converges within 5-6 iterations, matches the human-expert implementation (mean error of $3.1\\times10^{-3}$ %), with a $\\sim$33.4 % faster runtime and a $\\sim$30 % efficient memory usage at a cost comparable to mid-sized commercial APIs, yielding a practical template for physics-grounded scientific code generation. As datasets and models evolve, zero-shot code accuracy will improve; however, the Chain of Unit-Physics framework goes further by embedding first-principles analysis that is foundational to scientific codes.",
    "arxiv_url": "https://arxiv.org/abs/2512.01010",
    "authors": [
      "Vansh Sharma",
      "Venkat Raman"
    ],
    "first_author": "Vansh Sharma",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Unit-physics tests",
      "Test-driven scientific code synthesis",
      "Multi-agent orchestration for code generation",
      "Branching top-k decoding with confidence-margin scoring",
      "Sandboxed execution and diagnostic agent",
      "Physics-grounded verification and numerical consistency checks",
      "Combustion solver case study",
      "Iterative human-in-the-loop verification"
    ],
    "summary": "本文提出 Chain of Unit-Physics 框架：通过人类专家编写的基于物理第一性原理的“单元物理”测试、分支解码与多智能体（诊断/验证）流水线，引导并验证科学计算代码生成，在一个真实的燃烧求解器任务中收敛到接近人工实现的高精度解并提升运行与内存效率。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.01010v1",
    "published": "2025-11-30",
    "update_time": "2025-11-30",
    "download_time": "2025-12-16 14:29:30"
  },
  {
    "id": "2512.00867",
    "title": "The AI Attribution Paradox: Transparency as Social Strategy in Open-Source Software Development",
    "abstract": "AI coding assistants have transformed software development, raising questions about transparency and attribution practices. We examine the \"AI attribution paradox\": how developers strategically balance acknowledging AI assistance with managing community scrutiny. Analyzing 14,300 GitHub commits across 7,393 repositories from 2023-2025, we investigated attribution strategies and community responses across eight major AI tools. Results reveal widespread AI usage (95.2% of commits) but strategic attribution: only 29.5% employ explicit disclosure, with dramatic tool variation (Claude 80.5% versus Copilot 9.0%). Explicit attribution triggers modest scrutiny (23% more questions and 21% more comments) but tool choice matters 20-30 times more for predicting reception. Community sentiment remains neutral regardless of attribution type, suggesting curiosity rather than hostility. Temporal analyses show rapid norm evolution: explicit attribution increased from near-zero in early 2024 to 40% by late 2025, indicating community adaptation. These findings illuminate attribution as strategic communication rather than simple transparency, advancing understanding of algorithmic accountability and norm formation during technological transitions. We discuss implications for developers navigating disclosure decisions, platforms designing attribution mechanisms, and researchers studying emergent practices in AI-augmented collaborative work.",
    "arxiv_url": "https://arxiv.org/abs/2512.00867",
    "authors": [
      "Obada Kraishan"
    ],
    "first_author": "Obada Kraishan",
    "category": [
      "Empirical"
    ],
    "field": "Version Control & Collaboration",
    "task": "Git VCS",
    "tags": [
      "AI attribution practices",
      "Commit-message disclosure",
      "Impression management",
      "Tool-specific attribution norms",
      "Community engagement metrics",
      "Temporal norm evolution",
      "Emoji and comment sentiment analysis",
      "Open-source governance"
    ],
    "summary": "本文通过对2023–2025年14,300次GitHub提交的实证分析，揭示了“AI归属悖论”——开发者在公开源码中对AI协助的披露具有策略性（显式署名比例较低且随工具差异显著），显式披露会带来略增的审查但无明显负面情绪，且披露规范在短期内快速演进。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.00867v1",
    "published": "2025-11-30",
    "update_time": "2025-11-30",
    "download_time": "2025-12-16 14:30:03"
  },
  {
    "id": "2511.15665",
    "title": "Quantum-Guided Test Case Minimization for LLM-Based Code Generation",
    "abstract": "Precisely controlling Large Language Models (LLMs) to generate efficient and concise code is a central challenge in software engineering. We introduce a framework based on Test-Driven Development (TDD) that transforms code specification into a combinatorial optimization task. The framework first prompts an LLM to generate a test suite, then formulates the Test Case Minimization (TCM) problem as a Quadratic Unconstrained Binary Optimization (QUBO) model. This QUBO paradigm is compatible with both classical solvers and emerging hardware such as quantum annealers. Experimentally, quantum annealing solves the core TCM task 16 times faster than simulated annealing. This performance underpins our end-to-end framework, which reduces total token consumption by 36.5\\% and significantly improves code quality. This work demonstrates a powerful synergy between generative AI and combinatorial optimization in software engineering, highlighting the critical importance of precise model formulation.",
    "arxiv_url": "https://arxiv.org/abs/2511.15665",
    "authors": [
      "Huixiang Zhang",
      "Mahzabeen Emu"
    ],
    "first_author": "Huixiang Zhang",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Prompting",
    "tags": [
      "Test-Driven Development for LLMs",
      "Test Case Minimization",
      "QUBO formulation for test selection",
      "Quantum annealing acceleration",
      "LLM-generated test-suite pruning",
      "Token-cost aware specification",
      "LLM-guided code refactoring",
      "CI/CD per-commit optimization"
    ],
    "summary": "本文提出一个基于测试驱动开发和QUBO优化的端到端框架，通过对LLM生成的冗余测试用例进行量子退火加速的最小化，显著降低token消耗并提升生成代码的质量与生成效率。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.15665v1",
    "published": "2025-11-19",
    "update_time": "2025-11-19",
    "download_time": "2025-12-16 14:36:47"
  },
  {
    "id": "2511.02352",
    "title": "SWE-Sharp-Bench: A Reproducible Benchmark for C# Software Engineering Tasks",
    "abstract": "AI coding agents have shown great progress on Python software engineering benchmarks like SWE-Bench, and for other languages like Java and C in benchmarks like Multi-SWE-Bench. However, C# -- a prominent enterprise language ranking #5 in the TIOBE index -- remains absent from such benchmarks. We introduce SWE-Sharp-Bench, a reproducible software engineering benchmark for C# featuring 150 instances from 17 repositories. Evaluating identical model-agent configurations across languages reveals a significant performance gap: while 70% of Python tasks in SWE-Bench Verified are solved, only 40% of our C# tasks are resolved. We open-source SWE-Sharp-Bench and our entire curation pipeline.",
    "arxiv_url": "https://arxiv.org/abs/2511.02352",
    "authors": [
      "Sanket Mhatre",
      "Yasharth Bajpai",
      "Sumit Gulwani",
      "Emerson Murphy-Hill",
      "Gustavo Soares"
    ],
    "first_author": "Sanket Mhatre",
    "category": [
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Editing",
    "tags": [
      "C#/.NET ecosystem",
      "Reproducible containerized environments",
      "NuGet/MSBuild dependency resolution",
      "Execution-based filtering (pass→fail→pass)",
      "Patch complexity analysis",
      "Repository-level pull-request tasks",
      "Agent resolution-rate evaluation",
      "Multi-file coordinated edits"
    ],
    "summary": "本文提出SWE-Sharp-Bench——首个面向C#/.NET生态的可复现软件工程基准（150个实例、17个仓库）并开源构建管道，实验证明当前大模型代理在C#上解决率显著低于Python，主要受复杂多文件补丁和依赖/构建管理影响。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.02352v3",
    "published": "2025-11-04",
    "update_time": "2025-11-18",
    "download_time": "2025-12-11 15:55:19"
  },
  {
    "id": "2511.03404",
    "title": "Towards Realistic Project-Level Code Generation via Multi-Agent Collaboration and Semantic Architecture Modeling",
    "abstract": "In recent years, Large Language Models (LLMs) have achieved remarkable progress in automated code generation. In real-world software engineering, the growing demand for rapid iteration and continuous delivery underscores the importance of project-level code generation, where LLMs are expected to generate complete software projects directly from complex user requirements. Although existing studies have made initial explorations, they still face key limitations, including unrealistic datasets and unreliable evaluation metrics that fail to reflect real-world complexity, the semantic gap between human-written requirements and machine-interpretable structures, and difficulties in managing hierarchical dependencies and maintaining quality throughout the generation process. To address these limitations, we first introduce CodeProjectEval, a project-level code generation dataset built from 18 real-world repositories with 12.7 files and 2,388.6 lines of code per task on average, supplemented with documentation and executable test cases for automatic evaluation. We further propose ProjectGen, a multi-agent framework that decomposes projects into architecture design, skeleton generation, and code filling stages with iterative refinement and memory-based context management. Within this framework, we introduce the Semantic Software Architecture Tree (SSAT), a structured and semantically rich representation that effectively bridges user requirements and source code implementation. Experiments show that ProjectGen achieves state-of-the-art performance, passing 52/124 test cases on the small-scale project-level code generation dataset DevBench, a 57% improvement over the baseline approaches, and 310 test cases on CodeProjectEval, representing an improvement of roughly tenfold compared to the baselines.",
    "arxiv_url": "https://arxiv.org/abs/2511.03404",
    "authors": [
      "Qianhui Zhao",
      "Li Zhang",
      "Fang Liu",
      "Junhang Cheng",
      "Chengru Wu",
      "Junchen Ai",
      "Qiaoyuanhe Meng",
      "Lichen Zhang",
      "Xiaoli Lian",
      "Shubin Song",
      "Yuanping Guo"
    ],
    "first_author": "Qianhui Zhao",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Project-level Code Generation",
      "Multi-agent Collaboration",
      "Semantic Software Architecture Tree",
      "Architecture-aware Synthesis",
      "Skeleton-and-fill Pipeline",
      "Iterative Refinement",
      "Memory-based Context Management",
      "Executable Test-case Evaluation",
      "Hierarchical Dependency Management"
    ],
    "summary": "该论文提出了真实项目级代码生成数据集CodeProjectEval，并设计了多智能体ProjectGen框架与语义软件架构树（SSAT）作为中间表征，通过架构设计、骨架生成与代码填充的分阶段迭代及记忆化上下文管理显著提升了项目级代码生成的可执行性与质量。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.03404v1",
    "published": "2025-11-05",
    "update_time": "2025-11-05",
    "download_time": "2025-12-11 15:55:52"
  },
  {
    "id": "2511.06090",
    "title": "SWE-fficiency: Can Language Models Optimize Real-World Repositories on Real Workloads?",
    "abstract": "Optimizing the performance of large-scale software repositories demands expertise in code reasoning and software engineering (SWE) to reduce runtime while preserving program correctness. However, most benchmarks emphasize what to fix rather than how to fix code. We introduce SWE-fficiency, a benchmark for evaluating repository-level performance optimization on real workloads. Our suite contains 498 tasks across nine widely used data-science, machine-learning, and HPC repositories (e.g., numpy, pandas, scipy): given a complete codebase and a slow workload, an agent must investigate code semantics, localize bottlenecks and relevant tests, and produce a patch that matches or exceeds expert speedup while passing the same unit tests. To enable this how-to-fix evaluation, our automated pipeline scrapes GitHub pull requests for performance-improving edits, combining keyword filtering, static analysis, coverage tooling, and execution validation to both confirm expert speedup baselines and identify relevant repository unit tests. Empirical evaluation of state-of-the-art agents reveals significant underperformance. On average, agents achieve less than 0.15x the expert speedup: agents struggle in localizing optimization opportunities, reasoning about execution across functions, and maintaining correctness in proposed edits. We release the benchmark and accompanying data pipeline to facilitate research on automated performance engineering and long-horizon software reasoning.",
    "arxiv_url": "https://arxiv.org/abs/2511.06090",
    "authors": [
      "Jeffrey Jian Ma",
      "Milad Hashemi",
      "Amir Yazdanbakhsh",
      "Kevin Swersky",
      "Ofir Press",
      "Enhui Li",
      "Vijay Janapa Reddi",
      "Parthasarathy Ranganathan"
    ],
    "first_author": "Jeffrey Jian Ma",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Editing",
    "tags": [
      "Repository-level performance optimization",
      "Workload-driven patch generation",
      "Test localization via coverage",
      "Automated PR scraping pipeline",
      "Oracle-free evaluation",
      "Speedup ratio metric",
      "Real-world Python scientific libraries",
      "Correctness-preserving edits",
      "Long-horizon codebase reasoning"
    ],
    "summary": "本文提出SWE-FFICIENCY基准与可复现的数据采集管道，包含498个来自真实Python科学计算仓库的性能优化任务，要求在不破坏仓库单元测试的前提下通过代码修改加速指定工作负载，并发现现有语言模型在定位瓶颈、跨函数执行推理和保持正确性方面远落后于专家（平均仅达专家提速的0.15倍）。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.06090v2",
    "published": "2025-11-08",
    "update_time": "2025-11-11",
    "download_time": "2025-12-11 15:56:27"
  },
  {
    "id": "2511.02778",
    "title": "VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation",
    "abstract": "Code has emerged as a precise and executable medium for reasoning and action in the agent era. Yet, progress has largely focused on language-centric tasks such as program synthesis and debugging, leaving visual-centric coding underexplored. Inspired by how humans reason over sketches, we advocate SVG code as a compact, interpretable, and executable visual representation. We introduce VCode, a benchmark that reframes multimodal understanding as code generation: given an image, a model must produce SVG that preserves symbolic meaning for downstream reasoning. VCode covers three domains - general commonsense (MM-Vet), professional disciplines (MMMU), and visual-centric perception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel evaluation protocol in which a policy model answers questions over rendered SVGs; correct answers indicate faithful symbolic preservation. Empirically, frontier VLMs struggle to generate faithful SVGs, revealing a persistent gap between language-centric and visual-centric coding. To close this gap, we introduce VCoder, an agentic framework that augments VLMs along two axes: (i) Thinking with Revision, which iteratively analyzes discrepancies and refines SVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply structured cues such as objects, shapes, and text beyond the model's intrinsic capacity. Across benchmarks, frontier VLMs with strong reasoning capabilities score well overall yet remain limited in professional knowledge and 3D reasoning. VCoder delivers a 12.3-point overall gain over the top-performing Claude-4-Opus. Human studies show that both humans and VLMs perform worse on rendered SVGs, their consistency reveals the promise of symbolic visual representation. The benchmark and code are available at https://github.com/CSU-JPG/VCode.",
    "arxiv_url": "https://arxiv.org/abs/2511.02778",
    "authors": [
      "Kevin Qinghong Lin",
      "Yuhao Zheng",
      "Hangyu Ran",
      "Dantong Zhu",
      "Dongxing Mao",
      "Linjie Li",
      "Philip Torr",
      "Alex Jinpeng Wang"
    ],
    "first_author": "Kevin Qinghong Lin",
    "category": [
      "Benchmark",
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Translation",
    "tags": [
      "SVG as symbolic visual representation",
      "Image→SVG code generation",
      "Render→VQA evaluation (CodeVQA)",
      "Iterative discrepancy-driven refinement",
      "Integration of perception tools (detectors/segmenters)",
      "Symbolic abstraction for visual reasoning",
      "3D/spatial relation preservation",
      "Human vs. model consistency study"
    ],
    "summary": "本文提出VCode，一个将自然图像转换为可执行且可解释的SVG符号化视觉表示的多模态编码基准，并通过CodeVQA评估协议验证符号保真性，同时引入VCoder（迭代修订+视觉工具）以显著提升图像到SVG的生成质量。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2511.02778v1",
    "published": "2025-11-04",
    "update_time": "2025-11-04",
    "download_time": "2025-12-12 21:43:43"
  }
]