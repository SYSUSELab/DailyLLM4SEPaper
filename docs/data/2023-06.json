[
  {
    "id": "2306.08568",
    "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct",
    "abstract": "Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on HumanEval and HumanEval+. Our code, model weights, and data are public at https://github.com/nlpxucan/WizardLM",
    "arxiv_url": "https://arxiv.org/abs/2306.08568",
    "authors": [
      "Ziyang Luo",
      "Can Xu",
      "Pu Zhao",
      "Qingfeng Sun",
      "Xiubo Geng",
      "Wenxiang Hu",
      "Chongyang Tao",
      "Jing Ma",
      "Qingwei Lin",
      "Daxin Jiang"
    ],
    "first_author": "Ziyang Luo",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Instruction-Tuning",
    "tags": [
      "Code Evol-Instruct",
      "Heuristic-driven instruction evolution",
      "Adversarial instruction generation",
      "Instruction complexity augmentation",
      "Time/space complexity constraints",
      "Instruction-following fine-tuning",
      "Robustness via evolved prompts",
      "Performance-oriented instruction design"
    ],
    "summary": "本文提出面向代码的Code Evol-Instruct指令进化方法，对开源代码大模型进行指令微调以得到显著提升代码生成性能的模型，并实验证明在多项代码生成基准上优于现有开源（且部分闭源）模型，同时强调指令复杂性对性能的关键作用。",
    "quality": "High",
    "conference": "International Conference on Learning Representations (ICLR 2024)",
    "pdf_url": "https://arxiv.org/pdf/2306.08568v2",
    "published": "2023-06-14",
    "update_time": "2025-05-27",
    "download_time": "2025-12-16 10:38:29"
  },
  {
    "id": "2306.11644",
    "title": "Textbooks Are All You Need",
    "abstract": "We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality\" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.",
    "arxiv_url": "https://arxiv.org/abs/2306.11644",
    "authors": [
      "Suriya Gunasekar",
      "Yi Zhang",
      "Jyoti Aneja",
      "Caio César Teodoro Mendes",
      "Allie Del Giorno",
      "Sivakanth Gopi",
      "Mojan Javaheripi",
      "Piero Kauffmann",
      "Gustavo de Rosa",
      "Olli Saarikivi",
      "Adil Salim",
      "Shital Shah",
      "Harkirat Singh Behl",
      "Xin Wang",
      "Sébastien Bubeck",
      "Ronen Eldan",
      "Adam Tauman Kalai",
      "Yin Tat Lee",
      "Yuanzhi Li"
    ],
    "first_author": "Suriya Gunasekar",
    "category": [
      "Technical",
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Pre-Training",
    "tags": [
      "Textbook-quality data curation",
      "Synthetic textbook/exercise generation from a teacher LLM",
      "Data-driven scaling (quality over quantity)",
      "Finetuning on exercise-style prompts",
      "Compute- and parameter-efficient code model",
      "Emergence analysis in small code models",
      "Training data contamination analysis",
      "Pass@1-based code generation evaluation"
    ],
    "summary": "本文提出phi-1，一种仅1.3B参数但通过精选“教科书质量”语料与对抗性合成练习进行预训练与微调，从而在代码生成基准上以远小于常规模型的计算与数据规模取得极高性能，并观察到小模型的涌现行为与数据质量驱动的性能提升。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2306.11644v2",
    "published": "2023-06-20",
    "update_time": "2023-10-02",
    "download_time": "2025-12-16 10:41:36"
  },
  {
    "id": "2306.02907",
    "title": "SelfEvolve: A Code Evolution Framework via Large Language Models",
    "abstract": "Large language models (LLMs) have already revolutionized code generation, after being pretrained on publicly available code data. However, while various methods have been proposed to augment LLMs with retrieved knowledge and enhance the quality of code generation, the performance of these retrieval-based methods is limited by the strength of the retrievers used. In addition, while LLMs show great emergent ability, they still struggle to produce the correct code in one turn. To address these challenges, we propose a novel two-step pipeline, called \\autoknow, that leverages LLMs as both knowledge providers and self-reflective programmers. Unlike retrieval-based methods, \\autoknow~obtains the knowledge from input prompts and generates intermediate code based on the generated knowledge. After that, \\autoknow~asks LLM to act as an expert programmer to perform debugging for the generated code. This is achieved by receiving the error message from the interpreter, without requiring special test cases for correctness verification. We evaluate \\autoknow~on three code generation datasets, including DS-1000 for data science code, HumanEval for software engineering code, and TransCoder for C++-to-Python translation. Our empirical experiments show that \\autoknow~outperforms strong baselines by a significant margin on all datasets. We also conduct exhaustive analytical experiments to validate the effectiveness of the two stages of \\autoknow, and find that both are superior to other prompting-based methods. Further scalability analysis demonstrates that \\autoknow~can be adapted to other more advanced models, such as GPT-4, and bring consistent efficacy improvement.",
    "arxiv_url": "https://arxiv.org/abs/2306.02907",
    "authors": [
      "Shuyang Jiang",
      "Yuhao Wang",
      "Yu Wang"
    ],
    "first_author": "Shuyang Jiang",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Prompting",
    "tags": [
      "Self-generated knowledge extraction",
      "Interpreter-feedback debugging",
      "Iterative self-refinement",
      "Execution-guided program synthesis",
      "Retrieval-free augmentation",
      "Prompt-based code evolution",
      "Error-message-driven repair",
      "Two-stage generation-and-refinement pipeline"
    ],
    "summary": "该文提出SELF-EVOLVE，一个不依赖检索器的两阶段代码生成框架：先让模型自我生成问题相关知识并基于其生成初始代码，再通过解释器返回的错误信息让模型自我调试迭代改进，从而在多项代码生成与翻译数据集上显著提升性能。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2306.02907v1",
    "published": "2023-06-05",
    "update_time": "2023-06-05",
    "download_time": "2025-12-16 13:35:10"
  },
  {
    "id": "2306.09896",
    "title": "Is Self-Repair a Silver Bullet for Code Generation?",
    "abstract": "Large language models have shown remarkable aptitude in code generation, but still struggle to perform complex tasks. Self-repair -- in which the model debugs and repairs its own code -- has recently become a popular way to boost performance in these settings. However, despite its increasing popularity, existing studies of self-repair have been limited in scope; in many settings, its efficacy thus remains poorly understood. In this paper, we analyze Code Llama, GPT-3.5 and GPT-4's ability to perform self-repair on problems taken from HumanEval and APPS. We find that when the cost of carrying out repair is taken into account, performance gains are often modest, vary a lot between subsets of the data, and are sometimes not present at all. We hypothesize that this is because self-repair is bottlenecked by the model's ability to provide feedback on its own code; using a stronger model to artificially boost the quality of the feedback, we observe substantially larger performance gains. Similarly, a small-scale study in which we provide GPT-4 with feedback from human participants suggests that even for the strongest models, self-repair still lags far behind what can be achieved with human-level debugging.",
    "arxiv_url": "https://arxiv.org/abs/2306.09896",
    "authors": [
      "Theo X. Olausson",
      "Jeevana Priya Inala",
      "Chenglong Wang",
      "Jianfeng Gao",
      "Armando Solar-Lezama"
    ],
    "first_author": "Theo X. Olausson",
    "category": [
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Editing",
    "tags": [
      "Self-repair pipeline (generate→execute→feedback→repair)",
      "Feedback quality ablation",
      "Compute-budget vs sampling tradeoff",
      "Unit-test-driven debugging",
      "Cross-model feedback substitution",
      "Human-in-the-loop debugging comparison",
      "Pass@k baseline analysis",
      "Error-message to natural-language feedback"
    ],
    "summary": "本文在HumanEval和APPS上实证评估了CodeLlama、GPT-3.5与GPT-4的自我修复能力，发现计入修复成本后收益常常有限且不稳定，修复效果高度依赖反馈质量，且人类反馈显著优于自动反馈。",
    "quality": "High",
    "conference": "ICLR 2024",
    "pdf_url": "https://arxiv.org/pdf/2306.09896v5",
    "published": "2023-06-16",
    "update_time": "2024-02-02",
    "download_time": "2025-12-16 13:41:23"
  },
  {
    "id": "2306.03091",
    "title": "RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems",
    "abstract": "Large Language Models (LLMs) have greatly advanced code auto-completion systems, with a potential for substantial productivity enhancements for developers. However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios. To fill this gap, we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems. RepoBench supports both Python and Java and consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). Each task respectively measures the system's ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction. RepoBench aims to facilitate a more complete comparison of performance and encouraging continuous improvement in auto-completion systems. RepoBench is publicly available at https://github.com/Leolty/repobench.",
    "arxiv_url": "https://arxiv.org/abs/2306.03091",
    "authors": [
      "Tianyang Liu",
      "Canwen Xu",
      "Julian McAuley"
    ],
    "first_author": "Tianyang Liu",
    "category": [
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Completion",
    "tags": [
      "Repository-level evaluation",
      "Cross-file retrieval",
      "Next-line code prediction",
      "Retrieval–completion pipeline",
      "Python and Java multi-file projects",
      "Temporal test split to reduce leakage",
      "Long-context benchmarking",
      "Retrieval-augmented completion"
    ],
    "summary": "本文提出RepoBench，一个针对仓库级（跨文件）代码自动补全的基准，包含检索、补全与端到端流水线三项任务，并在大量 Python 和 Java 仓库上评估检索方法与补全模型以揭示处理长上下文与跨文件依赖的挑战。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2306.03091v2",
    "published": "2023-06-05",
    "update_time": "2023-10-04",
    "download_time": "2025-12-16 13:55:21"
  },
  {
    "id": "2306.14893",
    "title": "LongCoder: A Long-Range Pre-trained Language Model for Code Completion",
    "abstract": "In this paper, we introduce a new task for code completion that focuses on handling long code input and propose a sparse Transformer model, called LongCoder, to address this task. LongCoder employs a sliding window mechanism for self-attention and introduces two types of globally accessible tokens - bridge tokens and memory tokens - to improve performance and efficiency. Bridge tokens are inserted throughout the input sequence to aggregate local information and facilitate global interaction, while memory tokens are included to highlight important statements that may be invoked later and need to be memorized, such as package imports and definitions of classes, functions, or structures. We conduct experiments on a newly constructed dataset that contains longer code context and the publicly available CodeXGLUE benchmark. Experimental results demonstrate that LongCoder achieves superior performance on code completion tasks compared to previous models while maintaining comparable efficiency in terms of computational resources during inference. All the codes and data are available at https://github.com/microsoft/CodeBERT.",
    "arxiv_url": "https://arxiv.org/abs/2306.14893",
    "authors": [
      "Daya Guo",
      "Canwen Xu",
      "Nan Duan",
      "Jian Yin",
      "Julian McAuley"
    ],
    "first_author": "Daya Guo",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Completion",
    "tags": [
      "sliding-window sparse attention",
      "bridge tokens for local aggregation",
      "memory tokens for global memorization",
      "long-context / file-level code modeling",
      "linearized attention complexity",
      "autoregressive pre-training for completion",
      "construction of long-code completion benchmark",
      "code-driven dynamic global attention"
    ],
    "summary": "该文提出一种用于长代码上下文补全的稀疏Transformer预训练模型，通过滑动窗口注意力及两类全局令牌（桥接令牌与记忆令牌）在保持线性复杂度的同时建模文件级长上下文，并构建了面向长代码的补全基准以验证方法有效性。",
    "quality": "High",
    "conference": "ICML 2023",
    "pdf_url": "https://arxiv.org/pdf/2306.14893v1",
    "published": "2023-06-26",
    "update_time": "2023-06-26",
    "download_time": "2025-12-16 13:55:58"
  },
  {
    "id": "2306.10763",
    "title": "Guiding Language Models of Code with Global Context using Monitors",
    "abstract": "Language models of code (LMs) work well when the surrounding code provides sufficient context. This is not true when it becomes necessary to use types, functionality or APIs defined elsewhere in the repository or a linked library, especially those not seen during training. LMs suffer from limited awareness of such global context and end up hallucinating.   Integrated development environments (IDEs) assist developers in understanding repository context using static analysis. We extend this assistance, enjoyed by developers, to LMs. We propose monitor-guided decoding (MGD) where a monitor uses static analysis to guide the decoding. We construct a repository-level dataset PragmaticCode for method-completion in Java and evaluate MGD on it. On models of varying parameter scale, by monitoring for type-consistent object dereferences, MGD consistently improves compilation rates and agreement with ground truth. Further, LMs with fewer parameters, when augmented with MGD, can outperform larger LMs. With MGD, SantaCoder-1.1B achieves better compilation rate and next-identifier match than the much larger text-davinci-003 model.   We also conduct a generalizability study to evaluate the ability of MGD to generalize to multiple programming languages (Java, C# and Rust), coding scenarios (e.g., correct number of arguments to method calls), and to enforce richer semantic constraints (e.g., stateful API protocols). Our data and implementation are available at https://github.com/microsoft/monitors4codegen .",
    "arxiv_url": "https://arxiv.org/abs/2306.10763",
    "authors": [
      "Lakshya A Agrawal",
      "Aditya Kanade",
      "Navin Goyal",
      "Shuvendu K. Lahiri",
      "Sriram K. Rajamani"
    ],
    "first_author": "Lakshya A Agrawal",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Completion",
    "tags": [
      "Monitor-guided decoding",
      "Static-analysis-driven token masking",
      "Language Server Protocol integration",
      "Type-aware identifier resolution",
      "Decoding-time semantic constraints",
      "Repository-level context",
      "Stateful API protocol enforcement",
      "Compilation-focused evaluation"
    ],
    "summary": "本文提出监视器引导解码（MGD），在解码阶段调用静态分析（基于LSP）对模型生成的令牌进行约束，从而利用仓库级全局上下文提升类型一致性、编译率和与真实代码的匹配，并公开了相应的仓库级数据集与工具实现。",
    "quality": "High",
    "conference": "NeurIPS 2023",
    "pdf_url": "https://arxiv.org/pdf/2306.10763v2",
    "published": "2023-06-19",
    "update_time": "2023-11-03",
    "download_time": "2025-12-11 16:30:53"
  },
  {
    "id": "2306.10998",
    "title": "RepoFusion: Training Code Models to Understand Your Repository",
    "abstract": "Despite the huge success of Large Language Models (LLMs) in coding assistants like GitHub Copilot, these models struggle to understand the context present in the repository (e.g., imports, parent classes, files with similar names, etc.), thereby producing inaccurate code completions. This effect is more pronounced when using these assistants for repositories that the model has not seen during training, such as proprietary software or work-in-progress code projects. Recent work has shown the promise of using context from the repository during inference. In this work, we extend this idea and propose RepoFusion, a framework to train models to incorporate relevant repository context. Experiments on single-line code completion show that our models trained with repository context significantly outperform much larger code models as CodeGen-16B-multi ($\\sim73\\times$ larger) and closely match the performance of the $\\sim 70\\times$ larger StarCoderBase model that was trained with the Fill-in-the-Middle objective. We find these results to be a novel and compelling demonstration of the gains that training with repository context can bring. We carry out extensive ablation studies to investigate the impact of design choices such as context type, number of contexts, context length, and initialization within our framework. Lastly, we release Stack-Repo, a dataset of 200 Java repositories with permissive licenses and near-deduplicated files that are augmented with three types of repository contexts. Additionally, we are making available the code and trained checkpoints for our work. Our released resources can be found at \\url{https://huggingface.co/RepoFusion}.",
    "arxiv_url": "https://arxiv.org/abs/2306.10998",
    "authors": [
      "Disha Shrivastava",
      "Denis Kocetkov",
      "Harm de Vries",
      "Dzmitry Bahdanau",
      "Torsten Scholak"
    ],
    "first_author": "Disha Shrivastava",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Completion",
    "tags": [
      "Repository-level context integration",
      "Fusion-in-Decoder adaptation",
      "Repo-level prompt proposals",
      "Context retrieval (BM25 and embedding-based)",
      "Surrounding-context augmentation",
      "Single-line code completion",
      "Ablation studies on context design",
      "Java repository corpus release"
    ],
    "summary": "本文提出RepoFusion，通过将多个仓库级上下文用Fusion-in-Decoder训练融入代码模型以提升单行代码补全性能，并发布了带上下文增强的Java仓库语料与训练检查点，且小模型在多项评测中显著优于更大的基线模型。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2306.10998v1",
    "published": "2023-06-19",
    "update_time": "2023-06-19",
    "download_time": "2025-12-11 16:31:32"
  },
  {
    "id": "2306.17193",
    "title": "Uncovering the Limits of Machine Learning for Automatic Vulnerability Detection",
    "abstract": "Recent results of machine learning for automatic vulnerability detection (ML4VD) have been very promising. Given only the source code of a function $f$, ML4VD techniques can decide if $f$ contains a security flaw with up to 70% accuracy. However, as evident in our own experiments, the same top-performing models are unable to distinguish between functions that contain a vulnerability and functions where the vulnerability is patched. So, how can we explain this contradiction and how can we improve the way we evaluate ML4VD techniques to get a better picture of their actual capabilities?   In this paper, we identify overfitting to unrelated features and out-of-distribution generalization as two problems, which are not captured by the traditional approach of evaluating ML4VD techniques. As a remedy, we propose a novel benchmarking methodology to help researchers better evaluate the true capabilities and limits of ML4VD techniques. Specifically, we propose (i) to augment the training and validation dataset according to our cross-validation algorithm, where a semantic preserving transformation is applied during the augmentation of either the training set or the testing set, and (ii) to augment the testing set with code snippets where the vulnerabilities are patched.   Using six ML4VD techniques and two datasets, we find (a) that state-of-the-art models severely overfit to unrelated features for predicting the vulnerabilities in the testing data, (b) that the performance gained by data augmentation does not generalize beyond the specific augmentations applied during training, and (c) that state-of-the-art ML4VD techniques are unable to distinguish vulnerable functions from their patches.",
    "arxiv_url": "https://arxiv.org/abs/2306.17193",
    "authors": [
      "Niklas Risse",
      "Marcel Böhme"
    ],
    "first_author": "Niklas Risse",
    "category": [
      "Empirical",
      "Benchmark",
      "Technical"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "Semantic-preserving code transformations",
      "Overfitting to superficial features",
      "Cross-transformation generalization",
      "Patch-pair evaluation",
      "Data augmentation robustness",
      "Vuln–patch paired dataset",
      "Evaluation algorithms for ML4VD",
      "Out-of-distribution generalization"
    ],
    "summary": "本文提出两种用于评估自动化漏洞检测（ML4VD）的算法并发布包含漏洞与对应补丁对的新数据集，实验证明现有基于令牌的模型严重依赖与漏洞无关的特征且无法区分漏洞与其补丁。",
    "quality": "High",
    "conference": "Proceedings of the 33rd USENIX Security Symposium (USENIX Security 2024) 2024",
    "pdf_url": "https://arxiv.org/pdf/2306.17193v2",
    "published": "2023-06-28",
    "update_time": "2024-06-06",
    "download_time": "2025-12-11 17:14:01"
  }
]