[
  {
    "id": "2512.01939",
    "title": "An Empirical Study of Agent Developer Practices in AI Agent Frameworks",
    "abstract": "The rise of large language models (LLMs) has sparked a surge of interest in agents, leading to the rapid growth of agent frameworks. Agent frameworks are software toolkits and libraries that provide standardized components, abstractions, and orchestration mechanisms to simplify agent development. Despite widespread use of agent frameworks, their practical applications and how they influence the agent development process remain underexplored. Different agent frameworks encounter similar problems during use, indicating that these recurring issues deserve greater attention and call for further improvements in agent framework design. Meanwhile, as the number of agent frameworks continues to grow and evolve, more than 80% of developers report difficulties in identifying the frameworks that best meet their specific development requirements. In this paper, we conduct the first empirical study of LLM-based agent frameworks, exploring real-world experiences of developers in building AI agents. To compare how well the agent frameworks meet developer needs, we further collect developer discussions for the ten previously identified agent frameworks, resulting in a total of 11,910 discussions. Finally, by analyzing these discussions, we compare the frameworks across five dimensions: development efficiency, functional abstraction, learning cost, performance optimization, and maintainability, which refers to how easily developers can update and extend both the framework itself and the agents built upon it over time. Our comparative analysis reveals significant differences among frameworks in how they meet the needs of agent developers. Overall, we provide a set of findings and implications for the LLM-driven AI agent framework ecosystem and offer insights for the design of future LLM-based agent frameworks and agent developers.",
    "arxiv_url": "https://arxiv.org/abs/2512.01939",
    "authors": [
      "Yanlin Wang",
      "Xinyi Xu",
      "Jiachi Chen",
      "Tingting Bi",
      "Wenchao Gu",
      "Zibin Zheng"
    ],
    "first_author": "Yanlin Wang",
    "category": [
      "Empirical"
    ],
    "field": "Agent Frameworks & Developer Practices",
    "task": "Developer practices and SDLC challenges in LLM-based agent frameworks",
    "tags": [
      "Mining GitHub discussions",
      "Agent framework adoption patterns",
      "SDLC challenges taxonomy",
      "Logic control & task termination",
      "Tool integration and API compatibility",
      "Context retention and memory management",
      "Version and dependency compatibility",
      "Comparative framework evaluation (learning cost, efficiency, abstraction, performance, maintainability)",
      "Multi-framework composition"
    ],
    "summary": "本文通过分析1,575个基于LLM的代理项目及近1.2万条开发者讨论，构建了代理开发面向SDLC的挑战分类、识别十个代表性框架并基于五维度比较其对开发者需求的满足情况，揭示常见问题并提出改进建议。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.01939v1",
    "published": "2025-12-01",
    "update_time": "2025-12-01",
    "download_time": "2025-12-16 14:30:54"
  },
  {
    "id": "2512.01690",
    "title": "Generating REST API Tests With Descriptive Names",
    "abstract": "Automated test generation has become a key technique for ensuring software quality, particularly in modern API-based architectures. However, automatically generated test cases are typically assigned non-descriptive names (e.g., test0, test1), which reduces their readability and hinders their usefulness during comprehension and maintenance. In this work, we present three novel deterministic techniques to generate REST API test names. We then compare eight techniques in total for generating descriptive names for REST API tests automatically produced by the fuzzer EvoMaster, using 10 test cases generated for 9 different open-source APIs. The eight techniques include rule-based heuristics and large language model (LLM)-based approaches. Their effectiveness was empirically evaluated through two surveys (involving up to 39 people recruited via LinkedIn). Our results show that a rule-based approach achieves the highest clarity ratings among deterministic methods, performs on par with state-of-the-art LLM-based models such as Gemini and GPT-4o, and significantly outperforms GPT-3.5.   To further evaluate the practical impact of our results, an industrial case study was carried out with practitioners who actively use EvoMaster at Volkswagen AG. A developer questionnaire was then carried out based on the use of EvoMaster on four different APIs by four different users, for a total of 74 evaluated test cases. Feedback from practitioners further confirms that descriptive names produced by this approach improve test suite readability.   These findings highlight that lightweight, deterministic techniques can serve as effective alternatives to computationally expensive and security-sensitive LLM-based approaches for automated system-level test naming, providing a practical step toward more developer-friendly API test generation.",
    "arxiv_url": "https://arxiv.org/abs/2512.01690",
    "authors": [
      "Philip Garrett",
      "Juan P. Galeotti",
      "Andrea Arcuri",
      "Alexander Poth",
      "Olsi Rrjolli"
    ],
    "first_author": "Philip Garrett",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Software Testing",
    "task": "Test Generation",
    "tags": [
      "Test case naming",
      "Deterministic naming heuristics",
      "REST API semantics (endpoints, methods, params, status codes)",
      "EvoMaster integration",
      "Human readability evaluation",
      "LLM-based naming comparison",
      "Test-suite organization and sorting",
      "Industrial case study / practitioner feedback"
    ],
    "summary": "本文提出三种确定性方法为自动生成的REST API测试用例生成描述性名称，并通过问卷用户研究与大众及工业案例将这些规则方法与多种基于LLM的方法对比，结果表明规则方法在可读性上优于GPT‑3.5、与更先进模型相当且已集成到EvoMaster中提升了测试套件的可读性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.01690v1",
    "published": "2025-12-01",
    "update_time": "2025-12-01",
    "download_time": "2025-12-16 14:31:28"
  },
  {
    "id": "2512.01609",
    "title": "GPTrace: Effective Crash Deduplication Using LLM Embeddings",
    "abstract": "Fuzzing is a highly effective method for uncovering software vulnerabilities, but analyzing the resulting data typically requires substantial manual effort. This is amplified by the fact that fuzzing campaigns often find a large number of crashing inputs, many of which share the same underlying bug. Crash deduplication is the task of finding such duplicate crashing inputs and thereby reducing the data that needs to be examined. Many existing deduplication approaches rely on comparing stack traces or other information that is collected when a program crashes. Although various metrics for measuring the similarity of such pieces of information have been proposed, many do not yield satisfactory deduplication results. In this work, we present GPTrace, a deduplication workflow that leverages a large language model to evaluate the similarity of various data sources associated with crashes by computing embedding vectors and supplying those as input to a clustering algorithm. We evaluate our approach on over 300 000 crashing inputs belonging to 50 ground truth labels from 14 different targets. The deduplication results produced by GPTrace show a noticeable improvement over hand-crafted stack trace comparison methods and even more complex state-of-the-art approaches that are less flexible.",
    "arxiv_url": "https://arxiv.org/abs/2512.01609",
    "authors": [
      "Patrick Herter",
      "Vincent Ahlrichs",
      "Ridvan Açilan",
      "Julian Horsch"
    ],
    "first_author": "Patrick Herter",
    "category": [
      "Technical"
    ],
    "field": "Software Testing",
    "task": "Testing automation",
    "tags": [
      "Crash Deduplication",
      "Fuzzing Crash Triage",
      "Stack Trace Preprocessing",
      "ASan Report Sanitization",
      "Embedding-based Crash Similarity",
      "Summed Normalized Embeddings",
      "HDBSCAN-DBSCAN Hybrid Clustering"
    ],
    "summary": "GPTrace提出通过对栈跟踪和ASan报告计算并归一化求和的嵌入向量，结合密度聚类实现模糊测试崩溃去重，从而显著优于传统手工栈跟踪比对方法。",
    "quality": "High",
    "conference": "IEEE/ACM International Conference on Software Engineering (ICSE 2026)",
    "pdf_url": "https://arxiv.org/pdf/2512.01609v1",
    "published": "2025-12-01",
    "update_time": "2025-12-01",
    "download_time": "2025-12-16 14:32:38"
  },
  {
    "id": "2512.01396",
    "title": "BackportBench: A Multilingual Benchmark for Automated Backporting of Patches",
    "abstract": "Many modern software projects evolve rapidly to incorporate new features and security patches. It is important for users to update their dependencies to safer versions, but many still use older, vulnerable package versions because upgrading can be difficult and may break their existing codebase. Software developers can mitigate this problem by backporting security patches to older releases. However, manually backporting is time-consuming and error-prone. The effectiveness of existing automated backporting techniques on general software remains unclear since they typically target only code-hunk or function-level patch porting scenarios and are evaluated with imperfect metrics.   To facilitate the development and evaluation of automated backporting techniques, we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem. BackportBench is a multilingual benchmark that contains 202 patch backporting problems from PyPI, Maven, and npm, each with executable Docker environments and relevant test cases. We evaluated existing patch porting methods and LLM-based techniques that have the potential to adapt to this task using BackportBench. The results show that the agentic method has outperformed traditional patch porting methods, especially on cases that require logical and structural changes. However, the performance varies across different programming languages. Based on the findings, we draw several implications for researchers and software practitioners in future work on automated backporting.",
    "arxiv_url": "https://arxiv.org/abs/2512.01396",
    "authors": [
      "Zhiqing Zhong",
      "Jiaming Huang",
      "Pinjia He"
    ],
    "first_author": "Zhiqing Zhong",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Repair",
    "tags": [
      "Patch Backporting",
      "Multilingual (Python/Java/JavaScript)",
      "Repository-level Patch Adaptation",
      "Executable Docker Environments",
      "Test-suite Validation",
      "Agentic LLM Workflows",
      "Cross-file Incompatibility Handling",
      "Evaluation Metrics for Backporting"
    ],
    "summary": "本文提出BackportBench——一个包含来自PyPI、Maven和npm的202个多语言可执行补丁回移任务的基准，并用测试用例评估传统补丁迁移方法与多种（包括agentic）LLM驱动方法的效果与差异。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.01396v1",
    "published": "2025-12-01",
    "update_time": "2025-12-01",
    "download_time": "2025-12-16 14:33:18"
  },
  {
    "id": "2512.01356",
    "title": "LAURA: Enhancing Code Review Generation with Context-Enriched Retrieval-Augmented LLM",
    "abstract": "Code review is critical for ensuring software quality and maintainability. With the rapid growth in software scale and complexity, code review has become a bottleneck in the development process because of its time-consuming and knowledge-intensive nature and the shortage of experienced developers willing to review code. Several approaches have been proposed for automatically generating code reviews based on retrieval, neural machine translation, pre-trained models, or large language models (LLMs). These approaches mainly leverage historical code changes and review comments. However, a large amount of crucial information for code review, such as the context of code changes and prior review knowledge, has been overlooked. This paper proposes an LLM-based review knowledge-augmented, context-aware framework for code review generation, named LAURA. The framework integrates review exemplar retrieval, context augmentation, and systematic guidance to enhance the performance of ChatGPT-4o and DeepSeek v3 in generating code review comments. Besides, given the extensive low-quality reviews in existing datasets, we also constructed a high-quality dataset. Experimental results show that for both models, LAURA generates review comments that are either completely correct or at least helpful to developers in 42.2% and 40.4% of cases, respectively, significantly outperforming SOTA baselines. Furthermore, our ablation studies demonstrate that all components of LAURA contribute positively to improving comment quality.",
    "arxiv_url": "https://arxiv.org/abs/2512.01356",
    "authors": [
      "Yuxin Zhang",
      "Yuxia Zhang",
      "Zeyu Sun",
      "Yanjie Jiang",
      "Hui Liu"
    ],
    "first_author": "Yuxin Zhang",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Maintenance",
    "task": "Code Review",
    "tags": [
      "Context-extended diffs",
      "PR metadata augmentation",
      "Review exemplar retrieval",
      "Retrieval-augmented generation (RAG) for reviews",
      "Transformer-based code embeddings",
      "AST-based context expansion",
      "Systematic review prompting",
      "High-quality diff-comment dataset construction",
      "Human + LLM dual evaluation (I-Score/IH-Score)",
      "Ablation study of components"
    ],
    "summary": "本文提出LAURA框架，通过补充PR上下文、检索相似变更及其审查范例并结合系统化提示来增强LLM生成代码审查注释，并构建并公开了一个高质量的diff‑comment检索数据库以验证性能提升。",
    "quality": "High",
    "conference": "IEEE/ACM International Conference on Automated Software Engineering (ASE 2025)",
    "pdf_url": "https://arxiv.org/pdf/2512.01356v1",
    "published": "2025-12-01",
    "update_time": "2025-12-01",
    "download_time": "2025-12-16 14:33:59"
  },
  {
    "id": "2512.01255",
    "title": "Large Language Models Cannot Reliably Detect Vulnerabilities in JavaScript: The First Systematic Benchmark and Evaluation",
    "abstract": "Researchers have proposed numerous methods to detect vulnerabilities in JavaScript, especially those assisted by Large Language Models (LLMs). However, the actual capability of LLMs in JavaScript vulnerability detection remains questionable, necessitating systematic evaluation and comprehensive benchmarks. Unfortunately, existing benchmarks suffer from three critical limitations: (1) incomplete coverage, such as covering a limited subset of CWE types; (2) underestimation of LLM capabilities caused by unreasonable ground truth labeling; and (3) overestimation due to unrealistic cases such as using isolated vulnerable files rather than complete projects.   In this paper, we introduce, for the first time, three principles for constructing a benchmark for JavaScript vulnerability detection that directly address these limitations: (1) comprehensiveness, (2) no underestimation, and (3) no overestimation. Guided by these principles, we propose FORGEJS, the first automatic benchmark generation framework for evaluating LLMs' capability in JavaScript vulnerability detection. Then, we use FORGEJS to construct ARENAJS-the first systematic benchmark for LLM-based JavaScript vulnerability detection-and further propose JUDGEJS, an automatic evaluation framework.   We conduct the first systematic evaluation of LLMs for JavaScript vulnerability detection, leveraging JUDGEJS to assess seven popular commercial LLMs on ARENAJS. The results show that LLMs not only exhibit limited reasoning capabilities, but also suffer from severe robustness defects, indicating that reliable JavaScript vulnerability detection with LLMs remains an open challenge.",
    "arxiv_url": "https://arxiv.org/abs/2512.01255",
    "authors": [
      "Qingyuan Fei",
      "Xin Liu",
      "Song Li",
      "Shujiang Wu",
      "Jianwei Hou",
      "Ping Chen",
      "Zifeng Kang"
    ],
    "first_author": "Qingyuan Fei",
    "category": [
      "Benchmark",
      "Empirical",
      "Technical"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "JavaScript vulnerability benchmark",
      "Comprehensive CWE coverage",
      "Project-level vs. function-level evaluation",
      "Automatic benchmark generation",
      "Automated evaluation pipeline",
      "Semantic equivalence / fuzzy label alignment",
      "Vulnerability localization (file/function/line)",
      "Taint-flow and dependency reasoning assessment",
      "Robustness testing via code augmentations",
      "False positive rate and deployment-oriented metrics"
    ],
    "summary": "本文提出遵循全面性、避免低估和避免高估三项原则的自动化基准生成与评估体系，并基于该体系构建了系统性JavaScript漏洞检测基准与评估框架，实证评估多款主流商业大模型后发现其在推理能力、鲁棒性和可部署性方面表现不足且不可靠。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.01255v1",
    "published": "2025-12-01",
    "update_time": "2025-12-01",
    "download_time": "2025-12-16 14:34:37"
  },
  {
    "id": "2512.02795",
    "title": "Towards Observation Lakehouses: Living, Interactive Archives of Software Behavior",
    "abstract": "Code-generating LLMs are trained largely on static artifacts (source, comments, specifications) and rarely on materializations of run-time behavior. As a result, they readily internalize buggy or mislabeled code. Since non-trivial semantic properties are undecidable in general, the only practical way to obtain ground-truth functionality is by dynamic observation of executions. In prior work, we addressed representation with Sequence Sheets, Stimulus-Response Matrices (SRMs), and Stimulus-Response Cubes (SRCs) to capture and compare behavior across tests, implementations, and contexts. These structures make observation data analyzable offline and reusable, but they do not by themselves provide persistence, evolution, or interactive analytics at scale. In this paper, therefore, we introduce observation lakehouses that operationalize continual SRCs: a tall, append-only observations table storing every actuation (stimulus, response, context) and SQL queries that materialize SRC slices on demand. Built on Apache Parquet + Iceberg + DuckDB, the lakehouse ingests data from controlled pipelines (LASSO) and CI pipelines (e.g., unit test executions), enabling n-version assessment, behavioral clustering, and consensus oracles without re-execution. On a 509-problem benchmark, we ingest $\\approx$8.6M observation rows ($<$51MiB) and reconstruct SRM/SRC views and clusters in $<$100ms on a laptop, demonstrating that continual behavior mining is practical without a distributed cluster of machines. This makes behavioral ground truth first-class alongside other run-time data and provides an infrastructure path toward behavior-aware evaluation and training. The Observation Lakehouse, together with the accompanying dataset, is publicly available as an open-source project on GitHub: https://github.com/SoftwareObservatorium/observation-lakehouse",
    "arxiv_url": "https://arxiv.org/abs/2512.02795",
    "authors": [
      "Marcus Kessel"
    ],
    "first_author": "Marcus Kessel",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Software Testing",
    "task": "Testing automation",
    "tags": [
      "Observation Lakehouse",
      "Stimulus-Response Matrix (SRM)",
      "Stimulus-Response Cube (SRC)",
      "Sequence Sheets / Invocation step records",
      "Append-only observations table",
      "Parquet + Iceberg + DuckDB stack",
      "Schema evolution for continual observations",
      "Partitioned SRM reconstruction",
      "Behavioral clustering",
      "Consensus oracle / n-version assessment",
      "Interactive SQL-based analytics"
    ],
    "summary": "本文提出Observation Lakehouse，一种基于Parquet/Iceberg/DuckDB的数据湖仓架构，用细粒度调用步记录持续存储并按需重建SRM/SRC视图，从而在单机上高效完成行为聚类、n版评估与共识判定，推动运行时行为成为训练与评估的一等公民。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.02795v1",
    "published": "2025-12-02",
    "update_time": "2025-12-02",
    "download_time": "2025-12-16 14:35:14"
  },
  {
    "id": "2512.02750",
    "title": "\"Can you feel the vibes?\": An exploration of novice programmer engagement with vibe coding",
    "abstract": "Emerging alongside generative AI and the broader trend of AI-assisted coding, the term \"vibe coding\" refers to creating software via natural language prompts rather than direct code authorship. This approach promises to democratize software development, but its educational implications remain underexplored. This paper reports on a one-day educational hackathon investigating how novice programmers and mixed-experience teams engage with vibe coding. We organized an inclusive event at a Brazilian public university with 31 undergraduate participants from computing and non-computing disciplines, divided into nine teams. Through observations, an exit survey, and semi-structured interviews, we examined creative processes, tool usage patterns, collaboration dynamics, and learning outcomes. Findings reveal that vibe coding enabled rapid prototyping and cross-disciplinary collaboration, with participants developing prompt engineering skills and delivering functional demonstrations within time constraints. However, we observed premature convergence in ideation, uneven code quality requiring rework, and limited engagement with core software engineering practices. Teams adopted sophisticated workflows combining multiple AI tools in pipeline configurations, with human judgment remaining essential for critical refinement. The short format (9 hours) proved effective for confidence-building among newcomers while accommodating participants with limited availability. We conclude that vibe coding hackathons can serve as valuable low-stakes learning environments when coupled with explicit scaffolds for divergent thinking, critical evaluation of AI outputs, and realistic expectations about production quality.",
    "arxiv_url": "https://arxiv.org/abs/2512.02750",
    "authors": [
      "Kiev Gama",
      "Filipe Calegario",
      "Victoria Jackson",
      "Alexander Nolte",
      "Luiz Augusto Morais",
      "Vinicius Garcia"
    ],
    "first_author": "Kiev Gama",
    "category": [
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Prompting",
    "tags": [
      "Vibe Coding",
      "Hackathon Design",
      "Prompt Engineering",
      "Novice Programmer Engagement",
      "Cross-disciplinary Collaboration",
      "Toolchain Orchestration",
      "Premature Ideation Convergence",
      "Human-in-the-loop Validation"
    ],
    "summary": "本文通过对一次为期一天、面向新手的vibe coding黑客松的观察、问卷和访谈，探讨参与者如何用自然语言提示与多种AI工具协作快速原型、学到的提示工程技能、协作动态与学习成效，发现活动能提高信心与跨学科合作但也带来早熟收敛、代码质量参差及对软件工程实践的有限参与。",
    "quality": "Middle",
    "conference": "International Conference on Software Engineering, Education Track (SEET) 2026",
    "pdf_url": "https://arxiv.org/pdf/2512.02750v1",
    "published": "2025-12-02",
    "update_time": "2025-12-02",
    "download_time": "2025-12-16 14:35:58"
  },
  {
    "id": "2512.03421",
    "title": "Exploring the Potential and Limitations of Large Language Models for Novice Program Fault Localization",
    "abstract": "Novice programmers often face challenges in fault localization due to their limited experience and understanding of programming syntax and logic. Traditional methods like Spectrum-Based Fault Localization (SBFL) and Mutation-Based Fault Localization (MBFL) help identify faults but often lack the ability to understand code context, making them less effective for beginners. In recent years, Large Language Models (LLMs) have shown promise in overcoming these limitations by utilizing their ability to understand program syntax and semantics. LLM-based fault localization provides more accurate and context-aware results than traditional techniques. This study evaluates six closed-source and seven open-source LLMs using the Codeflaws, Condefects, and BugT datasets, with BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns. Advanced models with reasoning capabilities, such as OpenAI o3 and DeepSeekR1, achieve superior accuracy with minimal reliance on prompt engineering. In contrast, models without reasoning capabilities, like GPT-4, require carefully designed prompts to maintain performance. While LLMs perform well in simple fault localization, their accuracy decreases as problem difficulty increases, though top models maintain robust performance in the BugT dataset. Over-reasoning is another challenge, where some models generate excessive explanations that hinder fault localization clarity. Additionally, the computational cost of deploying LLMs remains a significant barrier for real-time debugging. LLM's explanations demonstrate significant value for novice programmer assistance, with one-year experience participants consistently rating them highly. Our findings demonstrate the potential of LLMs to improve debugging efficiency while stressing the need for further refinement in their reasoning and computational efficiency for practical adoption.",
    "arxiv_url": "https://arxiv.org/abs/2512.03421",
    "authors": [
      "Hexiang Xu",
      "Hengyuan Liu",
      "Yonghao Wu",
      "Xiaolan Kang",
      "Xiang Chen",
      "Yong Liu"
    ],
    "first_author": "Hexiang Xu",
    "category": [
      "Empirical",
      "Benchmark"
    ],
    "field": "Quality Management",
    "task": "Bug Localization",
    "tags": [
      "Novice-program fault dataset construction",
      "LLM vs SBFL/MBFL comparison",
      "Prompt engineering sensitivity",
      "Difficulty-level performance analysis",
      "Over-reasoning in model explanations",
      "User study on explanation usefulness",
      "Model size versus performance analysis",
      "Computational cost and real-time feasibility"
    ],
    "summary": "本文通过构建防止数据泄露的新数据集并在多套公开与自建数据上系统评估多款闭源/开源大模型，分析其在新手程序错误定位中的性能、提示工程敏感性、问题难度影响、过度推理与计算成本等局限，并通过用户研究验证解释性输出对初学者的教学价值。",
    "quality": "High",
    "conference": "Journal of Systems and Software",
    "pdf_url": "https://arxiv.org/pdf/2512.03421v1",
    "published": "2025-12-03",
    "update_time": "2025-12-03",
    "download_time": "2025-12-16 14:37:42"
  },
  {
    "id": "2512.03420",
    "title": "HarnessAgent: Scaling Automatic Fuzzing Harness Construction with Tool-Augmented LLM Pipelines",
    "abstract": "Large language model (LLM)-based techniques have achieved notable progress in generating harnesses for program fuzzing. However, applying them to arbitrary functions (especially internal functions) \\textit{at scale} remains challenging due to the requirement of sophisticated contextual information, such as specification, dependencies, and usage examples. State-of-the-art methods heavily rely on static or incomplete context provisioning, causing failure of generating functional harnesses. Furthermore, LLMs tend to exploit harness validation metrics, producing plausible yet logically useless code. % Therefore, harness generation across large and diverse projects continues to face challenges in reliable compilation, robust code retrieval, and comprehensive validation.   To address these challenges, we present HarnessAgent, a tool-augmented agentic framework that achieves fully automated, scalable harness construction over hundreds of OSS-Fuzz targets. HarnessAgent introduces three key innovations: 1) a rule-based strategy to identify and minimize various compilation errors; 2) a hybrid tool pool for precise and robust symbol source code retrieval; and 3) an enhanced harness validation pipeline that detects fake definitions. We evaluate HarnessAgent on 243 target functions from OSS-Fuzz projects (65 C projects and 178 C++ projects). It improves the three-shot success rate by approximately 20\\% compared to state-of-the-art techniques, reaching 87\\% for C and 81\\% for C++. Our one-hour fuzzing results show that more than 75\\% of the harnesses generated by HarnessAgent increase the target function coverage, surpassing the baselines by over 10\\%. In addition, the hybrid tool-pool system of HarnessAgent achieves a response rate of over 90\\% for source code retrieval, outperforming Fuzz Introspector by more than 30\\%.",
    "arxiv_url": "https://arxiv.org/abs/2512.03420",
    "authors": [
      "Kang Yang",
      "Yunhang Zhang",
      "Zichuan Li",
      "Guanhong Tao",
      "Jun Xu",
      "Xiaojing Liao"
    ],
    "first_author": "Kang Yang",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Software Testing",
    "task": "Test Generation",
    "tags": [
      "Fuzzing harness generation",
      "Compilation-error triage and routing",
      "Hybrid symbol/source retrieval",
      "LSP- and AST-based retrieval backend",
      "Agentic tool-augmented pipeline",
      "Detection of fake definitions during validation",
      "Scalable internal-function targeting",
      "Coverage-driven harness evaluation"
    ],
    "summary": "HarnessAgent 提出了一种工具增强的 agent 框架，通过编译错误分流、混合符号/源码检索与加强的验证管道，实现对大型 C/C++ 代码库中内部函数的可扩展自动化模糊测试 harness 生成，并在 243 个目标上显著提升生成成功率与 fuzz 覆盖率。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.03420v3",
    "published": "2025-12-03",
    "update_time": "2025-12-11",
    "download_time": "2025-12-16 14:38:19"
  },
  {
    "id": "2512.05073",
    "title": "David vs. Goliath: Can Small Models Win Big with Agentic AI in Hardware Design?",
    "abstract": "Large Language Model(LLM) inference demands massive compute and energy, making domain-specific tasks expensive and unsustainable. As foundation models keep scaling, we ask: Is bigger always better for hardware design? Our work tests this by evaluating Small Language Models coupled with a curated agentic AI framework on NVIDIA's Comprehensive Verilog Design Problems(CVDP) benchmark. Results show that agentic workflows: through task decomposition, iterative feedback, and correction - not only unlock near-LLM performance at a fraction of the cost but also create learning opportunities for agents, paving the way for efficient, adaptive solutions in complex design tasks.",
    "arxiv_url": "https://arxiv.org/abs/2512.05073",
    "authors": [
      "Shashwat Shankar",
      "Subhranshu Pandey",
      "Innocent Dengkhw Mochahari",
      "Bhabesh Mali",
      "Animesh Basak Chowdhury",
      "Sukanta Bhattacharjee",
      "Chandan Karfa"
    ],
    "first_author": "Shashwat Shankar",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Completion",
    "tags": [
      "Agentic task decomposition",
      "SLM-aware prompt engineering",
      "Verilog RTL generation",
      "I/O port usage checking",
      "Iterative validation and rollback",
      "Token-budgeted context management",
      "CocoTB-based functional testing",
      "Energy‑efficiency (intelligence-per-watt) tradeoffs"
    ],
    "summary": "该论文提出了一种面向小模型（SLM）的异构 agentic AI 框架，通过任务分解、结构化提示和迭代校验在 NVIDIA 的 CVDP Verilog 设计基准上实现了接近大型模型的硬件设计生成与理解性能，从而显著降低计算与能耗成本。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.05073v1",
    "published": "2025-12-04",
    "update_time": "2025-12-04",
    "download_time": "2025-12-16 14:39:06"
  },
  {
    "id": "2512.04680",
    "title": "Generative AI for Self-Adaptive Systems: State of the Art and Research Roadmap",
    "abstract": "Self-adaptive systems (SASs) are designed to handle changes and uncertainties through a feedback loop with four core functionalities: monitoring, analyzing, planning, and execution. Recently, generative artificial intelligence (GenAI), especially the area of large language models, has shown impressive performance in data comprehension and logical reasoning. These capabilities are highly aligned with the functionalities required in SASs, suggesting a strong potential to employ GenAI to enhance SASs. However, the specific benefits and challenges of employing GenAI in SASs remain unclear. Yet, providing a comprehensive understanding of these benefits and challenges is complex due to several reasons: limited publications in the SAS field, the technological and application diversity within SASs, and the rapid evolution of GenAI technologies. To that end, this paper aims to provide researchers and practitioners a comprehensive snapshot that outlines the potential benefits and challenges of employing GenAI's within SAS. Specifically, we gather, filter, and analyze literature from four distinct research fields and organize them into two main categories to potential benefits: (i) enhancements to the autonomy of SASs centered around the specific functions of the MAPE-K feedback loop, and (ii) improvements in the interaction between humans and SASs within human-on-the-loop settings. From our study, we outline a research roadmap that highlights the challenges of integrating GenAI into SASs. The roadmap starts with outlining key research challenges that need to be tackled to exploit the potential for applying GenAI in the field of SAS. The roadmap concludes with a practical reflection, elaborating on current shortcomings of GenAI and proposing possible mitigation strategies.",
    "arxiv_url": "https://arxiv.org/abs/2512.04680",
    "authors": [
      "Jialong Li",
      "Mingyue Zhang",
      "Nianyu Li",
      "Danny Weyns",
      "Zhi Jin",
      "Kenji Tei"
    ],
    "first_author": "Jialong Li",
    "category": [
      "Survey"
    ],
    "field": "Requirements & Design",
    "task": "Analysis",
    "tags": [
      "MAPE-K enhancement",
      "LLM-assisted analysis",
      "LLM-assisted planning",
      "LLM-assisted monitoring",
      "Human-on-the-loop (HOTL)",
      "Explainability and transparency",
      "Adaptation plan synthesis",
      "Safety and robustness mitigation"
    ],
    "summary": "本文综述了将大型语言模型等生成式AI应用于自适应系统（基于MAPE‑K）的研究现状，归纳其在监测、分析、规划、执行及人机协作方面的潜在增益与挑战，并提出了面向集成与实践的研究路线图与缓解策略。",
    "quality": "High",
    "conference": "ACM Transactions on Autonomous and Adaptive Systems 2024",
    "pdf_url": "https://arxiv.org/pdf/2512.04680v1",
    "published": "2025-12-04",
    "update_time": "2025-12-04",
    "download_time": "2025-12-16 14:39:58"
  },
  {
    "id": "2512.04702",
    "title": "POLARIS: Is Multi-Agentic Reasoning the Next Wave in Engineering Self-Adaptive Systems?",
    "abstract": "The growing scale, complexity, interconnectivity, and autonomy of modern software ecosystems introduce unprecedented uncertainty, challenging the foundations of traditional self-adaptation. Existing approaches, typically rule-driven controllers or isolated learning components, struggle to generalize to novel contexts or coordinate responses across distributed subsystems, leaving them ill-equipped for emergent unknown unknowns. Recent discussions on Self-Adaptation 2.0 emphasize an equal partnership between AI and adaptive systems, merging learning-driven intelligence with adaptive control for predictive and proactive behavior. Building on this foundation, we introduce POLARIS, a three-layer multi-agentic self-adaptation framework that advances beyond reactive adaptation. POLARIS integrates: (1) a low-latency Adapter layer for monitoring and safe execution, (2) a transparent Reasoning layer that generates and verifies plans using tool-aware, explainable agents, and (3) a Meta layer that records experiences and meta-learns improved adaptation policies over time. Through shared knowledge and predictive models, POLARIS handles uncertainty, learns from past actions, and evolves its strategies, enabling systems that anticipate change and maintain resilient, goal-directed behavior. Preliminary evaluation on two self-adaptive exemplars, SWIM and SWITCH, shows that POLARIS consistently outperforms state-of-the-art baselines. We argue this marks a shift toward Self-Adaptation 3.0, akin to Software 3.0: a paradigm where systems not only learn from their environment but also reason about and evolve their own adaptation processes, continuously improving to meet novel challenges.",
    "arxiv_url": "https://arxiv.org/abs/2512.04702",
    "authors": [
      "Divyansh Pandey",
      "Vyakhya Gupta",
      "Prakhar Singhal",
      "Karthik Vaidhyanathan"
    ],
    "first_author": "Divyansh Pandey",
    "category": [
      "Technical"
    ],
    "field": "Self-Adaptive Systems",
    "task": "Multi-Agentic Reasoning & Meta-Learning for Runtime Adaptation",
    "tags": [
      "Multi-agent orchestration for adaptation",
      "Tool-aware explainable reasoning agents",
      "Meta-learning of adaptation policies",
      "World-model based what-if simulation",
      "Verifier agent for plan validation",
      "Low-latency reactive controller (stabilization)",
      "Episodic knowledge base for experience replay"
    ],
    "summary": "本文提出POLARIS——一个三层多代理自适应框架，通过低延迟执行器、可解释的推理代理与记录并元学习的元层，结合世界模型与验证器实现主动、可演化的运行时自适应，并在SWIM与SWITCH示例上优于现有基线。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.04702v2",
    "published": "2025-12-04",
    "update_time": "2025-12-07",
    "download_time": "2025-12-16 14:40:38"
  },
  {
    "id": "2512.04673",
    "title": "Cross-Task Benchmarking and Evaluation of General-Purpose and Code-Specific Large Language Models",
    "abstract": "Large Language Models (LLMs) have revolutionized both general natural language processing and domain-specific applications such as code synthesis, legal reasoning, and finance. However, while prior studies have explored individual model capabilities, a systematic cross-domain comparison that unifies linguistic, reasoning, and code understanding abilities remains underexplored. In this work, we present a comprehensive evaluation of five general-purpose and three code-specific state-of-the-art LLMs across six diverse benchmarks encompassing linguistic competence, mathematical reasoning, and trustworthiness. Additionally, we analyze model behavior on the CoNaLa dataset for code explanation, comparing natural language and code-specialized LLMs. Our findings reveal that models optimized for code (e.g., CodeLLaMA variants) exhibit strong reasoning and syntactic precision, that even for non-coding tasks can show measurable performance gains, in contrast to general-purpose models like Mistral-7B and Llama-3-8B.",
    "arxiv_url": "https://arxiv.org/abs/2512.04673",
    "authors": [
      "Gunjan Das",
      "Paheli Bhattacharya",
      "Rishabh Gupta"
    ],
    "first_author": "Gunjan Das",
    "category": [
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Summarization",
    "tags": [
      "Cross-domain benchmarking",
      "Code vs general-purpose model comparison",
      "Code explanation / intent generation",
      "Trustworthiness evaluation",
      "Commonsense and mathematical reasoning assessment",
      "Consolidation of lexical and embedding-based metrics"
    ],
    "summary": "本文系统比较评估了多款通用与代码专用大型语言模型在多项自然语言与代码解释基准上的表现，发现代码专用模型在推理能力与语法精确性上具有显著优势并在某些非代码任务上也能带来提升。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.04673v1",
    "published": "2025-12-04",
    "update_time": "2025-12-04",
    "download_time": "2025-12-16 14:41:13"
  },
  {
    "id": "2512.05100",
    "title": "Structured Document Translation via Format Reinforcement Learning",
    "abstract": "Recent works on structured text translation remain limited to the sentence level, as they struggle to effectively handle the complex document-level XML or HTML structures. To address this, we propose \\textbf{Format Reinforcement Learning (FormatRL)}, which employs Group Relative Policy Optimization on top of a supervised fine-tuning model to directly optimize novel structure-aware rewards: 1) TreeSim, which measures structural similarity between predicted and reference XML trees and 2) Node-chrF, which measures translation quality at the level of XML nodes. Additionally, we apply StrucAUC, a fine-grained metric distinguishing between minor errors and major structural failures. Experiments on the SAP software-documentation benchmark demonstrate improvements across six metrics and an analysis further shows how different reward functions contribute to improvements in both structural and translation quality.",
    "arxiv_url": "https://arxiv.org/abs/2512.05100",
    "authors": [
      "Haiyue Song",
      "Johannes Eschbach-Dymanus",
      "Hour Kaing",
      "Sumire Honda",
      "Hideki Tanaka",
      "Bianka Buschbeck",
      "Masao Utiyama"
    ],
    "first_author": "Haiyue Song",
    "category": [
      "Technical"
    ],
    "field": "Structured Document Translation",
    "task": "Structured Document Translation",
    "tags": [
      "Format-aware Reinforcement Learning",
      "Group Relative Policy Optimization",
      "Tree edit-distance reward (TreeSim)",
      "Node-level chrF reward",
      "Structure-Aware AUC (StrucAUC)",
      "XML/HTML structure preservation",
      "Synthetic markup injection for data augmentation",
      "Document-level localization"
    ],
    "summary": "本文提出FORMATRL，一种在监督微调基础上使用GRPO并通过TreeSim和Node-chrF等结构感知奖励直接优化XML/HTML文档结构保真度与节点级翻译质量的强化学习方法，并在软件文档数据集上显著提升了结构与翻译评估指标。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.05100v1",
    "published": "2025-12-04",
    "update_time": "2025-12-04",
    "download_time": "2025-12-16 14:41:56"
  },
  {
    "id": "2512.04785",
    "title": "ASTRIDE: A Security Threat Modeling Platform for Agentic-AI Applications",
    "abstract": "AI agent-based systems are becoming increasingly integral to modern software architectures, enabling autonomous decision-making, dynamic task execution, and multimodal interactions through large language models (LLMs). However, these systems introduce novel and evolving security challenges, including prompt injection attacks, context poisoning, model manipulation, and opaque agent-to-agent communication, that are not effectively captured by traditional threat modeling frameworks. In this paper, we introduce ASTRIDE, an automated threat modeling platform purpose-built for AI agent-based systems. ASTRIDE extends the classical STRIDE framework by introducing a new threat category, A for AI Agent-Specific Attacks, which encompasses emerging vulnerabilities such as prompt injection, unsafe tool invocation, and reasoning subversion, unique to agent-based applications. To automate threat modeling, ASTRIDE combines a consortium of fine-tuned vision-language models (VLMs) with the OpenAI-gpt-oss reasoning LLM to perform end-to-end analysis directly from visual agent architecture diagrams, such as data flow diagrams(DFDs). LLM agents orchestrate the end-to-end threat modeling automation process by coordinating interactions between the VLM consortium and the reasoning LLM. Our evaluations demonstrate that ASTRIDE provides accurate, scalable, and explainable threat modeling for next-generation intelligent systems. To the best of our knowledge, ASTRIDE is the first framework to both extend STRIDE with AI-specific threats and integrate fine-tuned VLMs with a reasoning LLM to fully automate diagram-driven threat modeling in AI agent-based applications.",
    "arxiv_url": "https://arxiv.org/abs/2512.04785",
    "authors": [
      "Eranga Bandara",
      "Amin Hass",
      "Ross Gore",
      "Sachin Shetty",
      "Ravi Mukkamala",
      "Safdar H. Bouk",
      "Xueping Liang",
      "Ng Wee Keong",
      "Kasun De Zoysa",
      "Aruna Withanage",
      "Nilaan Loganathan"
    ],
    "first_author": "Eranga Bandara",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Requirements & Design",
    "task": "Analysis",
    "tags": [
      "Automated diagram-driven threat modeling",
      "STRIDE extension for AI agents",
      "AI agent-specific threats",
      "Vision-language ensemble for diagram parsing",
      "Prompt injection and instruction-manipulation detection",
      "Reasoning-based threat synthesis",
      "Quantized low-latency fine-tuning for edge inference",
      "Explainable security assessments"
    ],
    "summary": "本文提出ASTRIDE，一种将STRIDE扩展以包含AI代理特有攻击并结合微调视觉—语言模型集群与推理LLM，从架构图（如数据流图）自动生成可解释威胁模型的平台。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.04785v1",
    "published": "2025-12-04",
    "update_time": "2025-12-04",
    "download_time": "2025-12-16 14:42:40"
  },
  {
    "id": "2512.04611",
    "title": "PBFuzz: Agentic Directed Fuzzing for PoV Generation",
    "abstract": "Proof-of-Vulnerability (PoV) input generation is a critical task in software security and supports downstream applications such as path generation and validation. Generating a PoV input requires solving two sets of constraints: (1) reachability constraints for reaching vulnerable code locations, and (2) triggering constraints for activating the target vulnerability. Existing approaches, including directed greybox fuzzing and LLM-assisted fuzzing, struggle to efficiently satisfy these constraints. This work presents an agentic method that mimics human experts. Human analysts iteratively study code to extract semantic reachability and triggering constraints, form hypotheses about PoV triggering strategies, encode them as test inputs, and refine their understanding using debugging feedback. We automate this process with an agentic directed fuzzing framework called PBFuzz. PBFuzz tackles four challenges in agentic PoV generation: autonomous code reasoning for semantic constraint extraction, custom program-analysis tools for targeted inference, persistent memory to avoid hypothesis drift, and property-based testing for efficient constraint solving while preserving input structure. Experiments on the Magma benchmark show strong results. PBFuzz triggered 57 vulnerabilities, surpassing all baselines, and uniquely triggered 17 vulnerabilities not exposed by existing fuzzers. PBFuzz achieved this within a 30-minute budget per target, while conventional approaches use 24 hours. Median time-to-exposure was 339 seconds for PBFuzz versus 8680 seconds for AFL++ with CmpLog, giving a 25.6x efficiency improvement with an API cost of 1.83 USD per vulnerability.",
    "arxiv_url": "https://arxiv.org/abs/2512.04611",
    "authors": [
      "Haochen Zeng",
      "Andrew Bao",
      "Jiajun Cheng",
      "Chengyu Song"
    ],
    "first_author": "Haochen Zeng",
    "category": [
      "Technical"
    ],
    "field": "Software Testing",
    "task": "Test Generation",
    "tags": [
      "Agentic reasoning",
      "Directed fuzzing",
      "Proof-of-vulnerability generation",
      "Property-based test generation",
      "Semantic reachability and triggering constraint extraction",
      "Persistent agent memory",
      "On-demand program analysis tooling",
      "Fine-grained execution feedback"
    ],
    "summary": "本文提出PBFuzz，一种将自治式代码推理、按需程序分析与基于属性的测试相结合的定向模糊测试框架，用于高效自动生成Proof‑of‑Vulnerability输入并在Magma基准上显著超越现有方法。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.04611v1",
    "published": "2025-12-04",
    "update_time": "2025-12-04",
    "download_time": "2025-12-16 14:43:21"
  },
  {
    "id": "2512.04538",
    "title": "Completion by Comprehension: Guiding Code Generation with Multi-Granularity Understanding",
    "abstract": "As code completion task from function-level to repository-level, leveraging contextual information from large-scale codebases becomes a core challenge. However, existing retrieval-augmented generation (RAG) methods typically treat code as plain natural language, relying primarily on shallow semantic matching while overlooking structural semantics and code-specific dependencies. This limits their ability to capture control flow and underlying intent, ultimately constraining the quality of generated code. Therefore, we propose CoCo, a novel framework that enables code Completion by Comprehension of multi-granularity context from large-scale code repositories. CoCo employs static code analysis to extract structured context at the function, file, and project levels, capturing execution logic and semantic dependencies. It then adopts an graph-based multi-granularity context selection mechanism to filter out redundant information and remove noise. Consequently, the information is converted into natural language in a consistent manner, thereby functioning as explicit contextual prompts to guide subsequent code completion. Additionally, a structure-aware code re-ranker mechanism ensures alignment at both semantic and structural levels. Extensive experiments on CrossCodeEval and RepoEval benchmarks demonstrate that CoCo consistently surpasses state-of-the-art baselines, achieving up to 20.2% gains in EM. Moreover, the framework is model-agnostic and can be seamlessly integrated into existing methods, leading to significant performance.",
    "arxiv_url": "https://arxiv.org/abs/2512.04538",
    "authors": [
      "Xinkui Zhao",
      "Rongkai Liu",
      "Yifan Zhang",
      "Chen Zhi",
      "Lufei Zhang",
      "Guanjie Cheng",
      "Yueshen Xu",
      "Shuiguang Deng",
      "Jianwei Yin"
    ],
    "first_author": "Xinkui Zhao",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Completion",
    "tags": [
      "Static code analysis for multi-granularity context",
      "Function-file-repository context modeling",
      "Graph-based context selection",
      "Structure-aware code re-ranking",
      "Natural-language prompt synthesis from structural context",
      "Repository-level dependency and control-flow modeling",
      "Retrieval-augmented generation integration"
    ],
    "summary": "本文提出CoCo，通过静态分析提取函数/文件/仓库三级结构化上下文、用图筛选相关信息并将其转化为自然语言提示，结合结构感知的代码重排序，显著改进检索增强的仓库级代码补全性能。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.04538v1",
    "published": "2025-12-04",
    "update_time": "2025-12-04",
    "download_time": "2025-12-16 14:44:03"
  },
  {
    "id": "2512.04419",
    "title": "Solving LLM Repetition Problem in Production: A Comprehensive Study of Multiple Solutions",
    "abstract": "The repetition problem, where Large Language Models (LLMs) continuously generate repetitive content without proper termination, poses a critical challenge in production deployments, causing severe performance degradation and system stalling. This paper presents a comprehensive investigation and multiple practical solutions for the repetition problem encountered in real-world batch code interpretation tasks.   We identify three distinct repetition patterns: (1) business rule generation repetition, (2) method call relationship analysis repetition, and (3) PlantUML diagram syntax generation repetition. Through rigorous theoretical analysis based on Markov models, we establish that the root cause lies in greedy decoding's inability to escape repetitive loops, exacerbated by self-reinforcement effects.   Our comprehensive experimental evaluation demonstrates three viable solutions: (1) Beam Search decoding with early_stopping=True serves as a universal post-hoc mechanism that effectively resolves all three repetition patterns; (2) presence_penalty hyperparameter provides an effective solution specifically for BadCase 1; and (3) Direct Preference Optimization (DPO) fine-tuning offers a universal model-level solution for all three BadCases.   The primary value of this work lies in combining first-hand production experience with extensive experimental validation. Our main contributions include systematic theoretical analysis of repetition mechanisms, comprehensive evaluation of multiple solutions with task-specific applicability mapping, identification of early_stopping as the critical parameter for Beam Search effectiveness, and practical production-ready solutions validated in real deployment environments.",
    "arxiv_url": "https://arxiv.org/abs/2512.04419",
    "authors": [
      "Weiwei Wang",
      "Weijie Zou",
      "Jiyong Min"
    ],
    "first_author": "Weiwei Wang",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Understanding",
    "tags": [
      "Generation repetition (repeater)",
      "Greedy decoding loop",
      "Beam search with early_stopping",
      "Presence_penalty tuning",
      "Direct Preference Optimization (DPO) fine‑tuning",
      "Markov-chain theoretical analysis",
      "Production batch code interpretation",
      "Framework parameter integration (FastChat↔vLLM)"
    ],
    "summary": "本文基于批量代码解释的生产实践，利用马尔可夫模型分析LLM重复生成的根因，并通过大规模实验验证三种可行解决方案（启用early_stopping的束搜索、presence_penalty调优与DPO微调），给出任务适配与生产部署建议。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.04419v1",
    "published": "2025-12-04",
    "update_time": "2025-12-04",
    "download_time": "2025-12-16 14:44:59"
  },
  {
    "id": "2512.05908",
    "title": "Natural Language Summarization Enables Multi-Repository Bug Localization by LLMs in Microservice Architectures",
    "abstract": "Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code, LLM context limitations, and the need to first identify the correct repository. We propose reframing this as a natural language reasoning task by transforming codebases into hierarchical NL summaries and performing NL-to-NL search instead of cross-modal retrieval. Our approach builds context-aware summaries at file, directory, and repository levels, then uses a two-phase search: first routing bug reports to relevant repositories, then performing top-down localization within those repositories. Evaluated on DNext, an industrial system with 46 repositories and 1.1M lines of code, our method achieves Pass@10 of 0.82 and MRR of 0.50, significantly outperforming retrieval baselines and agentic RAG systems like GitHub Copilot and Cursor. This work demonstrates that engineered natural language representations can be more effective than raw source code for scalable bug localization, providing an interpretable repository -> directory -> file search path, which is vital for building trust in enterprise AI tools by providing essential transparency.",
    "arxiv_url": "https://arxiv.org/abs/2512.05908",
    "authors": [
      "Amirkia Rafiei Oskooei",
      "S. Selcan Yukcu",
      "Mehmet Cevheri Bozoglan",
      "Mehmet S. Aktas"
    ],
    "first_author": "Amirkia Rafiei Oskooei",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Quality Management",
    "task": "Bug Localization",
    "tags": [
      "Context-aware code summarization",
      "Hierarchical NL knowledge base",
      "Repository routing",
      "Top-down directory→file localization",
      "NL-to-NL retrieval",
      "Explainable repository→directory→file search",
      "Chain-of-Thought prompting",
      "Multi-repository microservice bug localization"
    ],
    "summary": "本文提出将多仓库微服务代码库转换为层次化、上下文感知的自然语言摘要，并通过先路由到候选仓库再自顶向下目录与文件检索的双阶段搜索，将错误定位转化为NL-to-NL推理，从而在工业规模数据上显著优于基于原始代码的检索与RAG方法。",
    "quality": "High",
    "conference": "LLM4Code Workshop, ICSE 2026",
    "pdf_url": "https://arxiv.org/pdf/2512.05908v1",
    "published": "2025-12-05",
    "update_time": "2025-12-05",
    "download_time": "2025-12-16 14:45:35"
  },
  {
    "id": "2512.05887",
    "title": "Bootstrapping Fuzzers for Compilers of Low-Resource Language Dialects Using Language Models",
    "abstract": "Modern extensible compiler frameworks-such as MLIR-enable rapid creation of domain-specific language dialects. This flexibility, however, makes correctness harder to ensure as the same extensibility that accelerates development also complicates maintaining the testing infrastructure. Extensible languages require automated test generation that is both dialect-agnostic (works across dialects without manual adaptation) and dialect-effective (targets dialect-specific features to find bugs). Existing approaches typically sacrifice one of these goals by either requiring manually constructed seed corpora for each dialect, or by failing to be effective. We present a dialect-agnostic and dialect-effective grammar-based and coverage-guided fuzzing approach for extensible compilers that combines two key insights from existing work: (i) the grammars of dialects, which already encode the structural and type constraints, can often be extracted automatically from the dialect specification; and (ii) these grammars can be used in combination with pre-trained large language models to automatically generate representative and diverse seed inputs from the full dialect space without requiring any manual input or training data. These seeds can then be used to bootstrap coverage-guided fuzzers. We built this approach into a tool, Germinator. When evaluated on six MLIR projects spanning 91 dialects, Germinator generated seeds improve line coverage by 10-120% over grammar-based baselines. We compare against grammar-based baselines because they are the only class of existing automatic seed generators that can be applied uniformly across MLIR's heterogeneous dialect ecosystem. Germinator discovers 88 previously unknown bugs (40 confirmed), including 23 in dialects with no prior automated test generators, demonstrating effective and controllable testing of low-resource dialects at scale.",
    "arxiv_url": "https://arxiv.org/abs/2512.05887",
    "authors": [
      "Sairam Vaidya",
      "Marcel Böhme",
      "Loris D'Antoni"
    ],
    "first_author": "Sairam Vaidya",
    "category": [
      "Technical"
    ],
    "field": "Software Testing",
    "task": "Test Generation",
    "tags": [
      "TableGen grammar extraction",
      "Grammar-constrained decoding",
      "Seed program generation",
      "Coverage-guided fuzzing integration",
      "Dialect-agnostic fuzzing",
      "Low-resource MLIR dialects",
      "Compiler fuzzing",
      "Bug discovery"
    ],
    "summary": "本文提出 Germinator，通过从 MLIR 的 TableGen 自动提取语法并对预训练语言模型进行语法约束采样生成多样化种子，以引导覆盖驱动的模糊测试，从而在 91 个方言上显著提升覆盖率并发现大量新缺陷。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.05887v1",
    "published": "2025-12-05",
    "update_time": "2025-12-05",
    "download_time": "2025-12-16 14:46:15"
  },
  {
    "id": "2512.07814",
    "title": "Understanding Privacy Risks in Code Models Through Training Dynamics: A Causal Approach",
    "abstract": "Large language models for code (LLM4Code) have greatly improved developer productivity but also raise privacy concerns due to their reliance on open-source repositories containing abundant personally identifiable information (PII). Prior work shows that commercial models can reproduce sensitive PII, yet existing studies largely treat PII as a single category and overlook the heterogeneous risks among different types. We investigate whether distinct PII types vary in their likelihood of being learned and leaked by LLM4Code, and whether this relationship is causal. Our methodology includes building a dataset with diverse PII types, fine-tuning representative models of different scales, computing training dynamics on real PII data, and formulating a structural causal model to estimate the causal effect of learnability on leakage. Results show that leakage risks differ substantially across PII types and correlate with their training dynamics: easy-to-learn instances such as IP addresses exhibit higher leakage, while harder types such as keys and passwords leak less frequently. Ambiguous types show mixed behaviors. This work provides the first causal evidence that leakage risks are type-dependent and offers guidance for developing type-aware and learnability-aware defenses for LLM4Code.",
    "arxiv_url": "https://arxiv.org/abs/2512.07814",
    "authors": [
      "Hua Yang",
      "Alejandro Velasco",
      "Sen Fang",
      "Bowen Xu",
      "Denys Poshyvanyk"
    ],
    "first_author": "Hua Yang",
    "category": [
      "Empirical",
      "Benchmark"
    ],
    "field": "Privacy & Security",
    "task": "PII Leakage Analysis (causal)",
    "tags": [
      "PII Types",
      "Training Dynamics",
      "Memorization and Leakage",
      "Structural Causal Model",
      "PII Dataset from Code",
      "LLM4Code Fine-tuning",
      "Type-aware Privacy Risk",
      "Leakage Attack Evaluation"
    ],
    "summary": "本文构建多类型PII数据集并在不同规模和架构的代码模型上计算训练动态，基于结构因果模型实证证明不同PII类型的可学性与推理时泄露风险存在因果关联，为类型感知的防护策略提供依据。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.07814v1",
    "published": "2025-12-08",
    "update_time": "2025-12-08",
    "download_time": "2025-12-10 01:51:03"
  },
  {
    "id": "2512.07666",
    "title": "Bridging Code Graphs and Large Language Models for Better Code Understanding",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in code intelligence tasks such as code generation, summarization, and translation. However, their reliance on linearized token sequences limits their ability to understand the structural semantics of programs. While prior studies have explored graphaugmented prompting and structure-aware pretraining, they either suffer from prompt length constraints or require task-specific architectural changes that are incompatible with large-scale instructionfollowing LLMs. To address these limitations, this paper proposes CGBridge, a novel plug-and-play method that enhances LLMs with Code Graph information through an external, trainable Bridge module. CGBridge first pre-trains a code graph encoder via selfsupervised learning on a large-scale dataset of 270K code graphs to learn structural code semantics. It then trains an external module to bridge the modality gap among code, graph, and text by aligning their semantics through cross-modal attention mechanisms. Finally, the bridge module generates structure-informed prompts, which are injected into a frozen LLM, and is fine-tuned for downstream code intelligence tasks. Experiments show that CGBridge achieves notable improvements over both the original model and the graphaugmented prompting method. Specifically, it yields a 16.19% and 9.12% relative gain in LLM-as-a-Judge on code summarization, and a 9.84% and 38.87% relative gain in Execution Accuracy on code translation. Moreover, CGBridge achieves over 4x faster inference than LoRA-tuned models, demonstrating both effectiveness and efficiency in structure-aware code understanding.",
    "arxiv_url": "https://arxiv.org/abs/2512.07666",
    "authors": [
      "Zeqi Chen",
      "Zhaoyang Chu",
      "Yi Gui",
      "Feng Guo",
      "Yao Wan",
      "Chuan Shi"
    ],
    "first_author": "Zeqi Chen",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Summarization",
    "tags": [
      "Code Property Graph (CPG)",
      "GNN-based code graph encoder",
      "Self-supervised graph pretraining",
      "Cross-modal attention / alignment",
      "Bridge module (plug-and-play)",
      "Structure-informed soft prompts",
      "Graph-text contrastive learning",
      "Code translation"
    ],
    "summary": "本文提出CGBridge——一种可插拔的桥接模块，通过对约27万条异构代码图进行自监督预训练、采用跨模态对齐（图-文本对比与匹配）并生成结构感知提示注入冻结的LLM，从而在代码摘要与翻译等代码理解任务上显著提升性能并提高推理效率。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.07666v1",
    "published": "2025-12-08",
    "update_time": "2025-12-08",
    "download_time": "2025-12-10 01:51:28"
  },
  {
    "id": "2512.07631",
    "title": "The Agent Capability Problem: Predicting Solvability Through Information-Theoretic Bounds",
    "abstract": "When should an autonomous agent commit resources to a task? We introduce the Agent Capability Problem (ACP), a framework for predicting whether an agent can solve a problem under resource constraints. Rather than relying on empirical heuristics, ACP frames problem-solving as information acquisition: an agent requires $\\Itotal$ bits to identify a solution and gains $\\Istep$ bits per action at cost $\\Cstep$, yielding an effective cost $\\Ceff = (\\Itotal/\\Istep), \\Cstep$ that predicts resource requirements before search. We prove that $\\Ceff$ lower-bounds expected cost and provide tight probabilistic upper bounds. Experimental validation shows that ACP predictions closely track actual agent performance, consistently bounding search effort while improving efficiency over greedy and random strategies. The framework generalizes across LLM-based and agentic workflows, linking principles from active learning, Bayesian optimization, and reinforcement learning through a unified information-theoretic lens. \\",
    "arxiv_url": "https://arxiv.org/abs/2512.07631",
    "authors": [
      "Shahar Lutati"
    ],
    "first_author": "Shahar Lutati",
    "category": [
      "Technical"
    ],
    "field": "Agent Decision & Planning",
    "task": "Solvability Prediction / Resource Allocation",
    "tags": [
      "Agent Capability Problem",
      "Solvability Prediction",
      "Information-Theoretic Bounds",
      "Effective Cost (Ceffective)",
      "Mutual Information per Action",
      "Stopping Time Analysis",
      "Lorden's Inequality",
      "Gaussian Process Approximation",
      "Resource Allocation for LLM-based Agents"
    ],
    "summary": "本文提出了代理能力问题（ACP），用信息论视角将解决所需信息量 Itotal 与每步信息增益 Is 及每步代价 Cs 结合成 Ceffective，并给出该量的下界与概率性上界以在资源受限下预测并引导代理求解可行性，实验以高斯过程近似验证了理论预测的有效性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.07631v1",
    "published": "2025-12-08",
    "update_time": "2025-12-08",
    "download_time": "2025-12-10 01:55:01"
  },
  {
    "id": "2512.08867",
    "title": "SimpleDevQA: Benchmarking Large Language Models on Development Knowledge QA",
    "abstract": "The Development Knowledge Question Answering (Dev Knowledge QA) task aims to provide natural language answers to knowledge-seeking questions during software development. To investigate its importance and to what extent it has been explored, we analyze real user-LLM dialogues from WildChat and find that: (1) The Dev Knowledge QA task accounts for 39.6% of interactions(highest among all tasks), revealing broad knowledge needs beyond code generation (32.3%). (2) Only 27.5% of real Dev Knowledge QA dialogues focus on code understanding, leaving out development knowledge-seeking. (3) Only 17.1% of real-world Dev Knowledge QA dialogues can be used for constructing a benchmark. Existing benchmarks have two primary limitations for evaluating the Dev Knowledge QA capability of LLMs. First, existing benchmarks offer a limited development knowledge scope, mainly focusing on code understanding and neglecting broader knowledge during development. Second, some benchmarks are not built from real user queries. To bridge this gap, we design a three-phase pipeline that transforms real-world dialogue into simple development knowledge-seeking QA pairs. Through this pipeline, we introduce SimpleDevQA, a multilingual benchmark derived from real user dialogues. It contains 2,740 QA pairs in three languages (English, Chinese, and Russian), and focuses on questions with unique, short, and verifiable answers for accurate and simple evaluation. Experiments show that: Code LLMs generally outperform general LLMs of similar scale; Knowledge injection with the Retrieval-Augmented Generation (RAG) strategy can boost LLM accuracy by 11.3% on average; LLMs show systematic overconfidence in Dev Knowledge QA, and the answering accuracy of LLMs shows a positive correlation with their stated confidence; Generally, LLMs with stronger code generation performance also exhibit stronger performance in Dev Knowledge QA.",
    "arxiv_url": "https://arxiv.org/abs/2512.08867",
    "authors": [
      "Jing Zhang",
      "Lianghong Guo",
      "Yanlin Wang",
      "Mingwei Liu",
      "Jiachi Chen",
      "Yuchi Ma",
      "Ensheng Shi",
      "Terry Yue Zhuo",
      "Hongyu Zhang",
      "Zibin Zheng"
    ],
    "first_author": "Jing Zhang",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Development Knowledge",
    "task": "Development Knowledge QA",
    "tags": [
      "Benchmark Construction",
      "Dev Knowledge QA",
      "Multilingual Dataset",
      "WildChat Dialogue Mining",
      "Data Pipeline",
      "Reference Retrieval",
      "RAG (Retrieval-Augmented Generation)",
      "LLM Evaluation",
      "Factuality Verification",
      "Human Annotation",
      "Closed-source vs Open-source Comparison",
      "Code LLM vs General LLM Performance",
      "Overconfidence Analysis"
    ],
    "summary": "本文提出SimpleDevQA——一个基于真实用户对话、覆盖英中俄三语的开发知识问答基准及其三阶段构建流水线，并通过对18种主流LLM的评测揭示了RAG可提升性能、闭源/代码模型普遍优于开源/通用模型以及模型过度自信等现象。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.08867v1",
    "published": "2025-12-09",
    "update_time": "2025-12-09",
    "download_time": "2025-12-11 01:52:20"
  },
  {
    "id": "2512.08810",
    "title": "Multicalibration for LLM-based Code Generation",
    "abstract": "As AI-based code generation becomes widespread, researchers are investigating the calibration of code LLMs - ensuring their confidence scores faithfully represent the true likelihood of code correctness. To do so, we investigate multicalibration, which can capture additional factors about a coding problem, such as complexity, code length, or programming language used. We study four multicalibration approaches on three function synthesis benchmarks, using latest-generation code LLMs (Qwen3 Coder, GPT-OSS, DeepSeek-R1-Distill). Our results demonstrate that multicalibration can yield distinct improvements over both uncalibrated token likelihoods (+1.03 in skill score) and baseline calibrations (+0.37 in skill score). We study the influence of the aforementioned factors in ablations, and make our dataset (consisting of code generations, likelihoods, and correctness labels) available for future research on code LLM calibration.",
    "arxiv_url": "https://arxiv.org/abs/2512.08810",
    "authors": [
      "Viola Campos",
      "Robin Kuschnereit",
      "Adrian Ulges"
    ],
    "first_author": "Viola Campos",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Uncertainty Estimation & Calibration",
    "tags": [
      "Multicalibration",
      "Post-hoc calibration",
      "Uncertainty estimation",
      "Group-aware calibration (complexity/code-length/prompt-length/language)",
      "Brier Skill Score",
      "Expected Calibration Error",
      "Token likelihood aggregation",
      "CALIBRI dataset",
      "Function synthesis benchmarks",
      "Code LLM evaluation (Qwen3/GPT-OSS/DeepSeek)"
    ],
    "summary": "本文首次将多组校准(multicalibration)方法应用于代码生成LLM，通过按复杂度、代码/提示长度及编程语言分组的后验校准显著提高置信度估计的可靠性，并发布了包含生成代码、似然与正确性标签的CALIBRI数据集以促进后续研究。",
    "quality": "High",
    "conference": "AI-SQE 2026 (The 1st International Workshop on AI for Software Quality Evaluation: Judgment, Metrics, Benchmarks, and Beyond) 2026",
    "pdf_url": "https://arxiv.org/pdf/2512.08810v1",
    "published": "2025-12-09",
    "update_time": "2025-12-09",
    "download_time": "2025-12-11 01:52:53"
  },
  {
    "id": "2512.08769",
    "title": "A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows",
    "abstract": "Agentic AI marks a major shift in how autonomous systems reason, plan, and execute multi-step tasks. Unlike traditional single model prompting, agentic workflows integrate multiple specialized agents with different Large Language Models(LLMs), tool-augmented capabilities, orchestration logic, and external system interactions to form dynamic pipelines capable of autonomous decision-making and action. As adoption accelerates across industry and research, organizations face a central challenge: how to design, engineer, and operate production-grade agentic AI workflows that are reliable, observable, maintainable, and aligned with safety and governance requirements. This paper provides a practical, end-to-end guide for designing, developing, and deploying production-quality agentic AI systems. We introduce a structured engineering lifecycle encompassing workflow decomposition, multi-agent design patterns, Model Context Protocol(MCP), and tool integration, deterministic orchestration, Responsible-AI considerations, and environment-aware deployment strategies. We then present nine core best practices for engineering production-grade agentic AI workflows, including tool-first design over MCP, pure-function invocation, single-tool and single-responsibility agents, externalized prompt management, Responsible-AI-aligned model-consortium design, clean separation between workflow logic and MCP servers, containerized deployment for scalable operations, and adherence to the Keep it Simple, Stupid (KISS) principle to maintain simplicity and robustness. To demonstrate these principles in practice, we present a comprehensive case study: a multimodal news-analysis and media-generation workflow. By combining architectural guidance, operational patterns, and practical implementation insights, this paper offers a foundational reference to build robust, extensible, and production-ready agentic AI workflows.",
    "arxiv_url": "https://arxiv.org/abs/2512.08769",
    "authors": [
      "Eranga Bandara",
      "Ross Gore",
      "Peter Foytik",
      "Sachin Shetty",
      "Ravi Mukkamala",
      "Abdul Rahman",
      "Xueping Liang",
      "Safdar H. Bouk",
      "Amin Hass",
      "Sachini Rajapakse",
      "Ng Wee Keong",
      "Kasun De Zoysa",
      "Aruna Withanage",
      "Nilaan Loganathan"
    ],
    "first_author": "Eranga Bandara",
    "category": [
      "Survey",
      "Technical"
    ],
    "field": "Requirements & Design",
    "task": "Management",
    "tags": [
      "Agentic AI",
      "Multi-Agent Workflows",
      "Model Context Protocol (MCP)",
      "Tool Integration",
      "Orchestration",
      "Deterministic Execution",
      "Responsible AI",
      "Deployment & Containerization",
      "Prompt Management",
      "Single-Responsibility Agents",
      "Case Study: Multimodal News Analysis"
    ],
    "summary": "本文提供了一套面向生产的 agentic AI 工作流的端到端工程指南，包含工作流分解、多代理设计模式、Model Context Protocol、工具集成、确定性编排、责任化 AI 考量及部署策略，并通过多模态新闻分析案例演示最佳实践。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.08769v1",
    "published": "2025-12-09",
    "update_time": "2025-12-09",
    "download_time": "2025-12-11 01:57:12"
  },
  {
    "id": "2512.09679",
    "title": "Understanding Chain-of-Thought Effectiveness in Code Generation: An Empirical and Information-Theoretic Analysis",
    "abstract": "Large language models (LLMs) achieve strong performance on code generation, but the mechanisms by which Chain-of-Thought (CoT) prompting helps remain unclear. We present a systematic empirical and information-theoretic study of CoT effectiveness in neural code generation, evaluating five paradigms (Zero-Shot, Zero-Shot CoT, Self-Planning, Structured CoT, Reasoning-CoT) across six Python benchmarks, a multilingual benchmark with 12 programming languages, and six models from 7B to 480B parameters, using conditional mutual information $I(Y;C|X)$ as a conceptual lens. Our results show that externally guided CoT consistently outperforms direct generation, with structured methods improving Pass@1 by 5--12\\% on average while using substantially fewer tokens than reflective reasoning, and that CoT benefits depend on language type systems and model capacity. We further find that reasoning \\emph{quality} is critical: high-quality structured CoT from strong generators yields significantly higher accuracy than lightweight alternatives with the same template, whereas naive Zero-Shot CoT can even degrade performance. These findings provide practical guidance for choosing CoT strategies based on model capacity, language characteristics, and task complexity.",
    "arxiv_url": "https://arxiv.org/abs/2512.09679",
    "authors": [
      "Naizhu Jin",
      "Zhong Li",
      "Guang Yang",
      "Tian Zhang",
      "Qingkai Zeng"
    ],
    "first_author": "Naizhu Jin",
    "category": [
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Chain-of-Thought prompting",
      "Structured CoT (SCoT)",
      "Self-planning prompts",
      "Reflective reasoning",
      "Conditional mutual information",
      "Information density metric",
      "Cross-language generalization",
      "Model scale / capacity effects",
      "Token-efficiency vs accuracy",
      "Reasoning quality evaluation"
    ],
    "summary": "本文提出基于条件互信息的信息论框架，并通过跨语言、跨模型的大规模实证实验系统地评估多种 Chain-of-Thought 提示范式在代码生成中对准确性与效率的影响，发现结构化 CoT 在令牌效率和稳定性上优于反思式方法且推理质量与模型容量显著影响效果。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.09679v1",
    "published": "2025-12-10",
    "update_time": "2025-12-10",
    "download_time": "2025-12-12 01:51:32"
  },
  {
    "id": "2512.09627",
    "title": "LogICL: Distilling LLM Reasoning to Bridge the Semantic Gap in Cross-Domain Log Anomaly Detection",
    "abstract": "Effective log anomaly detection is critical to sustaining reliability in large-scale IT infrastructures. Transformer-based models require substantial resources and labeled data, exacerbating the cold-start problem in target domains where logs are scarce. Existing cross-domain methods leverage source logs but struggle with generalization due to reliance on surface lexical similarity, failing to capture latent semantic equivalence amid structural divergences. To address this, we propose LogICL, a framework distilling Large Language Model (LLM) reasoning into a lightweight encoder for cross-domain anomaly detection. During training, LogICL constructs a delta matrix measuring the utility of demonstrations selected via Maximal Marginal Relevance relative to zero-shot inference. The encoder is optimized via a multi-objective loss comprising an ICL-Guided term that aligns representations based on reasoning assistance utility, maximum mean discrepancy for domain alignment, and supervised contrastive loss. At inference, the optimized encoder retrieves reasoning-aware demonstrations using semantic similarity and delta scores, enabling frozen-LLM in-context learning with Chain-of-Thought for accurate and interpretable detection. Experiments on few-shot and zero-shot cross-domain benchmarks confirm LogICL achieves state-of-the-art performance across heterogeneous systems. Further analysis via visualizations and case studies confirms LogICL bridges the semantic gap beyond surface lexical similarity, effectively capturing latent semantic equivalence for rapid deployment.",
    "arxiv_url": "https://arxiv.org/abs/2512.09627",
    "authors": [
      "Jingwei Ye",
      "Zhi Wang",
      "Chenbin Su",
      "Jieshuai Yang",
      "Jiayi Ding",
      "Chunbo Liu",
      "Ge Chu"
    ],
    "first_author": "Jingwei Ye",
    "category": [
      "Technical"
    ],
    "field": "AIOps",
    "task": "Log Anomaly Detection",
    "tags": [
      "In-Context Learning Distillation",
      "Delta Matrix for Demonstration Utility",
      "Maximal Marginal Relevance Demonstration Selection",
      "ICL-guided Contrastive Representation Learning",
      "Maximum Mean Discrepancy Domain Alignment",
      "Supervised Contrastive Loss for Anomaly Discrimination",
      "Reasoning-aware Demonstration Retrieval",
      "Frozen-LLM Chain-of-Thought Inference",
      "Lightweight Log Sequence Encoder",
      "Cross-domain Few-shot/Zero-shot Adaptation"
    ],
    "summary": "本文提出LogICL，通过将大模型的推理能力蒸馏到轻量级日志编码器并结合基于ICL的示例选择、delta矩阵衡量、MMD域对齐和对比损失，实现对异构系统的跨域少样本/零样本日志异常检测与可解释推理。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.09627v1",
    "published": "2025-12-10",
    "update_time": "2025-12-10",
    "download_time": "2025-12-12 01:52:03"
  },
  {
    "id": "2512.09543",
    "title": "SWEnergy: An Empirical Study on Energy Efficiency in Agentic Issue Resolution Frameworks with SLMs",
    "abstract": "Context. LLM-based autonomous agents in software engineering rely on large, proprietary models, limiting local deployment. This has spurred interest in Small Language Models (SLMs), but their practical effectiveness and efficiency within complex agentic frameworks for automated issue resolution remain poorly understood.   Goal. We investigate the performance, energy efficiency, and resource consumption of four leading agentic issue resolution frameworks when deliberately constrained to using SLMs. We aim to assess the viability of these systems for this task in resource-limited settings and characterize the resulting trade-offs.   Method. We conduct a controlled evaluation of four leading agentic frameworks (SWE-Agent, OpenHands, Mini SWE Agent, AutoCodeRover) using two SLMs (Gemma-3 4B, Qwen-3 1.7B) on the SWE-bench Verified Mini benchmark. On fixed hardware, we measure energy, duration, token usage, and memory over 150 runs per configuration.   Results. We find that framework architecture is the primary driver of energy consumption. The most energy-intensive framework, AutoCodeRover (Gemma), consumed 9.4x more energy on average than the least energy-intensive, OpenHands (Gemma). However, this energy is largely wasted. Task resolution rates were near-zero, demonstrating that current frameworks, when paired with SLMs, consume significant energy on unproductive reasoning loops. The SLM's limited reasoning was the bottleneck for success, but the framework's design was the bottleneck for efficiency.   Conclusions. Current agentic frameworks, designed for powerful LLMs, fail to operate efficiently with SLMs. We find that framework architecture is the primary driver of energy consumption, but this energy is largely wasted due to the SLMs' limited reasoning. Viable low-energy solutions require shifting from passive orchestration to architectures that actively manage SLM weaknesses.",
    "arxiv_url": "https://arxiv.org/abs/2512.09543",
    "authors": [
      "Arihant Tripathy",
      "Ch Pavan Harshit",
      "Karthik Vaidhyanathan"
    ],
    "first_author": "Arihant Tripathy",
    "category": [
      "Empirical"
    ],
    "field": "Quality Management",
    "task": "Bug Repair",
    "tags": [
      "Energy-aware agentic frameworks",
      "Hardware-level CPU/GPU energy profiling",
      "Small-model multi-turn reasoning limitations",
      "Framework architectural trade-offs",
      "Resource-constrained autonomous bug repair",
      "Wasted inference energy from reasoning loops",
      "Reproducible energy measurement methodology",
      "Failure-mode analysis for agentic workflows"
    ],
    "summary": "本文在固定硬件上用两种小型语言模型对四种代理式问题修复框架进行大规模能耗与性能评估，发现框架架构决定能耗且大部分能耗因SLM推理能力受限导致无效循环被浪费，表明需设计主动管理SLM弱点的新型架构以实现低能耗可行性。",
    "quality": "High",
    "conference": "AGENT (ICSE 2026 workshop) 2026",
    "pdf_url": "https://arxiv.org/pdf/2512.09543v1",
    "published": "2025-12-10",
    "update_time": "2025-12-10",
    "download_time": "2025-12-12 01:58:17"
  },
  {
    "id": "2512.09108",
    "title": "Evolving Excellence: Automated Optimization of LLM-based Agents",
    "abstract": "Agentic AI systems built on large language models (LLMs) offer significant potential for automating complex workflows, from software development to customer support. However, LLM agents often underperform due to suboptimal configurations; poorly tuned prompts, tool descriptions, and parameters that typically require weeks of manual refinement. Existing optimization methods either are too complex for general use or treat components in isolation, missing critical interdependencies.   We present ARTEMIS, a no-code evolutionary optimization platform that jointly optimizes agent configurations through semantically-aware genetic operators. Given only a benchmark script and natural language goals, ARTEMIS automatically discovers configurable components, extracts performance signals from execution logs, and evolves configurations without requiring architectural modifications.   We evaluate ARTEMIS on four representative agent systems: the \\emph{ALE Agent} for competitive programming on AtCoder Heuristic Contest, achieving a \\textbf{$13.6\\%$ improvement} in acceptance rate; the \\emph{Mini-SWE Agent} for code optimization on SWE-Perf, with a statistically significant \\textbf{10.1\\% performance gain}; and the \\emph{CrewAI Agent} for cost and mathematical reasoning on Math Odyssey, achieving a statistically significant \\textbf{$36.9\\%$ reduction} in the number of tokens required for evaluation. We also evaluate the \\emph{MathTales-Teacher Agent} powered by a smaller open-source model (Qwen2.5-7B) on GSM8K primary-level mathematics problems, achieving a \\textbf{22\\% accuracy improvement} and demonstrating that ARTEMIS can optimize agents based on both commercial and local models.",
    "arxiv_url": "https://arxiv.org/abs/2512.09108",
    "authors": [
      "Paul Brookes",
      "Vardan Voskanyan",
      "Rafail Giavrimis",
      "Matthew Truscott",
      "Mina Ilieva",
      "Chrystalla Pavlou",
      "Alexandru Staicu",
      "Manal Adham",
      "Will Evers- Hood",
      "Jingzhi Gong",
      "Kejia Zhang",
      "Matvey Fedoseev",
      "Vishal Sharma",
      "Roman Bauer",
      "Zheng Wang",
      "Hema Nair",
      "Wei Jie",
      "Tianhua Xu",
      "Aurora Constantin",
      "Leslie Kanthan",
      "Michail Basios"
    ],
    "first_author": "Paul Brookes",
    "category": [
      "Technical"
    ],
    "field": "LLM Agents & Deployment",
    "task": "Agent Configuration Optimization",
    "tags": [
      "Black-box agent tuning",
      "Semantically-aware mutation and crossover",
      "No-code evolutionary platform",
      "Execution-log based fitness",
      "LLM-ensemble guided edits",
      "Joint textual and parametric optimization",
      "Prompt + tool description co-optimization",
      "Cost vs. performance trade-off analysis"
    ],
    "summary": "本文提出Artemis——一个无代码的基于进化算法的代理配置自动优化平台，通过语义感知的变异与交叉操作并利用执行日志与基准反馈，自动联合调优提示、工具描述和参数，从而显著提升不同LLM代理在多种任务上的准确性与效率。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.09108v1",
    "published": "2025-12-09",
    "update_time": "2025-12-09",
    "download_time": "2025-12-12 02:05:43"
  },
  {
    "id": "2512.04355",
    "title": "Counting Without Running: Evaluating LLMs' Reasoning About Code Complexity",
    "abstract": "Modern GPU software stacks demand developers who can anticipate performance bottlenecks before ever launching a kernel; misjudging floating-point workloads upstream can derail tuning, scheduling, and even hardware procurement. Yet despite rapid progress in code generation, today's Large Language Models (LLMs) are rarely tested on this kind of forward-looking reasoning. We close that gap with gpuFLOPBench, a benchmark that asks models to \"count without running\" by predicting single and double-precision FLOP counts for 577 CUDA kernels drawn from HeCBench, annotated with ground-truth profiles and eight execution attributes that distinguish trivially analyzable code from kernels whose FLOPs depend on hidden compiler or runtime behavior. Evaluating current closed-source reasoning models shows clear but uneven progress: the newest LLMs achieve perfect classification on straightforward kernels but still incur multiple order-of-magnitude errors whenever implicit FLOPs arise from division, intrinsic math functions, or common subexpressions. These results surface a core limitation of existing code assistants -- the inability to internalize hardware-specific microcode effects -- and position gpuFLOPBench as a focused testbed for developing LLM tooling that can reason about performance with the same rigor as experienced GPU developers. Sources are available at our repository: https://github.com/Scientific-Computing-Lab/gpuFLOPBench",
    "arxiv_url": "https://arxiv.org/abs/2512.04355",
    "authors": [
      "Gregory Bolet",
      "Giorgis Georgakoudis",
      "Konstantinos Parasyris",
      "Harshitha Menon",
      "Niranjan Hasabnis",
      "Kirk W. Cameron",
      "Gal Oren"
    ],
    "first_author": "Gregory Bolet",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Performance Optimization",
    "tags": [
      "Static FLOP counting",
      "CUDA kernel FLOP estimation",
      "Execution-attribute annotations",
      "Implicit/hidden FLOPs (division, intrinsics, templates)",
      "Single vs double-precision FLOP labels",
      "Prompting for structured performance predictions",
      "Hardware/microcode execution effects",
      "Benchmark for static performance reasoning"
    ],
    "summary": "本文提出GPUFLOPBENCH数据集与评测框架，包含577个CUDA内核的单/双精度FLOP计数与执行属性标注，并评估现有封闭式推理模型在静态预测代码FLOP时的表现，揭示其在处理隐式FLOP来源（如除法、内建函数、编译器/运行时行为）时的显著失败模式。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.04355v1",
    "published": "2025-12-04",
    "update_time": "2025-12-04",
    "download_time": "2025-12-12 21:46:22"
  },
  {
    "id": "2512.10713",
    "title": "PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code",
    "abstract": "Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks.",
    "arxiv_url": "https://arxiv.org/abs/2512.10713",
    "authors": [
      "Itay Dreyfuss",
      "Antonio Abu Nassar",
      "Samuel Ackerman",
      "Axel Ben David",
      "Rami Katan",
      "Orna Raz",
      "Marcel Zalmanovici"
    ],
    "first_author": "Itay Dreyfuss",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Automated benchmark generation",
      "Deterministic expected-result checking",
      "Code dry-running simulation",
      "Instruction concatenation pipelines",
      "Contamination-resistant variant generation",
      "Difficulty control via instruction count and output length",
      "Prompt vs chat mode evaluation"
    ],
    "summary": "本文提出了PACIFIC框架，用可控的指令拼接与引用实现自动生成可确定性评估样本，以检测大型模型的代码逐步推理（干运行）与指令遵循能力，并通过可变难度和去污染的基准区分模型表现。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.10713v1",
    "published": "2025-12-11",
    "update_time": "2025-12-11",
    "download_time": "2025-12-13 01:45:41"
  },
  {
    "id": "2512.10493",
    "title": "Decoding Human-LLM Collaboration in Coding: An Empirical Study of Multi-Turn Conversations in the Wild",
    "abstract": "Large language models (LLMs) are increasingly acting as dynamic conversational interfaces, supporting multi-turn interactions that mimic human-like conversation and facilitate complex tasks like coding. While datasets such as LMSYS-Chat-1M and WildChat capture real-world user-LLM conversations, few studies systematically explore the mechanisms of human-LLM collaboration in coding scenarios. What tortuous paths do users experience during the interaction process? How well do the LLMs follow instructions? Are users satisfied? In this paper, we conduct an empirical analysis on human-LLM coding collaboration using LMSYS-Chat-1M and WildChat datasets to explore the human-LLM collaboration mechanism, LLMs' instruction following ability, and human satisfaction. This study yields interesting findings: 1) Task types shape interaction patterns(linear, star and tree), with code quality optimization favoring linear patterns, design-driven tasks leaning toward tree structures, and queries preferring star patterns; 2) Bug fixing and code refactoring pose greater challenges to LLMs' instruction following, with non-compliance rates notably higher than in information querying; 3) Code quality optimization and requirements-driven development tasks show lower user satisfaction, whereas structured knowledge queries and algorithm designs yield higher levels. These insights offer recommendations for improving LLM interfaces and user satisfaction in coding collaborations, while highlighting avenues for future research on adaptive dialogue systems. We believe this work broadens understanding of human-LLM synergies and supports more effective AI-assisted development.",
    "arxiv_url": "https://arxiv.org/abs/2512.10493",
    "authors": [
      "Binquan Zhang",
      "Li Zhang",
      "Haoyuan Zhang",
      "Fang Liu",
      "Song Wang",
      "Bo Shen",
      "An Fu",
      "Lin Shi"
    ],
    "first_author": "Binquan Zhang",
    "category": [
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Prompting",
    "tags": [
      "Multi-turn dialogue patterns",
      "Interaction topology (linear/star/tree)",
      "Instruction-following compliance",
      "User satisfaction trajectory",
      "Task taxonomy for coding interactions",
      "LLM-assisted data disentanglement",
      "Open card sorting annotation",
      "Statistical significance testing of interaction effects"
    ],
    "summary": "本文基于真实多轮对话数据实证分析了编码场景下的人-LLM协作，归纳出五类任务与线性/星状/树状三种交互模式，评估模型指令遵循能力与用户满意度并提出改进建议。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.10493v1",
    "published": "2025-12-11",
    "update_time": "2025-12-11",
    "download_time": "2025-12-13 01:46:15"
  },
  {
    "id": "2512.10789",
    "title": "Natural Language Interface for Firewall Configuration",
    "abstract": "This paper presents the design and prototype implementation of a natural language interface for configuring enterprise firewalls. The framework allows administrators to express access control policies in plain language, which are then translated into vendor specific configurations. A compact schema bound intermediate representation separates human intent from device syntax and in the current prototype compiles to Palo Alto PAN OS command line configuration while remaining extensible to other platforms. Large language models are used only as assistive parsers that generate typed intermediate representation objects, while compilation and enforcement remain deterministic. The prototype integrates three validation layers, namely a static linter that checks structural and vendor specific constraints, a safety gate that blocks overly permissive rules such as any to any allows, and a Batfish based simulator that validates configuration syntax and referential integrity against a synthetic device model. The paper describes the architecture, implementation, and test methodology on synthetic network context datasets and discusses how this approach can evolve into a scalable auditable and human centered workflow for firewall policy management.",
    "arxiv_url": "https://arxiv.org/abs/2512.10789",
    "authors": [
      "F. Taghiyev",
      "A. Aslanbayli"
    ],
    "first_author": "F. Taghiyev",
    "category": [
      "Technical"
    ],
    "field": "Network Security & Configuration",
    "task": "Natural Language to Firewall Configuration",
    "tags": [
      "Schema-bound intermediate representation",
      "Schema-constrained LLM parsing",
      "Resolver agent for entity grounding",
      "Vendor-agnostic compiler (Palo Alto backend prototype)",
      "IR linter (general and vendor-specific)",
      "Safety Gate preventing any-to-any and missing-fields",
      "Batfish-based configuration simulation",
      "Role-conditioned prompting and constrained decoding",
      "Least-privilege policy synthesis",
      "Audit logging and stage-only (side-effect free) compilation"
    ],
    "summary": "本文提出并实现了一个原型系统，利用受约束的LLM将管理员的自然语言访问策略解析为类型化中间表示，并通过静态lint、安全门控和Batfish仿真将其无副作用地编译为供应商特定的防火墙配置以确保安全与可审计性。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.10789v1",
    "published": "2025-12-11",
    "update_time": "2025-12-11",
    "download_time": "2025-12-13 01:50:04"
  },
  {
    "id": "2512.10563",
    "title": "NormCode: A Semi-Formal Language for Context-Isolated AI Planning",
    "abstract": "Multistep workflows that chain large language model (LLM) calls suffer from context pollution: as information accumulates across steps, models hallucinate, confuse intermediate outputs, and lose track of task constraints. We present NormCode, a semiformal language for constructing plans of inferences, structured decompositions where each step operates in data isolation and receives only explicitly passed inputs, which eliminates crossstep contamination by design. NormCode enforces a strict separation between semantic operations (LLMdriven reasoning, nondeterministic) and syntactic operations (deterministic data restructuring), enabling precise cost and reliability tracing. The language exists in three isomorphic formats: .ncds for human authoring, .ncd for machine execution, and .ncn for human verification, supporting progressive formalization from sketch to production. We validate NormCode through two demonstrations: (1) a base X addition algorithm achieving 100 percent accuracy on arbitrary length inputs, and (2) self hosted execution of NormCode's own five phase compiler pipeline. The working orchestrator provides dependency driven scheduling, SQLite backed checkpointing, and loop management, making AI workflows auditable by design and addressing a critical need for transparency in high stakes domains such as legal reasoning, medical decision making, and financial analysis.",
    "arxiv_url": "https://arxiv.org/abs/2512.10563",
    "authors": [
      "Xin Guan"
    ],
    "first_author": "Xin Guan",
    "category": [
      "Technical"
    ],
    "field": "Requirements & Design",
    "task": "Specification & Validation",
    "tags": [
      "Context Isolation",
      "Inference Decomposition",
      "Semi-Formal Intermediate Representation",
      "Semantic vs Syntactic Separation",
      "Progressive Formalization (.ncds/.ncd/.ncn)",
      "Auditable AI Workflows",
      "Dependency-Driven Orchestration",
      "Checkpointed Execution and Loop Management"
    ],
    "summary": "NormCode提出一种半形式化语言与执行框架，通过在多步LLM推理中强制显式数据隔离、将语义（非确定性LLM推理）与句法（确定性数据重构）分离，并提供三种同构格式与可检查的编译/运行时支持，从而实现可审计、可靠的AI工作流编排与执行。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.10563v1",
    "published": "2025-12-11",
    "update_time": "2025-12-11",
    "download_time": "2025-12-13 01:53:27"
  },
  {
    "id": "2512.10485",
    "title": "From Lab to Reality: A Practical Evaluation of Deep Learning Models and LLMs for Vulnerability Detection",
    "abstract": "Vulnerability detection methods based on deep learning (DL) have shown strong performance on benchmark datasets, yet their real-world effectiveness remains underexplored. Recent work suggests that both graph neural network (GNN)-based and transformer-based models, including large language models (LLMs), yield promising results when evaluated on curated benchmark datasets. These datasets are typically characterized by consistent data distributions and heuristic or partially noisy labels. In this study, we systematically evaluate two representative DL models-ReVeal and LineVul-across four representative datasets: Juliet, Devign, BigVul, and ICVul. Each model is trained independently on each respective dataset, and their code representations are analyzed using t-SNE to uncover vulnerability related patterns. To assess realistic applicability, we deploy these models along with four pretrained LLMs, Claude 3.5 Sonnet, GPT-o3-mini, GPT-4o, and GPT-5 on a curated dataset, VentiVul, comprising 20 recently (May 2025) fixed vulnerabilities from the Linux kernel. Our experiments reveal that current models struggle to distinguish vulnerable from non-vulnerable code in representation space and generalize poorly across datasets with differing distributions. When evaluated on VentiVul, our newly constructed time-wise out-of-distribution dataset, performance drops sharply, with most models failing to detect vulnerabilities reliably. These results expose a persistent gap between academic benchmarks and real-world deployment, emphasizing the value of our deployment-oriented evaluation framework and the need for more robust code representations and higher-quality datasets.",
    "arxiv_url": "https://arxiv.org/abs/2512.10485",
    "authors": [
      "Chaomeng Lu",
      "Bert Lagaisse"
    ],
    "first_author": "Chaomeng Lu",
    "category": [
      "Empirical",
      "Benchmark"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "Time-wise out-of-distribution evaluation",
      "Whole-file evaluation mode",
      "Function-pair evaluation mode",
      "Cross-dataset generalization",
      "Code representation clustering (t-SNE & centroid distance)",
      "Graph-based vs token-based code representations",
      "Zero-shot LLM vulnerability probing",
      "Linux CVE fix-based testbed",
      "Label quality and dataset bias analysis"
    ],
    "summary": "本文构建了一个时间分离的真实漏洞测试集并提出部署导向的评估框架，系统性比较图/序列深度模型与LLM在漏洞检测上的表示能力与跨数据集泛化性，结果表明现有模型在真实场景中表现显著下降。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.10485v1",
    "published": "2025-12-11",
    "update_time": "2025-12-11",
    "download_time": "2025-12-14 01:58:04"
  },
  {
    "id": "2512.10452",
    "title": "UniCoR: Modality Collaboration for Robust Cross-Language Hybrid Code Retrieval",
    "abstract": "Effective code retrieval is indispensable and it has become an important paradigm to search code in hybrid mode using both natural language and code snippets. Nevertheless, it remains unclear whether existing approaches can effectively leverage such hybrid queries, particularly in cross-language contexts. We conduct a comprehensive empirical study of representative code models and reveal three challenges: (1) insufficient semantic understanding; (2) inefficient fusion in hybrid code retrieval; and (3) weak generalization in cross-language scenarios. To address these challenges, we propose UniCoR, a novel self-supervised framework that learns Unified Code Representations framework designed to learn unified and robust code representations. Firstly, we design a multi-perspective supervised contrastive learning module to enhance semantic understanding and modality fusion. It aligns representations from multiple perspectives, including code-to-code, natural language-to-code, and natural language-to-natural language, enforcing the model to capture a semantic essence among modalities. Secondly, we introduce a representation distribution consistency learning module to improve cross-language generalization, which explicitly aligns the feature distributions of different programming languages, enabling language-agnostic representation learning. Extensive experiments on both empirical benchmark and large-scale benchmark show that UniCoR outperforms all baseline models, achieving an average improvement of 8.64% in MRR and 11.54% in MAP over the best-performing baseline. Furthermore, UniCoR exhibits stability in hybrid code retrieval and generalization capability in cross-language scenarios.",
    "arxiv_url": "https://arxiv.org/abs/2512.10452",
    "authors": [
      "Yang Yang",
      "Li Kuang",
      "Jiakun Liu",
      "Zhongxin Liu",
      "Yingjie Xia",
      "David Lo"
    ],
    "first_author": "Yang Yang",
    "category": [
      "Technical",
      "Empirical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Retrieval",
    "tags": [
      "Hybrid query fusion (code + natural language)",
      "Cross-language alignment",
      "Multi-perspective supervised contrastive learning",
      "Representation distribution consistency",
      "Maximum Mean Discrepancy (MMD) for language-agnostic alignment",
      "Modality collaboration and fusion",
      "Functionally equivalent code pairing",
      "Automated natural-language query generation",
      "Self-supervised code representation learning",
      "Fusion strategies: input remix / vector concat / score weighting"
    ],
    "summary": "本文提出UniCoR，一种结合多视角监督对比学习与表示分布一致性约束的自监督框架，通过强化模态协同与跨语言对齐，显著提升混合（代码+自然语言）跨语言代码检索的语义理解、融合效果与泛化能力。",
    "quality": "High",
    "conference": "IEEE/ACM International Conference on Software Engineering (ICSE 2026)",
    "pdf_url": "https://arxiv.org/pdf/2512.10452v1",
    "published": "2025-12-11",
    "update_time": "2025-12-11",
    "download_time": "2025-12-14 01:58:31"
  },
  {
    "id": "2512.10415",
    "title": "How to Trick Your AI TA: A Systematic Study of Academic Jailbreaking in LLM Code Evaluation",
    "abstract": "The use of Large Language Models (LLMs) as automatic judges for code evaluation is becoming increasingly prevalent in academic environments. But their reliability can be compromised by students who may employ adversarial prompting strategies in order to induce misgrading and secure undeserved academic advantages. In this paper, we present the first large-scale study of jailbreaking LLM-based automated code evaluators in academic context. Our contributions are: (i) We systematically adapt 20+ jailbreaking strategies for jailbreaking AI code evaluators in the academic context, defining a new class of attacks termed academic jailbreaking. (ii) We release a poisoned dataset of 25K adversarial student submissions, specifically designed for the academic code-evaluation setting, sourced from diverse real-world coursework and paired with rubrics and human-graded references, and (iii) In order to capture the multidimensional impact of academic jailbreaking, we systematically adapt and define three jailbreaking metrics (Jailbreak Success Rate, Score Inflation, and Harmfulness). (iv) We comprehensively evalulate the academic jailbreaking attacks using six LLMs. We find that these models exhibit significant vulnerability, particularly to persuasive and role-play-based attacks (up to 97% JSR). Our adversarial dataset and benchmark suite lay the groundwork for next-generation robust LLM-based evaluators in academic code assessment.",
    "arxiv_url": "https://arxiv.org/abs/2512.10415",
    "authors": [
      "Devanshu Sahoo",
      "Vasudev Majhi",
      "Arjun Neekhra",
      "Yash Sinha",
      "Murari Mandal",
      "Dhruv Kumar"
    ],
    "first_author": "Devanshu Sahoo",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Understanding",
    "tags": [
      "Academic Jailbreaking",
      "Adversarial Prompt Injection",
      "Poisoned Student Submissions",
      "Rubric-aware Attack Design",
      "Jailbreak Metrics (JSR/SIR/Harmfulness)",
      "Persona/Role-play Attacks",
      "Token/Emoji-level Manipulation",
      "Cross-model Robustness Benchmark"
    ],
    "summary": "本文首次系统研究了针对LLM自动代码评分器的“学术越狱”攻击，构建了约25K条对抗性学生提交数据集、提出专门的评价指标并在六种模型上进行基准评估，揭示了角色扮演与说服类攻击下的严重脆弱性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.10415v1",
    "published": "2025-12-11",
    "update_time": "2025-12-11",
    "download_time": "2025-12-14 02:06:36"
  },
  {
    "id": "2512.10398",
    "title": "Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale",
    "abstract": "Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale.",
    "arxiv_url": "https://arxiv.org/abs/2512.10398",
    "authors": [
      "Zhaodong Wang",
      "Zhenting Qi",
      "Sherman Wong",
      "Nathan Hu",
      "Samuel Lin",
      "Jun Ge",
      "Erwin Gao",
      "Yining Yang",
      "Ben Maurer",
      "Wenlin Chen",
      "David Recordon",
      "Yilun Du",
      "Minlan Yu",
      "Ying Zhang"
    ],
    "first_author": "Zhaodong Wang",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Hierarchical working memory",
      "Adaptive context compression",
      "Persistent note-taking",
      "Cross-session continual learning",
      "Meta-agent build–test–improve loop",
      "Modular tool extensions with typed callbacks",
      "Agent orchestration for long-horizon tasks",
      "Developer observability and DX-focused tooling",
      "Industrial-scale repository navigation",
      "Ablation-driven agent evaluation"
    ],
    "summary": "本文提出并开源了Confucius SDK及其实例化的Confucius Code Agent，通过层次化工作内存、自适应上下文压缩、持久化笔记和模块化扩展，并借助元代理的构建-测试-改进循环，实现面向工业规模代码库的长期推理与跨会话记忆，在多种软件工程任务上表现优异。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.10398v1",
    "published": "2025-12-11",
    "update_time": "2025-12-11",
    "download_time": "2025-12-14 02:07:20"
  },
  {
    "id": "2512.10393",
    "title": "Cross-modal Retrieval Models for Stripped Binary Analysis",
    "abstract": "LLM-agent based binary code analysis has demonstrated significant potential across a wide range of software security scenarios, including vulnerability detection, malware analysis, etc. In agent workflow, however, retrieving the positive from thousands of stripped binary functions based on user query remains under-studied and challenging, as the absence of symbolic information distinguishes it from source code retrieval. In this paper, we introduce, BinSeek, the first two-stage cross-modal retrieval framework for stripped binary code analysis. It consists of two models: BinSeekEmbedding is trained on large-scale dataset to learn the semantic relevance of the binary code and the natural language description, furthermore, BinSeek-Reranker learns to carefully judge the relevance of the candidate code to the description with context augmentation. To this end, we built an LLM-based data synthesis pipeline to automate training construction, also deriving a domain benchmark for future research. Our evaluation results show that BinSeek achieved the state-of-the-art performance, surpassing the the same scale models by 31.42% in Rec@3 and 27.17% in MRR@3, as well as leading the advanced general-purpose models that have 16 times larger parameters.",
    "arxiv_url": "https://arxiv.org/abs/2512.10393",
    "authors": [
      "Guoqiang Chen",
      "Lingyun Ying",
      "Ziyang Song",
      "Daguang Liu",
      "Qiang Wang",
      "Zhiqi Wang",
      "Li Hu",
      "Shaoyin Cheng",
      "Weiming Zhang",
      "Nenghai Yu"
    ],
    "first_author": "Guoqiang Chen",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Retrieval",
    "tags": [
      "stripped-binary retrieval",
      "decompiled pseudocode embeddings",
      "two-stage retrieval (embedding + reranker)",
      "context-augmented reranking with calling-context",
      "LLM-driven synthetic label generation",
      "contrastive embedding training (InfoNCE)",
      "domain-specific binary retrieval benchmark",
      "retrieval-augmented binary analysis for security"
    ],
    "summary": "本文提出了BinSeek，一种针对无符号（stripped）二进制函数的两阶段跨模态检索框架（嵌入召回 + 上下文增强重排序），并通过LLM驱动的数据合成构建训练集与首个领域基准，显著提升二进制代码检索性能。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.10393v1",
    "published": "2025-12-11",
    "update_time": "2025-12-11",
    "download_time": "2025-12-15 01:56:55"
  },
  {
    "id": "2512.10238",
    "title": "Studying and Automating Issue Resolution for Software Quality",
    "abstract": "Effective issue resolution is crucial for maintaining software quality. Yet developers frequently encounter challenges such as low-quality issue reports, limited understanding of real-world workflows, and a lack of automated support. This research aims to address these challenges through three complementary directions. First, we enhance issue report quality by proposing techniques that leverage LLM reasoning and application-specific information. Second, we empirically characterize developer workflows in both traditional and AI-augmented systems. Third, we automate cognitively demanding resolution tasks, including buggy UI localization and solution identification, through ML, DL, and LLM-based approaches. Together, our work delivers empirical insights, practical tools, and automated methods to advance AI-driven issue resolution, supporting more maintainable and high-quality software systems.",
    "arxiv_url": "https://arxiv.org/abs/2512.10238",
    "authors": [
      "Antu Saha"
    ],
    "first_author": "Antu Saha",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Quality Management",
    "task": "Bug Localization",
    "tags": [
      "Issue report quality assessment",
      "Steps-to-Reproduce (S2R) enhancement",
      "LLM reasoning aligned with dynamic app execution",
      "Execution model of screens and interactions",
      "Context-aware issue report generation",
      "Buggy UI localization",
      "Multi-modal text+vision retrieval for UI mapping",
      "Solution identification in issue discussions",
      "Empirical analysis of issue resolution workflows",
      "Cross-project generalizability of classifiers"
    ],
    "summary": "本文结合基于应用执行模型的多模态信息与LLM推理，提升缺陷报告（OB/EB/S2R）质量，实证分析传统与AI/ML集成系统的问题解决流程，并自动化实现Buggy UI定位与解决方案识别以加速问题定位与修复。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.10238v1",
    "published": "2025-12-11",
    "update_time": "2025-12-11",
    "download_time": "2025-12-15 01:57:20"
  },
  {
    "id": "2512.10501",
    "title": "Zero-shot 3D Map Generation with LLM Agents: A Dual-Agent Architecture for Procedural Content Generation",
    "abstract": "Procedural Content Generation (PCG) offers scalable methods for algorithmically creating complex, customizable worlds. However, controlling these pipelines requires the precise configuration of opaque technical parameters. We propose a training-free architecture that utilizes LLM agents for zero-shot PCG parameter configuration. While Large Language Models (LLMs) promise a natural language interface for PCG tools, off-the-shelf models often fail to bridge the semantic gap between abstract user instructions and strict parameter specifications. Our system pairs an Actor agent with a Critic agent, enabling an iterative workflow where the system autonomously reasons over tool parameters and refines configurations to progressively align with human design preferences. We validate this approach on the generation of various 3D maps, establishing a new benchmark for instruction-following in PCG. Experiments demonstrate that our approach outperforms single-agent baselines, producing diverse and structurally valid environments from natural language descriptions. These results demonstrate that off-the-shelf LLMs can be effectively repurposed as generalized agents for arbitrary PCG tools. By shifting the burden from model training to architectural reasoning, our method offers a scalable framework for mastering complex software without task-specific fine-tuning.",
    "arxiv_url": "https://arxiv.org/abs/2512.10501",
    "authors": [
      "Lim Chien Her",
      "Ming Yan",
      "Yunshu Bai",
      "Ruihao Li",
      "Hao Zhang"
    ],
    "first_author": "Lim Chien Her",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Procedural Content Generation",
    "task": "Zero-shot PCG parameter configuration for 3D map generation (dual-agent Actor–Critic)",
    "tags": [
      "Dual-agent Actor–Critic architecture",
      "Zero-shot parameter synthesis",
      "Documentation-grounded verification",
      "Iterative agent dialogic refinement",
      "Training-free LLM tool control",
      "3D map / terrain synthesis",
      "PCG parameter hallucination correction",
      "Instruction-following benchmark for PCG"
    ],
    "summary": "本文提出一种训练免费、双代理（Actor–Critic）的架构，利用离线大模型在零样本情形下通过迭代对话将自然语言提示映射为可执行的 PCG 参数，从而生成多样且结构有效的 3D 地图，并在新建的指令跟随基准上显著优于单代理基线。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.10501v1",
    "published": "2025-12-11",
    "update_time": "2025-12-11",
    "download_time": "2025-12-15 02:05:01"
  },
  {
    "id": "2512.10317",
    "title": "Translating Informal Proofs into Formal Proofs Using a Chain of States",
    "abstract": "We address the problem of translating informal mathematical proofs expressed in natural language into formal proofs in Lean4 under a constrained computational budget. Our approach is grounded in two key insights. First, informal proofs tend to proceed via a sequence of logical transitions - often implications or equivalences - without explicitly specifying intermediate results or auxiliary lemmas. In contrast, formal systems like Lean require an explicit representation of each proof state and the tactics that connect them. Second, each informal reasoning step can be viewed as an abstract transformation between proof states, but identifying the corresponding formal tactics often requires nontrivial domain knowledge and precise control over proof context. To bridge this gap, we propose a two stage framework. Rather than generating formal tactics directly, we first extract a Chain of States (CoS), a sequence of intermediate formal proof states aligned with the logical structure of the informal argument. We then generate tactics to transition between adjacent states in the CoS, thereby constructing the full formal proof. This intermediate representation significantly reduces the complexity of tactic generation and improves alignment with informal reasoning patterns. We build dedicated datasets and benchmarks for training and evaluation, and introduce an interactive framework to support tactic generation from formal states. Empirical results show that our method substantially outperforms existing baselines, achieving higher proof success rates.",
    "arxiv_url": "https://arxiv.org/abs/2512.10317",
    "authors": [
      "Ziyu Wang",
      "Bowen Yang",
      "Shihao Zhou",
      "Chenyi Li",
      "Yuan Zhang",
      "Bin Dong",
      "Zaiwen Wen"
    ],
    "first_author": "Ziyu Wang",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Formal Methods & Theorem Proving",
    "task": "Informal-to-Formal Proof Translation",
    "tags": [
      "Chain of States (CoS) intermediate representation",
      "proof-state extraction from informal text",
      "state-aligned tactic synthesis",
      "error-aware tactic regeneration",
      "Lean4-specific proof rewriting heuristics",
      "metaprogramming-based dataset construction",
      "interactive tactic generation framework",
      "reduction of prover invocations / compute-efficient proving"
    ],
    "summary": "本文提出了CoS两阶段框架：先从非正式数学证明抽取对齐的形式化证明状态链，再在相邻状态之间生成tactic以在Lean4中还原完整形式化证明，并通过元编程构建数据集与基准，显著提高了证明成功率与效率。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.10317v1",
    "published": "2025-12-11",
    "update_time": "2025-12-11",
    "download_time": "2025-12-15 02:09:36"
  },
  {
    "id": "2512.11589",
    "title": "A Study of Library Usage in Agent-Authored Pull Requests",
    "abstract": "Coding agents are becoming increasingly capable of completing end-to-end software engineering workflows that previously required a human developer, including raising pull requests (PRs) to propose their changes. However, we still know little about how these agents use libraries when generating code, a core part of real-world software development. To fill this gap, we study 26,760 agent-authored PRs from the AIDev dataset to examine three questions: how often do agents import libraries, how often do they introduce new dependencies (and with what versioning), and which specific libraries do they choose? We find that agents often import libraries (29.5% of PRs) but rarely add new dependencies (1.3% of PRs); and when they do, they follow strong versioning practices (75.0% specify a version), an improvement on direct LLM usage where versions are rarely mentioned. Generally, agents draw from a surprisingly diverse set of external libraries, contrasting with the limited \"library preferences\" seen in prior non-agentic LLM studies. Our results offer an early empirical view into how AI coding agents interact with today's software ecosystems.",
    "arxiv_url": "https://arxiv.org/abs/2512.11589",
    "authors": [
      "Lukas Twist"
    ],
    "first_author": "Lukas Twist",
    "category": [
      "Empirical"
    ],
    "field": "Version Control & Collaboration",
    "task": "Git VCS",
    "tags": [
      "Agent-authored pull requests",
      "Library import frequency",
      "New dependency introduction",
      "Dependency version specification (version hygiene)",
      "Library diversity across ecosystems",
      "Multi-language empirical analysis (TypeScript, Python, Go, C#)",
      "Import and manifest regex extraction",
      "Agentic workflow behavior"
    ],
    "summary": "本文通过分析26,760个由代码代理提交的拉取请求，实证研究代理在导入库、引入新依赖及选择库上的行为，发现代理常导入外部库但很少新增依赖且通常会指定版本，同时展现出较高的库多样性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.11589v1",
    "published": "2025-12-12",
    "update_time": "2025-12-12",
    "download_time": "2025-12-16 01:52:27"
  },
  {
    "id": "2512.11482",
    "title": "Towards Privacy-Preserving Code Generation: Differentially Private Code Language Models",
    "abstract": "Large language models specialized for code (CodeLLMs) have demonstrated remarkable capabilities in generating code snippets, documentation, and test cases. However, despite their promising capabilities, CodeLLMs can inadvertently memorize and reproduce snippets from their training data, which poses risks of privacy breaches and intellectual property violations. These risks restrict the deployment of CodeLLMs in sensitive domains and limit their training datasets to publicly available sources. To mitigate the memorization risk without compromising their task performance, we apply Differential Privacy (DP) to CodeLLMs. To the best of our knowledge, this is the first comprehensive study that systematically evaluates the effectiveness of DP in CodeLLMs. DP adds calibrated noise to the training process to protect individual data points while still allowing the model to learn useful patterns. To this end, we first identify and understand the driving reasons of the memorization behaviour of the CodeLLMs during their fine-tuning. Then, to address this issue, we empirically evaluate the effect of DP on mitigating memorization while preserving code generation capabilities. Our findings show that DP substantially reduces memorization in CodeLLMs across all the tested snippet types. The snippet types most prone to memorization are also the most effectively mitigated by DP. Furthermore, we observe that DP slightly increases perplexity but preserves, and can even enhance, the code generation capabilities of CodeLLMs, which makes it feasible to apply DP in practice without significantly compromising model utility. Finally, we analyze the impact of DP on training efficiency and energy consumption, finding that DP does not significantly affect training time or energy usage, making it a practical choice for privacy-preserving CodeLLMs training.",
    "arxiv_url": "https://arxiv.org/abs/2512.11482",
    "authors": [
      "Melih Catal",
      "Pooja Rani",
      "Harald C. Gall"
    ],
    "first_author": "Melih Catal",
    "category": [
      "Empirical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Pre-Training",
    "tags": [
      "DP-SGD training",
      "Memorization taxonomy",
      "Data‑extraction and membership‑inference attacks",
      "Privacy‑utility trade-off",
      "Snippet entropy and frequency analysis",
      "Functional correctness evaluation",
      "Training efficiency & energy analysis",
      "Privacy evaluation benchmarks"
    ],
    "summary": "本文系统评估在代码生成模型中应用差分隐私（DP）的可行性，发现DP显著降低对训练片段的记忆风险且在大多数情况下不会显著损害甚至可提升代码生成功能性，并发布了两个用于隐私与效用评估的新基准数据集。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.11482v1",
    "published": "2025-12-12",
    "update_time": "2025-12-12",
    "download_time": "2025-12-16 01:53:12"
  },
  {
    "id": "2512.11783",
    "title": "Super Suffixes: Bypassing Text Generation Alignment and Guard Models Simultaneously",
    "abstract": "The rapid deployment of Large Language Models (LLMs) has created an urgent need for enhanced security and privacy measures in Machine Learning (ML). LLMs are increasingly being used to process untrusted text inputs and even generate executable code, often while having access to sensitive system controls. To address these security concerns, several companies have introduced guard models, which are smaller, specialized models designed to protect text generation models from adversarial or malicious inputs. In this work, we advance the study of adversarial inputs by introducing Super Suffixes, suffixes capable of overriding multiple alignment objectives across various models with different tokenization schemes. We demonstrate their effectiveness, along with our joint optimization technique, by successfully bypassing the protection mechanisms of Llama Prompt Guard 2 on five different text generation models for malicious text and code generation. To the best of our knowledge, this is the first work to reveal that Llama Prompt Guard 2 can be compromised through joint optimization.   Additionally, by analyzing the changing similarity of a model's internal state to specific concept directions during token sequence processing, we propose an effective and lightweight method to detect Super Suffix attacks. We show that the cosine similarity between the residual stream and certain concept directions serves as a distinctive fingerprint of model intent. Our proposed countermeasure, DeltaGuard, significantly improves the detection of malicious prompts generated through Super Suffixes. It increases the non-benign classification rate to nearly 100%, making DeltaGuard a valuable addition to the guard model stack and enhancing robustness against adversarial prompt attacks.",
    "arxiv_url": "https://arxiv.org/abs/2512.11783",
    "authors": [
      "Andrew Adiletta",
      "Kathryn Adiletta",
      "Kemal Derya",
      "Berk Sunar"
    ],
    "first_author": "Andrew Adiletta",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "Super Suffixes",
      "Joint optimization across tokenization schemes",
      "Adversarial suffix attacks",
      "Guard model bypass",
      "Refusal concept direction",
      "Dynamic residual-similarity detection",
      "DeltaGuard",
      "Malicious code-generation dataset"
    ],
    "summary": "本文提出 Super Suffixes —— 通过在不同分词方案下联合优化以同时绕过文本生成模型的对齐与守卫模型，并基于残差流与拒绝概念方向的动态余弦相似度提出 DeltaGuard 检测方法，同时构建恶意代码生成数据集进行评估。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.11783v1",
    "published": "2025-12-12",
    "update_time": "2025-12-12",
    "download_time": "2025-12-16 01:53:54"
  },
  {
    "id": "2512.11402",
    "title": "REMODEL-LLM: Transforming C code to Java using LLMs",
    "abstract": "The automated translation of C code to Java code is a notoriously difficult task, fraught with challenges stemming from fundamental paradigm shifts (procedural vs. Object Oriented), memory models (manual pointers vs. Garbage Collection), and incompatible data types. This paper investigates the efficacy of 19 small, quantized LLMs (under 20 billion parameters) for the C to Java translation task. We use a novel, hybrid pipeline that leverages Abstract Syntax Trees (ASTs) for semantic decomposition and employs a highly constrained, rule based prompting strategy. The results are stark: a clear multi tiered performance divide emerged. The vast majority of models (Tier 3, e.g., llama3.1, gemma3, starcoder2) failed 100\\% of the tests, proving incapable of generating even basic, runnable Java boilerplate. A small middle tier (Tier 2, e.g., mistral-nemo and mistral) produced runnable code but was plagued by dangerous semantic failures and wrong translations. Only three models (Tier 1: phi4, deepseek-coder-v2, codeqwen) proved viable, passing over 50\\% of the test suite. Even these top models failed on the most complex C concepts, such as function pointers, sizeof, and enum logic, revealing a hard ceiling for the reasoning capabilities of current quantized models.",
    "arxiv_url": "https://arxiv.org/abs/2512.11402",
    "authors": [
      "Aryan Gupta",
      "Y. Raghu Reddy"
    ],
    "first_author": "Aryan Gupta",
    "category": [
      "Empirical",
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Translation",
    "tags": [
      "C-to-Java migration",
      "AST-guided decomposition",
      "Constrained rule-based prompting",
      "Quantized edge LLM evaluation",
      "Semantic vs. syntactic failure analysis",
      "Translation edge cases (function pointers, sizeof, enums)",
      "20-case C→Java benchmark"
    ],
    "summary": "本文提出基于AST的混合翻译流水线和受限提示策略，评估了19个量化小型LLM在C到Java迁移任务上的表现并构建了20个边界用例基准，结果显示大多数模型在关键语义转换（如函数指针、sizeof、枚举逻辑）上失败，仅三款模型在测试中通过率超过50%。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.11402v1",
    "published": "2025-12-12",
    "update_time": "2025-12-12",
    "download_time": "2025-12-16 01:59:31"
  },
  {
    "id": "2512.12719",
    "title": "Towards AI Agents Supported Research Problem Formulation",
    "abstract": "Poorly formulated research problems can compromise the practical relevance of Software Engineering studies by not reflecting the complexities of industrial practice. This vision paper explores the use of artificial intelligence agents to support SE researchers during the early stage of a research project, the formulation of the research problem. Based on the Lean Research Inception framework and using a published study on code maintainability in machine learning as a reference, we developed a descriptive evaluation of a scenario illustrating how AI agents, integrated into LRI, can support SE researchers by pre filling problem attributes, aligning stakeholder perspectives, refining research questions, simulating multiperspective assessments, and supporting decision making. The descriptive evaluation of the scenario suggests that AI agent support can enrich collaborative discussions and enhance critical reflection on the value, feasibility, and applicability of the research problem. Although the vision of integrating AI agents into LRI was perceived as promising to support the context aware and practice oriented formulation of research problems, empirical validation is needed to confirm and refine the integration of AI agents into problem formulation.",
    "arxiv_url": "https://arxiv.org/abs/2512.12719",
    "authors": [
      "Anrafel Fernandes Pereira",
      "Maria Teresa Baldassarre",
      "Daniel Mendez",
      "Marcos Kalinowski"
    ],
    "first_author": "Anrafel Fernandes Pereira",
    "category": [
      "Technical"
    ],
    "field": "Research Process & Methodology",
    "task": "Research Problem Formulation",
    "tags": [
      "AI-assisted problem framing",
      "Problem Vision board pre-filling",
      "Multi-agent reasoning for research design",
      "Stakeholder-perspective simulation",
      "Semantic-differential assessment augmentation",
      "Literature-and-industry evidence synthesis",
      "Decision-support for go/pivot/abort"
    ],
    "summary": "该愿景论文提出将 AI 多代理体整合入 Lean Research Inception 框架，通过预填问题属性、促进利益相关者对齐、生成多视角评估并支持决策流程以改善软件工程研究问题的表述，但仍需实证验证。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.12719v1",
    "published": "2025-12-14",
    "update_time": "2025-12-14",
    "download_time": "2025-12-16 02:40:02"
  },
  {
    "id": "2512.12706",
    "title": "Synergizing Code Coverage and Gameplay Intent: Coverage-Aware Game Playtesting with LLM-Guided Reinforcement Learning",
    "abstract": "The widespread adoption of the \"Games as a Service\" model necessitates frequent content updates, placing immense pressure on quality assurance. In response, automated game testing has been viewed as a promising solution to cope with this demanding release cadence. However, existing automated testing approaches typically create a dichotomy: code-centric methods focus on structural coverage without understanding gameplay context, while player-centric agents validate high-level intent but often fail to cover specific underlying code changes. To bridge this gap, we propose SMART (Structural Mapping for Augmented Reinforcement Testing), a novel framework that synergizes structural verification and functional validation for game update testing. SMART leverages large language models (LLMs) to interpret abstract syntax tree (AST) differences and extract functional intent, constructing a context-aware hybrid reward mechanism. This mechanism guides reinforcement learning agents to sequentially fulfill gameplay goals while adaptively exploring modified code branches. We evaluate SMART on two environments, Overcooked and Minecraft. The results demonstrate that SMART significantly outperforms state-of-the-art baselines; it achieves over 94% branch coverage of modified code, nearly double that of traditional reinforcement learning methods, while maintaining a 98% task completion rate, effectively balancing structural comprehensiveness with functional correctness.",
    "arxiv_url": "https://arxiv.org/abs/2512.12706",
    "authors": [
      "Enhong Mu",
      "Minami Yoda",
      "Yan Zhang",
      "Mingyue Zhang",
      "Yutaka Matsuno",
      "Jialong Li"
    ],
    "first_author": "Enhong Mu",
    "category": [
      "Technical"
    ],
    "field": "Software Testing",
    "task": "Testing automation",
    "tags": [
      "AST-diff semantic interpretation",
      "LLM-driven subgoal decomposition",
      "Hybrid structural-functional reward",
      "Coverage-aware reinforcement learning",
      "Context-aware mapping of subgoals to code anchors",
      "Game-update regression testing"
    ],
    "summary": "本文提出SMART框架，利用LLM解析AST差异将更新意图分解为语义子目标，并通过将这些子目标与结构性代码锚点动态映射并构建混合奖励来引导强化学习代理在游戏更新测试中同时实现高分支覆盖率与高任务完成率。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.12706v1",
    "published": "2025-12-14",
    "update_time": "2025-12-14",
    "download_time": "2025-12-16 02:40:31"
  },
  {
    "id": "2512.11296",
    "title": "Few-Shot VLM-Based G-Code and HMI Verification in CNC Machining",
    "abstract": "Manual generation of G-code is important for learning the operation of CNC machines. Prior work in G-code verification uses Large-Language Models (LLMs), which primarily examine errors in the written programming. However, CNC machining requires extensive use and knowledge of the Human-Machine Interface (HMI), which displays machine status and errors. LLMs currently lack the capability to leverage knowledge of HMIs due to their inability to access the vision modality. This paper proposes a few-shot VLM-based verification approach that simultaneously evaluates the G-code and the HMI display for errors and safety status. The input dataset includes paired G-code text and associated HMI screenshots from a 15-slant-PRO lathe, including both correct and error-prone cases. To enable few-shot learning, the VLM is provided with a structured JSON schema based on prior heuristic knowledge. After determining the prompts, instances of G-code and HMI that either contain errors or are error free are used as few-shot examples to guide the VLM. The model was then evaluated in comparison to a zero-shot VLM through multiple scenarios of incorrect G-code and HMI errors with respect to per-slot accuracy. The VLM showed that few-shot prompting led to overall enhancement of detecting HMI errors and discrepancies with the G-code for more comprehensive debugging. Therefore, the proposed framework was demonstrated to be suitable for verification of manually generated G-code that is typically developed in CNC training.",
    "arxiv_url": "https://arxiv.org/abs/2512.11296",
    "authors": [
      "Yasaman Hashem Pour",
      "Nazanin Mahjourian",
      "Vinh Nguyen"
    ],
    "first_author": "Yasaman Hashem Pour",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Understanding",
    "tags": [
      "G-code verification",
      "HMI indicator recognition",
      "Multimodal VLM few-shot prompting",
      "Structured JSON slot-filling",
      "G-code–machine-state alignment",
      "CNC safety/status checking"
    ],
    "summary": "本文提出一种基于视觉-语言模型的少样本联合校验方法，通过将G-code文本与机床HMI截图配对并使用结构化JSON schema进行few-shot提示，从而同时检测G-code语法错误与HMI显示与代码的不一致以提升人工编写G-code的验证与安全性。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.11296v1",
    "published": "2025-12-12",
    "update_time": "2025-12-12",
    "download_time": "2025-12-16 02:48:32"
  },
  {
    "id": "2512.11270",
    "title": "A-LAMP: Agentic LLM-Based Framework for Automated MDP Modeling and Policy Generation",
    "abstract": "Applying reinforcement learning (RL) to real-world tasks requires converting informal descriptions into a formal Markov decision process (MDP), implementing an executable environment, and training a policy agent. Automating this process is challenging due to modeling errors, fragile code, and misaligned objectives, which often impede policy training. We introduce an agentic large language model (LLM)-based framework for automated MDP modeling and policy generation (A-LAMP), that automatically translates free-form natural language task descriptions into an MDP formulation and trained policy. The framework decomposes modeling, coding, and training into verifiable stages, ensuring semantic alignment throughout the pipeline. Across both classic control and custom RL domains, A-LAMP consistently achieves higher policy generation capability than a single state-of-the-art LLM model. Notably, even its lightweight variant, which is built on smaller language models, approaches the performance of much larger models. Failure analysis reveals why these improvements occur. In addition, a case study also demonstrates that A-LAMP generates environments and policies that preserve the task's optimality, confirming its correctness and reliability.",
    "arxiv_url": "https://arxiv.org/abs/2512.11270",
    "authors": [
      "Hong Je-Gal",
      "Chan-Bin Yi",
      "Hyun-Suk Lee"
    ],
    "first_author": "Hong Je-Gal",
    "category": [
      "Technical"
    ],
    "field": "Requirements & Design",
    "task": "Specification & Validation",
    "tags": [
      "Agentic multi-agent orchestration",
      "Automated MDP synthesis",
      "Natural-language to environment code generation",
      "Reward function specification",
      "Policy training automation",
      "Modular verifiable pipeline",
      "Failure-mode analysis"
    ],
    "summary": "该论文提出A-LAMP，一个基于多智能体LLM的框架，能够将自由文本任务描述自动转为形式化MDP、生成可执行环境与训练代码并产出可验证的策略，且在多域实验中优于单一大模型。",
    "quality": "High",
    "conference": "NeurIPS 2025 Workshop: Multi-Turn Interactions in Large Language Models 2025",
    "pdf_url": "https://arxiv.org/pdf/2512.11270v1",
    "published": "2025-12-12",
    "update_time": "2025-12-12",
    "download_time": "2025-12-16 02:49:40"
  },
  {
    "id": "2512.13515",
    "title": "Fine-tuned LLM-based Code Migration Framework",
    "abstract": "The study presents the outcomes of research and experimental validation in the domain of automated codebase migration, with a focus on addressing challenges in transitioning SQL-based systems. The proposed method for migration essentially appears as a framework that leverages the best aspects of traditional software engineering techniques and provides an iterative, scalable, precise and efficient solution for modern database transformations. The central piece of the approach is the integration of a fine-tuned Large Language Model to address critical issues in SQL code conversion, such as syntax mapping, resolving discrepancies between Oracle PL/SQL and PostgreSQL, and optimising database elements such as stored procedures, triggers, views, and overall database logic. Thus, the method involves a trade-off between fine-tuning and prompt engineering. Special attention is given to a fine-tuning approach, which enhances the adaptability and compatibility with migration requirements across the entire database. According to the achieved results, fine-tuning plays a very important role. The study employs targeted evaluation methodologies along with computational metrics to measure the success of iterative conversion cycles. Core innovations include automated SQL feature detection, semi-supervised error analysis and integration of Subject Matter Experts feedback within a systematic migration workflow. The methodology achieves significant reductions in Syntax Error Rates, enhances feature alignment throughout migration iterations, and leverages dataset sampling to ensure continual improvement. By embedding GAI into the migration process, the framework facilitates precise feature mapping, semi-automated error resolution, and data-driven optimisation loops, improving workflow efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2512.13515",
    "authors": [
      "Oleg Grynets",
      "Vasyl Lyashkevych",
      "Dmytro Baran",
      "Maksym Orliansky",
      "Taras Zelenyy",
      "Markiian Leshchyshyn"
    ],
    "first_author": "Oleg Grynets",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Translation",
    "tags": [
      "Two-stage fine-tuning",
      "Hybrid Code Feature Profiling Engine (HCFPE)",
      "Retrieval-Augmented Generation for migration",
      "Closed-loop human-in-the-loop error correction",
      "SQL feature detection and annotation",
      "Multi-repository vs unified FAISS knowledge base",
      "Syntactic-description and direct transformation dataset configurator",
      "Semi-supervised error analysis and failure taxonomy"
    ],
    "summary": "本文提出了一个面向Oracle到PostgreSQL的自动化代码迁移框架，结合两阶段微调、特征分析引擎、RAG知识库与人机闭环错误分析以提升迁移的语法与语义准确性。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.13515v1",
    "published": "2025-12-15",
    "update_time": "2025-12-15",
    "download_time": "2025-12-17 01:48:11"
  },
  {
    "id": "2512.13438",
    "title": "From User Interface to Agent Interface: Efficiency Optimization of UI Representations for LLM Agents",
    "abstract": "While Large Language Model (LLM) agents show great potential for automated UI navigation such as automated UI testing and AI assistants, their efficiency has been largely overlooked. Our motivating study reveals that inefficient UI representation creates a critical performance bottleneck. However, UI representation optimization, formulated as the task of automatically generating programs that transform UI representations, faces two unique challenges. First, the lack of Boolean oracles, which traditional program synthesis uses to decisively validate semantic correctness, poses a fundamental challenge to co-optimization of token efficiency and completeness. Second, the need to process large, complex UI trees as input while generating long, compositional transformation programs, making the search space vast and error-prone. Toward addressing the preceding limitations, we present UIFormer, the first automated optimization framework that synthesizes UI transformation programs by conducting constraint-based optimization with structured decomposition of the complex synthesis task. First, UIFormer restricts the program space using a domain-specific language (DSL) that captures UI-specific operations. Second, UIFormer conducts LLM-based iterative refinement with correctness and efficiency rewards, providing guidance for achieving the efficiency-completeness co-optimization. UIFormer operates as a lightweight plugin that applies transformation programs for seamless integration with existing LLM agents, requiring minimal modifications to their core logic. Evaluations across three UI navigation benchmarks spanning Android and Web platforms with five LLMs demonstrate that UIFormer achieves 48.7% to 55.8% token reduction with minimal runtime overhead while maintaining or improving agent performance. Real-world industry deployment at WeChat further validates the practical impact of UIFormer.",
    "arxiv_url": "https://arxiv.org/abs/2512.13438",
    "authors": [
      "Dezhi Ran",
      "Zhi Gong",
      "Yuzhe Guo",
      "Mengzhou Wu",
      "Yuan Cao",
      "Haochuan Lu",
      "Hengyu Zhang",
      "Xia Zeng",
      "Gang Cao",
      "Liangchao Yao",
      "Yuetang Deng",
      "Wei Yang",
      "Tao Xie"
    ],
    "first_author": "Dezhi Ran",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Performance Optimization",
    "tags": [
      "UI representation optimization",
      "DSL for UI transformations",
      "Program synthesis for UI trees",
      "Iterative LLM-based refinement",
      "Efficiency–completeness co-optimization",
      "Local decomposition and tree merging",
      "Lightweight plugin integration for agents",
      "Constraint-based evaluation rewards"
    ],
    "summary": "本文提出UIFORMER，通过设计面向UI的DSL并结合基于LLM的迭代约束优化与结构化分解，自动合成将复杂UI树转换为低令牌开销且保持语义完备的表示的转换程序，从而显著降低LLM代理的token成本并在多平台基准与实际微信部署中保持或提升代理性能。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.13438v1",
    "published": "2025-12-15",
    "update_time": "2025-12-15",
    "download_time": "2025-12-17 01:49:07"
  },
  {
    "id": "2512.13607",
    "title": "Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models",
    "abstract": "Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes.",
    "arxiv_url": "https://arxiv.org/abs/2512.13607",
    "authors": [
      "Boxin Wang",
      "Chankyu Lee",
      "Nayeon Lee",
      "Sheng-Chieh Lin",
      "Wenliang Dai",
      "Yang Chen",
      "Yangyi Chen",
      "Zhuolin Yang",
      "Zihan Liu",
      "Mohammad Shoeybi",
      "Bryan Catanzaro",
      "Wei Ping"
    ],
    "first_author": "Boxin Wang",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Cascaded domain-wise reinforcement learning",
      "RLHF as a pre-step to boost reasoning",
      "Instruction-following RL (IF-RL)",
      "Math-specific RL curriculum",
      "Code RL for competitive programming",
      "Software-engineering RL (SWE RL)",
      "Execution-free reward modeling for SWE",
      "Generation–retrieval for code localization",
      "Test-time scaling / thinking modes",
      "Sequential-domain curriculum to mitigate forgetting"
    ],
    "summary": "本文提出级联领域强化学习（Cascade RL）来训练通用推理模型Nemotron‑Cascade，通过序贯的领域级RL（包含RLHF、IF‑RL、Math RL、Code RL、SWE RL）与工程化技巧提升代码推理与竞赛编程等任务的表现，并公开训练与数据配方。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.13607v1",
    "published": "2025-12-15",
    "update_time": "2025-12-15",
    "download_time": "2025-12-17 01:50:27"
  },
  {
    "id": "2512.13102",
    "title": "Socratic Students: Teaching Language Models to Learn by Asking Questions",
    "abstract": "Large Language Models (LLMs) excel at static interactions, where they answer user queries by retrieving knowledge encoded in their parameters. However, in many real-world settings, such as educational tutoring or medical assistance, relevant information is not directly available and must be actively acquired through dynamic interactions. An interactive agent would recognize its own uncertainty, ask targeted questions, and retain new knowledge efficiently. Prior work has primarily explored effective ways for a teacher to instruct the student, where the teacher identifies student gaps and provides guidance. In this work, we shift the focus to the student and investigate effective strategies to actively query the teacher in seeking useful information. Across math and coding benchmarks, where baseline student models begin with near-zero performance, we show that student-led approaches consistently yield absolute Pass@k improvements of at least 0.5 over static baselines. To improve question quality, we train students using Direct Preference Optimization (DPO) with guidance from either self or stronger students. We find that this guided training enables smaller models to learn how to ask better questions, further enhancing learning efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2512.13102",
    "authors": [
      "Rajeev Bhatt Ambati",
      "Tianyi Niu",
      "Aashu Singh",
      "Shlok Mishra",
      "Shashank Srivastava",
      "Snigdha Chaturvedi"
    ],
    "first_author": "Rajeev Bhatt Ambati",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Student-led interaction",
      "Active question generation",
      "Direct Preference Optimization for question selection",
      "Self- and peer-guidance",
      "Chain-of-Thought guided questioning",
      "Pre- and mid-assessment grounding",
      "Teacher-student dialogue protocol",
      "Pass@k-based preference collection"
    ],
    "summary": "本文提出一种学生主导的交互学习框架，训练小型模型通过主动提问（包含CoT指导、评估插入和基于DPO的自我/同辈指导）与强教师交互，在数学与编程任务上显著提升了Pass@k表现。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.13102v1",
    "published": "2025-12-15",
    "update_time": "2025-12-15",
    "download_time": "2025-12-17 01:57:25"
  },
  {
    "id": "2512.14429",
    "title": "Seismology modeling agent: A smart assistant for geophysical researchers",
    "abstract": "To address the steep learning curve and reliance on complex manual file editing and command-line operations in the traditional workflow of the mainstream open-source seismic wave simulation software SPECFEM, this paper proposes an intelligent, interactive workflow powered by Large Language Models (LLMs). We introduce the first Model Context Protocol (MCP) server suite for SPECFEM (supporting 2D, 3D Cartesian, and 3D Globe versions), which decomposes the entire simulation process into discrete, agent-executable tools spanning from parameter generation and mesh partitioning to solver execution and visualization. This approach enables a paradigm shift from file-driven to intent-driven conversational interactions. The framework supports both fully automated execution and human-in-the-loop collaboration, allowing researchers to guide simulation strategies in real time and retain scientific decision-making authority while significantly reducing tedious low-level operations. Validated through multiple case studies, the workflow operates seamlessly in both autonomous and interactive modes, yielding high-fidelity results consistent with standard baselines. As the first application of MCP technology to computational seismology, this study significantly lowers the entry barrier, enhances reproducibility, and offers a promising avenue for advancing computational geophysics toward AI-assisted and automated scientific research. The complete source code is available at https://github.com/RenYukun1563/specfem-mcp.",
    "arxiv_url": "https://arxiv.org/abs/2512.14429",
    "authors": [
      "Yukun Ren",
      "Siwei Yu",
      "Kai Chen",
      "Jianwei Ma"
    ],
    "first_author": "Yukun Ren",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Agents",
    "tags": [
      "Code Agents",
      "MCP Integration",
      "Agentic Workflow",
      "Seismic Simulation",
      "Human-in-the-loop",
      "Legacy Software Wrapping",
      "Intent-driven Interface",
      "Reproducibility"
    ],
    "summary": "本文为SPECFEM构建了首个Model Context Protocol (MCP) 服务器套件，并结合LLM驱动的代理实现以自然语言为中心的自动化与交互式地震波模拟工作流，从而降低使用门槛并提升可重复性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.14429v1",
    "published": "2025-12-16",
    "update_time": "2025-12-16",
    "download_time": "2025-12-18 01:48:51"
  },
  {
    "id": "2512.14233",
    "title": "PentestEval: Benchmarking LLM-based Penetration Testing with Modular and Stage-Level Design",
    "abstract": "Penetration testing is essential for assessing and strengthening system security against real-world threats, yet traditional workflows remain highly manual, expertise-intensive, and difficult to scale. Although recent advances in Large Language Models (LLMs) offer promising opportunities for automation, existing applications rely on simplistic prompting without task decomposition or domain adaptation, resulting in unreliable black-box behavior and limited insight into model capabilities across penetration testing stages. To address this gap, we introduce PentestEval, the first comprehensive benchmark for evaluating LLMs across six decomposed penetration testing stages: Information Collection, Weakness Gathering and Filtering, Attack Decision-Making, Exploit Generation and Revision. PentestEval integrates expert-annotated ground truth with a fully automated evaluation pipeline across 346 tasks covering all stages in 12 realistic vulnerable scenarios. Our stage-level evaluation of 9 widely used LLMs reveals generally weak performance and distinct limitations across the stages of penetration-testing workflow. End-to-end pipelines reach only 31% success rate, and existing LLM-powered systems such as PentestGPT, PentestAgent, and VulnBot exhibit similar limitations, with autonomous agents failing almost entirely. These findings highlight that autonomous penetration testing demands stronger structured reasoning, where modularization enhances each individual stage and improves overall performance. PentestEval provides the foundational benchmark needed for future research on fine-grained, stage-level evaluation, paving the way toward more reliable LLM-based automation.",
    "arxiv_url": "https://arxiv.org/abs/2512.14233",
    "authors": [
      "Ruozhao Yang",
      "Mingfei Cheng",
      "Gelei Deng",
      "Tianwei Zhang",
      "Junjie Wang",
      "Xiaofei Xie"
    ],
    "first_author": "Ruozhao Yang",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Security Testing",
    "task": "Penetration Testing Evaluation",
    "tags": [
      "Exploit Generation",
      "Exploit Revision",
      "Attack Decision-Making",
      "Weakness Gathering",
      "Weakness Filtering",
      "Stage-level Evaluation",
      "Modular Benchmark",
      "Ground-truth Annotations",
      "Autonomous Agent Failure",
      "Structured Reasoning",
      "End-to-End Evaluation"
    ],
    "summary": "本文提出PentestEval——一个针对渗透测试六个分阶段的模块化基准，配备专家注释与自动化评测管线，用以细粒度评估LLM在信息收集、弱点过滤、攻击决策与利用生成等环节的能力并揭示现有模型与自动化系统的显著局限性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.14233v1",
    "published": "2025-12-16",
    "update_time": "2025-12-16",
    "download_time": "2025-12-18 01:49:18"
  },
  {
    "id": "2512.14481",
    "title": "SASQ: Static Activation Scaling for Quantization-Aware Training in Large Language Models",
    "abstract": "Large language models (LLMs) excel at natural language tasks but face deployment challenges due to their growing size outpacing GPU memory advancements. Model quantization mitigates this issue by lowering weight and activation precision, but existing solutions face fundamental trade-offs: dynamic quantization incurs high computational overhead and poses deployment challenges on edge devices, while static quantization sacrifices accuracy. Existing approaches of quantization-aware training (QAT) further suffer from weight training costs. We propose SASQ: a lightweight QAT framework specifically tailored for activation quantization factors. SASQ exclusively optimizes only the quantization factors (without changing pre-trained weights), enabling static inference with high accuracy while maintaining deployment efficiency. SASQ adaptively truncates some outliers, thereby reducing the difficulty of quantization while preserving the distributional characteristics of the activations. SASQ not only surpasses existing SOTA quantization schemes but also outperforms the corresponding FP16 models. On LLaMA2-7B, it achieves 5.2% lower perplexity than QuaRot and 4.7% lower perplexity than the FP16 model on WikiText2.",
    "arxiv_url": "https://arxiv.org/abs/2512.14481",
    "authors": [
      "Shizhuo Mao",
      "Song Chen",
      "Yi Kang"
    ],
    "first_author": "Shizhuo Mao",
    "category": [
      "Technical"
    ],
    "field": "Model Compression & Deployment",
    "task": "Activation Quantization-Aware Training",
    "tags": [
      "Activation Quantization",
      "Quantization-Aware Training",
      "Static Quantization",
      "Outlier Truncation",
      "Phased Quantization",
      "Static Inference",
      "Inference Efficiency",
      "Per-token Quantization"
    ],
    "summary": "本文提出SASQ，一种仅优化激活量化因子的轻量级量化感知训练方法，通过自适应截断异常值与分阶段量化实现静态推理，从而在不修改预训练权重的情况下提升大模型静态量化精度与部署效率。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.14481v1",
    "published": "2025-12-16",
    "update_time": "2025-12-16",
    "download_time": "2025-12-18 01:52:36"
  },
  {
    "id": "2512.14417",
    "title": "PortAgent: LLM-driven Vehicle Dispatching Agent for Port Terminals",
    "abstract": "Vehicle Dispatching Systems (VDSs) are critical to the operational efficiency of Automated Container Terminals (ACTs). However, their widespread commercialization is hindered due to their low transferability across diverse terminals. This transferability challenge stems from three limitations: high reliance on port operational specialists, a high demand for terminal-specific data, and time-consuming manual deployment processes. Leveraging the emergence of Large Language Models (LLMs), this paper proposes PortAgent, an LLM-driven vehicle dispatching agent that fully automates the VDS transferring workflow. It bears three features: (1) no need for port operations specialists; (2) low need of data; and (3) fast deployment. Specifically, specialist dependency is eliminated by the Virtual Expert Team (VET). The VET collaborates with four virtual experts, including a Knowledge Retriever, Modeler, Coder, and Debugger, to emulate a human expert team for the VDS transferring workflow. These experts specialize in the domain of terminal VDS via a few-shot example learning approach. Through this approach, the experts are able to learn VDS-domain knowledge from a few VDS examples. These examples are retrieved via a Retrieval-Augmented Generation (RAG) mechanism, mitigating the high demand for terminal-specific data. Furthermore, an automatic VDS design workflow is established among these experts to avoid extra manual interventions. In this workflow, a self-correction loop inspired by the LLM Reflexion framework is created",
    "arxiv_url": "https://arxiv.org/abs/2512.14417",
    "authors": [
      "Jia Hu",
      "Junqi Li",
      "Weimeng Lin",
      "Peng Jia",
      "Yuxiong Ji",
      "Jintao Lai"
    ],
    "first_author": "Jia Hu",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Agents",
    "tags": [
      "Code Agents",
      "Virtual Expert Team",
      "RAG",
      "Few-shot Adaptation",
      "Self-Correction",
      "Automated Deployment",
      "Vehicle Dispatching",
      "AGV Scheduling",
      "Domain Transferability",
      "Human-Free Validation"
    ],
    "summary": "本文提出PortAgent——一种基于大型语言模型的港口车辆调度迁移代理，通过构建虚拟专家团队、检索增强的少样本学习与自我修正的代码生成-执行-调试闭环，实现低数据、无专家且快速部署的VDS迁移，在未见场景上取得高成功率并显著缩短部署时间。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.14417v1",
    "published": "2025-12-16",
    "update_time": "2025-12-16",
    "download_time": "2025-12-18 01:54:02"
  },
  {
    "id": "2512.15699",
    "title": "FrontierCS: Evolving Challenges for Evolving Intelligence",
    "abstract": "We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. Models solve these tasks by implementing executable programs rather than outputting a direct answer. FrontierCS includes algorithmic problems, which are often NP-hard variants of competitive programming problems with objective partial scoring, and research problems with the same property. For each problem we provide an expert reference solution and an automatic evaluator. Combining open-ended design, measurable progress, and expert curation, FrontierCS provides a benchmark at the frontier of computer-science difficulty. Empirically, we find that frontier reasoning models still lag far behind human experts on both the algorithmic and research tracks, that increasing reasoning budgets alone does not close this gap, and that models often over-optimize for generating merely workable code instead of discovering high-quality algorithms and system designs.",
    "arxiv_url": "https://arxiv.org/abs/2512.15699",
    "authors": [
      "Qiuyang Mang",
      "Wenhao Chai",
      "Zhifei Li",
      "Huanzhi Mao",
      "Shang Zhou",
      "Alexander Du",
      "Hanchen Li",
      "Shu Liu",
      "Edwin Chen",
      "Yichuan Wang",
      "Xieting Chu",
      "Zerui Cheng",
      "Yuan Xu",
      "Tian Xia",
      "Zirui Wang",
      "Tianneng Shi",
      "Jianzhu Yao",
      "Yilong Zhao",
      "Qizheng Zhang",
      "Charlie Ruan",
      "Zeyu Shen",
      "Kaiyuan Liu",
      "Runyuan He",
      "Dong Xing",
      "Zerui Li",
      "Zirong Zeng",
      "Yige Jiang",
      "Lufeng Cheng",
      "Ziyi Zhao",
      "Youran Sun",
      "Wesley Zheng",
      "Meiyuwang Zhang",
      "Ruyi Ji",
      "Xuechang Tu",
      "Zihan Zheng",
      "Zexing Chen",
      "Kangyang Zhou",
      "Zhaozi Wang",
      "Jingbang Chen",
      "Aleksandra Korolova",
      "Peter Henderson",
      "Pramod Viswanath",
      "Vijay Ganesh",
      "Saining Xie",
      "Zhuang Liu",
      "Dawn Song",
      "Sewon Min",
      "Ion Stoica",
      "Joseph E. Gonzalez",
      "Jingbo Shang",
      "Alvin Cheung"
    ],
    "first_author": "Qiuyang Mang",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Algorithmic Optimization",
      "Open-Ended Benchmark",
      "Executable Solver Evaluation",
      "Automatic Scoring",
      "Parametric Instance Generation",
      "Expert Reference Solutions",
      "Human-Model Gap"
    ],
    "summary": "FrontierCS 提出一个含 156 个多领域、开放式且可量化评分的计算机科学编程基准，要求模型生成可执行求解器并通过自动评估器在无已知最优解的问题上进行定量比较，实验证明当前模型在这些前沿问题上远落后于人类专家。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.15699v1",
    "published": "2025-12-17",
    "update_time": "2025-12-17",
    "download_time": "2025-12-19 01:51:14"
  },
  {
    "id": "2512.15468",
    "title": "How Do Semantically Equivalent Code Transformations Impact Membership Inference on LLMs for Code?",
    "abstract": "The success of large language models for code relies on vast amounts of code data, including public open-source repositories, such as GitHub, and private, confidential code from companies. This raises concerns about intellectual property compliance and the potential unauthorized use of license-restricted code. While membership inference (MI) techniques have been proposed to detect such unauthorized usage, their effectiveness can be undermined by semantically equivalent code transformation techniques, which modify code syntax while preserving semantic.   In this work, we systematically investigate whether semantically equivalent code transformation rules might be leveraged to evade MI detection. The results reveal that model accuracy drops by only 1.5% in the worst case for each rule, demonstrating that transformed datasets can effectively serve as substitutes for fine-tuning. Additionally, we find that one of the rules (RenameVariable) reduces MI success by 10.19%, highlighting its potential to obscure the presence of restricted code. To validate these findings, we conduct a causal analysis confirming that variable renaming has the strongest causal effect in disrupting MI detection. Notably, we find that combining multiple transformations does not further reduce MI effectiveness. Our results expose a critical loophole in license compliance enforcement for training large language models for code, showing that MI detection can be substantially weakened by transformation-based obfuscation techniques.",
    "arxiv_url": "https://arxiv.org/abs/2512.15468",
    "authors": [
      "Hua Yang",
      "Alejandro Velasco",
      "Thanh Le-Cong",
      "Md Nazmul Haque",
      "Bowen Xu",
      "Denys Poshyvanyk"
    ],
    "first_author": "Hua Yang",
    "category": [
      "Empirical",
      "Benchmark"
    ],
    "field": "Training Data Auditing & Privacy",
    "task": "Membership Inference",
    "tags": [
      "Semantics-Preserving Obfuscation",
      "RenameVariable",
      "Membership Inference",
      "Causal Analysis",
      "Model Memorization",
      "Fine-tuning Robustness",
      "License Compliance"
    ],
    "summary": "本文系统研究了23种语义等价代码变换对代码型大语言模型会员推断（MI）的影响，发现变换后模型性能几乎不降但MI成功率显著下降（尤其是变量重命名），并通过结构因果模型验证了该因果效应，揭示了基于变换的规避许可合规检测的风险。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.15468v1",
    "published": "2025-12-17",
    "update_time": "2025-12-17",
    "download_time": "2025-12-19 01:51:46"
  },
  {
    "id": "2512.15688",
    "title": "BashArena: A Control Setting for Highly Privileged AI Agents",
    "abstract": "Future AI agents might run autonomously with elevated privileges. If these agents are misaligned, they might abuse these privileges to cause serious damage. The field of AI control develops techniques that make it harder for misaligned AIs to cause such damage, while preserving their usefulness. We introduce BashArena, a setting for studying AI control techniques in security-critical environments. BashArena contains 637 Linux system administration and infrastructure engineering tasks in complex, realistic environments, along with four sabotage objectives (execute malware, exfiltrate secrets, escalate privileges, and disable firewall) for a red team to target. We evaluate multiple frontier LLMs on their ability to complete tasks, perform sabotage undetected, and detect sabotage attempts. Claude Sonnet 4.5 successfully executes sabotage while evading monitoring by GPT-4.1 mini 26% of the time, at 4% trajectory-wise FPR. Our findings provide a baseline for designing more effective control protocols in BashArena. We release the dataset as a ControlArena setting and share our task generation pipeline.",
    "arxiv_url": "https://arxiv.org/abs/2512.15688",
    "authors": [
      "Adam Kaufman",
      "James Lucassen",
      "Tyler Tracy",
      "Cody Rushing",
      "Aryan Bhatt"
    ],
    "first_author": "Adam Kaufman",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "AI Control & Security",
    "task": "Privileged-Agent Control Benchmarking",
    "tags": [
      "Red Teaming",
      "Control Protocols",
      "Monitoring Evasion",
      "Privileged Execution",
      "Multi-Step Interaction",
      "Docker Environments",
      "Adversarial Benchmark",
      "Side-Task Detection",
      "Security Testing",
      "Autonomous Agents"
    ],
    "summary": "BashArena 提出一个包含637个复杂 Linux 运维主任务和四类破坏（下载恶意软件、泄露秘密、权限提升、禁用防火墙）的对抗性控制基准，用于评估在高权限环境中 AI 代理的破坏能力与监测防护效果并发布了任务生成管道与数据集。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.15688v1",
    "published": "2025-12-17",
    "update_time": "2025-12-17",
    "download_time": "2025-12-19 01:52:32"
  },
  {
    "id": "2512.15466",
    "title": "On Assessing the Relevance of Code Reviews Authored by Generative Models",
    "abstract": "The use of large language models like ChatGPT in code review offers promising efficiency gains but also raises concerns about correctness and safety. Existing evaluation methods for code review generation either rely on automatic comparisons to a single ground truth, which fails to capture the variability of human perspectives, or on subjective assessments of \"usefulness\", a highly ambiguous concept. We propose a novel evaluation approach based on what we call multi-subjective ranking. Using a dataset of 280 self-contained code review requests and corresponding comments from CodeReview StackExchange, multiple human judges ranked the quality of ChatGPT-generated comments alongside the top human responses from the platform. Results show that ChatGPT's comments were ranked significantly better than human ones, even surpassing StackExchange's accepted answers. Going further, our proposed method motivates and enables more meaningful assessments of generative AI's performance in code review, while also raising awareness of potential risks of unchecked integration into review processes.",
    "arxiv_url": "https://arxiv.org/abs/2512.15466",
    "authors": [
      "Robert Heumüller",
      "Frank Ortmeier"
    ],
    "first_author": "Robert Heumüller",
    "category": [
      "Empirical",
      "Benchmark"
    ],
    "field": "Maintenance",
    "task": "Code Review",
    "tags": [
      "Multi-Subjective Ranking",
      "Human Annotation",
      "Code Review Comment Generation",
      "Evaluation Tooling",
      "Statistical Analysis",
      "Stylistic Bias"
    ],
    "summary": "本文提出并应用多主观排序（multi-subjective ranking）的评估方法，对来自CodeReview StackExchange的280个实例由四名评审者对比排序，结果显示ChatGPT生成的代码审查评论在排名上显著优于人类评论，并发布了数据集与Rankr评估工具，同时讨论了生成评论因语体更为精炼导致的表面偏好风险。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.15466v1",
    "published": "2025-12-17",
    "update_time": "2025-12-17",
    "download_time": "2025-12-19 01:54:45"
  },
  {
    "id": "2512.16816",
    "title": "Toward Systematic Counterfactual Fairness Evaluation of Large Language Models: The CAFFE Framework",
    "abstract": "Nowadays, Large Language Models (LLMs) are foundational components of modern software systems. As their influence grows, concerns about fairness have become increasingly pressing. Prior work has proposed metamorphic testing to detect fairness issues, applying input transformations to uncover inconsistencies in model behavior. This paper introduces an alternative perspective for testing counterfactual fairness in LLMs, proposing a structured and intent-aware framework coined CAFFE (Counterfactual Assessment Framework for Fairness Evaluation). Inspired by traditional non-functional testing, CAFFE (1) formalizes LLM-Fairness test cases through explicitly defined components, including prompt intent, conversational context, input variants, expected fairness thresholds, and test environment configuration, (2) assists testers by automatically generating targeted test data, and (3) evaluates model responses using semantic similarity metrics. Our experiments, conducted on three different architectural families of LLM, demonstrate that CAFFE achieves broader bias coverage and more reliable detection of unfair behavior than existing metamorphic approaches.",
    "arxiv_url": "https://arxiv.org/abs/2512.16816",
    "authors": [
      "Alessandra Parziale",
      "Gianmario Voria",
      "Valeria Pontillo",
      "Gemma Catolino",
      "Andrea De Lucia",
      "Fabio Palomba"
    ],
    "first_author": "Alessandra Parziale",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Software Testing",
    "task": "Test Generation",
    "tags": [
      "Counterfactual Fairness",
      "Intent-aware Testing",
      "Structured Test Cases",
      "Semantic Similarity Evaluation",
      "Stereotype-aware Prompt Generation",
      "Fairness Bug Detection",
      "Reproducibility"
    ],
    "summary": "本文提出CAFFE框架，通过结构化的意图感知反事实测试用例生成和基于语义相似度的评估，系统性地检测并量化大语言模型的公平性缺陷，在多种模型上较现有变形测试方法提升了明显的检测覆盖与准确性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.16816v1",
    "published": "2025-12-18",
    "update_time": "2025-12-18",
    "download_time": "2025-12-20 01:45:28"
  },
  {
    "id": "2512.16790",
    "title": "Inside Out: Uncovering How Comment Internalization Steers LLMs for Better or Worse",
    "abstract": "While comments are non-functional elements of source code, Large Language Models (LLM) frequently rely on them to perform Software Engineering (SE) tasks. Yet, where in the model this reliance resides, and how it affects performance, remains poorly understood. We present the first concept-level interpretability study of LLMs in SE, analyzing three tasks - code completion, translation, and refinement - through the lens of internal comment representation. Using Concept Activation Vectors (CAV), we show that LLMs not only internalize comments as distinct latent concepts but also differentiate between subtypes such as Javadocs, inline, and multiline comments. By systematically activating and deactivating these concepts in the LLMs' embedding space, we observed significant, model-specific, and task-dependent shifts in performance ranging from -90% to +67%. Finally, we conducted a controlled experiment using the same set of code inputs, prompting LLMs to perform 10 distinct SE tasks while measuring the activation of the comment concept within their latent representations. We found that code summarization consistently triggered the strongest activation of comment concepts, whereas code completion elicited the weakest sensitivity. These results open a new direction for building SE tools and models that reason about and manipulate internal concept representations rather than relying solely on surface-level input.",
    "arxiv_url": "https://arxiv.org/abs/2512.16790",
    "authors": [
      "Aaron Imani",
      "Mohammad Moshirpour",
      "Iftekhar Ahmed"
    ],
    "first_author": "Aaron Imani",
    "category": [
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Understanding",
    "tags": [
      "Interpretability",
      "Comment Internalization",
      "Concept Activation Vectors",
      "Comment Type Differentiation",
      "Latent Space Intervention",
      "Task-Conditional Sensitivity",
      "Model-Specific Effects"
    ],
    "summary": "本文首次使用概念激活向量（CAV）在LLM的隐藏表示中实证揭示了代码注释作为独立概念的内部化及不同注释类型的区分，并通过激活/抑制这些概念展示了对代码补全、翻译和重构等任务性能可正可负的显著影响。",
    "quality": "High",
    "conference": "IEEE/ACM International Conference on Software Engineering (ICSE 2026)",
    "pdf_url": "https://arxiv.org/pdf/2512.16790v1",
    "published": "2025-12-18",
    "update_time": "2025-12-18",
    "download_time": "2025-12-20 01:45:49"
  },
  {
    "id": "2512.16814",
    "title": "Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs",
    "abstract": "Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.",
    "arxiv_url": "https://arxiv.org/abs/2512.16814",
    "authors": [
      "William English",
      "Dominic Simon",
      "Sumit Kumar Jha",
      "Rickard Ewetz"
    ],
    "first_author": "William English",
    "category": [
      "Technical"
    ],
    "field": "Requirements & Design",
    "task": "Specification & Validation",
    "tags": [
      "Grammar Forcing",
      "Grammar-Constrained Decoding",
      "Atomic Predicate Lifting",
      "Masked AP Prediction",
      "Co-reference Handling",
      "Theoretical Learning Analysis",
      "Out-of-Distribution Robustness"
    ],
    "summary": "本文提出GraFT框架，通过对原子命题使用掩码语言模型进行提升并在序列到序列翻译中基于时序逻辑语法动态约束输出词元，从而提高自然语言到时序逻辑的翻译准确性，并给出理论证明与多基准实证评估。",
    "quality": "High",
    "conference": "Proceedings of the 42nd International Conference on Machine Learning (ICML 2025)",
    "pdf_url": "https://arxiv.org/pdf/2512.16814v1",
    "published": "2025-12-18",
    "update_time": "2025-12-18",
    "download_time": "2025-12-20 01:48:23"
  },
  {
    "id": "2512.16770",
    "title": "GinSign: Grounding Natural Language Into System Signatures for Temporal Logic Translation",
    "abstract": "Natural language (NL) to temporal logic (TL) translation enables engineers to specify, verify, and enforce system behaviors without manually crafting formal specifications-an essential capability for building trustworthy autonomous systems. While existing NL-to-TL translation frameworks have demonstrated encouraging initial results, these systems either explicitly assume access to accurate atom grounding or suffer from low grounded translation accuracy. In this paper, we propose a framework for Grounding Natural Language Into System Signatures for Temporal Logic translation called GinSign. The framework introduces a grounding model that learns the abstract task of mapping NL spans onto a given system signature: given a lifted NL specification and a system signature $\\mathcal{S}$, the classifier must assign each lifted atomic proposition to an element of the set of signature-defined atoms $\\mathcal{P}$. We decompose the grounding task hierarchically -- first predicting predicate labels, then selecting the appropriately typed constant arguments. Decomposing this task from a free-form generation problem into a structured classification problem permits the use of smaller masked language models and eliminates the reliance on expensive LLMs. Experiments across multiple domains show that frameworks which omit grounding tend to produce syntactically correct lifted LTL that is semantically nonequivalent to grounded target expressions, whereas our framework supports downstream model checking and achieves grounded logical-equivalence scores of $95.5\\%$, a $1.4\\times$ improvement over SOTA.",
    "arxiv_url": "https://arxiv.org/abs/2512.16770",
    "authors": [
      "William English",
      "Chase Walker",
      "Dominic Simon",
      "Rickard Ewetz"
    ],
    "first_author": "William English",
    "category": [
      "Technical"
    ],
    "field": "Requirements & Design",
    "task": "Specification & Validation",
    "tags": [
      "Predicate Grounding",
      "Argument Grounding",
      "System Signatures",
      "Lifted NL Translation",
      "Hierarchical Grounding",
      "Executable LTL",
      "Model Checking",
      "Logical Equivalence"
    ],
    "summary": "本文提出GinSign框架，通过将自然语言中提升的原子命题分层地分类为系统签名中的谓词和类型化参数来实现可执行的时序逻辑（LTL）翻译，从而显著提升了有语义的地面化翻译准确率并支持下游模型检验。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.16770v1",
    "published": "2025-12-18",
    "update_time": "2025-12-18",
    "download_time": "2025-12-20 01:49:12"
  },
  {
    "id": "2512.16272",
    "title": "Beyond Blind Spots: Analytic Hints for Mitigating LLM-Based Evaluation Pitfalls",
    "abstract": "Large Language Models are increasingly deployed as judges (LaaJ) in code generation pipelines. While attractive for scalability, LaaJs tend to overlook domain specific issues raising concerns about their reliability in critical evaluation tasks. To better understand these limitations in practice, we examine LaaJ behavior in a concrete industrial use case: legacy code modernization via COBOL code generation. In this setting, we find that even production deployed LaaJs can miss domain critical errors, revealing consistent blind spots in their evaluation capabilities.   To better understand these blind spots, we analyze generated COBOL programs and associated LaaJs judgments, drawing on expert knowledge to construct a preliminary taxonomy. Based on this taxonomy, we develop a lightweight analytic checker tool that flags over 30 domain specific issues observed in practice. We use its outputs as analytic hints, dynamically injecting them into the judges prompt to encourage LaaJ to revisit aspects it may have overlooked.   Experiments on a test set of 100 programs using four production level LaaJs show that LaaJ alone detects only about 45% of the errors present in the code (in all judges we tested), while the analytic checker alone lacks explanatory depth. When combined, the LaaJ+Hints configuration achieves up to 94% coverage (for the best performing judge and injection prompt) and produces qualitatively richer, more accurate explanations, demonstrating that analytic-LLM hybrids can substantially enhance evaluation reliability in deployed pipelines. We release the dataset and all used prompts.",
    "arxiv_url": "https://arxiv.org/abs/2512.16272",
    "authors": [
      "Ora Nova Fandina",
      "Eitan Farchi",
      "Shmulik Froimovich",
      "Raviv Gal",
      "Wesam Ibraheem",
      "Rami Katan",
      "Alice Podolsky"
    ],
    "first_author": "Ora Nova Fandina",
    "category": [
      "Technical",
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Analytic Hints",
      "LaaJ Blind Spots",
      "Rule-based Checker",
      "Hint Injection",
      "Evaluation Taxonomy",
      "COBOL Modernization",
      "Prompt Engineering",
      "Hybrid Evaluation",
      "Non-local Reasoning"
    ],
    "summary": "本文在COBOL代码现代化评估中发现并分类了LLM作为评判器（LaaJ）的领域盲点，构建了含30+问题类型的基于规则的分析检查器，并通过将解析性提示注入评判模型的提示中显著提升了错误检测覆盖率，且发布了数据集与提示。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.16272v1",
    "published": "2025-12-18",
    "update_time": "2025-12-18",
    "download_time": "2025-12-21 01:58:24"
  },
  {
    "id": "2512.16070",
    "title": "LLM4Perf: Large Language Models Are Effective Samplers for Multi-Objective Performance Modeling (Copy)",
    "abstract": "The performance of modern software systems is critically dependent on their complex configuration options. Building accurate performance models to navigate this vast space requires effective sampling strategies, yet existing methods often struggle with multi-objective optimization and cannot leverage semantic information from documentation. The recent success of Large Language Models (LLMs) motivates the central question of this work: Can LLMs serve as effective samplers for multi-objective performance modeling? To explore this, we present a comprehensive empirical study investigating the capabilities and characteristics of LLM-driven sampling. We design and implement LLM4Perf, a feedback-based framework, and use it to systematically evaluate the LLM-guided sampling process across four highly configurable, real-world systems. Our study reveals that the LLM-guided approach outperforms traditional baselines in most cases. Quantitatively, LLM4Perf achieves the best performance in nearly 68.8% (77 out of 112) of all evaluation scenarios, demonstrating its superior effectiveness. We find this effectiveness stems from the LLM's dual capabilities of configuration space pruning and feedback-driven strategy refinement. The effectiveness of this pruning is further validated by the fact that it also improves the performance of the baseline methods in nearly 91.5% (410 out of 448) of cases. Furthermore, we show how the LLM choices for each component and hyperparameters within LLM4Perf affect its effectiveness. Overall, this paper provides strong evidence for the effectiveness of LLMs in performance engineering and offers concrete insights into the mechanisms that drive their success.",
    "arxiv_url": "https://arxiv.org/abs/2512.16070",
    "authors": [
      "Xin Wang",
      "Zhenhao Li",
      "Zishuo Ding"
    ],
    "first_author": "Xin Wang",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Performance Engineering",
    "task": "Multi-Objective Configuration Sampling & Performance Modeling",
    "tags": [
      "Configuration Pruning",
      "Feedback-Driven Sampling",
      "Multi-Objective Optimization",
      "Sampling Strategy Design",
      "Configuration Generation",
      "Configuration Voting",
      "Performance Trend Analysis",
      "Hyperparameter Sensitivity"
    ],
    "summary": "本文提出LLM4Perf，一个利用大语言模型通过文档驱动的配置空间剪枝与迭代反馈引导多目标性能建模采样的框架，并构建了一个新的多目标性能数据集；实验证明LLM引导的采样在多数情景下优于传统基线，能提升性能模型的准确性与稳定性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.16070v1",
    "published": "2025-12-18",
    "update_time": "2025-12-18",
    "download_time": "2025-12-21 01:58:47"
  },
  {
    "id": "2512.16465",
    "title": "cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution",
    "abstract": "Optimizing CUDA kernels is a challenging and labor-intensive task, given the need for hardware-software co-design expertise and the proprietary nature of high-performance kernel libraries. While recent large language models (LLMs) combined with evolutionary algorithms show promise in automatic kernel optimization, existing approaches often fall short in performance due to their suboptimal agent designs and mismatched evolution representations. This work identifies these mismatches and proposes cuPilot, a strategy-coordinated multi-agent framework that introduces strategy as an intermediate semantic representation for kernel evolution. Key contributions include a strategy-coordinated evolution algorithm, roofline-guided prompting, and strategy-level population initialization. Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09$\\times$ over PyTorch on a benchmark of 100 kernels. On the GEMM tasks, cuPilot showcases sophisticated optimizations and achieves high utilization of critical hardware units. The generated kernels are open-sourced at https://github.com/champloo2878/cuPilot-Kernels.git.",
    "arxiv_url": "https://arxiv.org/abs/2512.16465",
    "authors": [
      "Jinwu Chen",
      "Qidie Wu",
      "Bin Li",
      "Lin Ma",
      "Xin Si",
      "Yang Hu",
      "Shouyi Yin",
      "Jun Yang"
    ],
    "first_author": "Jinwu Chen",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Performance Optimization",
    "tags": [
      "Strategy-Level Crossover",
      "Multi-Agent Framework",
      "Code Agents",
      "Roofline-Guided Prompting",
      "Strategy-Level Initialization",
      "Evolutionary Optimization",
      "CUDA Kernel Optimization",
      "Hardware-Aware Optimization",
      "Code RAG"
    ],
    "summary": "本文提出 cuPilot——一种以“策略”作为中间表示的多智能体进化框架，通过策略级交叉与翻译、roofline 引导的提示和策略级种群初始化，有效提升 CUDA 内核性能，在 100 个 KernelBench 基准上对比 PyTorch 平均提速 3.09×。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.16465v1",
    "published": "2025-12-18",
    "update_time": "2025-12-18",
    "download_time": "2025-12-21 02:04:47"
  },
  {
    "id": "2512.16262",
    "title": "Learning to Wait: Synchronizing Agents with the Physical World",
    "abstract": "Real-world agentic tasks, unlike synchronous Markov Decision Processes (MDPs), often involve non-blocking actions with variable latencies, creating a fundamental \\textit{Temporal Gap} between action initiation and completion. Existing environment-side solutions, such as blocking wrappers or frequent polling, either limit scalability or dilute the agent's context window with redundant observations. In this work, we propose an \\textbf{Agent-side Approach} that empowers Large Language Models (LLMs) to actively align their \\textit{Cognitive Timeline} with the physical world. By extending the Code-as-Action paradigm to the temporal domain, agents utilize semantic priors and In-Context Learning (ICL) to predict precise waiting durations (\\texttt{time.sleep(t)}), effectively synchronizing with asynchronous environment without exhaustive checking. Experiments in a simulated Kubernetes cluster demonstrate that agents can precisely calibrate their internal clocks to minimize both query overhead and execution latency, validating that temporal awareness is a learnable capability essential for autonomous evolution in open-ended environments.",
    "arxiv_url": "https://arxiv.org/abs/2512.16262",
    "authors": [
      "Yifei She",
      "Ping Zhang",
      "He Liu",
      "Yanmin Jia",
      "Yang Jing",
      "Zijun Liu",
      "Peng Sun",
      "Xiangbin Li",
      "Xiaohe Hu"
    ],
    "first_author": "Yifei She",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Agent Interaction",
    "task": "Temporal Synchronization",
    "tags": [
      "Code-as-Action",
      "Temporal Synchronization",
      "Active Wait",
      "In-Context Learning",
      "Asynchronous Environment",
      "Latency Modeling",
      "Regret Score",
      "Semantic Latency Estimation",
      "Context Efficiency"
    ],
    "summary": "本文提出一种Agent端方法，通过将Code-as-Action扩展到时间域，让LLM代理利用语义先验与In-Context Learning预测time.sleep(t)以同步其认知时间线与物理世界的异步延迟，并在模拟Kubernetes集群中实验证明可在减少查询开销的同时降低执行延迟。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.16262v1",
    "published": "2025-12-18",
    "update_time": "2025-12-18",
    "download_time": "2025-12-21 02:08:10"
  },
  {
    "id": "2512.15980",
    "title": "Embedding Software Intent: Lightweight Java Module Recovery",
    "abstract": "As an increasing number of software systems reach unprecedented scale, relying solely on code-level abstractions is becoming impractical. While architectural abstractions offer a means to manage these systems, maintaining their consistency with the actual code has been problematic. The Java Platform Module System (JPMS), introduced in Java 9, addresses this limitation by enabling explicit module specification at the language level. JPMS enhances architectural implementation through improved encapsulation and direct specification of ground-truth architectures within Java projects. Although many projects are written in Java, modularizing existing monolithic projects to JPMS modules is an open challenge due to ineffective module recovery by existing architecture recovery techniques. To address this challenge, this paper presents ClassLAR (Class-and Language model-based Architectural Recovery), a novel, lightweight, and efficient approach that recovers Java modules from monolithic Java systems using fully-qualified class names. ClassLAR leverages language models to extract semantic information from package and class names, capturing both structural and functional intent. In evaluations across 20 popular Java projects, ClassLAR outperformed all state-of-the-art techniques in architectural-level similarity metrics while achieving execution times that were 3.99 to 10.50 times faster.",
    "arxiv_url": "https://arxiv.org/abs/2512.15980",
    "authors": [
      "Yirui He",
      "Yuqi Huai",
      "Xingyu Chen",
      "Joshua Garcia"
    ],
    "first_author": "Yirui He",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Architecture Recovery",
    "task": "Module Recovery",
    "tags": [
      "Module Recovery",
      "Class-name Semantics",
      "LM Topic Modeling",
      "JPMS Ground Truth",
      "Encapsulation",
      "Architecture Matching",
      "Lightweight",
      "Efficiency"
    ],
    "summary": "本文提出ClassLAR，一种仅基于类的全限定名并利用语言模型的轻量级Java模块（JPMS）恢复方法，在20个开源项目上相比现有方法在架构相似性和封装性上表现更好且运行更快。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.15980v1",
    "published": "2025-12-17",
    "update_time": "2025-12-17",
    "download_time": "2025-12-22 01:57:54"
  },
  {
    "id": "2512.15979",
    "title": "OLAF: Towards Robust LLM-Based Annotation Framework in Empirical Software Engineering",
    "abstract": "Large Language Models (LLMs) are increasingly used in empirical software engineering (ESE) to automate or assist annotation tasks such as labeling commits, issues, and qualitative artifacts. Yet the reliability and reproducibility of such annotations remain underexplored. Existing studies often lack standardized measures for reliability, calibration, and drift, and frequently omit essential configuration details. We argue that LLM-based annotation should be treated as a measurement process rather than a purely automated activity. In this position paper, we outline the \\textbf{Operationalization for LLM-based Annotation Framework (OLAF)}, a conceptual framework that organizes key constructs: \\textit{reliability, calibration, drift, consensus, aggregation}, and \\textit{transparency}. The paper aims to motivate methodological discussion and future empirical work toward more transparent and reproducible LLM-based annotation in software engineering research.",
    "arxiv_url": "https://arxiv.org/abs/2512.15979",
    "authors": [
      "Mia Mohammad Imran",
      "Tarannum Shaila Zaman"
    ],
    "first_author": "Mia Mohammad Imran",
    "category": [
      "Survey",
      "Technical"
    ],
    "field": "Empirical Methods",
    "task": "LLM-based Annotation & Measurement",
    "tags": [
      "Operationalization",
      "Annotator Reliability",
      "Consensus Measurement",
      "Calibration",
      "Drift Analysis",
      "Probabilistic Aggregation",
      "Prompt Sensitivity",
      "Human-in-the-Loop",
      "Transparency",
      "Annotation Workflows"
    ],
    "summary": "本文提出了OLAF（一套面向软件工程中基于大模型的标注的操作化框架），将标注视为测量过程并定义了可靠性、共识、校准、漂移、聚合与透明性六个维度及相应的配置与报告指南，以提升LLM标注的可重复性与可审计性。",
    "quality": "Middle",
    "conference": "3rd International Workshop on Methodological Issues with Empirical Studies in Software Engineering (WSESE) 2026",
    "pdf_url": "https://arxiv.org/pdf/2512.15979v1",
    "published": "2025-12-17",
    "update_time": "2025-12-17",
    "download_time": "2025-12-22 01:58:12"
  },
  {
    "id": "2512.16855",
    "title": "TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge",
    "abstract": "Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.",
    "arxiv_url": "https://arxiv.org/abs/2512.16855",
    "authors": [
      "Khurram Khalil",
      "Khaza Anuarul Hoque"
    ],
    "first_author": "Khurram Khalil",
    "category": [
      "Technical"
    ],
    "field": "Model Compression & Deployment",
    "task": "LLM Compression with Formal Guarantees",
    "tags": [
      "Temporal Logic",
      "STL Robustness",
      "Robustness-Guided Optimization",
      "Layer-wise Quantization",
      "Layer-wise Pruning",
      "Bayesian Optimization",
      "Formal Guarantees",
      "Edge Deployment",
      "Runtime Adaptability",
      "Coherence Preservation"
    ],
    "summary": "本文提出TOGGLE，一种利用信号时序逻辑（STL）作为形式化约束、通过稳健性驱动的贝叶斯优化在无需重训练的情况下联合搜索层级量化与剪枝配置，从而在保证连贯性、长程依赖、上下文一致性和事实准确性等语言属性的前提下将LLM压缩并部署到边缘设备的框架。",
    "quality": "High",
    "conference": "IEEE ICCAD 2025",
    "pdf_url": "https://arxiv.org/pdf/2512.16855v1",
    "published": "2025-12-18",
    "update_time": "2025-12-18",
    "download_time": "2025-12-22 01:59:54"
  },
  {
    "id": "2512.16626",
    "title": "Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game",
    "abstract": "We introduce Stackelberg Learning from Human Feedback (SLHF), a new framework for preference optimization. SLHF frames the alignment problem as a sequential-move game between two policies: a Leader, which commits to an action, and a Follower, which responds conditionally on the Leader's action. This approach decomposes preference optimization into a refinement problem for the Follower and an optimization problem against an adversary for the Leader. Unlike Reinforcement Learning from Human Feedback (RLHF), which assigns scalar rewards to actions, or Nash Learning from Human Feedback (NLHF), which seeks a simultaneous-move equilibrium, SLHF leverages the asymmetry of sequential play to capture richer preference structures. The sequential design of SLHF naturally enables inference-time refinement, as the Follower learns to improve the Leader's actions, and these refinements can be leveraged through iterative sampling. We compare the solution concepts of SLHF, RLHF, and NLHF, and lay out key advantages in consistency, data sensitivity, and robustness to intransitive preferences. Experiments on large language models demonstrate that SLHF achieves strong alignment across diverse preference datasets, scales from 0.5B to 8B parameters, and yields inference-time refinements that transfer across model families without further fine-tuning.",
    "arxiv_url": "https://arxiv.org/abs/2512.16626",
    "authors": [
      "Barna Pásztor",
      "Thomas Kleine Buening",
      "Andreas Krause"
    ],
    "first_author": "Barna Pásztor",
    "category": [
      "Technical"
    ],
    "field": "LLM Alignment & Preference Learning",
    "task": "Preference Optimization (Sequential Stackelberg Learning from Human Feedback)",
    "tags": [
      "Stackelberg Game",
      "Preference Optimization",
      "Inference-Time Refinement",
      "Follower Refinement",
      "Two-Timescale GDA",
      "Pairwise Preference Modeling",
      "Intransitive Preferences",
      "Transferable Refinements",
      "Human-LLM Interaction",
      "Robustness"
    ],
    "summary": "本文提出了Stackelberg Learning from Human Feedback (SLHF)，将偏好优化建模为Leader-First、Follower-响应的序贯博弈，给出近似求解算法STACKELBERGGDA并展示其在推理时通过Follower迭代采样实现无微调的输出改进与跨模型迁移能力。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.16626v1",
    "published": "2025-12-18",
    "update_time": "2025-12-18",
    "download_time": "2025-12-22 02:11:57"
  },
  {
    "id": "2512.17814",
    "title": "LLM-based Behaviour Driven Development for Hardware Design",
    "abstract": "Test and verification are essential activities in hardware and system design, but their complexity grows significantly with increasing system sizes. While Behavior Driven Development (BDD) has proven effective in software engineering, it is not yet well established in hardware design, and its practical use remains limited. One contributing factor is the manual effort required to derive precise behavioral scenarios from textual specifications.   Recent advances in Large Language Models (LLMs) offer new opportunities to automate this step. In this paper, we investigate the use of LLM-based techniques to support BDD in the context of hardware design.",
    "arxiv_url": "https://arxiv.org/abs/2512.17814",
    "authors": [
      "Rolf Drechsler",
      "Qian Liu"
    ],
    "first_author": "Rolf Drechsler",
    "category": [
      "Technical"
    ],
    "field": "Software Testing",
    "task": "Test Generation",
    "tags": [
      "Behavior-Driven Development",
      "Gherkin Scenario Generation",
      "NL-to-Verilog",
      "Hardware Verification",
      "BDD Automation",
      "Prompt Engineering",
      "Executable Test Scenarios",
      "Simulation"
    ],
    "summary": "本文提出将大型语言模型应用于硬件设计的行为驱动开发（BDD），自动从自然语言规格生成可执行的Gherkin场景并生成对应的Verilog实现，以在ALU案例上进行仿真验证。",
    "quality": "Middle",
    "conference": "2nd International Symposium on Artificial Intelligence and Internet of Things (AIIoT-25) 2025",
    "pdf_url": "https://arxiv.org/pdf/2512.17814v1",
    "published": "2025-12-19",
    "update_time": "2025-12-19",
    "download_time": "2025-12-23 01:52:18"
  },
  {
    "id": "2512.17540",
    "title": "SGCR: A Specification-Grounded Framework for Trustworthy LLM Code Review",
    "abstract": "Automating code review with Large Language Models (LLMs) shows immense promise, yet practical adoption is hampered by their lack of reliability, context-awareness, and control. To address this, we propose Specification-Grounded Code Review (SGCR), a framework that grounds LLMs in human-authored specifications to produce trustworthy and relevant feedback. SGCR features a novel dual-pathway architecture: an explicit path ensures deterministic compliance with predefined rules derived from these specifications, while an implicit path heuristically discovers and verifies issues beyond those rules. Deployed in a live industrial environment at HiThink Research, SGCR's suggestions achieved a 42% developer adoption rate-a 90.9% relative improvement over a baseline LLM (22%). Our work demonstrates that specification-grounding is a powerful paradigm for bridging the gap between the generative power of LLMs and the rigorous reliability demands of software engineering.",
    "arxiv_url": "https://arxiv.org/abs/2512.17540",
    "authors": [
      "Kai Wang",
      "Bingcheng Mao",
      "Shuai Jia",
      "Yujie Ding",
      "Dongming Han",
      "Tianyi Ma",
      "Bin Cao"
    ],
    "first_author": "Kai Wang",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Maintenance",
    "task": "Code Review",
    "tags": [
      "Specification Grounding",
      "Dual-Pathway",
      "Explicit Specification Injection",
      "Implicit Specification Discovery",
      "Code RAG",
      "Ensemble Aggregation",
      "Specification Segmentation",
      "Explainability",
      "Reliability",
      "Production Deployment"
    ],
    "summary": "本文提出SGCR，一种将人工编写规范与双路径（显式规范注入与隐式规范发现）相结合的LLM驱动代码审查框架，结合RAG检索、模型集成与聚合以提高可控性与可信度，并在生产环境中验证了显著的开发者采纳率提升。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.17540v1",
    "published": "2025-12-19",
    "update_time": "2025-12-19",
    "download_time": "2025-12-23 01:52:38"
  },
  {
    "id": "2512.17419",
    "title": "SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories",
    "abstract": "Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.",
    "arxiv_url": "https://arxiv.org/abs/2512.17419",
    "authors": [
      "Lilin Wang",
      "Lucas Ramalho",
      "Alan Celestino",
      "Phuc Anthony Pham",
      "Yu Liu",
      "Umang Kumar Sinha",
      "Andres Portillo",
      "Onassis Osunwa",
      "Gabriel Maduekwe"
    ],
    "first_author": "Lilin Wang",
    "category": [
      "Benchmark",
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Repository-Level Coding",
    "tags": [
      "Repository-Level Coding",
      "Environment Synthesis",
      "Adaptive Log Parsing",
      "State-Differential Oracle",
      "Hint-Guided Trajectories",
      "Multilingual Benchmark",
      "Contamination-Aware Evaluation",
      "Automated Benchmark Generation"
    ],
    "summary": "SWE-Bench++ 提出一个自动化、多语言且可扩展的框架，能够从真实 GitHub pull request 自动合成可复现的容器环境和自适应日志解析器、以三态差分判定区分缺陷与功能请求，并通过提示引导生成可用于训练的轨迹来构建大规模仓库级评测集。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.17419v1",
    "published": "2025-12-19",
    "update_time": "2025-12-19",
    "download_time": "2025-12-23 01:56:12"
  },
  {
    "id": "2512.17259",
    "title": "Verifiability-First Agents: Provable Observability and Lightweight Audit Agents for Controlling Autonomous LLM Systems",
    "abstract": "As LLM-based agents grow more autonomous and multi-modal, ensuring they remain controllable, auditable, and faithful to deployer intent becomes critical. Prior benchmarks measured the propensity for misaligned behavior and showed that agent personalities and tool access significantly influence misalignment. Building on these insights, we propose a Verifiability-First architecture that (1) integrates run-time attestations of agent actions using cryptographic and symbolic methods, (2) embeds lightweight Audit Agents that continuously verify intent versus behavior using constrained reasoning, and (3) enforces challenge-response attestation protocols for high-risk operations. We introduce OPERA (Observability, Provable Execution, Red-team, Attestation), a benchmark suite and evaluation protocol designed to measure (i) detectability of misalignment, (ii) time to detection under stealthy strategies, and (iii) resilience of verifiability mechanisms to adversarial prompt and persona injection. Our approach shifts the evaluation focus from how likely misalignment is to how quickly and reliably misalignment can be detected and remediated.",
    "arxiv_url": "https://arxiv.org/abs/2512.17259",
    "authors": [
      "Abhivansh Gupta"
    ],
    "first_author": "Abhivansh Gupta",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Autonomous Agents & Safety",
    "task": "Agent Verifiability and Audit",
    "tags": [
      "Action Attestation",
      "Intent Specification",
      "Audit Agents",
      "Provenance Log",
      "Challenge–Response Attestation",
      "Signed Receipts",
      "Runtime Verification",
      "Verifiability Benchmark",
      "Adversarial Prompt Robustness",
      "Remediation Controls"
    ],
    "summary": "本文提出可验证性优先的智能体架构，通过意图规范、加密动作证明与轻量级审计代理以及挑战-响应协议实现运行时可观测性与可审计性，并引入评估检测速度与鲁棒性的OPERA基准。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.17259v1",
    "published": "2025-12-19",
    "update_time": "2025-12-19",
    "download_time": "2025-12-23 01:57:34"
  },
  {
    "id": "2512.19644",
    "title": "More code, less validation: Risk factors for over-reliance on AI coding tools among scientists",
    "abstract": "Programming is essential to modern scientific research, yet most scientists report inadequate training for the software development their work demands. Generative AI tools capable of code generation may support scientific programmers, but user studies indicate risks of over-reliance, particularly among inexperienced users. We surveyed 868 scientists who program, examining adoption patterns, tool preferences, and factors associated with perceived productivity. Adoption is highest among students and less experienced programmers, with variation across fields. Scientific programmers overwhelmingly prefer general-purpose conversational interfaces like ChatGPT over developer-specific tools. Both inexperience and limited use of development practices (like testing, code review, and version control) are associated with greater perceived productivity-but these factors interact, suggesting formal practices may partially compensate for inexperience. The strongest predictor of perceived productivity is the number of lines of generated code typically accepted at once. These findings suggest scientific programmers using generative AI may gauge productivity by code generation rather than validation, raising concerns about research code integrity.",
    "arxiv_url": "https://arxiv.org/abs/2512.19644",
    "authors": [
      "Gabrielle O'Brien",
      "Alexis Parker",
      "Nasir Eisty",
      "Jeffrey Carver"
    ],
    "first_author": "Gabrielle O'Brien",
    "category": [
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Prompting",
    "tags": [
      "Human-LLM Interaction",
      "Over-reliance",
      "Scientific Software",
      "Survey Study",
      "Code Validation",
      "Testing Adoption",
      "Version Control Usage",
      "Perceived Productivity",
      "Experience Effects",
      "Tool Preference"
    ],
    "summary": "本文基于对868名从事编程的科学家的问卷调查，发现学生与经验较少的研究人员更常使用以对话为主的生成式AI（如ChatGPT），且缺乏测试、代码审查与版本控制等实践与更高的自感生产力相关，尤其是一次性接受较多生成代码是感知生产力的最强预测因子，提示科研人员可能以生成量而非验证为依据过度依赖AI，威胁研究代码完整性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.19644v1",
    "published": "2025-12-22",
    "update_time": "2025-12-22",
    "download_time": "2025-12-24 01:50:13"
  },
  {
    "id": "2512.19509",
    "title": "Beyond Language Boundaries: Uncovering Programming Language Families for Code Language Models",
    "abstract": "The rapid proliferation of diverse programming languages presents both opportunities and challenges for developing multilingual code LLMs. While existing techniques often train code LLMs by simply aggregating multilingual code data, few explore the deeper relationships between programming languages(PLs) and how such relationships can be utilized to optimize the training and inference of code LLMs. In this work, we investigate 2 fundamental questions: 1) What are the deep linguistic relationships among PLs? and 2) How can these relationships be leveraged to improve multilingual code LLMs? We propose an embedding-based framework to uncover the latent families of PLs. Our approach begins by defining 21 primary linguistic features of programming languages, such as variable definition, control structures, and method declarations, and then employs LLMs to generate feature-aligned code samples across multiple languages. By embedding these semantically parallel code snippets from 19 languages, we construct a similarity matrix and perform hierarchical clustering to uncover inherent language relationships. Our analysis reveals clear hierarchical structures among programming languages. Closely related languages form well-defined clusters (e.g., C, C++, Java, and Swift group together), while Go exhibits as a central language with the highest cross-language similarity. Building on the uncovered language families, we propose three strategies to enhance multilingual LLM training: transfer learning across linguistically related languages, linguistic proximity-guided curriculum learning, and centroid-based intermediary code translation. Experiments on 4 code intelligence tasks demonstrate that our methods significantly improve multilingual LLM performance. This work offers a universal perspective on programming languages and advances more effective strategies for multilingual code LLM training.",
    "arxiv_url": "https://arxiv.org/abs/2512.19509",
    "authors": [
      "Shangbo Yun",
      "Xiaodong Gu",
      "Jianghong Huang",
      "Beijun Shen"
    ],
    "first_author": "Shangbo Yun",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Representation Learning",
    "tags": [
      "Code Representation Learning",
      "Programming Language Families",
      "Language Similarity",
      "Feature-aligned Generation",
      "Transfer Learning",
      "Curriculum Learning",
      "Intermediary Code Translation",
      "Centroid Language"
    ],
    "summary": "本文通过定义21项编程语言特征并生成跨19种语言的语义对齐代码片段，基于代码模型嵌入发现编程语言族谱，并利用语言相似性提出迁移学习、相似度引导的课程学习和基于质心的中介代码翻译策略以提升多语言代码LLM性能。",
    "quality": "High",
    "conference": "FSE 2026",
    "pdf_url": "https://arxiv.org/pdf/2512.19509v1",
    "published": "2025-12-22",
    "update_time": "2025-12-22",
    "download_time": "2025-12-24 01:50:36"
  },
  {
    "id": "2512.19481",
    "title": "A Dataset and Preliminary Study of Using GPT-5 for Code-change Impact Analysis",
    "abstract": "Understanding source code changes and their impact on other code entities is a crucial skill in software development. However, the analysis of code changes and their impact is often performed manually and therefore is time-consuming. Recent advancements in AI, and in particular large language models (LLMs) show promises to help developers in various code analysis tasks. However, the extent to which this potential can be utilized for understanding code changes and their impact is underexplored. To address this gap, we study the capabilities of GPT-5 and GPT-5-mini to predict the code entities impacted by given source code changes. We construct a dataset containing information about seed-changes, change pairs, and change types for each commit. Existing datasets lack crucial information about seed changes and impacted code entities. Our experiments evaluate the LLMs in two configurations: (1) seed-change information and the parent commit tree and (2) seed-change information, the parent commit tree, and the diff hunk of each seed change. We found that both LLMs perform poorly in the two experiments, whereas GPT-5 outperforms GPT-5-mini. Furthermore, the provision of the diff hunks helps both models to slightly improve their performance.",
    "arxiv_url": "https://arxiv.org/abs/2512.19481",
    "authors": [
      "Katharina Stengg",
      "Christian Macho",
      "Martin Pinzger"
    ],
    "first_author": "Katharina Stengg",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Maintenance",
    "task": "Change Impact Analysis",
    "tags": [
      "Change Impact Prediction",
      "Commit-level Dataset",
      "Seed-change Annotation",
      "Diff-hunk Prompting",
      "Prompt Optimization",
      "Manual Annotation",
      "Java Code Analysis",
      "LLM Evaluation"
    ],
    "summary": "本文构建了一个扩展的代码变更影响数据集（包含种子变更、变更对与diff hunk），并对GPT-5与GPT-5-mini在代码变更影响预测任务上进行了初步评估，结果表明模型总体表现较差但在提供diff hunk时略有改善。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.19481v1",
    "published": "2025-12-22",
    "update_time": "2025-12-22",
    "download_time": "2025-12-24 01:52:55"
  },
  {
    "id": "2512.19396",
    "title": "EchoTrail-GUI: Building Actionable Memory for GUI Agents via Critic-Guided Self-Exploration",
    "abstract": "Contemporary GUI agents, while increasingly capable due to advances in Large Vision-Language Models (VLMs), often operate with a critical limitation: they treat each task in isolation, lacking a mechanism to systematically learn from past successes. This digital ''amnesia'' results in sub-optimal performance, repeated errors, and poor generalization to novel challenges. To bridge this gap, we introduce EchoTrail-GUI, a novel framework designed to mimic human-like experiential learning by equipping agents with a dynamic, accessible memory. Our framework operates in three distinct stages. First, during Experience Exploration, an agent autonomously interacts with GUI environments to build a curated database of successful task trajectories, validated by a reward model. Crucially, the entire knowledge base construction is thus fully automated, requiring no human supervision. Second, in the Memory Injection stage, upon receiving a new task, our system efficiently retrieves the most relevant past trajectories to serve as actionable ''memories''. Finally, during GUI Task Inference, these memories are injected as in-context guidance to inform the agent's reasoning and decision-making process. We demonstrate the efficacy of our approach on benchmarks including Android World and AndroidLab. The results show that EchoTrail-GUI significantly improves the task success rate and operational efficiency of baseline agents, validating the power of structured memory in creating more robust and intelligent GUI automation.",
    "arxiv_url": "https://arxiv.org/abs/2512.19396",
    "authors": [
      "Runze Li",
      "Yuwen Zhai",
      "Bo Xu",
      "LiWu Xu",
      "Nian Shi",
      "Wei Zhang",
      "Ran Lin",
      "Liang Wang"
    ],
    "first_author": "Runze Li",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "GUI Interaction & Agents",
    "task": "Memory-Augmented GUI Automation",
    "tags": [
      "Memory-Augmented Agents",
      "Critic-Guided Exploration",
      "Autonomous Trajectory Collection",
      "Trajectory Curation",
      "Dense-Sparse Retrieval",
      "Memory Injection",
      "Retrieval-Augmented Inference",
      "GUI Automation"
    ],
    "summary": "本文提出EchoTrail-GUI，通过评论家引导的自主探索自动构建高质量的GUI操作轨迹库，并采用稠密-稀疏检索将相关成功轨迹作为记忆注入到代理的推理过程中，从而在Android基准上显著提升任务成功率与效率。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.19396v1",
    "published": "2025-12-22",
    "update_time": "2025-12-22",
    "download_time": "2025-12-24 01:53:43"
  },
  {
    "id": "2512.20482",
    "title": "SweRank+: Multilingual, Multi-Turn Code Ranking for Software Issue Localization",
    "abstract": "Maintaining large-scale, multilingual codebases hinges on accurately localizing issues, which requires mapping natural-language error descriptions to the relevant functions that need to be modified. However, existing ranking approaches are often Python-centric and perform a single-pass search over the codebase. This work introduces SweRank+, a framework that couples SweRankMulti, a cross-lingual code ranking tool, with SweRankAgent, an agentic search setup, for iterative, multi-turn reasoning over the code repository. SweRankMulti comprises a code embedding retriever and a listwise LLM reranker, and is trained using a carefully curated large-scale issue localization dataset spanning multiple popular programming languages. SweRankAgent adopts an agentic search loop that moves beyond single-shot localization with a memory buffer to reason and accumulate relevant localization candidates over multiple turns. Our experiments on issue localization benchmarks spanning various languages demonstrate new state-of-the-art performance with SweRankMulti, while SweRankAgent further improves localization over single-pass ranking.",
    "arxiv_url": "https://arxiv.org/abs/2512.20482",
    "authors": [
      "Revanth Gangi Reddy",
      "Ye Liu",
      "Wenting Zhao",
      "JaeHyeok Doo",
      "Tarun Suresh",
      "Daniel Lee",
      "Caiming Xiong",
      "Yingbo Zhou",
      "Semih Yavuz",
      "Shafiq Joty"
    ],
    "first_author": "Revanth Gangi Reddy",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Quality Management",
    "task": "Bug Localization",
    "tags": [
      "Multilingual Code",
      "Code Ranking",
      "Retriever-Reranker",
      "Listwise Reranking",
      "Agentic Search",
      "Multi-Turn Reasoning",
      "Memory Buffer",
      "Hard-Negative Mining",
      "Repository-Level Localization"
    ],
    "summary": "本文提出SWERANK+，通过构建多语言数据集SWELOCMULTI并结合基于检索-重排序的SWERANKMULTI与具备记忆缓冲的多轮迭代搜索代理SWERANKAGENT，实现跨多种编程语言的高精度问题定位并刷新多项基线性能。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.20482v1",
    "published": "2025-12-23",
    "update_time": "2025-12-23",
    "download_time": "2025-12-25 01:52:43"
  },
  {
    "id": "2512.20334",
    "title": "Comment Traps: How Defective Commented-out Code Augment Defects in AI-Assisted Code Generation",
    "abstract": "With the rapid development of large language models in code generation, AI-powered editors such as GitHub Copilot and Cursor are revolutionizing software development practices. At the same time, studies have identified potential defects in the generated code. Previous research has predominantly examined how code context influences the generation of defective code, often overlooking the impact of defects within commented-out code (CO code). AI coding assistants' interpretation of CO code in prompts affects the code they generate.   This study evaluates how AI coding assistants, GitHub Copilot and Cursor, are influenced by defective CO code. The experimental results show that defective CO code in the context causes AI coding assistants to generate more defective code, reaching up to 58.17 percent. Our findings further demonstrate that the tools do not simply copy the defective code from the context. Instead, they actively reason to complete incomplete defect patterns and continue to produce defective code despite distractions such as incorrect indentation or tags. Even with explicit instructions to ignore the defective CO code, the reduction in defects does not exceed 21.84 percent. These findings underscore the need for improved robustness and security measures in AI coding assistants.",
    "arxiv_url": "https://arxiv.org/abs/2512.20334",
    "authors": [
      "Yuan Huang",
      "Yukang Zhou",
      "Xiangping Chen",
      "Zibin Zheng"
    ],
    "first_author": "Yuan Huang",
    "category": [
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Prompting",
    "tags": [
      "Commented-out Code",
      "Defect Amplification",
      "Prompt Contamination",
      "Prompt Engineering",
      "Context Sparsity",
      "Robustness",
      "Security and Vulnerabilities",
      "Human-LLM Interaction"
    ],
    "summary": "本研究在大规模真实Python代码库中注入带缺陷的注释掉代码并在GitHub Copilot与Cursor上测试，发现这些注释掉的缺陷代码会显著增加AI生成代码中的缺陷率，且通过提示工程难以完全缓解该问题。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.20334v1",
    "published": "2025-12-23",
    "update_time": "2025-12-23",
    "download_time": "2025-12-25 01:53:26"
  },
  {
    "id": "2512.20387",
    "title": "Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems",
    "abstract": "We propose a Vision-Language Simulation Model (VLSM) that unifies visual and textual understanding to synthesize executable FlexScript from layout sketches and natural-language prompts, enabling cross-modal reasoning for industrial simulation systems. To support this new paradigm, the study constructs the first large-scale dataset for generative digital twins, comprising over 120,000 prompt-sketch-code triplets that enable multimodal learning between textual descriptions, spatial structures, and simulation logic. In parallel, three novel evaluation metrics, Structural Validity Rate (SVR), Parameter Match Rate (PMR), and Execution Success Rate (ESR), are proposed specifically for this task to comprehensively evaluate structural integrity, parameter fidelity, and simulator executability. Through systematic ablation across vision encoders, connectors, and code-pretrained language backbones, the proposed models achieve near-perfect structural accuracy and high execution robustness. This work establishes a foundation for generative digital twins that integrate visual reasoning and language understanding into executable industrial simulation systems.",
    "arxiv_url": "https://arxiv.org/abs/2512.20387",
    "authors": [
      "YuChe Hsu",
      "AnJui Wang",
      "TsaiChing Ni",
      "YuanFu Yang"
    ],
    "first_author": "YuChe Hsu",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Translation",
    "tags": [
      "Multimodal code generation",
      "Layout-to-code",
      "Simulation DSL synthesis",
      "Sketch-conditioned synthesis",
      "Vision-Language Fusion",
      "Structural Validity",
      "Execution Robustness",
      "Parameter Fidelity",
      "Industrial Digital Twins",
      "Human-AI Prompting"
    ],
    "summary": "本文提出视觉-语言仿真模型（VLSM）并构建首个大规模多模态数据集与专用评测指标，将布局草图和自然语言提示自动生成可执行的仿真脚本以实现工业数字孪生的自动化建模。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.20387v1",
    "published": "2025-12-23",
    "update_time": "2025-12-23",
    "download_time": "2025-12-25 01:55:57"
  },
  {
    "id": "2512.20328",
    "title": "Toward Explaining Large Language Models in Software Engineering Tasks",
    "abstract": "Recent progress in Large Language Models (LLMs) has substantially advanced the automation of software engineering (SE) tasks, enabling complex activities such as code generation and code summarization. However, the black-box nature of LLMs remains a major barrier to their adoption in high-stakes and safety-critical domains, where explainability and transparency are vital for trust, accountability, and effective human supervision. Despite increasing interest in explainable AI for software engineering, existing methods lack domain-specific explanations aligned with how practitioners reason about SE artifacts. To address this gap, we introduce FeatureSHAP, the first fully automated, model-agnostic explainability framework tailored to software engineering tasks. Based on Shapley values, FeatureSHAP attributes model outputs to high-level input features through systematic input perturbation and task-specific similarity comparisons, while remaining compatible with both open-source and proprietary LLMs. We evaluate FeatureSHAP on two bi-modal SE tasks: code generation and code summarization. The results show that FeatureSHAP assigns less importance to irrelevant input features and produces explanations with higher fidelity than baseline methods. A practitioner survey involving 37 participants shows that FeatureSHAP helps practitioners better interpret model outputs and make more informed decisions. Collectively, FeatureSHAP represents a meaningful step toward practical explainable AI in software engineering. FeatureSHAP is available at https://github.com/deviserlab/FeatureSHAP.",
    "arxiv_url": "https://arxiv.org/abs/2512.20328",
    "authors": [
      "Antonio Vitale",
      "Khai-Nguyen Nguyen",
      "Denys Poshyvanyk",
      "Rocco Oliveto",
      "Simone Scalabrino",
      "Antonio Mastropaolo"
    ],
    "first_author": "Antonio Vitale",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Explainability & Interpretability",
    "task": "Feature-level Attribution",
    "tags": [
      "Feature-level Attribution",
      "Shapley Values",
      "Model-Agnostic Explanation",
      "Perturbation-based Attribution",
      "Prompt Feature Decomposition",
      "Actionable Explanations",
      "Faithfulness",
      "Human-LLM Interaction",
      "Code Generation",
      "Code Summarization",
      "User Study"
    ],
    "summary": "本文提出FeatureSHAP，一种基于Shapley值的模型不可知的特征级可解释性框架，通过对语义层面输入特征进行系统性扰动与相似性比较，为代码生成和代码摘要任务提供更具可操作性与高保真性的解释，并通过定量实验与37位从业者调研验证其实用性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.20328v1",
    "published": "2025-12-23",
    "update_time": "2025-12-23",
    "download_time": "2025-12-25 01:56:42"
  },
  {
    "id": "2512.21238",
    "title": "Assessing the Software Security Comprehension of Large Language Models",
    "abstract": "Large language models (LLMs) are increasingly used in software development, but their level of software security expertise remains unclear. This work systematically evaluates the security comprehension of five leading LLMs: GPT-4o-Mini, GPT-5-Mini, Gemini-2.5-Flash, Llama-3.1, and Qwen-2.5, using Blooms Taxonomy as a framework. We assess six cognitive dimensions: remembering, understanding, applying, analyzing, evaluating, and creating. Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments from an Introduction to Software Security course, real-world case studies (XBOW), and project-based creation tasks from a Secure Software Engineering course. Results show that while LLMs perform well on lower-level cognitive tasks such as recalling facts and identifying known vulnerabilities, their performance degrades significantly on higher-order tasks that require reasoning, architectural evaluation, and secure system creation. Beyond reporting aggregate accuracy, we introduce a software security knowledge boundary that identifies the highest cognitive level at which a model consistently maintains reliable performance. In addition, we identify 51 recurring misconception patterns exhibited by LLMs across Blooms levels.",
    "arxiv_url": "https://arxiv.org/abs/2512.21238",
    "authors": [
      "Mohammed Latif Siddiq",
      "Natalie Sekerak",
      "Antonio Karam",
      "Maria Leal",
      "Arvin Islam-Gomes",
      "Joanna C. S. Santos"
    ],
    "first_author": "Mohammed Latif Siddiq",
    "category": [
      "Empirical",
      "Benchmark"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "Security and Vulnerabilities",
      "Knowledge Boundary",
      "Bloom's Taxonomy",
      "Misconception Taxonomy",
      "Concept Inventory",
      "Evaluation Framework",
      "Higher-Order Reasoning",
      "Educational Assessment",
      "Hallucination"
    ],
    "summary": "本文提出了基于布鲁姆认知分类的Basket评估框架，系统性地测评五种主流大模型在软件安全领域的理解能力，发现模型在记忆与识别层面表现良好但在分析、评估与创建等高阶认知任务上显著退化，并归纳出51类常见误解与知识边界。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.21238v1",
    "published": "2025-12-24",
    "update_time": "2025-12-24",
    "download_time": "2025-12-26 01:51:53"
  },
  {
    "id": "2512.21236",
    "title": "Casting a SPELL: Sentence Pairing Exploration for LLM Limitation-breaking",
    "abstract": "Large language models (LLMs) have revolutionized software development through AI-assisted coding tools, enabling developers with limited programming expertise to create sophisticated applications. However, this accessibility extends to malicious actors who may exploit these powerful tools to generate harmful software. Existing jailbreaking research primarily focuses on general attack scenarios against LLMs, with limited exploration of malicious code generation as a jailbreak target. To address this gap, we propose SPELL, a comprehensive testing framework specifically designed to evaluate the weakness of security alignment in malicious code generation. Our framework employs a time-division selection strategy that systematically constructs jailbreaking prompts by intelligently combining sentences from a prior knowledge dataset, balancing exploration of novel attack patterns with exploitation of successful techniques. Extensive evaluation across three advanced code models (GPT-4.1, Claude-3.5, and Qwen2.5-Coder) demonstrates SPELL's effectiveness, achieving attack success rates of 83.75%, 19.38%, and 68.12% respectively across eight malicious code categories. The generated prompts successfully produce malicious code in real-world AI development tools such as Cursor, with outputs confirmed as malicious by state-of-the-art detection systems at rates exceeding 73%. These findings reveal significant security gaps in current LLM implementations and provide valuable insights for improving AI safety alignment in code generation applications.",
    "arxiv_url": "https://arxiv.org/abs/2512.21236",
    "authors": [
      "Yifan Huang",
      "Xiaojun Jia",
      "Wenbo Guo",
      "Yuqiang Sun",
      "Yihao Huang",
      "Chong Wang",
      "Yang Liu"
    ],
    "first_author": "Yifan Huang",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Security & Misuse",
    "task": "Malicious Prompt Generation",
    "tags": [
      "Jailbreaking",
      "Sentence Pairing",
      "Prompt Component Discovery",
      "Exploit-Exploration Strategy",
      "Time-Division Selection",
      "Automated Attack Generation",
      "Defense Strategies",
      "Black-box Evaluation",
      "IDE Real-world Deployment",
      "Security and Vulnerabilities"
    ],
    "summary": "本文提出SPELL框架，通过自动发现并组合提示句子组件以生成新的越狱提示，从而在多款代码LLM上高效生成可执行恶意代码并同时验证与提出可部署的防御策略。",
    "quality": "High",
    "conference": "ACM SIGSOFT International Symposium on the Foundations of Software Engineering (FSE 2026)",
    "pdf_url": "https://arxiv.org/pdf/2512.21236v1",
    "published": "2025-12-24",
    "update_time": "2025-12-24",
    "download_time": "2025-12-26 01:52:15"
  },
  {
    "id": "2512.21336",
    "title": "Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty",
    "abstract": "Masked Diffusion Models (MDMs) offer flexible, non-autoregressive generation, but this freedom introduces a challenge: final output quality is highly sensitive to the decoding order. We are the first to formalize this issue, attributing the variability in output quality to the cumulative predictive uncertainty along a generative path. To quantify this uncertainty, we introduce Denoising Entropy, a computable metric that serves as an internal signal for evaluating generative process. Leveraging this metric, we propose two algorithms designed to optimize the decoding path: a post-hoc selection method and a real-time guidance strategy. Experiments demonstrate that our entropy-guided methods significantly improve generation quality, consistently boosting accuracy on challenging reasoning, planning, and code benchmarks. Our work establishes Denoising Entropy as a principled tool for understanding and controlling generation, effectively turning the uncertainty in MDMs from a liability into a key advantage for discovering high-quality solutions.",
    "arxiv_url": "https://arxiv.org/abs/2512.21336",
    "authors": [
      "Ziyu Chen",
      "Xinbei Jiang",
      "Peng Sun",
      "Tao Lin"
    ],
    "first_author": "Ziyu Chen",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Path Uncertainty",
      "Denoising Entropy",
      "Entropy-guided Decoding",
      "Post-hoc Reranking",
      "Sequential Monte Carlo",
      "Masked Diffusion Models",
      "Non-autoregressive Generation",
      "Code Generation"
    ],
    "summary": "本文提出了量化MDM解码路径不确定性的Denoising Entropy度量，并基于此设计后验重排和实时引导的熵驱动解码算法，以显著提升推理、规划与代码生成任务的生成质量。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.21336v1",
    "published": "2025-12-24",
    "update_time": "2025-12-24",
    "download_time": "2025-12-26 01:52:37"
  },
  {
    "id": "2512.21332",
    "title": "C2LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention Pooling",
    "abstract": "We present C2LLM - Contrastive Code Large Language Models, a family of code embedding models in both 0.5B and 7B sizes. Building upon Qwen-2.5-Coder backbones, C2LLM adopts a Pooling by Multihead Attention (PMA) module for generating sequence embedding from token embeddings, effectively 1) utilizing the LLM's causal representations acquired during pretraining, while also 2) being able to aggregate information from all tokens in the sequence, breaking the information bottleneck in EOS-based sequence embeddings, and 3) supporting flexible adaptation of embedding dimension, serving as an alternative to MRL. Trained on three million publicly available data, C2LLM models set new records on MTEB-Code among models of similar sizes, with C2LLM-7B ranking 1st on the overall leaderboard.",
    "arxiv_url": "https://arxiv.org/abs/2512.21332",
    "authors": [
      "Jin Qin",
      "Zihan Liao",
      "Ziyin Zhang",
      "Hang Yu",
      "Peng Di",
      "Rui Wang"
    ],
    "first_author": "Jin Qin",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Retrieval",
    "tags": [
      "Cross-Attention Pooling",
      "Contrastive Learning",
      "Causal Attention Preservation",
      "Embedding Dimension Adaptation",
      "Code Embeddings",
      "Retrieval Optimization"
    ],
    "summary": "本文提出C2LLM——在Qwen-2.5-Coder基础上通过在LLM输出上加入Pooling by Multihead Attention（PMA）模块并采用对比学习训练的代码嵌入模型家族，从而在保持因果注意力的同时改进序列聚合与维度适配，在MTEB-Code上取得同规模模型的领先检索性能。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.21332v1",
    "published": "2025-12-24",
    "update_time": "2025-12-24",
    "download_time": "2025-12-26 01:52:58"
  },
  {
    "id": "2512.21028",
    "title": "Artificial or Just Artful? Do LLMs Bend the Rules in Programming?",
    "abstract": "Large Language Models (LLMs) are widely used for automated code generation, yet their apparent successes often mask a tension between pretraining objectives and alignment choices. While pretraining encourages models to exploit all available signals to maximize success, alignment, whether through fine-tuning or prompting, may restrict their use. This conflict is especially salient in agentic AI settings, for instance when an agent has access to unit tests that, although intended for validation, act as strong contextual signals that can be leveraged regardless of explicit prohibitions. In this paper, we investigate how LLMs adapt their code generation strategies when exposed to test cases under different prompting conditions. Using the BigCodeBench (Hard) dataset, we design five prompting conditions that manipulate test visibility and impose explicit or implicit restrictions on their use. We evaluate five LLMs (four open-source and one closed-source) across correctness, code similarity, program size, and code churn, and analyze cross-model consistency to identify recurring adaptation strategies. Our results show that test visibility dramatically alters performance, correctness nearly doubles for some models, while explicit restrictions or partial exposure only partially mitigate this effect. Beyond raw performance, we identify four recurring adaptation strategies, with test-driven refinement emerging as the most frequent. These results highlight how LLMs adapt their behavior when exposed to contextual signals that conflict with explicit instructions, providing useful insight into how models reconcile pretraining objectives with alignment constraints.",
    "arxiv_url": "https://arxiv.org/abs/2512.21028",
    "authors": [
      "Oussama Ben Sghaier",
      "Kevin Delcourt",
      "Houari Sahraoui"
    ],
    "first_author": "Oussama Ben Sghaier",
    "category": [
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Prompting",
    "tags": [
      "Test-Driven Refinement",
      "Contextual Signal Exploitation",
      "Alignment Conflict",
      "Instruction Override",
      "Prompt Restrictions",
      "Correctness Improvement",
      "Code Similarity",
      "Code Churn",
      "Cross-Model Consistency",
      "Empirical Analysis"
    ],
    "summary": "本文通过对五种提示条件下（完整/部分/显式禁止使用单元测试等）五个LLM在BigCodeBench上的对比实证研究，发现测试可见性显著提升正确率且模型常常在显式禁令下仍利用测试信号，表现出以测试为驱动的多种适应策略。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.21028v1",
    "published": "2025-12-24",
    "update_time": "2025-12-24",
    "download_time": "2025-12-27 01:49:29"
  },
  {
    "id": "2512.20957",
    "title": "One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents",
    "abstract": "Locating the files and functions requiring modification in large open-source software (OSS) repositories is challenging due to their scale and structural complexity. Existing large language model (LLM)-based methods typically treat this as a repository-level retrieval task and rely on multiple auxiliary tools, which overlook code execution logic and complicate model control. We propose RepoNavigator, an LLM agent equipped with a single execution-aware tool-jumping to the definition of an invoked symbol. This unified design reflects the actual flow of code execution while simplifying tool manipulation. RepoNavigator is trained end-to-end via Reinforcement Learning (RL) directly from a pretrained model, without any closed-source distillation. Experiments demonstrate that RL-trained RepoNavigator achieves state-of-the-art performance, with the 7B model outperforming 14B baselines, the 14B model surpassing 32B competitors, and even the 32B model exceeding closed-source models such as Claude-3.7. These results confirm that integrating a single, structurally grounded tool with RL training provides an efficient and scalable solution for repository-level issue localization.",
    "arxiv_url": "https://arxiv.org/abs/2512.20957",
    "authors": [
      "Zhaoxi Zhang",
      "Yitong Duan",
      "Yanzhi Zhang",
      "Yiming Xu",
      "Jiyan He",
      "Yunfang Wu"
    ],
    "first_author": "Zhaoxi Zhang",
    "category": [
      "Technical"
    ],
    "field": "Quality Management",
    "task": "Bug Localization",
    "tags": [
      "Repository-Level Coding",
      "Issue Localization",
      "Jump Tool",
      "Language-Server Integration",
      "Reinforcement Learning",
      "Tool-Augmented Agent",
      "Execution-Aware Navigation",
      "End-to-End Training",
      "Model Distillation-Free"
    ],
    "summary": "本文提出RepoNavigator，一种配备单一执行感知“jump”工具并通过在预训练模型上直接进行强化学习端到端训练的仓库级定位代理，可高效定位大型开源代码库中的相关文件与函数并在多项基准上取得SOTA性能。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.20957v1",
    "published": "2025-12-24",
    "update_time": "2025-12-24",
    "download_time": "2025-12-27 01:49:46"
  },
  {
    "id": "2512.21132",
    "title": "AutoBaxBuilder: Bootstrapping Code Security Benchmarking",
    "abstract": "As LLMs see wide adoption in software engineering, the reliable assessment of the correctness and security of LLM-generated code is crucial. Notably, prior work has demonstrated that security is often overlooked, exposing that LLMs are prone to generating code with security vulnerabilities. These insights were enabled by specialized benchmarks, crafted through significant manual effort by security experts. However, relying on manually-crafted benchmarks is insufficient in the long term, because benchmarks (i) naturally end up contaminating training data, (ii) must extend to new tasks to provide a more complete picture, and (iii) must increase in difficulty to challenge more capable LLMs. In this work, we address these challenges and present AutoBaxBuilder, a framework that generates tasks and tests for code security benchmarking from scratch. We introduce a robust pipeline with fine-grained plausibility checks, leveraging the code understanding capabilities of LLMs to construct functionality tests and end-to-end security-probing exploits. To confirm the quality of the generated benchmark, we conduct both a qualitative analysis and perform quantitative experiments, comparing it against tasks constructed by human experts. We use AutoBaxBuilder to construct entirely new tasks and release them to the public as AutoBaxBench, together with a thorough evaluation of the security capabilities of LLMs on these tasks. We find that a new task can be generated in under 2 hours, costing less than USD 10.",
    "arxiv_url": "https://arxiv.org/abs/2512.21132",
    "authors": [
      "Tobias von Arx",
      "Niels Mündler",
      "Mark Vero",
      "Maximilian Baader",
      "Martin Vechev"
    ],
    "first_author": "Tobias von Arx",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "Security and Vulnerabilities",
      "Automated benchmark generation",
      "End-to-end exploit synthesis",
      "Functional test generation",
      "Scenario synthesis",
      "CWE classification",
      "Benchmark contamination mitigation",
      "Difficulty stratification",
      "Reproducibility",
      "Code Agents",
      "Efficiency"
    ],
    "summary": "本文提出AUTOBAXBUILDER，一种基于LLM的自动化流水线，能从零生成后端安全基准任务（场景、功能测试和端到端漏洞利用脚本），并以此构建并公开AUTOBAXBENCH以评估模型的安全与正确性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.21132v1",
    "published": "2025-12-24",
    "update_time": "2025-12-24",
    "download_time": "2025-12-27 01:51:28"
  },
  {
    "id": "2512.21024",
    "title": "Policy-Conditioned Policies for Multi-Agent Task Solving",
    "abstract": "In multi-agent tasks, the central challenge lies in the dynamic adaptation of strategies. However, directly conditioning on opponents' strategies is intractable in the prevalent deep reinforcement learning paradigm due to a fundamental ``representational bottleneck'': neural policies are opaque, high-dimensional parameter vectors that are incomprehensible to other agents. In this work, we propose a paradigm shift that bridges this gap by representing policies as human-interpretable source code and utilizing Large Language Models (LLMs) as approximate interpreters. This programmatic representation allows us to operationalize the game-theoretic concept of \\textit{Program Equilibrium}. We reformulate the learning problem by utilizing LLMs to perform optimization directly in the space of programmatic policies. The LLM functions as a point-wise best-response operator that iteratively synthesizes and refines the ego agent's policy code to respond to the opponent's strategy. We formalize this process as \\textit{Programmatic Iterated Best Response (PIBR)}, an algorithm where the policy code is optimized by textual gradients, using structured feedback derived from game utility and runtime unit tests. We demonstrate that this approach effectively solves several standard coordination matrix games and a cooperative Level-Based Foraging environment.",
    "arxiv_url": "https://arxiv.org/abs/2512.21024",
    "authors": [
      "Yue Lin",
      "Shuhui Zhu",
      "Wenhao Li",
      "Ang Li",
      "Dan Qiao",
      "Pascal Poupart",
      "Hongyuan Zha",
      "Baoxiang Wang"
    ],
    "first_author": "Yue Lin",
    "category": [
      "Technical"
    ],
    "field": "Multi-Agent Reinforcement Learning",
    "task": "Programmatic policy conditioning / Programmatic Iterated Best Response (PIBR)",
    "tags": [
      "Programmatic Policies",
      "Code Agents",
      "Program Equilibrium",
      "Textual Gradients",
      "Iterated Best Response",
      "Runtime Unit Tests",
      "Multi-Agent Coordination",
      "LLM Policy Synthesis",
      "Interpretability"
    ],
    "summary": "本文提出将策略表示为可执行源代码并利用大语言模型作为近似解释器，提出Programmatic Iterated Best Response (PIBR)算法，通过文本梯度结合博弈效用与运行时单元测试在代码空间对策略进行迭代优化以实现多智能体协调。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.21024v1",
    "published": "2025-12-24",
    "update_time": "2025-12-24",
    "download_time": "2025-12-27 01:52:36"
  },
  {
    "id": "2512.20732",
    "title": "FEM-Bench: A Structured Scientific Reasoning Benchmark for Evaluating Code-Generating LLMs",
    "abstract": "As LLMs advance their reasoning capabilities about the physical world, the absence of rigorous benchmarks for evaluating their ability to generate scientifically valid physical models has become a critical gap. Computational mechanics, which develops and applies mathematical models and numerical methods to predict the behavior of physical systems under forces, deformation, and constraints, provides an ideal foundation for structured scientific reasoning evaluation. Problems follow clear mathematical structure, enforce strict physical and numerical constraints, and support objective verification. The discipline requires constructing explicit models of physical systems and reasoning about geometry, spatial relationships, and material behavior, connecting directly to emerging AI goals in physical reasoning and world modeling. We introduce FEM-Bench, a computational mechanics benchmark designed to evaluate the ability of LLMs to generate correct finite element method (FEM) and related code. FEM-Bench 2025 contains a suite of introductory but nontrivial tasks aligned with material from a first graduate course on computational mechanics. These tasks capture essential numerical and physical modeling challenges while representing only a small fraction of the complexity present in the discipline. Despite their simplicity, state-of-the-art LLMs do not reliably solve all of them. In a five attempt run, the best performing model at function writing, Gemini 3 Pro, completed 30/33 tasks at least once and 26/33 tasks all five times. The best performing model at unit test writing, GPT-5, had an Average Joint Success Rate of 73.8%. Other popular models showed broad performance variation. FEM-Bench establishes a structured foundation for evaluating AI-generated scientific code, and future iterations will incorporate increasingly sophisticated tasks to track progress as models evolve.",
    "arxiv_url": "https://arxiv.org/abs/2512.20732",
    "authors": [
      "Saeed Mohammadzadeh",
      "Erfan Hamdi",
      "Joel Shor",
      "Emma Lejeune"
    ],
    "first_author": "Saeed Mohammadzadeh",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "FEM Code Generation",
      "Scientific Computing",
      "Numerical Verification",
      "Mesh Convergence",
      "Physical Modeling",
      "Unit Test Generation",
      "Floating-point Sensitivity",
      "Structured Scientific Reasoning"
    ],
    "summary": "本文提出FEM-Bench，一个用于评估大模型生成有限元与计算力学代码的结构化基准，包含入门但有挑战性的任务集、自动化评估工具和多模型基线测试以量化科学计算代码生成能力。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.20732v1",
    "published": "2025-12-23",
    "update_time": "2025-12-23",
    "download_time": "2025-12-28 02:03:21"
  },
  {
    "id": "2512.20203",
    "title": "Well Begun is Half Done: Location-Aware and Trace-Guided Iterative Automated Vulnerability Repair",
    "abstract": "The advances of large language models (LLMs) have paved the way for automated software vulnerability repair approaches, which iteratively refine the patch until it becomes plausible. Nevertheless, existing LLM-based vulnerability repair approaches face notable limitations: 1) they ignore the concern of locations that need to be patched and focus solely on the repair content. 2) they lack quality assessment for generated candidate patches in the iterative process.   To tackle the two limitations, we propose \\sysname, an LLM-based approach that provides information about where should be patched first. Furthermore, \\sysname improves the iterative repair strategy by assessing the quality of test-failing patches and selecting the best patch for the next iteration. We introduce two dimensions to assess the quality of patches: whether they introduce new vulnerabilities and the taint statement coverage. We evaluated \\sysname on a real-world C/C++ vulnerability repair dataset VulnLoc+, which contains 40 vulnerabilities and their Proofs-of-Vulnerability. The experimental results demonstrate that \\sysname exhibits substantial improvements compared with the Neural Machine Translation-based, Program Analysis-based, and LLM-based state-of-the-art vulnerability repair approaches. Specifically, \\sysname is able to generate 27 plausible patches, which is comparable to or even 8 to 22 more plausible patches than the baselines. In terms of correct patch generation, \\sysname repairs 8 to 13 additional vulnerabilities compared with existing approaches.",
    "arxiv_url": "https://arxiv.org/abs/2512.20203",
    "authors": [
      "Zhenlei Ye",
      "Xiaobing Sun",
      "Sicong Cao",
      "Lili Bo",
      "Bin Li"
    ],
    "first_author": "Zhenlei Ye",
    "category": [
      "Technical"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Repair",
    "tags": [
      "Program Repair",
      "Security and Vulnerabilities",
      "Location-Aware Patch Prediction",
      "Taint-Guided Ranking",
      "Iterative Repair",
      "Patch Quality Assessment",
      "Multi-Hunk Vulnerability",
      "PoV Verification"
    ],
    "summary": "本文提出LoopRepair，一种结合补丁位置预测与污点追踪引导的迭代自动漏洞修复方法，通过预测需修复的代码行并基于是否引入新漏洞与污点语句覆盖对候选补丁排序选择，从而在多块（multi-hunk）C/C++漏洞修复上显著优于现有方法。",
    "quality": "High",
    "conference": "ICSE 2026",
    "pdf_url": "https://arxiv.org/pdf/2512.20203v1",
    "published": "2025-12-23",
    "update_time": "2025-12-23",
    "download_time": "2025-12-28 02:04:32"
  },
  {
    "id": "2512.20968",
    "title": "Mesh-Attention: A New Communication-Efficient Distributed Attention with Improved Data Locality",
    "abstract": "Distributed attention is a fundamental problem for scaling context window for Large Language Models (LLMs). The state-of-the-art method, Ring-Attention, suffers from scalability limitations due to its excessive communication traffic. This paper proposes a new distributed attention algorithm, Mesh-Attention, by rethinking the design space of distributed attention with a new matrix-based model. Our method assigns a two-dimensional tile -- rather than one-dimensional row or column -- of computation blocks to each GPU to achieve higher efficiency through lower communication-computation (CommCom) ratio. The general approach covers Ring-Attention as a special case, and allows the tuning of CommCom ratio with different tile shapes. Importantly, we propose a greedy algorithm that can efficiently search the scheduling space within the tile with restrictions that ensure efficient communication among GPUs. The theoretical analysis shows that Mesh-Attention leads to a much lower communication complexity and exhibits good scalability comparing to other current algorithms.   Our extensive experiment results show that Mesh-Attention can achieve up to 3.4x speedup (2.9x on average) and reduce the communication volume by up to 85.4% (79.0% on average) on 256 GPUs. Our scalability results further demonstrate that Mesh-Attention sustains superior performance as the system scales, substantially reducing overhead in large-scale deployments. The results convincingly confirm the advantage of Mesh-Attention.",
    "arxiv_url": "https://arxiv.org/abs/2512.20968",
    "authors": [
      "Sirui Chen",
      "Jingji Chen",
      "Siqi Zhu",
      "Ziheng Jiang",
      "Yanghua Peng",
      "Xuehai Qian"
    ],
    "first_author": "Sirui Chen",
    "category": [
      "Technical"
    ],
    "field": "Model Infrastructure",
    "task": "Distributed Attention / Model Parallelism",
    "tags": [
      "Mesh Tiling",
      "Assignment Matrix",
      "CommCom Ratio",
      "Local Q-KV Property",
      "Greedy Scheduling",
      "Computation-Communication Overlap",
      "Communication Reduction",
      "GPU Scalability"
    ],
    "summary": "本文提出Mesh-Attention，一种基于二维tile分配与赋值矩阵的分布式注意力算法，通过提高数据局部性和贪心调度最大化计算-通信重叠，在256 GPU上平均实现2.9×加速并显著降低通信量（最高85.4%）。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.20968v1",
    "published": "2025-12-24",
    "update_time": "2025-12-24",
    "download_time": "2025-12-28 02:07:59"
  },
  {
    "id": "2512.20845",
    "title": "MAR:Multi-Agent Reflexion Improves Reasoning Abilities in LLMs",
    "abstract": "LLMs have shown the capacity to improve their performance on reasoning tasks through reflecting on their mistakes, and acting with these reflections in mind. However, continual reflections of the same LLM onto itself exhibit degeneration of thought, where the LLM continues to repeat the same errors again and again even with the knowledge that its wrong. To address this problem, we instead introduce multi-agent with multi-persona debators as the method to generate reflections. Through out extensive experimentation, we've found that the leads to better diversity of in the reflections generated by the llm agent. We demonstrate an accuracy of 47% EM HotPot QA (question answering) and 82.7% on HumanEval (programming), both performances surpassing reflection with a single llm.",
    "arxiv_url": "https://arxiv.org/abs/2512.20845",
    "authors": [
      "Onat Ozer",
      "Grace Wu",
      "Yuchen Wang",
      "Daniel Dosti",
      "Honghao Zhang",
      "Vivi De La Rue"
    ],
    "first_author": "Onat Ozer",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Multi-Agent Debate",
      "Episodic Memory",
      "Self-Reflection",
      "Judge Aggregation",
      "Persona Diversity",
      "Iterative Refinement",
      "Code Agents",
      "Human-LLM Interaction",
      "Failure Mode Analysis",
      "Correctness"
    ],
    "summary": "本文复现并分析了单代理 Reflexion 的失效模式，提出 Multi‑Agent Reflexion（MAR）——通过多样化人格的批评者和一个裁判聚合反思来减少认知盲点，从而在 HotPotQA 与 HumanEval 上提升了推理与代码生成正确率。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.20845v1",
    "published": "2025-12-23",
    "update_time": "2025-12-23",
    "download_time": "2025-12-28 02:09:42"
  },
  {
    "id": "2512.22113",
    "title": "Agentic Structured Graph Traversal for Root Cause Analysis of Code-related Incidents in Cloud Applications",
    "abstract": "Cloud incidents pose major operational challenges in production, with unresolved production cloud incidents cost on average over $2M per hour. Prior research identifies code- and configuration-related issues as the predominant category of root causes in cloud incidents. This paper introduces PRAXIS, an orchestrator that manages and deploys an agentic workflow for diagnosing code- and configuration-caused cloud incidents. PRAXIS employs an LLM-driven structured traversal over two types of graph: (1) a service dependency graph (SDG) that captures microservice-level dependencies; and (2) a hammock-block program dependence graph (PDG) that captures code-level dependencies for each microservice. Together, these graphs encode microservice- and code-level dependencies and the LLM acts as a traversal policy over these graphs, moving between services and code dependencies to localize and explain failures. Compared to state-of-the-art ReAct baselines, PRAXIS improves RCA accuracy by up to 3.1x while reducing token consumption by 3.8x. PRAXIS is demonstrated on a set of 30 comprehensive real-world incidents that is being compiled into an RCA benchmark.",
    "arxiv_url": "https://arxiv.org/abs/2512.22113",
    "authors": [
      "Shengkun Cui",
      "Rahul Krishna",
      "Saurabh Jha",
      "Ravishankar K. Iyer"
    ],
    "first_author": "Shengkun Cui",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "SRE & Incident Diagnosis",
    "task": "Root Cause Analysis",
    "tags": [
      "Agentic Orchestration",
      "Cross-SDG-PDG Traversal",
      "Service Dependency Graph",
      "Hammock-block PDG",
      "Structure-aware Reasoning",
      "Observability Integration",
      "Root-Cause Identification",
      "Token Efficiency"
    ],
    "summary": "本文提出PRAXIS，一种基于LLM驱动的结构化图遍历代理化编排器，通过联合利用微服务依赖图和hammock块程序依赖图对云应用中代码与配置引起的故障进行跨层根因分析，在30个真实场景基准上显著提高诊断准确率并降低token消耗。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.22113v1",
    "published": "2025-12-26",
    "update_time": "2025-12-26",
    "download_time": "2025-12-29 02:01:25"
  },
  {
    "id": "2512.21818",
    "title": "Analyzing Code Injection Attacks on LLM-based Multi-Agent Systems in Software Development",
    "abstract": "Agentic AI and Multi-Agent Systems are poised to dominate industry and society imminently. Powered by goal-driven autonomy, they represent a powerful form of generative AI, marking a transition from reactive content generation into proactive multitasking capabilities. As an exemplar, we propose an architecture of a multi-agent system for the implementation phase of the software engineering process. We also present a comprehensive threat model for the proposed system. We demonstrate that while such systems can generate code quite accurately, they are vulnerable to attacks, including code injection. Due to their autonomous design and lack of humans in the loop, these systems cannot identify and respond to attacks by themselves. This paper analyzes the vulnerability of multi-agent systems and concludes that the coder-reviewer-tester architecture is more resilient than both the coder and coder-tester architectures, but is less efficient at writing code. We find that by adding a security analysis agent, we mitigate the loss in efficiency while achieving even better resiliency. We conclude by demonstrating that the security analysis agent is vulnerable to advanced code injection attacks, showing that embedding poisonous few-shot examples in the injected code can increase the attack success rate from 0% to 71.95%.",
    "arxiv_url": "https://arxiv.org/abs/2512.21818",
    "authors": [
      "Brian Bowers",
      "Smita Khapre",
      "Jugal Kalita"
    ],
    "first_author": "Brian Bowers",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Maintenance",
    "task": "Code Review",
    "tags": [
      "Code Agents",
      "Code Injection",
      "Few-shot Poisoning",
      "Security Analysis Agent",
      "Threat Modeling",
      "Architectural Resilience",
      "Adversarial Robustness",
      "Data Exfiltration"
    ],
    "summary": "本文提出并评估了用于软件实现阶段的多代理系统架构和相应的威胁模型，展示在代码注入攻击下不同架构（coder、coder-tester、coder-reviewer-tester）在安全性与效率间的权衡，并提出加入安全分析代理作为缓解措施但同时揭示该代理可被嵌入有害少样本示例的高级注入攻击显著绕过（成功率从0%提升到71.95%）。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.21818v1",
    "published": "2025-12-26",
    "update_time": "2025-12-26",
    "download_time": "2025-12-29 02:01:53"
  },
  {
    "id": "2512.20861",
    "title": "Memory-Efficient Acceleration of Block Low-Rank Foundation Models on Resource Constrained GPUs",
    "abstract": "Recent advances in transformer-based foundation models have made them the default choice for many tasks, but their rapidly growing size makes fitting a full model on a single GPU increasingly difficult and their computational cost prohibitive. Block low-rank (BLR) compression techniques address this challenge by learning compact representations of weight matrices. While traditional low-rank (LR) methods often incur sharp accuracy drops, BLR approaches such as Monarch and BLAST can better capture the underlying structure, thus preserving accuracy while reducing computations and memory footprints. In this work, we use roofline analysis to show that, although BLR methods achieve theoretical savings and practical speedups for single-token inference, multi-token inference often becomes memory-bound in practice, increasing latency despite compiler-level optimizations in PyTorch. To address this, we introduce custom Triton kernels with partial fusion and memory layout optimizations for both Monarch and BLAST. On memory-constrained NVIDIA GPUs such as Jetson Orin Nano and A40, our kernels deliver up to $3.76\\times$ speedups and $3\\times$ model size compression over PyTorch dense baselines using CUDA backend and compiler-level optimizations, while supporting various models including Llama-7/1B, GPT2-S, DiT-XL/2, and ViT-B. Our code is available at https://github.com/pabillam/mem-efficient-blr .",
    "arxiv_url": "https://arxiv.org/abs/2512.20861",
    "authors": [
      "Pierre Abillama",
      "Changwoo Lee",
      "Juechu Dong",
      "David Blaauw",
      "Dennis Sylvester",
      "Hun-Seok Kim"
    ],
    "first_author": "Pierre Abillama",
    "category": [
      "Technical"
    ],
    "field": "Model Deployment & Systems",
    "task": "Inference Acceleration & Memory Optimization",
    "tags": [
      "Block Low-Rank",
      "Triton Kernels",
      "Roofline Analysis",
      "Memory Layout Optimization",
      "Partial Fusion",
      "Inference Acceleration",
      "Edge GPUs",
      "Model Compression",
      "Intermediate Data Movement",
      "Hardware-aware Optimization"
    ],
    "summary": "本文通过屋顶线分析揭示了块低秩（BLR）分解在多令牌推理中因中间数据移动而变为内存瓶颈，并提出基于Triton的部分融合与内存布局优化内核，在受限GPU上显著加速并压缩模型。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.20861v1",
    "published": "2025-12-24",
    "update_time": "2025-12-24",
    "download_time": "2025-12-29 02:07:00"
  },
  {
    "id": "2512.20823",
    "title": "NotSoTiny: A Large, Living Benchmark for RTL Code Generation",
    "abstract": "LLMs have shown early promise in generating RTL code, yet evaluating their capabilities in realistic setups remains a challenge. So far, RTL benchmarks have been limited in scale, skewed toward trivial designs, offering minimal verification rigor, and remaining vulnerable to data contamination. To overcome these limitations and to push the field forward, this paper introduces NotSoTiny, a benchmark that assesses LLM on the generation of structurally rich and context-aware RTL. Built from hundreds of actual hardware designs produced by the Tiny Tapeout community, our automated pipeline removes duplicates, verifies correctness and periodically incorporates new designs to mitigate contamination, matching Tiny Tapeout release schedule. Evaluation results show that NotSoTiny tasks are more challenging than prior benchmarks, emphasizing its effectiveness in overcoming current limitations of LLMs applied to hardware design, and in guiding the improvement of such promising technology.",
    "arxiv_url": "https://arxiv.org/abs/2512.20823",
    "authors": [
      "Razine Moundir Ghorab",
      "Emanuele Parisi",
      "Cristian Gutierrez",
      "Miquel Alberti-Binimelis",
      "Miquel Moreto",
      "Dario Garcia-Gasulla",
      "Gokcen Kestor"
    ],
    "first_author": "Razine Moundir Ghorab",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Completion",
    "tags": [
      "RTL Code Generation",
      "Contextual Module Completion",
      "Formal Equivalence Checking",
      "Simulation-based Evaluation",
      "Temporal Deduplication",
      "Near-duplicate Detection",
      "MinHash-LSH",
      "Contamination Mitigation",
      "Structural Complexity",
      "Living Benchmark"
    ],
    "summary": "本文提出NotSoTiny——一个基于Tiny Tapeout真实硬件设计、经过去重与周期性刷新以降低数据污染的规模化RTL模块补全基准，并结合仿真与形式等价检查提供严格可扩展的评测流程以衡量LLM在复杂结构化硬件代码生成中的表现。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.20823v1",
    "published": "2025-12-23",
    "update_time": "2025-12-23",
    "download_time": "2025-12-29 02:07:41"
  },
  {
    "id": "2512.21757",
    "title": "How Do Agents Perform Code Optimization? An Empirical Study",
    "abstract": "Performance optimization is a critical yet challenging aspect of software development, often requiring a deep understanding of system behavior, algorithmic tradeoffs, and careful code modifications. Although recent advances in AI coding agents have accelerated code generation and bug fixing, little is known about how these agents perform on real-world performance optimization tasks. We present the first empirical study comparing agent- and human-authored performance optimization commits, analyzing 324 agent-generated and 83 human-authored PRs from the AIDev dataset across adoption, maintainability, optimization patterns, and validation practices. We find that AI-authored performance PRs are less likely to include explicit performance validation than human-authored PRs (45.7\\% vs. 63.6\\%, $p=0.007$). In addition, AI-authored PRs largely use the same optimization patterns as humans. We further discuss limitations and opportunities for advancing agentic code optimization.",
    "arxiv_url": "https://arxiv.org/abs/2512.21757",
    "authors": [
      "Huiyun Peng",
      "Antonio Zhong",
      "Ricardo Andrés Calvo Méndez",
      "Kelechi G. Kalu",
      "James C. Davis"
    ],
    "first_author": "Huiyun Peng",
    "category": [
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Performance Optimization",
    "tags": [
      "Performance validation",
      "Optimization patterns",
      "LLM-assisted annotation",
      "Catalog refinement",
      "Agent-human comparison",
      "Merge/adoption metrics",
      "Static reasoning",
      "Profiling-based validation"
    ],
    "summary": "本文基于 AIDev 数据集实证分析了 407 个性能相关 PR（含 324 个 AI 代理生成的 PR），比较了 AI 与人类在性能优化模式、验证实践、合并率与可维护性上的差异，发现 AI 提交虽然采用与人类相似的优化模式，但较少提供性能验证且更依赖静态推理，并据此扩展并修订了性能优化模式目录。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.21757v1",
    "published": "2025-12-25",
    "update_time": "2025-12-25",
    "download_time": "2025-12-30 01:53:39"
  },
  {
    "id": "2512.21681",
    "title": "Exploring the Security Threats of Retriever Backdoors in Retrieval-Augmented Code Generation",
    "abstract": "Retrieval-Augmented Code Generation (RACG) is increasingly adopted to enhance Large Language Models for software development, yet its security implications remain dangerously underexplored. This paper conducts the first systematic exploration of a critical and stealthy threat: backdoor attacks targeting the retriever component, which represents a significant supply-chain vulnerability. It is infeasible to assess this threat realistically, as existing attack methods are either too ineffective to pose a real danger or are easily detected by state-of-the-art defense mechanisms spanning both latent-space analysis and token-level inspection, which achieve consistently high detection rates. To overcome this barrier and enable a realistic analysis, we first developed VenomRACG, a new class of potent and stealthy attack that serves as a vehicle for our investigation. Its design makes poisoned samples statistically indistinguishable from benign code, allowing the attack to consistently maintain low detectability across all evaluated defense mechanisms. Armed with this capability, our exploration reveals a severe vulnerability: by injecting vulnerable code equivalent to only 0.05% of the entire knowledge base size, an attacker can successfully manipulate the backdoored retriever to rank the vulnerable code in its top-5 results in 51.29% of cases. This translates to severe downstream harm, causing models like GPT-4o to generate vulnerable code in over 40% of targeted scenarios, while leaving the system's general performance intact. Our findings establish that retriever backdooring is not a theoretical concern but a practical threat to the software development ecosystem that current defenses are blind to, highlighting the urgent need for robust security measures.",
    "arxiv_url": "https://arxiv.org/abs/2512.21681",
    "authors": [
      "Tian Li",
      "Bo Lin",
      "Shangwen Wang",
      "Yusong Tan"
    ],
    "first_author": "Tian Li",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Retrieval",
    "tags": [
      "Retriever Backdoor",
      "Supply-Chain Attack",
      "Semantic Disruption Injection",
      "Vulnerability-Aware Trigger",
      "Syntax-Semantic Injection",
      "Stealthy Poisoning",
      "Defense Evasion",
      "Transferability",
      "Downstream Vulnerability Induction",
      "ASR Evaluation"
    ],
    "summary": "本文提出VenomRACG，一种针对检索器的高效且难以检测的后门攻击，通过语义扰动注入与脆弱性感知触发器在极少量投毒样本下使检索器优先返回易受攻击代码并诱导下游生成器输出脆弱代码，且能规避现有检测方法。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.21681v1",
    "published": "2025-12-25",
    "update_time": "2025-12-25",
    "download_time": "2025-12-30 01:53:55"
  },
  {
    "id": "2512.21613",
    "title": "AMS-IO-Bench and AMS-IO-Agent: Benchmarking and Structured Reasoning for Analog and Mixed-Signal Integrated Circuit Input/Output Design",
    "abstract": "In this paper, we propose AMS-IO-Agent, a domain-specialized LLM-based agent for structure-aware input/output (I/O) subsystem generation in analog and mixed-signal (AMS) integrated circuits (ICs). The central contribution of this work is a framework that connects natural language design intent with industrial-level AMS IC design deliverables. AMS-IO-Agent integrates two key capabilities: (1) a structured domain knowledge base that captures reusable constraints and design conventions; (2) design intent structuring, which converts ambiguous user intent into verifiable logic steps using JSON and Python as intermediate formats. We further introduce AMS-IO-Bench, a benchmark for wirebond-packaged AMS I/O ring automation. On this benchmark, AMS-IO-Agent achieves over 70\\% DRC+LVS pass rate and reduces design turnaround time from hours to minutes, outperforming the baseline LLM. Furthermore, an agent-generated I/O ring was fabricated and validated in a 28 nm CMOS tape-out, demonstrating the practical effectiveness of the approach in real AMS IC design flows. To our knowledge, this is the first reported human-agent collaborative AMS IC design in which an LLM-based agent completes a nontrivial subtask with outputs directly used in silicon.",
    "arxiv_url": "https://arxiv.org/abs/2512.21613",
    "authors": [
      "Zhishuai Zhang",
      "Xintian Li",
      "Shilong Liu",
      "Aodong Zhang",
      "Lu Jie",
      "Nan Sun"
    ],
    "first_author": "Zhishuai Zhang",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Code Agents",
      "Intent Graph",
      "EDA Script Generation",
      "I/O Ring Automation",
      "Domain-Specific Knowledge Base",
      "Design Intent Structuring",
      "Constraint Resolution",
      "DRC+LVS Validation",
      "Tape-out Validation",
      "Human-Agent Collaboration"
    ],
    "summary": "本文提出AMS-IO-Agent，一种结合领域知识库与结构化设计意图推理的LLM智能体，并构建AMS-IO-Bench基准，实现自动生成可用于工业流程的AMS I/O环的可执行EDA脚本与版图，在28nm芯片流片中验证并取得70%+ DRC/LVS通过率与显著缩短设计周期。",
    "quality": "High",
    "conference": "AAAI 2026",
    "pdf_url": "https://arxiv.org/pdf/2512.21613v1",
    "published": "2025-12-25",
    "update_time": "2025-12-25",
    "download_time": "2025-12-30 01:58:59"
  },
  {
    "id": "2512.21540",
    "title": "Leash: Adaptive Length Penalty and Reward Shaping for Efficient Large Reasoning Model",
    "abstract": "Existing approaches typically rely on fixed length penalties, but such penalties are hard to tune and fail to adapt to the evolving reasoning abilities of LLMs, leading to suboptimal trade-offs between accuracy and conciseness. To address this challenge, we propose Leash (adaptive LEngth penAlty and reward SHaping), a reinforcement learning framework for efficient reasoning in LLMs. We formulate length control as a constrained optimization problem and employ a Lagrangian primal-dual method to dynamically adjust the penalty coefficient. When generations exceed the target length, the penalty is intensified; when they are shorter, it is relaxed. This adaptive mechanism guides models toward producing concise reasoning without sacrificing task performance. Experiments on Deepseek-R1-Distill-Qwen-1.5B and Qwen3-4B-Thinking-2507 show that Leash reduces the average reasoning length by 60% across diverse tasks - including in-distribution mathematical reasoning and out-of-distribution domains such as coding and instruction following - while maintaining competitive performance. Our work thus presents a practical and effective paradigm for developing controllable and efficient LLMs that balance reasoning capabilities with computational budgets.",
    "arxiv_url": "https://arxiv.org/abs/2512.21540",
    "authors": [
      "Yanhao Li",
      "Lu Ma",
      "Jiaran Zhang",
      "Lexiang Tang",
      "Wentao Zhang",
      "Guibo Luo"
    ],
    "first_author": "Yanhao Li",
    "category": [
      "Technical"
    ],
    "field": "Model Efficiency & Control",
    "task": "Reasoning Length Control",
    "tags": [
      "Length Control",
      "Primal-Dual Optimization",
      "One-Sided Penalty",
      "Reward Shaping",
      "Chain-of-Thought Compression",
      "Constraint RL",
      "Efficiency",
      "Training Stability"
    ],
    "summary": "本文提出LEASH，一种基于拉格朗日对偶的自适应长度惩罚与奖励塑形的强化学习方法，通过动态调整罚项并采用单向惩罚在训练中控制推理链长度，从而在不牺牲任务性能的前提下显著减少生成长度并提升推理效率。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.21540v1",
    "published": "2025-12-25",
    "update_time": "2025-12-25",
    "download_time": "2025-12-30 02:00:03"
  },
  {
    "id": "2512.23385",
    "title": "Securing the AI Supply Chain: What Can We Learn From Developer-Reported Security Issues and Solutions of AI Projects?",
    "abstract": "The rapid growth of Artificial Intelligence (AI) models and applications has led to an increasingly complex security landscape. Developers of AI projects must contend not only with traditional software supply chain issues but also with novel, AI-specific security threats. However, little is known about what security issues are commonly encountered and how they are resolved in practice. This gap hinders the development of effective security measures for each component of the AI supply chain. We bridge this gap by conducting an empirical investigation of developer-reported issues and solutions, based on discussions from Hugging Face and GitHub. To identify security-related discussions, we develop a pipeline that combines keyword matching with an optimal fine-tuned distilBERT classifier, which achieved the best performance in our extensive comparison of various deep learning and large language models. This pipeline produces a dataset of 312,868 security discussions, providing insights into the security reporting practices of AI applications and projects. We conduct a thematic analysis of 753 posts sampled from our dataset and uncover a fine-grained taxonomy of 32 security issues and 24 solutions across four themes: (1) System and Software, (2) External Tools and Ecosystem, (3) Model, and (4) Data. We reveal that many security issues arise from the complex dependencies and black-box nature of AI components. Notably, challenges related to Models and Data often lack concrete solutions. Our insights can offer evidence-based guidance for developers and researchers to address real-world security threats across the AI supply chain.",
    "arxiv_url": "https://arxiv.org/abs/2512.23385",
    "authors": [
      "The Anh Nguyen",
      "Triet Huynh Minh Le",
      "M. Ali Babar"
    ],
    "first_author": "The Anh Nguyen",
    "category": [
      "Empirical",
      "Benchmark",
      "Technical"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "Security and Vulnerabilities",
      "AI Supply Chain",
      "Developer Discussions",
      "Learning-based Classification",
      "Security Taxonomy",
      "Dependency Vulnerabilities",
      "Data Leakage",
      "Community-driven Detection"
    ],
    "summary": "本文构建关键词+学习型分类器的检测管道，从 Hugging Face 与 GitHub 抽取并标注了约312,868条安全相关讨论，基于753条抽样帖进行主题分析，提出覆盖系统/工具/模型/数据四大类的32类安全问题与24类解决方案并开放复现包，为AI供应链安全提供实证证据与实践建议。",
    "quality": "High",
    "conference": "IEEE/ACM International Conference on Software Engineering (ICSE 2026)",
    "pdf_url": "https://arxiv.org/pdf/2512.23385v1",
    "published": "2025-12-29",
    "update_time": "2025-12-29",
    "download_time": "2026-01-01 02:04:42"
  },
  {
    "id": "2512.23327",
    "title": "An Empirical Study of Generative AI Adoption in Software Engineering",
    "abstract": "Context. GenAI tools are being increasingly adopted by practitioners in SE, promising support for several SE activities. Despite increasing adoption, we still lack empirical evidence on how GenAI is used in practice, the benefits it provides, the challenges it introduces, and its broader organizational and societal implications. Objective. This study aims to provide an overview of the status of GenAI adoption in SE. It investigates the status of GenAI adoption, associated benefits and challenges, institutionalization of tools and techniques, and anticipated long term impacts on SE professionals and the community. Results. The results indicate a wide adoption of GenAI tools and how they are deeply integrated into daily SE work, particularly for implementation, verification and validation, personal assistance, and maintenance-related tasks. Practitioners report substantial benefits, most notably reduction in cycle time, quality improvements, enhanced support in knowledge work, and productivity gains. However, objective measurement of productivity and quality remains limited in practice. Significant challenges persist, including incorrect or unreliable outputs, prompt engineering difficulties, validation overhead, security and privacy concerns, and risks of overreliance. Institutionalization of tools and techniques seems to be common, but it varies considerably, with a strong focus on tool access and less emphasis on training and governance. Practitioners expect GenAI to redefine rather than replace their roles, while expressing moderate concern about job market contraction and skill shifts.",
    "arxiv_url": "https://arxiv.org/abs/2512.23327",
    "authors": [
      "Görkem Giray",
      "Onur Demirörs",
      "Marcos Kalinowski",
      "Daniel Mendez"
    ],
    "first_author": "Görkem Giray",
    "category": [
      "Empirical"
    ],
    "field": "Adoption & Socio-technical",
    "task": "GenAI adoption and organizational impact",
    "tags": [
      "Human-LLM Interaction",
      "Productivity Gains",
      "Cycle Time Reduction",
      "Quality Improvement",
      "Validation Overhead",
      "Prompt Engineering",
      "Security and Privacy",
      "Tool Institutionalization",
      "Governance",
      "Overreliance",
      "Job Market Impact",
      "Empirical Survey"
    ],
    "summary": "本文基于对来自37个国家的204名软件工程从业者的问卷调查，实证分析了生成式AI在软件工程中的采用现状、带来的效益与挑战、工具制度化程度及对职业与社区的长期影响。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.23327v1",
    "published": "2025-12-29",
    "update_time": "2025-12-29",
    "download_time": "2026-01-01 02:05:01"
  },
  {
    "id": "2512.23631",
    "title": "BOAD: Discovering Hierarchical Software Engineering Agents via Bandit Optimization",
    "abstract": "Large language models (LLMs) have shown strong reasoning and coding capabilities, yet they struggle to generalize to real-world software engineering (SWE) problems that are long-horizon and out of distribution. Existing systems often rely on a single agent to handle the entire workflow-interpreting issues, navigating large codebases, and implementing fixes-within one reasoning chain. Such monolithic designs force the model to retain irrelevant context, leading to spurious correlations and poor generalization. Motivated by how human engineers decompose complex problems, we propose structuring SWE agents as orchestrators coordinating specialized sub-agents for sub-tasks such as localization, editing, and validation. The challenge lies in discovering effective hierarchies automatically: as the number of sub-agents grows, the search space becomes combinatorial, and it is difficult to attribute credit to individual sub-agents within a team. We address these challenges by formulating hierarchy discovery as a multi-armed bandit (MAB) problem, where each arm represents a candidate sub-agent and the reward measures its helpfulness when collaborating with others. This framework, termed Bandit Optimization for Agent Design (BOAD), enables efficient exploration of sub-agent designs under limited evaluation budgets. On SWE-bench-Verified, BOAD outperforms single-agent and manually designed multi-agent systems. On SWE-bench-Live, featuring more recent and out-of-distribution issues, our 36B system ranks second on the leaderboard at the time of evaluation, surpassing larger models such as GPT-4 and Claude. These results demonstrate that automatically discovered hierarchical multi-agent systems significantly improve generalization on challenging long-horizon SWE tasks. Code is available at https://github.com/iamxjy/BOAD-SWE-Agent.",
    "arxiv_url": "https://arxiv.org/abs/2512.23631",
    "authors": [
      "Iris Xu",
      "Guangtao Zeng",
      "Zexue He",
      "Charles Jin",
      "Aldo Pareja",
      "Dan Gutfreund",
      "Chuang Gan",
      "Zhang-Wei Hong"
    ],
    "first_author": "Iris Xu",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Hierarchical Multi-Agent Orchestration",
    "tags": [
      "Code Agents",
      "Hierarchical Agents",
      "Bandit Optimization",
      "Helpfulness Estimation",
      "LLM-as-Judge",
      "Multi-Agent Coordination",
      "Orchestrator",
      "Sub-agent Specialization",
      "Sample Efficiency",
      "Long-Horizon Generalization"
    ],
    "summary": "本文提出BOAD，一种将多臂赌博机用于自动发现分层软件工程多智能体体系结构的方法，通过评估子智能体的“有用性”并高效组合以提升在长时程、分布外的软件工程问题上的泛化能力，且在实际基准上取得优异成绩。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.23631v1",
    "published": "2025-12-29",
    "update_time": "2025-12-29",
    "download_time": "2026-01-01 02:05:32"
  },
  {
    "id": "2512.23557",
    "title": "Toward Trustworthy Agentic AI: A Multimodal Framework for Preventing Prompt Injection Attacks",
    "abstract": "Powerful autonomous systems, which reason, plan, and converse using and between numerous tools and agents, are made possible by Large Language Models (LLMs), Vision-Language Models (VLMs), and new agentic AI systems, like LangChain and GraphChain. Nevertheless, this agentic environment increases the probability of the occurrence of multimodal prompt injection (PI) attacks, in which concealed or malicious instructions carried in text, pictures, metadata, or agent-to-agent messages may spread throughout the graph and lead to unintended behavior, a breach of policy, or corruption of state. In order to mitigate these risks, this paper suggests a Cross-Agent Multimodal Provenanc- Aware Defense Framework whereby all the prompts, either user-generated or produced by upstream agents, are sanitized and all the outputs generated by an LLM are verified independently before being sent to downstream nodes. This framework contains a Text sanitizer agent, visual sanitizer agent, and output validator agent all coordinated by a provenance ledger, which keeps metadata of modality, source, and trust level throughout the entire agent network. This architecture makes sure that agent-to-agent communication abides by clear trust frames such such that injected instructions are not propagated down LangChain or GraphChain-style-workflows. The experimental assessments show that multimodal injection detection accuracy is significantly enhanced, and the cross-agent trust leakage is minimized, as well as, agentic execution pathways become stable. The framework, which expands the concept of provenance tracking and validation to the multi-agent orchestration, enhances the establishment of secure, understandable and reliable agentic AI systems.",
    "arxiv_url": "https://arxiv.org/abs/2512.23557",
    "authors": [
      "Toqeer Ali Syed",
      "Mishal Ateeq Almutairi",
      "Mahmoud Abdel Moaty"
    ],
    "first_author": "Toqeer Ali Syed",
    "category": [
      "Technical"
    ],
    "field": "LLM Security & Safety",
    "task": "Prompt Injection Defense",
    "tags": [
      "Multimodal Sanitization",
      "Provenance Tracking",
      "Cross-Agent Validation",
      "Output Validation",
      "Trust Scoring",
      "Zero-Trust Agent Network",
      "Visual Prompt Injection Detection",
      "Agentic AI Security"
    ],
    "summary": "本文提出了一个跨代理的多模态可溯源防御框架，通过文本与图像清洗器、信任评分与可溯源账本以及输出验证器，防止提示注入在LangChain/GraphChain式多代理管道中传播并提升系统安全性与执行稳定性。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.23557v1",
    "published": "2025-12-29",
    "update_time": "2025-12-29",
    "download_time": "2026-01-01 02:06:13"
  },
  {
    "id": "2512.24636",
    "title": "How Do Agentic AI Systems Deal With Software Energy Concerns? A Pull Request-Based Study",
    "abstract": "As Software Engineering enters its new era (SE 3.0), AI coding agents increasingly automate software development workflows. However, it remains unclear how exactly these agents recognize and address software energy concerns-an issue growing in importance due to large-scale data centers, energy-hungry language models, and battery-constrained devices. In this paper, we examined the energy awareness of agent-authored pull requests (PRs) using a publicly available dataset. We identified 216 energy-explicit PRs and conducted a thematic analysis, deriving a taxonomy of energy-aware work. Our further analysis of the applied optimization techniques shows that most align with established research recommendations. Although building and running these agents is highly energy intensive, encouragingly, the results indicate that they exhibit energy awareness when generating software artifacts. However, optimization-related PRs are accepted less frequently than others, largely due to their negative impact on maintainability.",
    "arxiv_url": "https://arxiv.org/abs/2512.24636",
    "authors": [
      "Tanjum Motin Mitul",
      "Md. Masud Mazumder",
      "Md Nahidul Islam Opu",
      "Shaiful Chowdhury"
    ],
    "first_author": "Tanjum Motin Mitul",
    "category": [
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Performance Optimization",
    "tags": [
      "Energy-aware Code",
      "Energy Profiling",
      "Optimization Techniques",
      "Taxonomy",
      "Maintainability Tradeoff",
      "Pull Request Analysis",
      "Agentic AI",
      "Merge Time Impact"
    ],
    "summary": "本文基于AIDev数据集对216个由AI代理提交的能耗相关Pull Request进行主题性分析，构建五类能耗关注点的分类并归纳出21种优化技术，发现代理具备能耗意识但优化类PR因降低可维护性而更易被拒绝且合并耗时更长。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.24636v1",
    "published": "2025-12-31",
    "update_time": "2025-12-31",
    "download_time": "2026-01-02 01:55:15"
  },
  {
    "id": "2512.24635",
    "title": "DynaFix: Iterative Automated Program Repair Driven by Execution-Level Dynamic Information",
    "abstract": "Automated Program Repair (APR) aims to automatically generate correct patches for buggy programs. Recent approaches leveraging large language models (LLMs) have shown promise but face limitations. Most rely solely on static analysis, ignoring runtime behaviors. Some attempt to incorporate dynamic signals, but these are often restricted to training or fine-tuning, or injected only once into the repair prompt, without iterative use. This fails to fully capture program execution. Current iterative repair frameworks typically rely on coarse-grained feedback, such as pass/fail results or exception types, and do not leverage fine-grained execution-level information effectively. As a result, models struggle to simulate human stepwise debugging, limiting their effectiveness in multi-step reasoning and complex bug repair.   To address these challenges, we propose DynaFix, an execution-level dynamic information-driven APR method that iteratively leverages runtime information to refine the repair process. In each repair round, DynaFix captures execution-level dynamic information such as variable states, control-flow paths, and call stacks, transforming them into structured prompts to guide LLMs in generating candidate patches. If a patch fails validation, DynaFix re-executes the modified program to collect new execution information for the next attempt. This iterative loop incrementally improves patches based on updated feedback, similar to the stepwise debugging practices of human developers. We evaluate DynaFix on the Defects4J v1.2 and v2.0 benchmarks. DynaFix repairs 186 single-function bugs, a 10% improvement over state-of-the-art baselines, including 38 bugs previously unrepaired. It achieves correct patches within at most 35 attempts, reducing the patch search space by 70% compared with existing methods, thereby demonstrating both effectiveness and efficiency in repairing complex bugs.",
    "arxiv_url": "https://arxiv.org/abs/2512.24635",
    "authors": [
      "Zhili Huang",
      "Ling Xu",
      "Chao Liu",
      "Weifeng Sun",
      "Xu Zhang",
      "Yan Lei",
      "Meng Yan",
      "Hongyu Zhang"
    ],
    "first_author": "Zhili Huang",
    "category": [
      "Technical"
    ],
    "field": "Quality Management",
    "task": "Bug Repair",
    "tags": [
      "Program Repair",
      "Dynamic Analysis",
      "Runtime Instrumentation",
      "Iterative Debugging",
      "Execution Traces",
      "Fault Localization",
      "LLM Prompting",
      "Search Space Reduction",
      "Java Instrumentation",
      "Patch Validation"
    ],
    "summary": "DynaFix提出一种基于执行级动态信息的迭代自动程序修复框架，通过轻量级Java运行时插桩收集变量状态、控制流和调用栈并将其结构化为提示驱动LLM生成与验证补丁，从而在Defects4J上显著提升修复率并减少补丁搜索空间。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.24635v1",
    "published": "2025-12-31",
    "update_time": "2025-12-31",
    "download_time": "2026-01-02 01:55:30"
  },
  {
    "id": "2512.25065",
    "title": "Vulcan: Instance-Optimal Systems Heuristics Through LLM-Driven Search",
    "abstract": "Resource-management tasks in modern operating and distributed systems continue to rely primarily on hand-designed heuristics for tasks such as scheduling, caching, or active queue management. Designing performant heuristics is an expensive, time-consuming process that we are forced to continuously go through due to the constant flux of hardware, workloads and environments.   We propose a new alternative: synthesizing instance-optimal heuristics -- specialized for the exact workloads and hardware where they will be deployed -- using code-generating large language models (LLMs). To make this synthesis tractable, Vulcan separates policy and mechanism through LLM-friendly, task-agnostic interfaces. With these interfaces, users specify the inputs and objectives of their desired policy, while Vulcan searches for performant policies via evolutionary search over LLM-generated code. This interface is expressive enough to capture a wide range of system policies, yet sufficiently constrained to allow even small, inexpensive LLMs to generate correct and executable code.   We use Vulcan to synthesize performant heuristics for cache eviction and memory tiering, and find that these heuristics outperform all human-designed state-of-the-art algorithms by upto 69% and 7.9% in performance for each of these tasks respectively.",
    "arxiv_url": "https://arxiv.org/abs/2512.25065",
    "authors": [
      "Rohit Dwivedula",
      "Divyanshu Saxena",
      "Sujay Yadalam",
      "Daehyeok Kim",
      "Aditya Akella"
    ],
    "first_author": "Rohit Dwivedula",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Systems & Resource Management",
    "task": "Instance-optimal Heuristic Synthesis",
    "tags": [
      "Instance-Optimal Heuristics",
      "Policy Synthesis",
      "VALUE-RANK Interface",
      "Evolutionary Search",
      "LLM Code Generation",
      "Interface Separation",
      "Cache Eviction",
      "Memory Tiering",
      "Interpretable Heuristics"
    ],
    "summary": "本文提出VULCAN框架，通过将策略与机制分离并在受限的VALUE/RANK接口上结合LLM生成代码与进化搜索，自动合成针对具体工作负载和硬件的实例最优系统启发式策略，在缓存淘汰和内存分层任务上显著超过多个人类设计的基线。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.25065v1",
    "published": "2025-12-31",
    "update_time": "2025-12-31",
    "download_time": "2026-01-02 01:56:16"
  },
  {
    "id": "2512.24873",
    "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem",
    "abstract": "Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.",
    "arxiv_url": "https://arxiv.org/abs/2512.24873",
    "authors": [
      "Weixun Wang",
      "XiaoXiao Xu",
      "Wanhe An",
      "Fangwen Dai",
      "Wei Gao",
      "Yancheng He",
      "Ju Huang",
      "Qiang Ji",
      "Hanqi Jin",
      "Xiaoyang Li",
      "Yang Li",
      "Zhongwen Li",
      "Shirong Lin",
      "Jiashun Liu",
      "Zenan Liu",
      "Tao Luo",
      "Dilxat Muhtar",
      "Yuanbin Qu",
      "Jiaqiang Shi",
      "Qinghui Sun",
      "Yingshui Tan",
      "Hao Tang",
      "Runze Wang",
      "Yi Wang",
      "Zhaoguo Wang",
      "Yanan Wu",
      "Shaopan Xiong",
      "Binchen Xu",
      "Xander Xu",
      "Yuchi Xu",
      "Qipeng Zhang",
      "Xixia Zhang",
      "Haizhou Zhao",
      "Jie Zhao",
      "Shuaibing Zhao",
      "Baihui Zheng",
      "Jianhui Zheng",
      "Suhang Zheng",
      "Yanni Zhu",
      "Mengze Cai",
      "Kerui Cao",
      "Xitong Chen",
      "Yue Dai",
      "Lifan Du",
      "Tao Feng",
      "Tao He",
      "Jin Hu",
      "Yijie Hu",
      "Ziyu Jiang",
      "Cheng Li",
      "Xiang Li",
      "Jing Liang",
      "Chonghuan Liu",
      "ZhenDong Liu",
      "Haodong Mi",
      "Yanhu Mo",
      "Junjia Ni",
      "Shixin Pei",
      "Jingyu Shen",
      "XiaoShuai Song",
      "Cecilia Wang",
      "Chaofan Wang",
      "Kangyu Wang",
      "Pei Wang",
      "Tao Wang",
      "Wei Wang",
      "Ke Xiao",
      "Mingyu Xu",
      "Tiange Xu",
      "Nan Ya",
      "Siran Yang",
      "Jianan Ye",
      "Yaxing Zang",
      "Duo Zhang",
      "Junbo Zhang",
      "Boren Zheng",
      "Wanxi Deng",
      "Ling Pan",
      "Lin Qu",
      "Wenbo Su",
      "Jiamang Wang",
      "Wei Wang",
      "Hu Wei",
      "Minggang Wu",
      "Cheng Yu",
      "Bing Zhao",
      "Zhicheng Zheng",
      "Bo Zheng"
    ],
    "first_author": "Weixun Wang",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Agentic Systems",
    "task": "Agentic Reinforcement Learning",
    "tags": [
      "Code Agents",
      "Agentic Reinforcement Learning",
      "Chunk-level Credit Assignment",
      "Chunk-level Optimization",
      "Sandboxed Environment Orchestration",
      "Trajectory Synthesis",
      "Safety Verification",
      "Terminal Agent Benchmark",
      "End-to-end Training Pipeline",
      "Production Deployment"
    ],
    "summary": "本文提出了面向多回合、工具化工作流的 Agentic Learning Ecosystem（ALE）（包含 ROLL、ROCK、iFlow CLI），基于百万级轨迹训练出开源代理模型 ROME，提出基于语义交互块的信用分配与策略优化方法以提升长时序稳定性，并发布更严格的 Terminal Bench Pro 基准与安全校验流程，展示了在终端代理任务上的竞争性表现与生产部署能力。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.24873v1",
    "published": "2025-12-31",
    "update_time": "2025-12-31",
    "download_time": "2026-01-02 01:58:19"
  },
  {
    "id": "2512.24630",
    "title": "How Do Agentic AI Systems Address Performance Optimizations? A BERTopic-Based Analysis of Pull Requests",
    "abstract": "LLM-based software engineering is influencing modern software development. In addition to correctness, prior studies have also examined the performance of software artifacts generated by AI agents. However, it is unclear how exactly the agentic AI systems address performance concerns in practice. In this paper, we present an empirical study of performance-related pull requests generated by AI agents. Using LLM-assisted detection and BERTopic-based topic modeling, we identified 52 performance-related topics grouped into 10 higher-level categories. Our results show that AI agents apply performance optimizations across diverse layers of the software stack and that the type of optimization significantly affects pull request acceptance rates and review times. We also found that performance optimization by AI agents primarily occurs during the development phase, with less focus on the maintenance phase. Our findings provide empirical evidence that can support the evaluation and improvement of agentic AI systems with respect to their performance optimization behaviors and review outcomes.",
    "arxiv_url": "https://arxiv.org/abs/2512.24630",
    "authors": [
      "Md Nahidul Islam Opu",
      "Shahidul Islam",
      "Muhammad Asaduzzaman",
      "Shaiful Chowdhury"
    ],
    "first_author": "Md Nahidul Islam Opu",
    "category": [
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Performance Optimization",
    "tags": [
      "Performance Optimization Taxonomy",
      "BERTopic Clustering",
      "LLM-based Classification",
      "Pull Request Analysis",
      "PR Acceptance Rate",
      "Merge Time Analysis",
      "SDLC Activity Attribution",
      "Stack-level Optimizations",
      "Agentic AI Behavior"
    ],
    "summary": "本文通过对AIDev数据集中1221个由代理AI生成的性能相关PR进行LLM辅助识别与BERTopic主题建模，归纳出52个性能主题并分为10类，实证分析了不同优化类型在PR接受率、合并时长和软件生命周期活动中的分布差异。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.24630v1",
    "published": "2025-12-31",
    "update_time": "2025-12-31",
    "download_time": "2026-01-03 01:49:29"
  },
  {
    "id": "2512.24594",
    "title": "A Tale of 1001 LoC: Potential Runtime Error-Guided Specification Synthesis for Verifying Large-Scale Programs",
    "abstract": "Fully automated verification of large-scale software and hardware systems is arguably the holy grail of formal methods. Large language models (LLMs) have recently demonstrated their potential for enhancing the degree of automation in formal verification by, e.g., generating formal specifications as essential to deductive verification, yet exhibit poor scalability due to long-context reasoning limitations and, more importantly, the difficulty of inferring complex, interprocedural specifications. This paper presents Preguss -- a modular, fine-grained framework for automating the generation and refinement of formal specifications. Preguss synergizes between static analysis and deductive verification by steering two components in a divide-and-conquer fashion: (i) potential runtime error-guided construction and prioritization of verification units, and (ii) LLM-aided synthesis of interprocedural specifications at the unit level. We show that Preguss substantially outperforms state-of-the-art LLM-based approaches and, in particular, it enables highly automated RTE-freeness verification for real-world programs with over a thousand LoC, with a reduction of 80.6%~88.9% human verification effort.",
    "arxiv_url": "https://arxiv.org/abs/2512.24594",
    "authors": [
      "Zhongyi Wang",
      "Tengjie Lin",
      "Mingshuai Chen",
      "Haokun Li",
      "Mingqi Yang",
      "Xiao Yi",
      "Shengchao Qin",
      "Yixing Luo",
      "Xiaofeng Li",
      "Bin Gu",
      "Liqiang Lu",
      "Jianwei Yin"
    ],
    "first_author": "Zhongyi Wang",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Requirements & Design",
    "task": "Specification & Validation",
    "tags": [
      "Specification Synthesis",
      "RTE-guided Synthesis",
      "Interprocedural Contracts",
      "Static-Analysis Integration",
      "Deductive Verification",
      "Modular Verification",
      "Scalability",
      "Verification Dataset"
    ],
    "summary": "本文提出Preguss，一种结合静态分析与LLM的模块化细粒度规范合成框架，通过以潜在运行时错误引导的划分与优先级策略，在单元级合成跨过程规范，从而实现对千行级C程序的RTE无害性验证并大幅减少人工参与。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.24594v1",
    "published": "2025-12-31",
    "update_time": "2025-12-31",
    "download_time": "2026-01-03 01:49:48"
  },
  {
    "id": "2512.24796",
    "title": "LeanCat: A Benchmark Suite for Formal Category Theory in Lean (Part I: 1-Categories)",
    "abstract": "Large language models (LLMs) have made rapid progress in formal theorem proving, yet current benchmarks under-measure the kind of abstraction and library-mediated reasoning that organizes modern mathematics. In parallel with FATE's emphasis on frontier algebra, we introduce LeanCat, a Lean benchmark for category-theoretic formalization -- a unifying language for mathematical structure and a core layer of modern proof engineering -- serving as a stress test of structural, interface-level reasoning. Part I: 1-Categories contains 100 fully formalized statement-level tasks, curated into topic families and three difficulty tiers via an LLM-assisted + human grading process. The best model solves 8.25% of tasks at pass@1 (32.50%/4.17%/0.00% by Easy/Medium/High) and 12.00% at pass@4 (50.00%/4.76%/0.00%). We also evaluate LeanBridge which use LeanExplore to search Mathlib, and observe consistent gains over single-model baselines. LeanCat is intended as a compact, reusable checkpoint for tracking both AI and human progress toward reliable, research-level formalization in Lean.",
    "arxiv_url": "https://arxiv.org/abs/2512.24796",
    "authors": [
      "Rongge Xu",
      "Hui Dai",
      "Yiming Fu",
      "Jiedong Jiang",
      "Tianjiao Nie",
      "Hongwei Wang",
      "Junkai Wang",
      "Holiverse Yang",
      "Jiatong Yang",
      "Zhi-Hao Zhang"
    ],
    "first_author": "Rongge Xu",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Formalization & Theorem Proving",
    "task": "Theorem Proving Benchmark (library-grounded formalization)",
    "tags": [
      "Formalization Benchmark",
      "Category Theory",
      "Proof Synthesis",
      "Library-Grounded Reasoning",
      "Search-Augmented Proving",
      "Retrieve-Generate-Verify",
      "Difficulty Annotation",
      "Natural-to-Formal Gap",
      "Math Library Navigation",
      "Tool-Augmented Proving"
    ],
    "summary": "本文提出LeanCat基准（第一部分：1-范畴），包含100个在Lean中正式化的范畴论定理以测试模型在库驱动的抽象推理与证明合成能力，并给出难度标注、基线评估及基于检索的增强工作流。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.24796v1",
    "published": "2025-12-31",
    "update_time": "2025-12-31",
    "download_time": "2026-01-03 01:52:30"
  },
  {
    "id": "2512.24615",
    "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization",
    "abstract": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose \\textbf{Youtu-Agent}, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a \\textbf{Workflow} mode for standard tasks and a \\textbf{Meta-Agent} mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an \\textbf{Agent Practice} module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an \\textbf{Agent RL} module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.",
    "arxiv_url": "https://arxiv.org/abs/2512.24615",
    "authors": [
      "Yuchen Shi",
      "Yuzheng Cai",
      "Siqi Cai",
      "Zihan Xu",
      "Lichao Chen",
      "Yulei Qin",
      "Zhijian Zhou",
      "Xiang Fei",
      "Chaofan Qiu",
      "Xiaoyu Tan",
      "Gang Li",
      "Zongyi Li",
      "Haojia Lin",
      "Guocan Cai",
      "Yong Mao",
      "Yunsheng Wu",
      "Ke Li",
      "Xing Sun"
    ],
    "first_author": "Yuchen Shi",
    "category": [
      "Technical"
    ],
    "field": "Agent Systems",
    "task": "Agent Construction & Optimization",
    "tags": [
      "Code Agents",
      "Automated Tool Synthesis",
      "Meta-Agent",
      "Workflow Automation",
      "In-Context Optimization",
      "Agent Reinforcement Learning",
      "YAML Configuration",
      "Context Management"
    ],
    "summary": "Youtu-Agent 提出一个模块化的 LLM agent 框架，通过 YAML 结构化配置实现自动生成工具与代理，并结合基于上下文的 Practice 模块与可扩展的 RL 模块进行混合策略优化以持续提升代理性能。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.24615v1",
    "published": "2025-12-31",
    "update_time": "2025-12-31",
    "download_time": "2026-01-03 01:54:18"
  },
  {
    "id": "2512.24570",
    "title": "On the Effectiveness of Training Data Optimization for LLM-based Code Generation: An Empirical Study",
    "abstract": "Large language models (LLMs) have achieved remarkable progress in code generation, largely driven by the availability of high-quality code datasets for effective training. To further improve data quality, numerous training data optimization techniques have been proposed; however, their overall effectiveness has not been systematically evaluated. To bridge this gap, we conduct the first large-scale empirical study, examining five widely-used training data optimization techniques and their pairwise combinations for LLM-based code generation across three benchmarks and four LLMs. Our results show that data synthesis is the most effective technique for improving functional correctness and reducing code smells, although it performs relatively worse on code maintainability compared to data refactoring, cleaning, and selection. Regarding combinations, we find that most combinations do not further improve functional correctness but can effectively enhance code quality (code smells and maintainability). Among all combinations, data synthesis combined with data refactoring achieves the strongest overall performance. Furthermore, our fine-grained analysis reinforces these findings and provides deeper insights into how individual techniques and their combinations influence code generation effectiveness. Overall, this work represents a first step toward a systematic understanding of training data optimization and combination strategies, offering practical guidance for future research and deployment in LLM-based code generation.",
    "arxiv_url": "https://arxiv.org/abs/2512.24570",
    "authors": [
      "Shiqi Kuang",
      "Zhao Tian",
      "Tao Xiao",
      "Dong Wang",
      "Junjie Chen"
    ],
    "first_author": "Shiqi Kuang",
    "category": [
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Pre-Training",
    "tags": [
      "Training Data Optimization",
      "Data Synthesis",
      "Data Refactoring",
      "Data Cleaning",
      "Data Selection",
      "Data Augmentation",
      "Technique Combination",
      "Correctness",
      "Code Smells",
      "Maintainability",
      "Complementarity",
      "Empirical Evaluation"
    ],
    "summary": "本文通过大规模实证比较五类训练数据优化技术及其组合在多种LLM和基准上的效果，发现数据合成在提升功能正确性和减少代码异味方面最有效，而组合增益有限且互补性比数量更重要，为训练数据优化提供了实践指导。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.24570v1",
    "published": "2025-12-31",
    "update_time": "2025-12-31",
    "download_time": "2026-01-04 02:05:06"
  },
  {
    "id": "2512.24560",
    "title": "Localized Calibrated Uncertainty in Code Language Models",
    "abstract": "Large Language models (LLMs) can generate complicated source code from natural language prompts. However, LLMs can generate output that deviates from what the user wants, requiring supervision and editing. To support this process, we offer techniques to localize where generations might be misaligned from user intent. We first create a dataset of \"Minimal Intent Aligning Patches\" of repaired LLM generated programs. Each program uses test cases to verify correctness. After creating a dataset of programs, we measure how well various techniques can assign a well-calibrated probability to indicate which parts of code will be edited in a minimal patch (i.e., give a probability that corresponds with empirical odds it is edited). We compare white-box probing (where we propose a technique for efficient arbitrary-span querying), against black-box reflective and self-consistency based approaches. We find probes with a small supervisor model can achieve low calibration error and Brier Skill Score of approx 0.2 estimating edited lines on code generated by models many orders of magnitude larger. We discuss the generalizability of the techniques, and the connections to AI oversight and control, finding a probe trained only on code shows some signs of generalizing to natural language errors if new probability scaling is allowed.",
    "arxiv_url": "https://arxiv.org/abs/2512.24560",
    "authors": [
      "David Gros",
      "Prem Devanbu"
    ],
    "first_author": "David Gros",
    "category": [
      "Technical",
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Alignment",
    "tags": [
      "Localized Calibration",
      "Line-level Confidence",
      "Token-level Confidence",
      "Minimal Patch",
      "White-box Probing",
      "Self-Consistency Sampling",
      "Reflective Confidence",
      "Brier Skill Score"
    ],
    "summary": "本文构建了一个包含最小意图对齐补丁的数据集，并提出并比较了白盒探针、基于多次采样的一致性和反思式方法，以在行/标记级别产生校准良好的局部不确定性估计，辅助定位需修复的代码片段。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.24560v1",
    "published": "2025-12-31",
    "update_time": "2025-12-31",
    "download_time": "2026-01-04 02:05:23"
  },
  {
    "id": "2512.24609",
    "title": "Reinforcement Learning-Augmented LLM Agents for Collaborative Decision Making and Performance Optimization",
    "abstract": "Large Language Models (LLMs) perform well in language tasks but often lack collaborative awareness and struggle to optimize global performance in multi-agent settings. We present a reinforcement learning-augmented LLM agent framework that formulates cooperation as a decentralized partially observable Markov decision process (Dec-POMDP) and adopts centralized training with decentralized execution (CTDE). We introduce Group Relative Policy Optimization (GRPO) to jointly optimize agent policies with access to global signals during training, together with a simplified joint reward that balances task quality, speed, and coordination cost. On collaborative writing and coding benchmarks, our framework delivers a 3x increase in task processing speed over single-agent baselines, 98.7% structural/style consistency in writing, and a 74.6% test pass rate in coding. The approach consistently outperforms strong multi-agent LLM baselines and provides a practical path toward reliable collaboration in complex workflows.",
    "arxiv_url": "https://arxiv.org/abs/2512.24609",
    "authors": [
      "Dong Qiu",
      "Duo Xu",
      "Limengxi Yue"
    ],
    "first_author": "Dong Qiu",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "LLM Agents & Collaboration",
    "task": "Multi-Agent Coordination and Policy Optimization",
    "tags": [
      "Code Agents",
      "Dec-POMDP",
      "CTDE",
      "Group Relative Policy Optimization",
      "Credit Assignment",
      "Joint Reward Design",
      "Role-based Agents",
      "Throughput Optimization"
    ],
    "summary": "本文提出一种基于强化学习的LLM多智能体协作框架，将协作建模为Dec‑POMDP、采用集中训练-分散执行并引入Group Relative Policy Optimization与联合奖励设计，从而在协作写作与角色分工编码任务上显著提升速度、质量与协调效率。",
    "quality": "High",
    "conference": "IEEE ICFTIC 2025",
    "pdf_url": "https://arxiv.org/pdf/2512.24609v1",
    "published": "2025-12-31",
    "update_time": "2025-12-31",
    "download_time": "2026-01-04 02:10:02"
  },
  {
    "id": "2512.24571",
    "title": "SynRAG: A Large Language Model Framework for Executable Query Generation in Heterogeneous SIEM System",
    "abstract": "Security Information and Event Management (SIEM) systems are essential for large enterprises to monitor their IT infrastructure by ingesting and analyzing millions of logs and events daily. Security Operations Center (SOC) analysts are tasked with monitoring and analyzing this vast data to identify potential threats and take preventive actions to protect enterprise assets. However, the diversity among SIEM platforms, such as Palo Alto Networks Qradar, Google SecOps, Splunk, Microsoft Sentinel and the Elastic Stack, poses significant challenges. As these systems differ in attributes, architecture, and query languages, making it difficult for analysts to effectively monitor multiple platforms without undergoing extensive training or forcing enterprises to expand their workforce. To address this issue, we introduce SynRAG, a unified framework that automatically generates threat detection or incident investigation queries for multiple SIEM platforms from a platform-agnostic specification. SynRAG can generate platformspecific queries from a single high-level specification written by analysts. Without SynRAG, analysts would need to manually write separate queries for each SIEM platform, since query languages vary significantly across systems. This framework enables seamless threat detection and incident investigation across heterogeneous SIEM environments, reducing the need for specialized training and manual query translation. We evaluate SynRAG against state-of-the-art language models, including GPT, Llama, DeepSeek, Gemma, and Claude, using Qradar and SecOps as representative SIEM systems. Our results demonstrate that SynRAG generates significantly better queries for crossSIEM threat detection and incident investigation compared to the state-of-the-art base models.",
    "arxiv_url": "https://arxiv.org/abs/2512.24571",
    "authors": [
      "Md Hasan Saju",
      "Austin Page",
      "Akramul Azim",
      "Jeff Gardiner",
      "Farzaneh Abazari",
      "Frank Eargle"
    ],
    "first_author": "Md Hasan Saju",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "SIEM & Security Operations",
    "task": "SIEM Query Generation",
    "tags": [
      "SIEM Query Translation",
      "Retrieval-Augmented Generation",
      "YAML Threat Specification",
      "Syntax-aware Generation",
      "Cross-SIEM Interoperability",
      "Executable Query Synthesis",
      "Threat Detection",
      "Benchmark for SIEM queries"
    ],
    "summary": "本文提出 SynRAG——一个基于 RAG 的统一框架，可从平台无关的 YAML 威胁规范自动生成各异 SIEM 平台（如 QRadar、SecOps）上可执行的查询，包含语法服务与向量检索并构建了用于评估的基准，实验显示优于若干主流大模型基线。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.24571v1",
    "published": "2025-12-31",
    "update_time": "2025-12-31",
    "download_time": "2026-01-04 02:10:30"
  }
]