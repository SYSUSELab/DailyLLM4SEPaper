[
  {
    "id": "2307.14936",
    "title": "PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback",
    "abstract": "Large Language Models for Code (Code LLM) are flourishing. New and powerful models are released on a weekly basis, demonstrating remarkable performance on the code generation task. Various approaches have been proposed to boost the code generation performance of pre-trained Code LLMs, such as supervised fine-tuning, instruction tuning, reinforcement learning, etc. In this paper, we propose a novel RRTF (Rank Responses to align Test&Teacher Feedback) framework, which can effectively and efficiently boost pre-trained large language models for code generation. Under this framework, we present PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval benchmark. Furthermore, through an extensive evaluation on CoderEval and LeetCode benchmarks, we show that PanGu-Coder2 consistently outperforms all previous Code LLMs.",
    "arxiv_url": "https://arxiv.org/abs/2307.14936",
    "authors": [
      "Bo Shen",
      "Jiaxin Zhang",
      "Taihong Chen",
      "Daoguang Zan",
      "Bing Geng",
      "An Fu",
      "Muhan Zeng",
      "Ailun Yu",
      "Jichuan Ji",
      "Jingyang Zhao",
      "Yuenan Guo",
      "Qianxiang Wang"
    ],
    "first_author": "Bo Shen",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Instruction-Tuning",
    "tags": [
      "Ranking-feedback fine-tuning",
      "Test-case-guided ranking",
      "Preference-ranking objective",
      "RLHF-alternative (ranking-based)",
      "Evol-Instruct sampling",
      "Instruction tuning for code",
      "Data-quality focused curation",
      "Efficient SFT-style optimization",
      "Pass-rate optimization"
    ],
    "summary": "本文提出RRTF（Rank Responses to align Test&Teacher Feedback）框架，使用基于测试与教师偏好的响应排序作为反馈对预训练代码模型进行高效指令微调，从而得到PanGu‑Coder2并在多项基准上显著提升代码生成通过率。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2307.14936v1",
    "published": "2023-07-27",
    "update_time": "2023-07-27",
    "download_time": "2025-12-16 10:37:54"
  },
  {
    "id": "2307.04349",
    "title": "RLTF: Reinforcement Learning from Unit Test Feedback",
    "abstract": "The goal of program synthesis, or code generation, is to generate executable code based on given descriptions. Recently, there has been an increasing number of studies employing reinforcement learning (RL) to improve the performance of large language models (LLMs) for code. However, current representative works either rely solely on offline frameworks, limiting the exploration of new sample spaces, or fall short in the utilization of unit test signals, not accounting for specific error locations within the code. To address these issues, we propose RLTF, i.e., Reinforcement Learning from Unit Test Feedback, a novel online RL framework with unit test feedback of multi-granularity for refining code LLMs. Our approach generates data in real-time during training and simultaneously utilizes fine-grained feedback signals to guide the model towards producing higher-quality code. Extensive experiments show that RLTF achieves state-of-the-art performance on the APPS and the MBPP benchmarks. Our code is available at: https://github.com/Zyq-scut/RLTF.",
    "arxiv_url": "https://arxiv.org/abs/2307.04349",
    "authors": [
      "Jiate Liu",
      "Yiqin Zhu",
      "Kaiwen Xiao",
      "Qiang Fu",
      "Xiao Han",
      "Wei Yang",
      "Deheng Ye"
    ],
    "first_author": "Jiate Liu",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Completion",
    "tags": [
      "Online Reinforcement Learning",
      "Unit-test-guided rewards",
      "Fine-grained error localization",
      "Adaptive reward scaling",
      "Online sample buffer",
      "Critic sampling",
      "Policy optimization for program synthesis"
    ],
    "summary": "该文提出RLTF，一种针对程序合成的在线强化学习框架，通过实时生成样本并从单元测试中提取多粒度（细粒度与自适应）反馈来对代码大模型进行优化，在APPS和MBPP上达到领先性能。",
    "quality": "High",
    "conference": "Transactions on Machine Learning Research (TMLR) 2024",
    "pdf_url": "https://arxiv.org/pdf/2307.04349v2",
    "published": "2023-07-10",
    "update_time": "2023-11-13",
    "download_time": "2025-12-16 13:25:42"
  },
  {
    "id": "2308.00147",
    "title": "Delving into Commit-Issue Correlation to Enhance Commit Message Generation Models",
    "abstract": "Commit message generation (CMG) is a challenging task in automated software engineering that aims to generate natural language descriptions of code changes for commits. Previous methods all start from the modified code snippets, outputting commit messages through template-based, retrieval-based, or learning-based models. While these methods can summarize what is modified from the perspective of code, they struggle to provide reasons for the commit. The correlation between commits and issues that could be a critical factor for generating rational commit messages is still unexplored.   In this work, we delve into the correlation between commits and issues from the perspective of dataset and methodology. We construct the first dataset anchored on combining correlated commits and issues. The dataset consists of an unlabeled commit-issue parallel part and a labeled part in which each example is provided with human-annotated rational information in the issue. Furthermore, we propose \\tool (\\underline{Ex}traction, \\underline{Gro}unding, \\underline{Fi}ne-tuning), a novel paradigm that can introduce the correlation between commits and issues into the training phase of models. To evaluate whether it is effective, we perform comprehensive experiments with various state-of-the-art CMG models. The results show that compared with the original models, the performance of \\tool-enhanced models is significantly improved.",
    "arxiv_url": "https://arxiv.org/abs/2308.00147",
    "authors": [
      "Liran Wang",
      "Xunzhu Tang",
      "Yichen He",
      "Changyu Ren",
      "Shuhua Shi",
      "Chaoran Yan",
      "Zhoujun Li"
    ],
    "first_author": "Liran Wang",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Summarization",
    "tags": [
      "Commit-Issue Correlation",
      "Rational Information Extraction",
      "Issue-Grounded Code Representation",
      "Extraction-Grounding-Fine-tuning Paradigm",
      "Commit-Issue Parallel Dataset",
      "Human-Annotated Rationale Labels",
      "Training-Time Issue Augmentation"
    ],
    "summary": "本文构建了首个提交—问题并行数据集并提出ExGroFi（提取—落地—微调）范式，通过将问题报告中的理性信息引入训练显著提升了自动生成提交信息的合理性与质量。",
    "quality": "High",
    "conference": "ASE (IEEE/ACM International Conference on Automated Software Engineering) 2023",
    "pdf_url": "https://arxiv.org/pdf/2308.00147v2",
    "published": "2023-07-31",
    "update_time": "2023-09-28",
    "download_time": "2025-12-11 16:29:10"
  }
]