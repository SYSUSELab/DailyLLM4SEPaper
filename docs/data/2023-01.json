[
  {
    "id": "2301.03988",
    "title": "SantaCoder: don't reach for the stars!",
    "abstract": "The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identifiable Information (PII) redaction pipeline, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data. We train 1.1B parameter models on the Java, JavaScript, and Python subsets of The Stack and evaluate them on the MultiPL-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ GitHub stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java, JavaScript, and Python portions of MultiPL-E, despite being a substantially smaller model. All models are released under an OpenRAIL license at https://hf.co/bigcode.",
    "arxiv_url": "https://arxiv.org/abs/2301.03988",
    "authors": [
      "Loubna Ben Allal",
      "Raymond Li",
      "Denis Kocetkov",
      "Chenghao Mou",
      "Christopher Akiki",
      "Carlos Munoz Ferrandis",
      "Niklas Muennighoff",
      "Mayank Mishra",
      "Alex Gu",
      "Manan Dey",
      "Logesh Kumar Umapathi",
      "Carolyn Jane Anderson",
      "Yangtian Zi",
      "Joel Lamy Poirier",
      "Hailey Schoelkopf",
      "Sergey Troshin",
      "Dmitry Abulkhanov",
      "Manuel Romero",
      "Michael Lappert",
      "Francesco De Toni",
      "Bernardo García del Río",
      "Qian Liu",
      "Shamik Bose",
      "Urvashi Bhattacharyya",
      "Terry Yue Zhuo",
      "Ian Yu",
      "Paulo Villegas",
      "Marco Zocca",
      "Sourab Mangrulkar",
      "David Lansky",
      "Huu Nguyen",
      "Danish Contractor",
      "Luis Villa",
      "Jia Li",
      "Dzmitry Bahdanau",
      "Yacine Jernite",
      "Sean Hughes",
      "Daniel Fried",
      "Arjun Guha",
      "Harm de Vries",
      "Leandro von Werra"
    ],
    "first_author": "Loubna Ben Allal",
    "category": [
      "Technical",
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Pre-Training",
    "tags": [
      "PII redaction pipeline",
      "PII annotation benchmark (400 files)",
      "Fill-in-the-middle pretraining objective",
      "Multi-Query Attention (MQA) ablations",
      "Near-duplicate filtering",
      "Repository-star filtering as data-quality proxy",
      "Comments-to-code and char-to-token ratio filters",
      "1.1B-parameter code model training",
      "Text-to-code benchmark evaluation",
      "Open-source release and governance"
    ],
    "summary": "本文总结了 BigCode 社区在训练 1.1B 参数代码模型过程中的工作，包含 PII 删除管道与 400 文件标注基准、对 MQA 与 FIM 的消融实验、对多种数据预处理（如近重复、仓库 star 筛选、注释比率等）对性能影响的研究，并在多语言 text-to-code 基准上展示小型模型优于更大开源模型的结果且公开发布模型与工具。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2301.03988v2",
    "published": "2023-01-09",
    "update_time": "2023-02-24",
    "download_time": "2025-12-16 10:44:24"
  },
  {
    "id": "2301.13816",
    "title": "Execution-based Code Generation using Deep Reinforcement Learning",
    "abstract": "The utilization of programming language (PL) models, pre-trained on large-scale code corpora, as a means of automating software engineering processes has demonstrated considerable potential in streamlining various code generation tasks such as code completion, code translation, and program synthesis. However, current approaches mainly rely on supervised fine-tuning objectives borrowed from text generation, neglecting unique sequence-level characteristics of code, including but not limited to compilability as well as syntactic and functional correctness. To address this limitation, we propose PPOCoder, a new framework for code generation that synergistically combines pre-trained PL models with Proximal Policy Optimization (PPO) which is a widely used deep reinforcement learning technique. By utilizing non-differentiable feedback from code execution and structure alignment, PPOCoder seamlessly integrates external code-specific knowledge into the model optimization process. It's important to note that PPOCoder is a task-agnostic and model-agnostic framework that can be used across different code generation tasks and PLs. Extensive experiments on three code generation tasks demonstrate the effectiveness of our proposed approach compared to SOTA methods, achieving significant improvements in compilation success rates and functional correctness across different PLs.",
    "arxiv_url": "https://arxiv.org/abs/2301.13816",
    "authors": [
      "Parshin Shojaee",
      "Aneesh Jain",
      "Sindhu Tipirneni",
      "Chandan K. Reddy"
    ],
    "first_author": "Parshin Shojaee",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Completion",
    "tags": [
      "Execution-based reward design",
      "PPO fine-tuning of pre-trained code models",
      "Compiler and unit-test feedback",
      "AST and DFG structural matching",
      "KL-divergence penalty to prevent forgetting",
      "Model- and task-agnostic RL framework"
    ],
    "summary": "本文提出PPOCoder，将PPO强化学习与基于执行（编译/单元测试）和AST/DFG结构对齐的非可微奖励相结合，对预训练代码模型进行微调，从而在代码补全、代码翻译和程序合成等任务上显著提升可编译性与功能正确性。",
    "quality": "High",
    "conference": "Transactions on Machine Learning Research (TMLR) 2023",
    "pdf_url": "https://arxiv.org/pdf/2301.13816v4",
    "published": "2023-01-31",
    "update_time": "2023-07-19",
    "download_time": "2025-12-16 13:26:46"
  },
  {
    "id": "2301.03270",
    "title": "A Survey of Learning-based Automated Program Repair",
    "abstract": "Automated program repair (APR) aims to fix software bugs automatically and plays a crucial role in software development and maintenance. With the recent advances in deep learning (DL), an increasing number of APR techniques have been proposed to leverage neural networks to learn bug-fixing patterns from massive open-source code repositories. Such learning-based techniques usually treat APR as a neural machine translation (NMT) task, where buggy code snippets (i.e., source language) are translated into fixed code snippets (i.e., target language) automatically. Benefiting from the powerful capability of DL to learn hidden relationships from previous bug-fixing datasets, learning-based APR techniques have achieved remarkable performance. In this paper, we provide a systematic survey to summarize the current state-of-the-art research in the learning-based APR community. We illustrate the general workflow of learning-based APR techniques and detail the crucial components, including fault localization, patch generation, patch ranking, patch validation, and patch correctness phases. We then discuss the widely-adopted datasets and evaluation metrics and outline existing empirical studies. We discuss several critical aspects of learning-based APR techniques, such as repair domains, industrial deployment, and the open science issue. We highlight several practical guidelines on applying DL techniques for future APR studies, such as exploring explainable patch generation and utilizing code features. Overall, our paper can help researchers gain a comprehensive understanding about the achievements of the existing learning-based APR techniques and promote the practical application of these techniques. Our artifacts are publicly available at \\url{https://github.com/QuanjunZhang/AwesomeLearningAPR}.",
    "arxiv_url": "https://arxiv.org/abs/2301.03270",
    "authors": [
      "Quanjun Zhang",
      "Chunrong Fang",
      "Yuxiang Ma",
      "Weisong Sun",
      "Zhenyu Chen"
    ],
    "first_author": "Quanjun Zhang",
    "category": [
      "Survey"
    ],
    "field": "Quality Management",
    "task": "Bug Repair",
    "tags": [
      "Neural Machine Translation formulation",
      "Fault Localization",
      "Patch Generation",
      "Patch Ranking & Validation",
      "Code Representation: Sequence/Tree/Graph",
      "Pre-trained Models for APR",
      "Evaluation: Plausible vs Correct Patches",
      "Multilingual and Multi-hunk Repair",
      "Explainable Patch Generation",
      "Open Science and Reproducibility"
    ],
    "summary": "本文系统综述了基于深度学习的自动程序修复（APR）研究，梳理了常见工作流程与关键组件、数据集与评测指标、实证研究、挑战与未来方向。",
    "quality": "High",
    "conference": "ACM Transactions on Software Engineering and Methodology (TOSEM) 2023",
    "pdf_url": "https://arxiv.org/pdf/2301.03270v3",
    "published": "2023-01-09",
    "update_time": "2023-11-01",
    "download_time": "2025-12-11 16:56:32"
  }
]