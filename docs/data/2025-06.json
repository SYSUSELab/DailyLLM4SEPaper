[
  {
    "id": "2506.04418",
    "title": "Characterizing Multi-Hunk Patches: Divergence, Proximity, and LLM Repair Challenges",
    "abstract": "Multi-hunk bugs, where fixes span disjoint regions of code, are common in practice, yet remain underrepresented in automated repair. Existing techniques and benchmarks pre-dominantly target single-hunk scenarios, overlooking the added complexity of coordinating semantically related changes across the codebase. In this work, we characterize HUNK4J, a dataset of multi-hunk patches derived from 372 real-world defects. We propose hunk divergence, a metric that quantifies the variation among edits in a patch by capturing lexical, structural, and file-level differences, while incorporating the number of hunks involved. We further define spatial proximity, a classification that models how hunks are spatially distributed across the program hierarchy. Our empirical study spanning six LLMs reveals that model success rates decline with increased divergence and spatial dispersion. Notably, when using the LLM alone, no model succeeds in the most dispersed Fragment class. These findings highlight a critical gap in LLM capabilities and motivate divergence-aware repair strategies.",
    "arxiv_url": "https://arxiv.org/abs/2506.04418",
    "authors": [
      "Noor Nashid",
      "Daniel Ding",
      "Keheliya Gallaba",
      "Ahmed E. Hassan",
      "Ali Mesbah"
    ],
    "first_author": "Noor Nashid",
    "category": [
      "Benchmark",
      "Empirical",
      "Technical"
    ],
    "field": "Quality Management",
    "task": "Bug Repair",
    "tags": [
      "Multi-hunk patch characterization",
      "Hunk divergence metric",
      "Spatial proximity classification",
      "Lexical/AST/file-level pairwise distance",
      "Divergence-aware repair analysis",
      "LLM-based program repair evaluation",
      "Context retrieval and scope effects"
    ],
    "summary": "本文提出了衡量多块补丁内部差异的 hunk divergence 指标与表示补丁空间分布的 spatial proximity 分类，并基于372个真实多块缺陷构建基准和评估平台，对六种LLM的多块修复能力进行系统实证，揭示随着散度和分散程度增加模型修复成功率显著下降。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2506.04418v2",
    "published": "2025-06-04",
    "update_time": "2025-11-17",
    "download_time": "2025-12-11 17:03:20"
  },
  {
    "id": "2506.02073",
    "title": "Flow2Code: Evaluating Large Language Models for Flowchart-based Code Generation Capability",
    "abstract": "While large language models (LLMs) show promise in code generation, existing benchmarks neglect the flowchart-based code generation. To promote further research on flowchart-based code generation, this work presents Flow2Code, a novel benchmark for flowchart-based code generation evaluation. The evaluation dataset spans 15 programming languages and includes 5,622 code segments paired with 16,866 flowcharts of three types: code, UML, and pseudocode. Extensive experiments with 13 multimodal LLMs reveal that current LLMs can not generate code based on flowcharts perfectly. Besides, experiment results show that the supervised fine-tuning technique contributes greatly to the models' performance. We publicly release our code and datasets at https://github.com/hml-github/Flow2Code.",
    "arxiv_url": "https://arxiv.org/abs/2506.02073",
    "authors": [
      "Mengliang He",
      "Jiayi Zeng",
      "Yankai Jiang",
      "Wei Zhang",
      "Zeming Liu",
      "Xiaoming Shi",
      "Aimin Zhou"
    ],
    "first_author": "Mengliang He",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Translation",
    "tags": [
      "Flowchart-to-code benchmark",
      "Multimodal code generation",
      "Code/UML/Pseudocode flowcharts",
      "Multilingual coverage (15 languages)",
      "DOT flowchart representation",
      "Human-in-the-loop verification",
      "Zero-shot vs supervised fine-tuning comparison",
      "Evaluation of 13 multimodal models",
      "Flowchart construction and transformation pipeline"
    ],
    "summary": "本文提出了 Flow2Code，一个包含代码、UML 与伪代码三类流程图并覆盖15种编程语言的大规模多模态基准，并在13个多模态模型上评测，发现当前模型在流程图驱动的代码生成上仍存在明显不足且有监督微调能显著提升性能。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2506.02073v1",
    "published": "2025-06-02",
    "update_time": "2025-06-02",
    "download_time": "2025-12-13 00:13:38"
  },
  {
    "id": "2506.11928",
    "title": "LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?",
    "abstract": "Recent reports claim that large language models (LLMs) now outperform elite humans in competitive programming. Drawing on knowledge from a group of medalists in international algorithmic contests, we revisit this claim, examining how LLMs differ from human experts and where limitations still remain. We introduce LiveCodeBench Pro, a benchmark composed of problems from Codeforces, ICPC, and IOI that are continuously updated to reduce the likelihood of data contamination. A team of Olympiad medalists annotates every problem for algorithmic categories and conducts a line-by-line analysis of failed model-generated submissions. Using this new data and benchmark, we find that frontier models still have significant limitations: without external tools, the best model achieves only 53% pass@1 on medium-difficulty problems and 0% on hard problems, domains where expert humans still excel. We also find that LLMs succeed at implementation-heavy problems but struggle with nuanced algorithmic reasoning and complex case analysis, often generating confidently incorrect justifications. High performance appears largely driven by implementation precision and tool augmentation, not superior reasoning. LiveCodeBench Pro thus highlights the significant gap to human grandmaster levels, while offering fine-grained diagnostics to steer future improvements in code-centric LLM reasoning.",
    "arxiv_url": "https://arxiv.org/abs/2506.11928",
    "authors": [
      "Zihan Zheng",
      "Zerui Cheng",
      "Zeyu Shen",
      "Shang Zhou",
      "Kaiyuan Liu",
      "Hansen He",
      "Dongruixuan Li",
      "Stanley Wei",
      "Hangyi Hao",
      "Jianzhu Yao",
      "Peiyao Sheng",
      "Zixuan Wang",
      "Wenhao Chai",
      "Aleksandra Korolova",
      "Peter Henderson",
      "Sanjeev Arora",
      "Pramod Viswanath",
      "Jingbo Shang",
      "Saining Xie"
    ],
    "first_author": "Zihan Zheng",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Competitive Programming Benchmark",
      "Contamination-Resistant Evaluation",
      "Olympiad-Expert Annotations",
      "Cognitive-Focus Taxonomy",
      "Elo-Based Model Rating",
      "Implementation vs Algorithmic Reasoning",
      "Failure Mode Analysis",
      "Tool-Augmentation Effects",
      "Test-Suite Robustness",
      "Pass@1 Evaluation"
    ],
    "summary": "LiveCodeBench Pro 提出一个由奥赛奖牌得主持续更新并注释的竞赛编程基准，通过细粒度技能标签、抗污染策略和逐行失败分析，揭示当代 LLM 在算法推理与复杂案例分析上的显著局限性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2506.11928v1",
    "published": "2025-06-13",
    "update_time": "2025-06-13",
    "download_time": "2025-12-17 18:37:43"
  },
  {
    "id": "2506.12713",
    "title": "Humanity's Last Code Exam: Can Advanced LLMs Conquer Human's Hardest Code Competition?",
    "abstract": "Code generation is a core capability of large language models (LLMs), yet mainstream benchmarks (e.g., APPs and LiveCodeBench) contain questions with medium-level difficulty and pose no challenge to advanced LLMs. To better reflected the advanced reasoning and code generation ability, We introduce Humanity's Last Code Exam (HLCE), comprising 235 most challenging problems from the International Collegiate Programming Contest (ICPC World Finals) and the International Olympiad in Informatics (IOI) spanning 2010 - 2024. As part of HLCE, we design a harmonized online-offline sandbox that guarantees fully reproducible evaluation. Through our comprehensive evaluation, we observe that even the strongest reasoning LLMs: o4-mini(high) and Gemini-2.5 Pro, achieve pass@1 rates of only 15.9% and 11.4%, respectively. Meanwhile, we propose a novel \"self-recognition\" task to measure LLMs' awareness of their own capabilities. Results indicate that LLMs' self-recognition abilities are not proportionally correlated with their code generation performance. Finally, our empirical validation of test-time scaling laws reveals that current advanced LLMs have substantial room for improvement on complex programming tasks. We expect HLCE to become a milestone challenge for code generation and to catalyze advances in high-performance reasoning and human-AI collaborative programming. Our code and dataset are also public available(https://github.com/Humanity-s-Last-Code-Exam/HLCE).",
    "arxiv_url": "https://arxiv.org/abs/2506.12713",
    "authors": [
      "Xiangyang Li",
      "Xiaopeng Li",
      "Kuicai Dong",
      "Quanhu Zhang",
      "Rongju Ruan",
      "Xinyi Dai",
      "Xiaoshuang Liu",
      "Shengchun Xu",
      "Yasheng Wang",
      "Ruiming Tang"
    ],
    "first_author": "Xiangyang Li",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Competitive Programming Benchmark",
      "Interactive Problem Evaluation",
      "ICPC/IOI Problem Curation",
      "Model Self-Assessment",
      "Test-time Scaling Laws",
      "Reproducible Sandbox Evaluation",
      "Pass@1 Performance Analysis",
      "Human-vs-Model Comparison",
      "Algorithmic Reasoning Challenges"
    ],
    "summary": "本文提出HLCE——一个包含2010–2024年IOI与ICPC世界决赛中235道高难度题目的代码生成基准（含交互式题型）并提供可复现的线上-线下评测平台，基于此对12个先进模型进行全面评估、引入模型自我识别任务并验证测试时扩展律，结果显示当前最强模型在该基准上的表现仍然有限。",
    "quality": "High",
    "conference": "EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2506.12713v2",
    "published": "2025-06-15",
    "update_time": "2025-10-18",
    "download_time": "2025-12-17 18:38:19"
  },
  {
    "id": "2506.16395",
    "title": "OJBench: A Competition Level Code Benchmark For Large Language Models",
    "abstract": "Recent advancements in large language models (LLMs) have demonstrated significant progress in math and code reasoning capabilities. However, existing code benchmark are limited in their ability to evaluate the full spectrum of these capabilities, particularly at the competitive level. To bridge this gap, we introduce OJBench, a novel and challenging benchmark designed to assess the competitive-level code reasoning abilities of LLMs. OJBench comprises 232 programming competition problems from NOI and ICPC, providing a more rigorous test of models' reasoning skills. We conducted a comprehensive evaluation using OJBench on 37 models, including both closed-source and open-source models, reasoning-oriented and non-reasoning-oriented models. Our results indicate that even state-of-the-art reasoning-oriented models, such as o4-mini and Gemini-2.5-pro-exp, struggle with highly challenging competition-level problems. This highlights the significant challenges that models face in competitive-level code reasoning.",
    "arxiv_url": "https://arxiv.org/abs/2506.16395",
    "authors": [
      "Zhexu Wang",
      "Yiping Liu",
      "Yejie Wang",
      "Wenyang He",
      "Bofei Gao",
      "Muxi Diao",
      "Yanxu Chen",
      "Kelin Fu",
      "Flood Sung",
      "Zhilin Yang",
      "Tianyu Liu",
      "Weiran Xu"
    ],
    "first_author": "Zhexu Wang",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Competition-level algorithmic problems",
      "Contest-sourced problem collection",
      "Testcase-based judging harness",
      "Difficulty calibration from contest metrics",
      "Dual-language evaluation (CPP vs Python)",
      "Execution-feedback iterative correction",
      "Cross-model comparative analysis",
      "Open-vs-closed-source performance gap",
      "Long-chain-of-thought reasoning assessment"
    ],
    "summary": "本文提出OJBench——一个包含232道NOI与ICPC竞赛题的竞赛级代码基准，并在37个模型上评测，揭示当前即使是顶尖推理型模型在高难度竞赛题上仍存在显著不足、CPP通常优于Python且执行反馈能提升模型表现的事实。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2506.16395v1",
    "published": "2025-06-19",
    "update_time": "2025-06-19",
    "download_time": "2025-12-17 18:38:51"
  },
  {
    "id": "2506.04894",
    "title": "ICPC-Eval: Probing the Frontiers of LLM Reasoning with Competitive Programming Contests",
    "abstract": "With the significant progress of large reasoning models in complex coding and reasoning tasks, existing benchmarks, like LiveCodeBench and CodeElo, are insufficient to evaluate the coding capabilities of large language models (LLMs) in real competition environments. Moreover, current evaluation metrics such as Pass@K fail to capture the reflective abilities of reasoning models. To address these challenges, we propose \\textbf{ICPC-Eval}, a top-level competitive coding benchmark designed to probing the frontiers of LLM reasoning. ICPC-Eval includes 118 carefully curated problems from 11 recent ICPC contests held in various regions of the world, offering three key contributions: 1) A challenging realistic ICPC competition scenario, featuring a problem type and difficulty distribution consistent with actual contests. 2) A robust test case generation method and a corresponding local evaluation toolkit, enabling efficient and accurate local evaluation. 3) An effective test-time scaling evaluation metric, Refine@K, which allows iterative repair of solutions based on execution feedback. The results underscore the significant challenge in evaluating complex reasoning abilities: top-tier reasoning models like DeepSeek-R1 often rely on multi-turn code feedback to fully unlock their in-context reasoning potential when compared to non-reasoning counterparts. Furthermore, despite recent advancements in code generation, these models still lag behind top-performing human teams. We release the benchmark at: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs",
    "arxiv_url": "https://arxiv.org/abs/2506.04894",
    "authors": [
      "Shiyi Xu",
      "Yiwen Hu",
      "Yingqian Min",
      "Zhipeng Chen",
      "Wayne Xin Zhao",
      "Ji-Rong Wen"
    ],
    "first_author": "Shiyi Xu",
    "category": [
      "Benchmark",
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Competitive Programming",
      "LLM Test-Case Synthesis",
      "Local Evaluation Toolkit",
      "Special Judge Support",
      "Refine@K",
      "Iterative Refinement",
      "Execution Feedback",
      "Test-time Scaling",
      "Difficulty Calibration",
      "Zero False Positives"
    ],
    "summary": "ICPC-Eval提出了一个包含118道ICPC高难度题目的基准，利用LLM生成并验证本地测试用例并引入Refine@K度量以评估基于执行反馈的迭代修正能力。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2506.04894v1",
    "published": "2025-06-05",
    "update_time": "2025-06-05",
    "download_time": "2025-12-17 19:04:10"
  }
]