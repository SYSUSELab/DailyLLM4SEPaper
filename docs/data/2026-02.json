[
  {
    "id": "2602.03462",
    "title": "RAL-Bench: Benchmarking for Application-Level Functional Correctness and Non-Functional Quality Attributes",
    "abstract": "Code generation has advanced rapidly with code-focused large language models (LLMs), especially on snippet-level tasks. However, application-level generation requires producing a runnable multi-file repository with correct structure, dependencies, and end-to-end executability, and real-world software must satisfy both functional correctness and non-functional quality (e.g., maintainability, security). Existing benchmarks provide a limited execution-based assessment of these requirements at the application level. We ask: Can current LLMs generate application-level repositories that meet both functional and non-functional criteria? We propose RAL-Bench, a benchmark and evaluation framework for application-level code generation. For each task, we distill a concise natural-language requirement from a high-quality reference project, build black-box system tests covering functional and non-functional attributes, and keep only tests that pass on the reference repository to ensure a sound oracle and an end-to-end executable suite. Functional correctness is measured by system-test pass rate. Non-functional quality is measured along five ISO/IEC 25010-inspired dimensions and aggregated with an Analytic Hierarchy Process (AHP)-derived weight vector, with per-dimension diagnostics and baseline-normalized scoring using reference measurements. Across 16 LLMs evaluated zero-shot with greedy decoding, functional correctness is the dominant bottleneck: no model exceeds a 45% functional pass rate under our requirement-driven, reference-validated tests. We release RAL-Bench at https://github.com/Wwstarry/RAL-Bench. .",
    "arxiv_url": "https://arxiv.org/abs/2602.03462",
    "authors": [
      "Ruwei Pan",
      "Yakun Zhang",
      "Qingyuan Liang",
      "Yueheng Zhu",
      "Chao Liu",
      "Lu Zhang",
      "Hongyu Zhang"
    ],
    "first_author": "Ruwei Pan",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Application-Level Code Generation",
    "tags": [
      "Application-Level Generation",
      "System Testing",
      "Non-functional Quality",
      "ISO25010",
      "AHP Weighting",
      "Requirement Distillation",
      "Reference-validated Tests",
      "Baseline-normalized Scoring",
      "End-to-End Executability",
      "Zero-shot Evaluation"
    ],
    "summary": "本文提出 RAL-Bench，一个用于评估 LLM 从自然语言需求生成可运行应用仓库的基准框架，通过对参考实现验证的黑盒系统测试同时度量功能正确性与基于 ISO/IEC 25010 的多维非功能质量（并用 AHP 加权聚合），并在零样本条件下评估了 16 款 LLM，发现功能正确性是主要瓶颈。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.03462v1",
    "published": "2026-02-03",
    "update_time": "2026-02-03",
    "download_time": "2026-02-04 02:21:26"
  },
  {
    "id": "2602.03419",
    "title": "SWE-World: Building Software Engineering Agents in Docker-Free Environments",
    "abstract": "Recent advances in large language models (LLMs) have enabled software engineering agents to tackle complex code modification tasks. Most existing approaches rely on execution feedback from containerized environments, which require dependency-complete setup and physical execution of programs and tests. While effective, this paradigm is resource-intensive and difficult to maintain, substantially complicating agent training and limiting scalability. We propose SWE-World, a Docker-free framework that replaces physical execution environments with a learned surrogate for training and evaluating software engineering agents. SWE-World leverages LLM-based models trained on real agent-environment interaction data to predict intermediate execution outcomes and final test feedback, enabling agents to learn without interacting with physical containerized environments. This design preserves the standard agent-environment interaction loop while eliminating the need for costly environment construction and maintenance during agent optimization and evaluation. Furthermore, because SWE-World can simulate the final evaluation outcomes of candidate trajectories without real submission, it enables selecting the best solution among multiple test-time attempts, thereby facilitating effective test-time scaling (TTS) in software engineering tasks. Experiments on SWE-bench Verified demonstrate that SWE-World raises Qwen2.5-Coder-32B from 6.2\\% to 52.0\\% via Docker-free SFT, 55.0\\% with Docker-free RL, and 68.2\\% with further TTS. The code is available at https://github.com/RUCAIBox/SWE-World",
    "arxiv_url": "https://arxiv.org/abs/2602.03419",
    "authors": [
      "Shuang Sun",
      "Huatong Song",
      "Lisheng Huang",
      "Jinhao Jiang",
      "Ran Le",
      "Zhihao Lv",
      "Zongchao Chen",
      "Yiwen Hu",
      "Wenyang Luo",
      "Wayne Xin Zhao",
      "Yang Song",
      "Hongteng Xu",
      "Tao Zhang",
      "Ji-Rong Wen"
    ],
    "first_author": "Shuang Sun",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Code Simulation",
      "Code Agents",
      "Surrogate Execution",
      "World Modeling",
      "Docker-Free Training",
      "Test-Time Scaling",
      "Transition Model",
      "Reward Modeling",
      "Repository-Level Coding",
      "Agentic Interaction"
    ],
    "summary": "本文提出SWE-World：用训练得到的LLM作为环境的替代（步骤级转移模型与终端奖励模型），以在无Docker容器的条件下训练和评估软件工程代理，从而显著降低资源开销并在SFT、RL与测试时扩展中提升修复性能。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.03419v1",
    "published": "2026-02-03",
    "update_time": "2026-02-03",
    "download_time": "2026-02-04 02:21:44"
  },
  {
    "id": "2602.02475",
    "title": "AgentRx: Diagnosing AI Agent Failures from Execution Trajectories",
    "abstract": "AI agents often fail in ways that are difficult to localize because executions are probabilistic, long-horizon, multi-agent, and mediated by noisy tool outputs. We address this gap by manually annotating failed agent runs and release a novel benchmark of 115 failed trajectories spanning structured API workflows, incident management, and open-ended web/file tasks. Each trajectory is annotated with a critical failure step and a category from a grounded-theory derived, cross-domain failure taxonomy. To mitigate the human cost of failure attribution, we present AGENTRX, an automated domain-agnostic diagnostic framework that pinpoints the critical failure step in a failed agent trajectory. It synthesizes constraints, evaluates them step-by-step, and produces an auditable validation log of constraint violations with associated evidence; an LLM-based judge uses this log to localize the critical step and category. Our framework improves step localization and failure attribution over existing baselines across three domains.",
    "arxiv_url": "https://arxiv.org/abs/2602.02475",
    "authors": [
      "Shraddha Barke",
      "Arnav Goyal",
      "Alind Khare",
      "Avaljot Singh",
      "Suman Nath",
      "Chetan Bansal"
    ],
    "first_author": "Shraddha Barke",
    "category": [
      "Benchmark",
      "Technical"
    ],
    "field": "AIOps",
    "task": "Agent Failure Localization",
    "tags": [
      "Failure Taxonomy",
      "Execution Trajectories",
      "Constraint Synthesis",
      "Validation Log",
      "LLM Adjudication",
      "Critical Failure Localization",
      "Grounded Theory Annotation",
      "Multi-Agent Diagnostics",
      "Tool Invocation Validation"
    ],
    "summary": "该论文发布了包含115个失败代理执行轨迹的基准与跨域失败分类法，并提出AGENTRX框架通过从工具模式与策略合成约束、逐步验证并由基于LLM的裁判利用可审计的违约日志来定位和归因关键失败步骤。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.02475v1",
    "published": "2026-02-02",
    "update_time": "2026-02-02",
    "download_time": "2026-02-04 02:22:32"
  },
  {
    "id": "2602.02419",
    "title": "SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration",
    "abstract": "Graphical User Interface (GUI) grounding aims to translate natural language instructions into executable screen coordinates, enabling automated GUI interaction. Nevertheless, incorrect grounding can result in costly, hard-to-reverse actions (e.g., erroneous payment approvals), raising concerns about model reliability. In this paper, we introduce SafeGround, an uncertainty-aware framework for GUI grounding models that enables risk-aware predictions through calibrations before testing. SafeGround leverages a distribution-aware uncertainty quantification method to capture the spatial dispersion of stochastic samples from outputs of any given model. Then, through the calibration process, SafeGround derives a test-time decision threshold with statistically guaranteed false discovery rate (FDR) control. We apply SafeGround on multiple GUI grounding models for the challenging ScreenSpot-Pro benchmark. Experimental results show that our uncertainty measure consistently outperforms existing baselines in distinguishing correct from incorrect predictions, while the calibrated threshold reliably enables rigorous risk control and potentials of substantial system-level accuracy improvements. Across multiple GUI grounding models, SafeGround improves system-level accuracy by up to 5.38\\% percentage points over Gemini-only inference.",
    "arxiv_url": "https://arxiv.org/abs/2602.02419",
    "authors": [
      "Qingni Wang",
      "Yue Fan",
      "Xin Eric Wang"
    ],
    "first_author": "Qingni Wang",
    "category": [
      "Technical"
    ],
    "field": "GUI Interaction",
    "task": "GUI Grounding",
    "tags": [
      "Uncertainty Calibration",
      "Selective Prediction",
      "False Discovery Rate Control",
      "Distribution-aware Uncertainty",
      "Cascaded Inference",
      "Spatial Uncertainty",
      "Black-box Models"
    ],
    "summary": "SAFEGROUND 提出了一种无需访问模型内部、基于随机采样的空间分布不确定性量化与学习-然后-测试校准方法，以在 GUI 定位任务中实现可控错误率的选择性预测与级联推理，从而提升系统级准确性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.02419v1",
    "published": "2026-02-02",
    "update_time": "2026-02-02",
    "download_time": "2026-02-04 02:23:45"
  },
  {
    "id": "2602.04640",
    "title": "Towards Structured, State-Aware, and Execution-Grounded Reasoning for Software Engineering Agents",
    "abstract": "Software Engineering (SE) agents have shown promising abilities in supporting various SE tasks. Current SE agents remain fundamentally reactive, making decisions mainly based on conversation history and the most recent response. However, this reactive design provides no explicit structure or persistent state within the agent's memory, making long-horizon reasoning challenging. As a result, SE agents struggle to maintain a coherent understanding across reasoning steps, adapt their hypotheses as new evidence emerges, or incorporate execution feedback into the mental reasoning model of the system state.   In this position paper, we argue that, to further advance SE agents, we need to move beyond reactive behavior toward a structured, state-aware, and execution-grounded reasoning. We outline how explicit structure, persistent and evolving state, and the integration of execution-grounded feedback can help SE agents perform more coherent and reliable reasoning in long-horizon tasks. We also provide an initial roadmap for developing next-generation SE agents that can more effectively perform real-world tasks.",
    "arxiv_url": "https://arxiv.org/abs/2602.04640",
    "authors": [
      "Tse-Hsun",
      "Chen"
    ],
    "first_author": "Tse-Hsun",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Stateful Reasoning",
      "Structured Memory",
      "Execution-Grounded Feedback",
      "Hypothesis Tracking",
      "Pre/Post Conditions",
      "Finite-State Abstraction",
      "Long-Horizon Reasoning",
      "Tool Integration"
    ],
    "summary": "本文主张将软件工程代理从基于对话的被动反应式设计，转向具有显式结构、持久可演化状态和执行反馈驱动的推理机制，并提出实现该目标的初步路线图。",
    "quality": "Middle",
    "conference": "BoatSE",
    "pdf_url": "https://arxiv.org/pdf/2602.04640v1",
    "published": "2026-02-04",
    "update_time": "2026-02-04",
    "download_time": "2026-02-05 02:22:50"
  },
  {
    "id": "2602.04449",
    "title": "What's in a Benchmark? The Case of SWE-Bench in Automated Program Repair",
    "abstract": "The rapid progress in Automated Program Repair (APR) has been fueled by advances in AI, particularly large language models (LLMs) and agent-based systems. SWE-Bench is a benchmark designed to evaluate repair systems using real issues mined from popular open-source Python repositories. Its public leaderboards-SWE-Bench Lite and Verified-have become central platforms for tracking progress and comparing solutions. In this paper, we present the first comprehensive study of these two leaderboards, examining who is submitting solutions, the products behind the submissions, the LLMs employed, and the openness of the approaches. We analyze 79 entries submitted to Lite leaderboard and 133 to Verified. Our results show that most entries on both leaderboards originate from industry, particularly small companies and large publicly traded companies. These submissions often achieve top results, although academic contributions-typically open source-also remain competitive. We also find a clear dominance of proprietary LLMs, especially Claude family, with state-of-the-art results on both leaderboards currently achieved by Claude 4 Sonnet. These findings offer insights into the SWE-Bench ecosystem that can guide greater transparency and diversity in future benchmark-driven research.",
    "arxiv_url": "https://arxiv.org/abs/2602.04449",
    "authors": [
      "Matias Martinez",
      "Xavier Franch"
    ],
    "first_author": "Matias Martinez",
    "category": [
      "Empirical"
    ],
    "field": "Quality Management",
    "task": "Bug Repair",
    "tags": [
      "Program Repair",
      "Benchmark Analysis",
      "Leaderboard Dynamics",
      "Submitter Profiling",
      "LLM Adoption",
      "Proprietary Model Dominance",
      "Open-source Availability",
      "Patch Correctness"
    ],
    "summary": "本文对SWE-Bench Lite和Verified排行榜上的79和133条提交进行系统性实证分析，揭示了提交者构成、产品可用性、开源程度与所用大型语言模型（以专有模型占优）对自动程序修复结果的影响，并讨论了基准的透明性与演化问题。",
    "quality": "High",
    "conference": "IEEE/ACM 48th International Conference on Software Engineering (ICSE)",
    "pdf_url": "https://arxiv.org/pdf/2602.04449v1",
    "published": "2026-02-04",
    "update_time": "2026-02-04",
    "download_time": "2026-02-05 02:23:11"
  },
  {
    "id": "2602.04572",
    "title": "From Competition to Collaboration: Designing Sustainable Mechanisms Between LLMs and Online Forums",
    "abstract": "While Generative AI (GenAI) systems draw users away from (Q&A) forums, they also depend on the very data those forums produce to improve their performance. Addressing this paradox, we propose a framework of sequential interaction, in which a GenAI system proposes questions to a forum that can publish some of them. Our framework captures several intricacies of such a collaboration, including non-monetary exchanges, asymmetric information, and incentive misalignment. We bring the framework to life through comprehensive, data-driven simulations using real Stack Exchange data and commonly used LLMs. We demonstrate the incentive misalignment empirically, yet show that players can achieve roughly half of the utility in an ideal full-information scenario. Our results highlight the potential for sustainable collaboration that preserves effective knowledge sharing between AI systems and human knowledge platforms.",
    "arxiv_url": "https://arxiv.org/abs/2602.04572",
    "authors": [
      "Niv Fono",
      "Yftah Ziser",
      "Omer Ben-Porat"
    ],
    "first_author": "Niv Fono",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Requirements & Design",
    "task": "Management",
    "tags": [
      "Human-LLM Interaction",
      "Incentive Misalignment",
      "Game-Theoretic Modeling",
      "Acceptance-Aware Mechanism",
      "Asymmetric Information",
      "Non-Monetary Exchange",
      "Data-Driven Simulation",
      "Forum Sustainability"
    ],
    "summary": "本文提出了一个博弈论框架，并通过基于真实Stack Exchange数据与开源LLM的大规模仿真实验，设计并验证了在非货币交换与信息不对称条件下，GenAI与问答社区之间能够部分恢复双方效用的可持续协作机制。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.04572v1",
    "published": "2026-02-04",
    "update_time": "2026-02-04",
    "download_time": "2026-02-05 02:24:42"
  },
  {
    "id": "2602.04466",
    "title": "Is Micro Domain-Adaptive Pre-Training Effective for Real-World Operations? Multi-Step Evaluation Reveals Potential and Bottlenecks",
    "abstract": "When applying LLMs to real-world enterprise operations, LLMs need to handle proprietary knowledge in small domains of specific operations ($\\textbf{micro domains}$). A previous study shows micro domain-adaptive pre-training ($\\textbf{mDAPT}$) with fewer documents is effective, similarly to DAPT in larger domains. However, it evaluates mDAPT only on multiple-choice questions; thus, its effectiveness for generative tasks in real-world operations remains unknown. We aim to reveal the potential and bottlenecks of mDAPT for generative tasks. To this end, we disentangle the answering process into three subtasks and evaluate the performance of each subtask: (1) $\\textbf{eliciting}$ facts relevant to questions from an LLM's own knowledge, (2) $\\textbf{reasoning}$ over the facts to obtain conclusions, and (3) $\\textbf{composing}$ long-form answers based on the conclusions. We verified mDAPT on proprietary IT product knowledge for real-world questions in IT technical support operations. As a result, mDAPT resolved the elicitation task that the base model struggled with but did not resolve other subtasks. This clarifies mDAPT's effectiveness in the knowledge aspect and its bottlenecks in other aspects. Further analysis empirically shows that resolving the elicitation and reasoning tasks ensures sufficient performance (over 90%), emphasizing the need to enhance reasoning capability.",
    "arxiv_url": "https://arxiv.org/abs/2602.04466",
    "authors": [
      "Masaya Tsunokake",
      "Yuta Koreeda",
      "Terufumi Morishita",
      "Koichi Nagatsuka",
      "Hikaru Tomonari",
      "Yasuhiro Sogawa"
    ],
    "first_author": "Masaya Tsunokake",
    "category": [
      "Empirical",
      "Technical"
    ],
    "field": "Domain Adaptation & Knowledge Grounding",
    "task": "Micro Domain-Adaptive Pre-Training Evaluation for Generative QA",
    "tags": [
      "Micro-domain Adaptation",
      "Multi-step Oracle Evaluation",
      "Knowledge Elicitation",
      "Knowledge Memorization",
      "Reasoning Bottleneck",
      "Answer Composition",
      "Enterprise IT QA"
    ],
    "summary": "本文通过提出多步oracle评估与知识评估方法，实证研究在企业微域（micro-domain）上进行领域自适应预训练（mDAPT）对生成式技术支持问答的效果，发现mDAPT能显著提升知识提取能力但在推理与长文生成上仍存瓶颈。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.04466v1",
    "published": "2026-02-04",
    "update_time": "2026-02-04",
    "download_time": "2026-02-05 02:25:42"
  },
  {
    "id": "2602.05762",
    "title": "RocqSmith: Can Automatic Optimization Forge Better Proof Agents?",
    "abstract": "This work studies the applicability of automatic AI agent optimization methods to real-world agents in formal verification settings, focusing on automated theorem proving in Rocq as a representative and challenging domain. We evaluate how different automatic agent optimizers perform when applied to the task of optimizing a Rocq proof-generation agent, and assess whether parts of the fine-grained tuning of agentic systems, such as prompt design, contextual knowledge, and control strategies, can be automated. Our results show that while several optimizers yield measurable improvements, simple few-shot bootstrapping is the most consistently effective; however, none of the studied methods matches the performance of a carefully engineered state-of-the-art proof agent.",
    "arxiv_url": "https://arxiv.org/abs/2602.05762",
    "authors": [
      "Andrei Kozyrev",
      "Nikita Khramov",
      "Denis Lochmelis",
      "Valerio Morelli",
      "Gleb Solovev",
      "Anton Podkopaev"
    ],
    "first_author": "Andrei Kozyrev",
    "category": [
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Program Proof",
      "Proof Agents",
      "Agent Optimization",
      "Prompt Optimization",
      "Few-shot Bootstrapping",
      "Contextual Knowledge Retrieval",
      "Control Flow Optimization",
      "Optimization Efficiency"
    ],
    "summary": "本文在Rocq自动定理证明场景中系统评估了多种自动化智能体优化方法，发现简单的few-shot自举最为稳定但仍未能超越精心设计的手工证明智能体。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.05762v1",
    "published": "2026-02-05",
    "update_time": "2026-02-05",
    "download_time": "2026-02-06 02:22:52"
  },
  {
    "id": "2602.05721",
    "title": "A Dual-Loop Agent Framework for Automated Vulnerability Reproduction",
    "abstract": "Automated vulnerability reproduction from CVE descriptions requires generating executable Proof-of-Concept (PoC) exploits and validating them in target environments. This process is critical in software security research and practice, yet remains time-consuming and demands specialized expertise when performed manually. While LLM agents show promise for automating this task, existing approaches often conflate exploring attack directions with fixing implementation details, which leads to unproductive debugging loops when reproduction fails. To address this, we propose Cve2PoC, an LLM-based dual-loop agent framework following a plan-execute-evaluate paradigm. The Strategic Planner analyzes vulnerability semantics and target code to produce structured attack plans. The Tactical Executor generates PoC code and validates it through progressive verification. The Adaptive Refiner evaluates execution results and routes failures to different loops: the \\textit{Tactical Loop} for code-level refinement, while the \\textit{Strategic Loop} for attack strategy replanning. This dual-loop design enables the framework to escape ineffective debugging by matching remediation to failure type. Evaluation on two benchmarks covering 617 real-world vulnerabilities demonstrates that Cve2PoC achieves 82.9\\% and 54.3\\% reproduction success rates on SecBench.js and PatchEval, respectively, outperforming the best baseline by 11.3\\% and 20.4\\%. Human evaluation confirms that generated PoCs achieve comparable code quality to human-written exploits in readability and reusability.",
    "arxiv_url": "https://arxiv.org/abs/2602.05721",
    "authors": [
      "Bin Liu",
      "Yanjie Zhao",
      "Zhenpeng Chen",
      "Guoai Xu",
      "Haoyu Wang"
    ],
    "first_author": "Bin Liu",
    "category": [
      "Technical"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Reproduction",
    "tags": [
      "Code Agents",
      "Security and Vulnerabilities",
      "PoC Generation",
      "Dual-Loop Agents",
      "Strategic Planning",
      "Tactical Execution",
      "Progressive Verification",
      "Adaptive Refinement",
      "Differential Testing",
      "Experience Retrieval"
    ],
    "summary": "本文提出Cve2PoC——一种将战略规划与战术执行分离的基于LLM的双环代理框架，通过结构化攻击计划、渐进式多层验证与自适应精炼，实现从CVE描述自动生成并验证可执行PoC，显著提升漏洞复现成功率。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.05721v1",
    "published": "2026-02-05",
    "update_time": "2026-02-05",
    "download_time": "2026-02-06 02:23:26"
  },
  {
    "id": "2602.04837",
    "title": "Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing",
    "abstract": "Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods.",
    "arxiv_url": "https://arxiv.org/abs/2602.04837",
    "authors": [
      "Zhaotian Weng",
      "Antonis Antoniades",
      "Deepak Nathani",
      "Zhen Zhang",
      "Xiao Pu",
      "Xin Eric Wang"
    ],
    "first_author": "Zhaotian Weng",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Group Evolution",
      "Experience Sharing",
      "Open-Ended Self-Improvement",
      "Meta-Learning",
      "Performance-Novelty Selection",
      "Diversity Consolidation",
      "Agent Robustness",
      "Transferability"
    ],
    "summary": "本文提出Group-Evolving Agents（GEA），通过将群体作为进化单元并在群体内显式共享与复用经验，实现开放式自我改进，在多个编码基准上显著优于现有自我进化方法并展现更强的鲁棒性与可迁移性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.04837v1",
    "published": "2026-02-04",
    "update_time": "2026-02-04",
    "download_time": "2026-02-06 02:24:45"
  },
  {
    "id": "2602.04811",
    "title": "SE-Bench: Benchmarking Self-Evolution with Knowledge Internalization",
    "abstract": "True self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hindered by two obstacles: the entanglement of prior knowledge, where ``new'' knowledge may appear in pre-training data, and the entanglement of reasoning complexity, where failures may stem from problem difficulty rather than an inability to recall learned knowledge. We introduce SE-Bench, a diagnostic environment that obfuscates the NumPy library and its API doc into a pseudo-novel package with randomized identifiers. Agents are trained to internalize this package and evaluated on simple coding tasks without access to documentation, yielding a clean setting where tasks are trivial with the new API doc but impossible for base models without it. Our investigation reveals three insights: (1) the Open-Book Paradox, where training with reference documentation inhibits retention, requiring \"Closed-Book Training\" to force knowledge compression into weights; (2) the RL Gap, where standard RL fails to internalize new knowledge completely due to PPO clipping and negative gradients; and (3) the viability of Self-Play for internalization, proving models can learn from self-generated, noisy tasks when coupled with SFT, but not RL. Overall, SE-Bench establishes a rigorous diagnostic platform for self-evolution with knowledge internalization. Our code and dataset can be found at https://github.com/thunlp/SE-Bench.",
    "arxiv_url": "https://arxiv.org/abs/2602.04811",
    "authors": [
      "Jiarui Yuan",
      "Tailin Jin",
      "Weize Chen",
      "Zeyuan Liu",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "first_author": "Jiarui Yuan",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Instruction-Tuning",
    "tags": [
      "Knowledge Internalization",
      "API Obfuscation",
      "Closed-Book Training",
      "Open-Book Paradox",
      "Self-Play Learning",
      "RL Gap",
      "PPO Clipping",
      "Compositional Generalization",
      "Diagnostic Benchmark",
      "Consensus Filtering"
    ],
    "summary": "该论文提出SE-BENCH，通过将NumPy函数和文档混淆为伪新库并构建可训练与测试题集，提供一个“必须内化才能解题”的诊断基准，用以评估和分析SFT、RL与自我博弈在将新API知识压缩入模型权重中的效果，并揭示了开卷悖论、RL内化缺陷与基于SFT的自我博弈可行性等重要发现。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.04811v1",
    "published": "2026-02-04",
    "update_time": "2026-02-04",
    "download_time": "2026-02-06 02:25:38"
  }
]