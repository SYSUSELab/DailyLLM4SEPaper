[
  {
    "id": "2602.03462",
    "title": "RAL-Bench: Benchmarking for Application-Level Functional Correctness and Non-Functional Quality Attributes",
    "abstract": "Code generation has advanced rapidly with code-focused large language models (LLMs), especially on snippet-level tasks. However, application-level generation requires producing a runnable multi-file repository with correct structure, dependencies, and end-to-end executability, and real-world software must satisfy both functional correctness and non-functional quality (e.g., maintainability, security). Existing benchmarks provide a limited execution-based assessment of these requirements at the application level. We ask: Can current LLMs generate application-level repositories that meet both functional and non-functional criteria? We propose RAL-Bench, a benchmark and evaluation framework for application-level code generation. For each task, we distill a concise natural-language requirement from a high-quality reference project, build black-box system tests covering functional and non-functional attributes, and keep only tests that pass on the reference repository to ensure a sound oracle and an end-to-end executable suite. Functional correctness is measured by system-test pass rate. Non-functional quality is measured along five ISO/IEC 25010-inspired dimensions and aggregated with an Analytic Hierarchy Process (AHP)-derived weight vector, with per-dimension diagnostics and baseline-normalized scoring using reference measurements. Across 16 LLMs evaluated zero-shot with greedy decoding, functional correctness is the dominant bottleneck: no model exceeds a 45% functional pass rate under our requirement-driven, reference-validated tests. We release RAL-Bench at https://github.com/Wwstarry/RAL-Bench. .",
    "arxiv_url": "https://arxiv.org/abs/2602.03462",
    "authors": [
      "Ruwei Pan",
      "Yakun Zhang",
      "Qingyuan Liang",
      "Yueheng Zhu",
      "Chao Liu",
      "Lu Zhang",
      "Hongyu Zhang"
    ],
    "first_author": "Ruwei Pan",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Application-Level Code Generation",
    "tags": [
      "Application-Level Generation",
      "System Testing",
      "Non-functional Quality",
      "ISO25010",
      "AHP Weighting",
      "Requirement Distillation",
      "Reference-validated Tests",
      "Baseline-normalized Scoring",
      "End-to-End Executability",
      "Zero-shot Evaluation"
    ],
    "summary": "本文提出 RAL-Bench，一个用于评估 LLM 从自然语言需求生成可运行应用仓库的基准框架，通过对参考实现验证的黑盒系统测试同时度量功能正确性与基于 ISO/IEC 25010 的多维非功能质量（并用 AHP 加权聚合），并在零样本条件下评估了 16 款 LLM，发现功能正确性是主要瓶颈。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.03462v1",
    "published": "2026-02-03",
    "update_time": "2026-02-03",
    "download_time": "2026-02-04 02:21:26"
  },
  {
    "id": "2602.03419",
    "title": "SWE-World: Building Software Engineering Agents in Docker-Free Environments",
    "abstract": "Recent advances in large language models (LLMs) have enabled software engineering agents to tackle complex code modification tasks. Most existing approaches rely on execution feedback from containerized environments, which require dependency-complete setup and physical execution of programs and tests. While effective, this paradigm is resource-intensive and difficult to maintain, substantially complicating agent training and limiting scalability. We propose SWE-World, a Docker-free framework that replaces physical execution environments with a learned surrogate for training and evaluating software engineering agents. SWE-World leverages LLM-based models trained on real agent-environment interaction data to predict intermediate execution outcomes and final test feedback, enabling agents to learn without interacting with physical containerized environments. This design preserves the standard agent-environment interaction loop while eliminating the need for costly environment construction and maintenance during agent optimization and evaluation. Furthermore, because SWE-World can simulate the final evaluation outcomes of candidate trajectories without real submission, it enables selecting the best solution among multiple test-time attempts, thereby facilitating effective test-time scaling (TTS) in software engineering tasks. Experiments on SWE-bench Verified demonstrate that SWE-World raises Qwen2.5-Coder-32B from 6.2\\% to 52.0\\% via Docker-free SFT, 55.0\\% with Docker-free RL, and 68.2\\% with further TTS. The code is available at https://github.com/RUCAIBox/SWE-World",
    "arxiv_url": "https://arxiv.org/abs/2602.03419",
    "authors": [
      "Shuang Sun",
      "Huatong Song",
      "Lisheng Huang",
      "Jinhao Jiang",
      "Ran Le",
      "Zhihao Lv",
      "Zongchao Chen",
      "Yiwen Hu",
      "Wenyang Luo",
      "Wayne Xin Zhao",
      "Yang Song",
      "Hongteng Xu",
      "Tao Zhang",
      "Ji-Rong Wen"
    ],
    "first_author": "Shuang Sun",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Code Simulation",
      "Code Agents",
      "Surrogate Execution",
      "World Modeling",
      "Docker-Free Training",
      "Test-Time Scaling",
      "Transition Model",
      "Reward Modeling",
      "Repository-Level Coding",
      "Agentic Interaction"
    ],
    "summary": "本文提出SWE-World：用训练得到的LLM作为环境的替代（步骤级转移模型与终端奖励模型），以在无Docker容器的条件下训练和评估软件工程代理，从而显著降低资源开销并在SFT、RL与测试时扩展中提升修复性能。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.03419v1",
    "published": "2026-02-03",
    "update_time": "2026-02-03",
    "download_time": "2026-02-04 02:21:44"
  },
  {
    "id": "2602.02475",
    "title": "AgentRx: Diagnosing AI Agent Failures from Execution Trajectories",
    "abstract": "AI agents often fail in ways that are difficult to localize because executions are probabilistic, long-horizon, multi-agent, and mediated by noisy tool outputs. We address this gap by manually annotating failed agent runs and release a novel benchmark of 115 failed trajectories spanning structured API workflows, incident management, and open-ended web/file tasks. Each trajectory is annotated with a critical failure step and a category from a grounded-theory derived, cross-domain failure taxonomy. To mitigate the human cost of failure attribution, we present AGENTRX, an automated domain-agnostic diagnostic framework that pinpoints the critical failure step in a failed agent trajectory. It synthesizes constraints, evaluates them step-by-step, and produces an auditable validation log of constraint violations with associated evidence; an LLM-based judge uses this log to localize the critical step and category. Our framework improves step localization and failure attribution over existing baselines across three domains.",
    "arxiv_url": "https://arxiv.org/abs/2602.02475",
    "authors": [
      "Shraddha Barke",
      "Arnav Goyal",
      "Alind Khare",
      "Avaljot Singh",
      "Suman Nath",
      "Chetan Bansal"
    ],
    "first_author": "Shraddha Barke",
    "category": [
      "Benchmark",
      "Technical"
    ],
    "field": "AIOps",
    "task": "Agent Failure Localization",
    "tags": [
      "Failure Taxonomy",
      "Execution Trajectories",
      "Constraint Synthesis",
      "Validation Log",
      "LLM Adjudication",
      "Critical Failure Localization",
      "Grounded Theory Annotation",
      "Multi-Agent Diagnostics",
      "Tool Invocation Validation"
    ],
    "summary": "该论文发布了包含115个失败代理执行轨迹的基准与跨域失败分类法，并提出AGENTRX框架通过从工具模式与策略合成约束、逐步验证并由基于LLM的裁判利用可审计的违约日志来定位和归因关键失败步骤。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.02475v1",
    "published": "2026-02-02",
    "update_time": "2026-02-02",
    "download_time": "2026-02-04 02:22:32"
  },
  {
    "id": "2602.02419",
    "title": "SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration",
    "abstract": "Graphical User Interface (GUI) grounding aims to translate natural language instructions into executable screen coordinates, enabling automated GUI interaction. Nevertheless, incorrect grounding can result in costly, hard-to-reverse actions (e.g., erroneous payment approvals), raising concerns about model reliability. In this paper, we introduce SafeGround, an uncertainty-aware framework for GUI grounding models that enables risk-aware predictions through calibrations before testing. SafeGround leverages a distribution-aware uncertainty quantification method to capture the spatial dispersion of stochastic samples from outputs of any given model. Then, through the calibration process, SafeGround derives a test-time decision threshold with statistically guaranteed false discovery rate (FDR) control. We apply SafeGround on multiple GUI grounding models for the challenging ScreenSpot-Pro benchmark. Experimental results show that our uncertainty measure consistently outperforms existing baselines in distinguishing correct from incorrect predictions, while the calibrated threshold reliably enables rigorous risk control and potentials of substantial system-level accuracy improvements. Across multiple GUI grounding models, SafeGround improves system-level accuracy by up to 5.38\\% percentage points over Gemini-only inference.",
    "arxiv_url": "https://arxiv.org/abs/2602.02419",
    "authors": [
      "Qingni Wang",
      "Yue Fan",
      "Xin Eric Wang"
    ],
    "first_author": "Qingni Wang",
    "category": [
      "Technical"
    ],
    "field": "GUI Interaction",
    "task": "GUI Grounding",
    "tags": [
      "Uncertainty Calibration",
      "Selective Prediction",
      "False Discovery Rate Control",
      "Distribution-aware Uncertainty",
      "Cascaded Inference",
      "Spatial Uncertainty",
      "Black-box Models"
    ],
    "summary": "SAFEGROUND 提出了一种无需访问模型内部、基于随机采样的空间分布不确定性量化与学习-然后-测试校准方法，以在 GUI 定位任务中实现可控错误率的选择性预测与级联推理，从而提升系统级准确性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.02419v1",
    "published": "2026-02-02",
    "update_time": "2026-02-02",
    "download_time": "2026-02-04 02:23:45"
  },
  {
    "id": "2602.04640",
    "title": "Towards Structured, State-Aware, and Execution-Grounded Reasoning for Software Engineering Agents",
    "abstract": "Software Engineering (SE) agents have shown promising abilities in supporting various SE tasks. Current SE agents remain fundamentally reactive, making decisions mainly based on conversation history and the most recent response. However, this reactive design provides no explicit structure or persistent state within the agent's memory, making long-horizon reasoning challenging. As a result, SE agents struggle to maintain a coherent understanding across reasoning steps, adapt their hypotheses as new evidence emerges, or incorporate execution feedback into the mental reasoning model of the system state.   In this position paper, we argue that, to further advance SE agents, we need to move beyond reactive behavior toward a structured, state-aware, and execution-grounded reasoning. We outline how explicit structure, persistent and evolving state, and the integration of execution-grounded feedback can help SE agents perform more coherent and reliable reasoning in long-horizon tasks. We also provide an initial roadmap for developing next-generation SE agents that can more effectively perform real-world tasks.",
    "arxiv_url": "https://arxiv.org/abs/2602.04640",
    "authors": [
      "Tse-Hsun",
      "Chen"
    ],
    "first_author": "Tse-Hsun",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Stateful Reasoning",
      "Structured Memory",
      "Execution-Grounded Feedback",
      "Hypothesis Tracking",
      "Pre/Post Conditions",
      "Finite-State Abstraction",
      "Long-Horizon Reasoning",
      "Tool Integration"
    ],
    "summary": "本文主张将软件工程代理从基于对话的被动反应式设计，转向具有显式结构、持久可演化状态和执行反馈驱动的推理机制，并提出实现该目标的初步路线图。",
    "quality": "Middle",
    "conference": "BoatSE",
    "pdf_url": "https://arxiv.org/pdf/2602.04640v1",
    "published": "2026-02-04",
    "update_time": "2026-02-04",
    "download_time": "2026-02-05 02:22:50"
  },
  {
    "id": "2602.04449",
    "title": "What's in a Benchmark? The Case of SWE-Bench in Automated Program Repair",
    "abstract": "The rapid progress in Automated Program Repair (APR) has been fueled by advances in AI, particularly large language models (LLMs) and agent-based systems. SWE-Bench is a benchmark designed to evaluate repair systems using real issues mined from popular open-source Python repositories. Its public leaderboards-SWE-Bench Lite and Verified-have become central platforms for tracking progress and comparing solutions. In this paper, we present the first comprehensive study of these two leaderboards, examining who is submitting solutions, the products behind the submissions, the LLMs employed, and the openness of the approaches. We analyze 79 entries submitted to Lite leaderboard and 133 to Verified. Our results show that most entries on both leaderboards originate from industry, particularly small companies and large publicly traded companies. These submissions often achieve top results, although academic contributions-typically open source-also remain competitive. We also find a clear dominance of proprietary LLMs, especially Claude family, with state-of-the-art results on both leaderboards currently achieved by Claude 4 Sonnet. These findings offer insights into the SWE-Bench ecosystem that can guide greater transparency and diversity in future benchmark-driven research.",
    "arxiv_url": "https://arxiv.org/abs/2602.04449",
    "authors": [
      "Matias Martinez",
      "Xavier Franch"
    ],
    "first_author": "Matias Martinez",
    "category": [
      "Empirical"
    ],
    "field": "Quality Management",
    "task": "Bug Repair",
    "tags": [
      "Program Repair",
      "Benchmark Analysis",
      "Leaderboard Dynamics",
      "Submitter Profiling",
      "LLM Adoption",
      "Proprietary Model Dominance",
      "Open-source Availability",
      "Patch Correctness"
    ],
    "summary": "本文对SWE-Bench Lite和Verified排行榜上的79和133条提交进行系统性实证分析，揭示了提交者构成、产品可用性、开源程度与所用大型语言模型（以专有模型占优）对自动程序修复结果的影响，并讨论了基准的透明性与演化问题。",
    "quality": "High",
    "conference": "IEEE/ACM 48th International Conference on Software Engineering (ICSE)",
    "pdf_url": "https://arxiv.org/pdf/2602.04449v1",
    "published": "2026-02-04",
    "update_time": "2026-02-04",
    "download_time": "2026-02-05 02:23:11"
  },
  {
    "id": "2602.04572",
    "title": "From Competition to Collaboration: Designing Sustainable Mechanisms Between LLMs and Online Forums",
    "abstract": "While Generative AI (GenAI) systems draw users away from (Q&A) forums, they also depend on the very data those forums produce to improve their performance. Addressing this paradox, we propose a framework of sequential interaction, in which a GenAI system proposes questions to a forum that can publish some of them. Our framework captures several intricacies of such a collaboration, including non-monetary exchanges, asymmetric information, and incentive misalignment. We bring the framework to life through comprehensive, data-driven simulations using real Stack Exchange data and commonly used LLMs. We demonstrate the incentive misalignment empirically, yet show that players can achieve roughly half of the utility in an ideal full-information scenario. Our results highlight the potential for sustainable collaboration that preserves effective knowledge sharing between AI systems and human knowledge platforms.",
    "arxiv_url": "https://arxiv.org/abs/2602.04572",
    "authors": [
      "Niv Fono",
      "Yftah Ziser",
      "Omer Ben-Porat"
    ],
    "first_author": "Niv Fono",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Requirements & Design",
    "task": "Management",
    "tags": [
      "Human-LLM Interaction",
      "Incentive Misalignment",
      "Game-Theoretic Modeling",
      "Acceptance-Aware Mechanism",
      "Asymmetric Information",
      "Non-Monetary Exchange",
      "Data-Driven Simulation",
      "Forum Sustainability"
    ],
    "summary": "本文提出了一个博弈论框架，并通过基于真实Stack Exchange数据与开源LLM的大规模仿真实验，设计并验证了在非货币交换与信息不对称条件下，GenAI与问答社区之间能够部分恢复双方效用的可持续协作机制。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.04572v1",
    "published": "2026-02-04",
    "update_time": "2026-02-04",
    "download_time": "2026-02-05 02:24:42"
  },
  {
    "id": "2602.04466",
    "title": "Is Micro Domain-Adaptive Pre-Training Effective for Real-World Operations? Multi-Step Evaluation Reveals Potential and Bottlenecks",
    "abstract": "When applying LLMs to real-world enterprise operations, LLMs need to handle proprietary knowledge in small domains of specific operations ($\\textbf{micro domains}$). A previous study shows micro domain-adaptive pre-training ($\\textbf{mDAPT}$) with fewer documents is effective, similarly to DAPT in larger domains. However, it evaluates mDAPT only on multiple-choice questions; thus, its effectiveness for generative tasks in real-world operations remains unknown. We aim to reveal the potential and bottlenecks of mDAPT for generative tasks. To this end, we disentangle the answering process into three subtasks and evaluate the performance of each subtask: (1) $\\textbf{eliciting}$ facts relevant to questions from an LLM's own knowledge, (2) $\\textbf{reasoning}$ over the facts to obtain conclusions, and (3) $\\textbf{composing}$ long-form answers based on the conclusions. We verified mDAPT on proprietary IT product knowledge for real-world questions in IT technical support operations. As a result, mDAPT resolved the elicitation task that the base model struggled with but did not resolve other subtasks. This clarifies mDAPT's effectiveness in the knowledge aspect and its bottlenecks in other aspects. Further analysis empirically shows that resolving the elicitation and reasoning tasks ensures sufficient performance (over 90%), emphasizing the need to enhance reasoning capability.",
    "arxiv_url": "https://arxiv.org/abs/2602.04466",
    "authors": [
      "Masaya Tsunokake",
      "Yuta Koreeda",
      "Terufumi Morishita",
      "Koichi Nagatsuka",
      "Hikaru Tomonari",
      "Yasuhiro Sogawa"
    ],
    "first_author": "Masaya Tsunokake",
    "category": [
      "Empirical",
      "Technical"
    ],
    "field": "Domain Adaptation & Knowledge Grounding",
    "task": "Micro Domain-Adaptive Pre-Training Evaluation for Generative QA",
    "tags": [
      "Micro-domain Adaptation",
      "Multi-step Oracle Evaluation",
      "Knowledge Elicitation",
      "Knowledge Memorization",
      "Reasoning Bottleneck",
      "Answer Composition",
      "Enterprise IT QA"
    ],
    "summary": "本文通过提出多步oracle评估与知识评估方法，实证研究在企业微域（micro-domain）上进行领域自适应预训练（mDAPT）对生成式技术支持问答的效果，发现mDAPT能显著提升知识提取能力但在推理与长文生成上仍存瓶颈。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.04466v1",
    "published": "2026-02-04",
    "update_time": "2026-02-04",
    "download_time": "2026-02-05 02:25:42"
  }
]