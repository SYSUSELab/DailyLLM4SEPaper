[
  {
    "id": "2602.03462",
    "title": "RAL-Bench: Benchmarking for Application-Level Functional Correctness and Non-Functional Quality Attributes",
    "abstract": "Code generation has advanced rapidly with code-focused large language models (LLMs), especially on snippet-level tasks. However, application-level generation requires producing a runnable multi-file repository with correct structure, dependencies, and end-to-end executability, and real-world software must satisfy both functional correctness and non-functional quality (e.g., maintainability, security). Existing benchmarks provide a limited execution-based assessment of these requirements at the application level. We ask: Can current LLMs generate application-level repositories that meet both functional and non-functional criteria? We propose RAL-Bench, a benchmark and evaluation framework for application-level code generation. For each task, we distill a concise natural-language requirement from a high-quality reference project, build black-box system tests covering functional and non-functional attributes, and keep only tests that pass on the reference repository to ensure a sound oracle and an end-to-end executable suite. Functional correctness is measured by system-test pass rate. Non-functional quality is measured along five ISO/IEC 25010-inspired dimensions and aggregated with an Analytic Hierarchy Process (AHP)-derived weight vector, with per-dimension diagnostics and baseline-normalized scoring using reference measurements. Across 16 LLMs evaluated zero-shot with greedy decoding, functional correctness is the dominant bottleneck: no model exceeds a 45% functional pass rate under our requirement-driven, reference-validated tests. We release RAL-Bench at https://github.com/Wwstarry/RAL-Bench. .",
    "arxiv_url": "https://arxiv.org/abs/2602.03462",
    "authors": [
      "Ruwei Pan",
      "Yakun Zhang",
      "Qingyuan Liang",
      "Yueheng Zhu",
      "Chao Liu",
      "Lu Zhang",
      "Hongyu Zhang"
    ],
    "first_author": "Ruwei Pan",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Application-Level Code Generation",
    "tags": [
      "Application-Level Generation",
      "System Testing",
      "Non-functional Quality",
      "ISO25010",
      "AHP Weighting",
      "Requirement Distillation",
      "Reference-validated Tests",
      "Baseline-normalized Scoring",
      "End-to-End Executability",
      "Zero-shot Evaluation"
    ],
    "summary": "本文提出 RAL-Bench，一个用于评估 LLM 从自然语言需求生成可运行应用仓库的基准框架，通过对参考实现验证的黑盒系统测试同时度量功能正确性与基于 ISO/IEC 25010 的多维非功能质量（并用 AHP 加权聚合），并在零样本条件下评估了 16 款 LLM，发现功能正确性是主要瓶颈。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.03462v1",
    "published": "2026-02-03",
    "update_time": "2026-02-03",
    "download_time": "2026-02-04 02:21:26"
  },
  {
    "id": "2602.03419",
    "title": "SWE-World: Building Software Engineering Agents in Docker-Free Environments",
    "abstract": "Recent advances in large language models (LLMs) have enabled software engineering agents to tackle complex code modification tasks. Most existing approaches rely on execution feedback from containerized environments, which require dependency-complete setup and physical execution of programs and tests. While effective, this paradigm is resource-intensive and difficult to maintain, substantially complicating agent training and limiting scalability. We propose SWE-World, a Docker-free framework that replaces physical execution environments with a learned surrogate for training and evaluating software engineering agents. SWE-World leverages LLM-based models trained on real agent-environment interaction data to predict intermediate execution outcomes and final test feedback, enabling agents to learn without interacting with physical containerized environments. This design preserves the standard agent-environment interaction loop while eliminating the need for costly environment construction and maintenance during agent optimization and evaluation. Furthermore, because SWE-World can simulate the final evaluation outcomes of candidate trajectories without real submission, it enables selecting the best solution among multiple test-time attempts, thereby facilitating effective test-time scaling (TTS) in software engineering tasks. Experiments on SWE-bench Verified demonstrate that SWE-World raises Qwen2.5-Coder-32B from 6.2\\% to 52.0\\% via Docker-free SFT, 55.0\\% with Docker-free RL, and 68.2\\% with further TTS. The code is available at https://github.com/RUCAIBox/SWE-World",
    "arxiv_url": "https://arxiv.org/abs/2602.03419",
    "authors": [
      "Shuang Sun",
      "Huatong Song",
      "Lisheng Huang",
      "Jinhao Jiang",
      "Ran Le",
      "Zhihao Lv",
      "Zongchao Chen",
      "Yiwen Hu",
      "Wenyang Luo",
      "Wayne Xin Zhao",
      "Yang Song",
      "Hongteng Xu",
      "Tao Zhang",
      "Ji-Rong Wen"
    ],
    "first_author": "Shuang Sun",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Code Simulation",
      "Code Agents",
      "Surrogate Execution",
      "World Modeling",
      "Docker-Free Training",
      "Test-Time Scaling",
      "Transition Model",
      "Reward Modeling",
      "Repository-Level Coding",
      "Agentic Interaction"
    ],
    "summary": "本文提出SWE-World：用训练得到的LLM作为环境的替代（步骤级转移模型与终端奖励模型），以在无Docker容器的条件下训练和评估软件工程代理，从而显著降低资源开销并在SFT、RL与测试时扩展中提升修复性能。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.03419v1",
    "published": "2026-02-03",
    "update_time": "2026-02-03",
    "download_time": "2026-02-04 02:21:44"
  },
  {
    "id": "2602.02475",
    "title": "AgentRx: Diagnosing AI Agent Failures from Execution Trajectories",
    "abstract": "AI agents often fail in ways that are difficult to localize because executions are probabilistic, long-horizon, multi-agent, and mediated by noisy tool outputs. We address this gap by manually annotating failed agent runs and release a novel benchmark of 115 failed trajectories spanning structured API workflows, incident management, and open-ended web/file tasks. Each trajectory is annotated with a critical failure step and a category from a grounded-theory derived, cross-domain failure taxonomy. To mitigate the human cost of failure attribution, we present AGENTRX, an automated domain-agnostic diagnostic framework that pinpoints the critical failure step in a failed agent trajectory. It synthesizes constraints, evaluates them step-by-step, and produces an auditable validation log of constraint violations with associated evidence; an LLM-based judge uses this log to localize the critical step and category. Our framework improves step localization and failure attribution over existing baselines across three domains.",
    "arxiv_url": "https://arxiv.org/abs/2602.02475",
    "authors": [
      "Shraddha Barke",
      "Arnav Goyal",
      "Alind Khare",
      "Avaljot Singh",
      "Suman Nath",
      "Chetan Bansal"
    ],
    "first_author": "Shraddha Barke",
    "category": [
      "Benchmark",
      "Technical"
    ],
    "field": "AIOps",
    "task": "Agent Failure Localization",
    "tags": [
      "Failure Taxonomy",
      "Execution Trajectories",
      "Constraint Synthesis",
      "Validation Log",
      "LLM Adjudication",
      "Critical Failure Localization",
      "Grounded Theory Annotation",
      "Multi-Agent Diagnostics",
      "Tool Invocation Validation"
    ],
    "summary": "该论文发布了包含115个失败代理执行轨迹的基准与跨域失败分类法，并提出AGENTRX框架通过从工具模式与策略合成约束、逐步验证并由基于LLM的裁判利用可审计的违约日志来定位和归因关键失败步骤。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.02475v1",
    "published": "2026-02-02",
    "update_time": "2026-02-02",
    "download_time": "2026-02-04 02:22:32"
  },
  {
    "id": "2602.02419",
    "title": "SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration",
    "abstract": "Graphical User Interface (GUI) grounding aims to translate natural language instructions into executable screen coordinates, enabling automated GUI interaction. Nevertheless, incorrect grounding can result in costly, hard-to-reverse actions (e.g., erroneous payment approvals), raising concerns about model reliability. In this paper, we introduce SafeGround, an uncertainty-aware framework for GUI grounding models that enables risk-aware predictions through calibrations before testing. SafeGround leverages a distribution-aware uncertainty quantification method to capture the spatial dispersion of stochastic samples from outputs of any given model. Then, through the calibration process, SafeGround derives a test-time decision threshold with statistically guaranteed false discovery rate (FDR) control. We apply SafeGround on multiple GUI grounding models for the challenging ScreenSpot-Pro benchmark. Experimental results show that our uncertainty measure consistently outperforms existing baselines in distinguishing correct from incorrect predictions, while the calibrated threshold reliably enables rigorous risk control and potentials of substantial system-level accuracy improvements. Across multiple GUI grounding models, SafeGround improves system-level accuracy by up to 5.38\\% percentage points over Gemini-only inference.",
    "arxiv_url": "https://arxiv.org/abs/2602.02419",
    "authors": [
      "Qingni Wang",
      "Yue Fan",
      "Xin Eric Wang"
    ],
    "first_author": "Qingni Wang",
    "category": [
      "Technical"
    ],
    "field": "GUI Interaction",
    "task": "GUI Grounding",
    "tags": [
      "Uncertainty Calibration",
      "Selective Prediction",
      "False Discovery Rate Control",
      "Distribution-aware Uncertainty",
      "Cascaded Inference",
      "Spatial Uncertainty",
      "Black-box Models"
    ],
    "summary": "SAFEGROUND 提出了一种无需访问模型内部、基于随机采样的空间分布不确定性量化与学习-然后-测试校准方法，以在 GUI 定位任务中实现可控错误率的选择性预测与级联推理，从而提升系统级准确性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.02419v1",
    "published": "2026-02-02",
    "update_time": "2026-02-02",
    "download_time": "2026-02-04 02:23:45"
  },
  {
    "id": "2602.04640",
    "title": "Towards Structured, State-Aware, and Execution-Grounded Reasoning for Software Engineering Agents",
    "abstract": "Software Engineering (SE) agents have shown promising abilities in supporting various SE tasks. Current SE agents remain fundamentally reactive, making decisions mainly based on conversation history and the most recent response. However, this reactive design provides no explicit structure or persistent state within the agent's memory, making long-horizon reasoning challenging. As a result, SE agents struggle to maintain a coherent understanding across reasoning steps, adapt their hypotheses as new evidence emerges, or incorporate execution feedback into the mental reasoning model of the system state.   In this position paper, we argue that, to further advance SE agents, we need to move beyond reactive behavior toward a structured, state-aware, and execution-grounded reasoning. We outline how explicit structure, persistent and evolving state, and the integration of execution-grounded feedback can help SE agents perform more coherent and reliable reasoning in long-horizon tasks. We also provide an initial roadmap for developing next-generation SE agents that can more effectively perform real-world tasks.",
    "arxiv_url": "https://arxiv.org/abs/2602.04640",
    "authors": [
      "Tse-Hsun",
      "Chen"
    ],
    "first_author": "Tse-Hsun",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Stateful Reasoning",
      "Structured Memory",
      "Execution-Grounded Feedback",
      "Hypothesis Tracking",
      "Pre/Post Conditions",
      "Finite-State Abstraction",
      "Long-Horizon Reasoning",
      "Tool Integration"
    ],
    "summary": "本文主张将软件工程代理从基于对话的被动反应式设计，转向具有显式结构、持久可演化状态和执行反馈驱动的推理机制，并提出实现该目标的初步路线图。",
    "quality": "Middle",
    "conference": "BoatSE",
    "pdf_url": "https://arxiv.org/pdf/2602.04640v1",
    "published": "2026-02-04",
    "update_time": "2026-02-04",
    "download_time": "2026-02-05 02:22:50"
  },
  {
    "id": "2602.04449",
    "title": "What's in a Benchmark? The Case of SWE-Bench in Automated Program Repair",
    "abstract": "The rapid progress in Automated Program Repair (APR) has been fueled by advances in AI, particularly large language models (LLMs) and agent-based systems. SWE-Bench is a benchmark designed to evaluate repair systems using real issues mined from popular open-source Python repositories. Its public leaderboards-SWE-Bench Lite and Verified-have become central platforms for tracking progress and comparing solutions. In this paper, we present the first comprehensive study of these two leaderboards, examining who is submitting solutions, the products behind the submissions, the LLMs employed, and the openness of the approaches. We analyze 79 entries submitted to Lite leaderboard and 133 to Verified. Our results show that most entries on both leaderboards originate from industry, particularly small companies and large publicly traded companies. These submissions often achieve top results, although academic contributions-typically open source-also remain competitive. We also find a clear dominance of proprietary LLMs, especially Claude family, with state-of-the-art results on both leaderboards currently achieved by Claude 4 Sonnet. These findings offer insights into the SWE-Bench ecosystem that can guide greater transparency and diversity in future benchmark-driven research.",
    "arxiv_url": "https://arxiv.org/abs/2602.04449",
    "authors": [
      "Matias Martinez",
      "Xavier Franch"
    ],
    "first_author": "Matias Martinez",
    "category": [
      "Empirical"
    ],
    "field": "Quality Management",
    "task": "Bug Repair",
    "tags": [
      "Program Repair",
      "Benchmark Analysis",
      "Leaderboard Dynamics",
      "Submitter Profiling",
      "LLM Adoption",
      "Proprietary Model Dominance",
      "Open-source Availability",
      "Patch Correctness"
    ],
    "summary": "本文对SWE-Bench Lite和Verified排行榜上的79和133条提交进行系统性实证分析，揭示了提交者构成、产品可用性、开源程度与所用大型语言模型（以专有模型占优）对自动程序修复结果的影响，并讨论了基准的透明性与演化问题。",
    "quality": "High",
    "conference": "IEEE/ACM 48th International Conference on Software Engineering (ICSE)",
    "pdf_url": "https://arxiv.org/pdf/2602.04449v1",
    "published": "2026-02-04",
    "update_time": "2026-02-04",
    "download_time": "2026-02-05 02:23:11"
  },
  {
    "id": "2602.04572",
    "title": "From Competition to Collaboration: Designing Sustainable Mechanisms Between LLMs and Online Forums",
    "abstract": "While Generative AI (GenAI) systems draw users away from (Q&A) forums, they also depend on the very data those forums produce to improve their performance. Addressing this paradox, we propose a framework of sequential interaction, in which a GenAI system proposes questions to a forum that can publish some of them. Our framework captures several intricacies of such a collaboration, including non-monetary exchanges, asymmetric information, and incentive misalignment. We bring the framework to life through comprehensive, data-driven simulations using real Stack Exchange data and commonly used LLMs. We demonstrate the incentive misalignment empirically, yet show that players can achieve roughly half of the utility in an ideal full-information scenario. Our results highlight the potential for sustainable collaboration that preserves effective knowledge sharing between AI systems and human knowledge platforms.",
    "arxiv_url": "https://arxiv.org/abs/2602.04572",
    "authors": [
      "Niv Fono",
      "Yftah Ziser",
      "Omer Ben-Porat"
    ],
    "first_author": "Niv Fono",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Requirements & Design",
    "task": "Management",
    "tags": [
      "Human-LLM Interaction",
      "Incentive Misalignment",
      "Game-Theoretic Modeling",
      "Acceptance-Aware Mechanism",
      "Asymmetric Information",
      "Non-Monetary Exchange",
      "Data-Driven Simulation",
      "Forum Sustainability"
    ],
    "summary": "本文提出了一个博弈论框架，并通过基于真实Stack Exchange数据与开源LLM的大规模仿真实验，设计并验证了在非货币交换与信息不对称条件下，GenAI与问答社区之间能够部分恢复双方效用的可持续协作机制。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.04572v1",
    "published": "2026-02-04",
    "update_time": "2026-02-04",
    "download_time": "2026-02-05 02:24:42"
  },
  {
    "id": "2602.04466",
    "title": "Is Micro Domain-Adaptive Pre-Training Effective for Real-World Operations? Multi-Step Evaluation Reveals Potential and Bottlenecks",
    "abstract": "When applying LLMs to real-world enterprise operations, LLMs need to handle proprietary knowledge in small domains of specific operations ($\\textbf{micro domains}$). A previous study shows micro domain-adaptive pre-training ($\\textbf{mDAPT}$) with fewer documents is effective, similarly to DAPT in larger domains. However, it evaluates mDAPT only on multiple-choice questions; thus, its effectiveness for generative tasks in real-world operations remains unknown. We aim to reveal the potential and bottlenecks of mDAPT for generative tasks. To this end, we disentangle the answering process into three subtasks and evaluate the performance of each subtask: (1) $\\textbf{eliciting}$ facts relevant to questions from an LLM's own knowledge, (2) $\\textbf{reasoning}$ over the facts to obtain conclusions, and (3) $\\textbf{composing}$ long-form answers based on the conclusions. We verified mDAPT on proprietary IT product knowledge for real-world questions in IT technical support operations. As a result, mDAPT resolved the elicitation task that the base model struggled with but did not resolve other subtasks. This clarifies mDAPT's effectiveness in the knowledge aspect and its bottlenecks in other aspects. Further analysis empirically shows that resolving the elicitation and reasoning tasks ensures sufficient performance (over 90%), emphasizing the need to enhance reasoning capability.",
    "arxiv_url": "https://arxiv.org/abs/2602.04466",
    "authors": [
      "Masaya Tsunokake",
      "Yuta Koreeda",
      "Terufumi Morishita",
      "Koichi Nagatsuka",
      "Hikaru Tomonari",
      "Yasuhiro Sogawa"
    ],
    "first_author": "Masaya Tsunokake",
    "category": [
      "Empirical",
      "Technical"
    ],
    "field": "Domain Adaptation & Knowledge Grounding",
    "task": "Micro Domain-Adaptive Pre-Training Evaluation for Generative QA",
    "tags": [
      "Micro-domain Adaptation",
      "Multi-step Oracle Evaluation",
      "Knowledge Elicitation",
      "Knowledge Memorization",
      "Reasoning Bottleneck",
      "Answer Composition",
      "Enterprise IT QA"
    ],
    "summary": "本文通过提出多步oracle评估与知识评估方法，实证研究在企业微域（micro-domain）上进行领域自适应预训练（mDAPT）对生成式技术支持问答的效果，发现mDAPT能显著提升知识提取能力但在推理与长文生成上仍存瓶颈。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.04466v1",
    "published": "2026-02-04",
    "update_time": "2026-02-04",
    "download_time": "2026-02-05 02:25:42"
  },
  {
    "id": "2602.05762",
    "title": "RocqSmith: Can Automatic Optimization Forge Better Proof Agents?",
    "abstract": "This work studies the applicability of automatic AI agent optimization methods to real-world agents in formal verification settings, focusing on automated theorem proving in Rocq as a representative and challenging domain. We evaluate how different automatic agent optimizers perform when applied to the task of optimizing a Rocq proof-generation agent, and assess whether parts of the fine-grained tuning of agentic systems, such as prompt design, contextual knowledge, and control strategies, can be automated. Our results show that while several optimizers yield measurable improvements, simple few-shot bootstrapping is the most consistently effective; however, none of the studied methods matches the performance of a carefully engineered state-of-the-art proof agent.",
    "arxiv_url": "https://arxiv.org/abs/2602.05762",
    "authors": [
      "Andrei Kozyrev",
      "Nikita Khramov",
      "Denis Lochmelis",
      "Valerio Morelli",
      "Gleb Solovev",
      "Anton Podkopaev"
    ],
    "first_author": "Andrei Kozyrev",
    "category": [
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Program Proof",
      "Proof Agents",
      "Agent Optimization",
      "Prompt Optimization",
      "Few-shot Bootstrapping",
      "Contextual Knowledge Retrieval",
      "Control Flow Optimization",
      "Optimization Efficiency"
    ],
    "summary": "本文在Rocq自动定理证明场景中系统评估了多种自动化智能体优化方法，发现简单的few-shot自举最为稳定但仍未能超越精心设计的手工证明智能体。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.05762v1",
    "published": "2026-02-05",
    "update_time": "2026-02-05",
    "download_time": "2026-02-06 02:22:52"
  },
  {
    "id": "2602.05721",
    "title": "A Dual-Loop Agent Framework for Automated Vulnerability Reproduction",
    "abstract": "Automated vulnerability reproduction from CVE descriptions requires generating executable Proof-of-Concept (PoC) exploits and validating them in target environments. This process is critical in software security research and practice, yet remains time-consuming and demands specialized expertise when performed manually. While LLM agents show promise for automating this task, existing approaches often conflate exploring attack directions with fixing implementation details, which leads to unproductive debugging loops when reproduction fails. To address this, we propose Cve2PoC, an LLM-based dual-loop agent framework following a plan-execute-evaluate paradigm. The Strategic Planner analyzes vulnerability semantics and target code to produce structured attack plans. The Tactical Executor generates PoC code and validates it through progressive verification. The Adaptive Refiner evaluates execution results and routes failures to different loops: the \\textit{Tactical Loop} for code-level refinement, while the \\textit{Strategic Loop} for attack strategy replanning. This dual-loop design enables the framework to escape ineffective debugging by matching remediation to failure type. Evaluation on two benchmarks covering 617 real-world vulnerabilities demonstrates that Cve2PoC achieves 82.9\\% and 54.3\\% reproduction success rates on SecBench.js and PatchEval, respectively, outperforming the best baseline by 11.3\\% and 20.4\\%. Human evaluation confirms that generated PoCs achieve comparable code quality to human-written exploits in readability and reusability.",
    "arxiv_url": "https://arxiv.org/abs/2602.05721",
    "authors": [
      "Bin Liu",
      "Yanjie Zhao",
      "Zhenpeng Chen",
      "Guoai Xu",
      "Haoyu Wang"
    ],
    "first_author": "Bin Liu",
    "category": [
      "Technical"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Reproduction",
    "tags": [
      "Code Agents",
      "Security and Vulnerabilities",
      "PoC Generation",
      "Dual-Loop Agents",
      "Strategic Planning",
      "Tactical Execution",
      "Progressive Verification",
      "Adaptive Refinement",
      "Differential Testing",
      "Experience Retrieval"
    ],
    "summary": "本文提出Cve2PoC——一种将战略规划与战术执行分离的基于LLM的双环代理框架，通过结构化攻击计划、渐进式多层验证与自适应精炼，实现从CVE描述自动生成并验证可执行PoC，显著提升漏洞复现成功率。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.05721v1",
    "published": "2026-02-05",
    "update_time": "2026-02-05",
    "download_time": "2026-02-06 02:23:26"
  },
  {
    "id": "2602.04837",
    "title": "Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing",
    "abstract": "Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods.",
    "arxiv_url": "https://arxiv.org/abs/2602.04837",
    "authors": [
      "Zhaotian Weng",
      "Antonis Antoniades",
      "Deepak Nathani",
      "Zhen Zhang",
      "Xiao Pu",
      "Xin Eric Wang"
    ],
    "first_author": "Zhaotian Weng",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Group Evolution",
      "Experience Sharing",
      "Open-Ended Self-Improvement",
      "Meta-Learning",
      "Performance-Novelty Selection",
      "Diversity Consolidation",
      "Agent Robustness",
      "Transferability"
    ],
    "summary": "本文提出Group-Evolving Agents（GEA），通过将群体作为进化单元并在群体内显式共享与复用经验，实现开放式自我改进，在多个编码基准上显著优于现有自我进化方法并展现更强的鲁棒性与可迁移性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.04837v1",
    "published": "2026-02-04",
    "update_time": "2026-02-04",
    "download_time": "2026-02-06 02:24:45"
  },
  {
    "id": "2602.04811",
    "title": "SE-Bench: Benchmarking Self-Evolution with Knowledge Internalization",
    "abstract": "True self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hindered by two obstacles: the entanglement of prior knowledge, where ``new'' knowledge may appear in pre-training data, and the entanglement of reasoning complexity, where failures may stem from problem difficulty rather than an inability to recall learned knowledge. We introduce SE-Bench, a diagnostic environment that obfuscates the NumPy library and its API doc into a pseudo-novel package with randomized identifiers. Agents are trained to internalize this package and evaluated on simple coding tasks without access to documentation, yielding a clean setting where tasks are trivial with the new API doc but impossible for base models without it. Our investigation reveals three insights: (1) the Open-Book Paradox, where training with reference documentation inhibits retention, requiring \"Closed-Book Training\" to force knowledge compression into weights; (2) the RL Gap, where standard RL fails to internalize new knowledge completely due to PPO clipping and negative gradients; and (3) the viability of Self-Play for internalization, proving models can learn from self-generated, noisy tasks when coupled with SFT, but not RL. Overall, SE-Bench establishes a rigorous diagnostic platform for self-evolution with knowledge internalization. Our code and dataset can be found at https://github.com/thunlp/SE-Bench.",
    "arxiv_url": "https://arxiv.org/abs/2602.04811",
    "authors": [
      "Jiarui Yuan",
      "Tailin Jin",
      "Weize Chen",
      "Zeyuan Liu",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "first_author": "Jiarui Yuan",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Instruction-Tuning",
    "tags": [
      "Knowledge Internalization",
      "API Obfuscation",
      "Closed-Book Training",
      "Open-Book Paradox",
      "Self-Play Learning",
      "RL Gap",
      "PPO Clipping",
      "Compositional Generalization",
      "Diagnostic Benchmark",
      "Consensus Filtering"
    ],
    "summary": "该论文提出SE-BENCH，通过将NumPy函数和文档混淆为伪新库并构建可训练与测试题集，提供一个“必须内化才能解题”的诊断基准，用以评估和分析SFT、RL与自我博弈在将新API知识压缩入模型权重中的效果，并揭示了开卷悖论、RL内化缺陷与基于SFT的自我博弈可行性等重要发现。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.04811v1",
    "published": "2026-02-04",
    "update_time": "2026-02-04",
    "download_time": "2026-02-06 02:25:38"
  },
  {
    "id": "2602.05891",
    "title": "When Elo Lies: Hidden Biases in Codeforces-Based Evaluation of Large Language Models",
    "abstract": "As Large Language Models (LLMs) achieve breakthroughs in complex reasoning, Codeforces-based Elo ratings have emerged as a prominent metric for evaluating competitive programming capabilities. However, these ratings are often reported without critical experimental details, leading to significant discrepancies illustrated by recent reports where the score of the same model version fluctuated by nearly 500 points. This paper presents a systematic empirical study on the hidden factors biasing Elo evaluations: (1) the temporal ordering of submissions, (2) contest difficulty selection, and (3) run to run stochastic variability of LLMs. Utilizing a controlled benchmark of 37 recent Codeforces contests and 13,691 generated test cases, we demonstrate that Elo scores are highly sensitive to these parameters. Our findings reveal that varying submission orders can shift scores by 394 points, while contest selection can cause differences of up to 1,122 points for the same model. Run to run performance exhibits substantial instability, with a maximum difference of 349 points in mean scores observed when evaluating identical contests. We conclude that direct Elo comparisons are unreliable and potentially misleading without strict standardization and transparent reporting of experimental settings.",
    "arxiv_url": "https://arxiv.org/abs/2602.05891",
    "authors": [
      "Shenyu Zheng",
      "Ximing Dong",
      "Xiaoshuang Liu",
      "Gustavo Oliva",
      "Chong Chun Yong",
      "Dayi Lin",
      "Boyuan Chen",
      "Shaowei Wang",
      "Ahmed E. Hassan"
    ],
    "first_author": "Shenyu Zheng",
    "category": [
      "Empirical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Elo Bias",
      "Reproducibility",
      "Robustness",
      "Run-to-Run Variability",
      "Submission Order Sensitivity",
      "Contest Selection Bias",
      "Test Case Generation",
      "Automated Verifier"
    ],
    "summary": "本文通过构建包含37场Codeforces竞赛与13,691个自动生成测试用例的本地评测基准，系统实证分析了提交时序、竞赛选择与模型运行随机性如何导致基于Codeforces的Elo评分产生数百至上千点的偏差，并强调需严格标准化与透明报告评测设置以避免误导性比较。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.05891v1",
    "published": "2026-02-05",
    "update_time": "2026-02-05",
    "download_time": "2026-02-08 02:44:52"
  },
  {
    "id": "2602.05780",
    "title": "Automated Customization of LLMs for Enterprise Code Repositories Using Semantic Scopes",
    "abstract": "Code completion (CC) is a task frequently used by developers when working in collaboration with LLM-based programming assistants. Despite the increased performance of LLMs on public benchmarks, out of the box LLMs still have a hard time generating code that aligns with a private code repository not previously seen by the model's training data. Customizing code LLMs to a private repository provides a way to improve the model performance. In this paper we present our approach for automated LLM customization based on semantic scopes in the code. We evaluate LLMs on real industry cases with two private enterprise code repositories with two customization strategies: Retrieval-Augmented Generation (RAG) and supervised Fine-Tuning (FT). Our mechanism for ingesting the repository's data and formulating the training data pairs with semantic scopes helps models to learn the underlying patterns specific to the repository, providing more precise code to developers and helping to boost their productivity. The code completions of moderately sized customized models can be significantly better than those of uncustomized models of much larger capacity. We also include an analysis of customization on two public benchmarks and present opportunities for future work.",
    "arxiv_url": "https://arxiv.org/abs/2602.05780",
    "authors": [
      "Ulrich Finkler",
      "Irene Manotas",
      "Wei Zhang",
      "Geert Janssen",
      "Octavian Popescu",
      "Shyam Ramji"
    ],
    "first_author": "Ulrich Finkler",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Completion",
    "tags": [
      "Semantic Scopes",
      "Repository-Level Fine-Tuning",
      "Code RAG",
      "Repository-Level Coding",
      "Automated Data Ingestion",
      "Style Adaptation",
      "Concise Completions",
      "Small-Model Efficiency"
    ],
    "summary": "本文提出一种基于代码语义作用域的自动化仓库数据摄取与训练对构建方法，并在两个私有企业代码库上比较了基于检索增强生成（RAG）与监督微调（FT）的定制化效果，结果表明语义作用域驱动的微调能在小模型上显著提升代码补全的准确性、风格一致性和延迟表现。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.05780v1",
    "published": "2026-02-05",
    "update_time": "2026-02-05",
    "download_time": "2026-02-08 02:45:10"
  },
  {
    "id": "2602.06039",
    "title": "DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching",
    "abstract": "Multi-agent systems built from prompted large language models can improve multi-round reasoning, yet most existing pipelines rely on fixed, trajectory-wide communication patterns that are poorly matched to the stage-dependent needs of iterative problem solving. We introduce DyTopo, a manager-guided multi-agent framework that reconstructs a sparse directed communication graph at each round. Conditioned on the manager's round goal, each agent outputs lightweight natural-language query (need) and \\key (offer) descriptors; DyTopo embeds these descriptors and performs semantic matching, routing private messages only along the induced edges. Across code generation and mathematical reasoning benchmarks and four LLM backbones, DyTopo consistently outperforms over the strongest baseline (avg. +6.2). Beyond accuracy, DyTopo yields an interpretable coordination trace via the evolving graphs, enabling qualitative inspection of how communication pathways reconfigure across rounds.",
    "arxiv_url": "https://arxiv.org/abs/2602.06039",
    "authors": [
      "Yuxing Lu",
      "Yucheng Hu",
      "Xukai Zhao",
      "Jiuxin Cao"
    ],
    "first_author": "Yuxing Lu",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Dynamic Topology",
      "Semantic Key-Query Matching",
      "Manager-guided Routing",
      "Multi-Agent Coordination",
      "Private Message Routing",
      "Single-Pass Inference",
      "Interpretable Communication Graphs",
      "Topology Evolution Analysis",
      "Robustness",
      "Efficiency"
    ],
    "summary": "本文提出DyTopo，一种由Manager引导的多智能体动态拓扑路由框架，代理在每轮生成自然语言的need/key描述并通过语义匹配构建稀疏有向通信图，从而按需路由私有消息，在代码生成与数学推理任务上显著提升性能并提供可解释的协同轨迹。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.06039v1",
    "published": "2026-02-05",
    "update_time": "2026-02-05",
    "download_time": "2026-02-08 02:45:34"
  },
  {
    "id": "2602.05885",
    "title": "Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations",
    "abstract": "High-quality kernel is critical for scalable AI systems, and enabling LLMs to generate such code would advance AI development. However, training LLMs for this task requires sufficient data, a robust environment, and the process is often vulnerable to reward hacking and lazy optimization. In these cases, models may hack training rewards and prioritize trivial correctness over meaningful speedup. In this paper, we systematically study reinforcement learning (RL) for kernel generation. We first design KernelGYM, a robust distributed GPU environment that supports reward hacking check, data collection from multi-turn interactions and long-term RL training. Building on KernelGYM, we investigate effective multi-turn RL methods and identify a biased policy gradient issue caused by self-inclusion in GRPO. To solve this, we propose Turn-level Reinforce-Leave-One-Out (TRLOO) to provide unbiased advantage estimation for multi-turn RL. To alleviate lazy optimization, we incorporate mismatch correction for training stability and introduce Profiling-based Rewards (PR) and Profiling-based Rejection Sampling (PRS) to overcome the issue. The trained model, Dr.Kernel-14B, reaches performance competitive with Claude-4.5-Sonnet in Kernelbench. Finally, we study sequential test-time scaling for Dr.Kernel-14B. On the KernelBench Level-2 subset, 31.6% of the generated kernels achieve at least a 1.2x speedup over the Torch reference, surpassing Claude-4.5-Sonnet (26.7%) and GPT-5 (28.6%). When selecting the best candidate across all turns, this 1.2x speedup rate further increases to 47.8%. All resources, including environment, training code, models, and dataset, are included in https://www.github.com/hkust-nlp/KernelGYM.",
    "arxiv_url": "https://arxiv.org/abs/2602.05885",
    "authors": [
      "Wei Liu",
      "Jiawei Xu",
      "Yingru Li",
      "Longtao Zheng",
      "Tianjian Li",
      "Qian Liu",
      "Junxian He"
    ],
    "first_author": "Wei Liu",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Performance Optimization",
    "tags": [
      "Kernel Generation",
      "Triton",
      "Multi-turn Reinforcement Learning",
      "Unbiased Advantage Estimation",
      "Profiling-based Reward",
      "Rejection Sampling",
      "Reward Hacking Detection",
      "Lazy Optimization",
      "Distributed GPU Evaluation Environment",
      "Test-time Scaling"
    ],
    "summary": "本文提出了面向 Triton 内核生成的分布式 RL 平台 KERNELGYM，提出无偏多回合策略梯度估计 TRLOO 及基于性能分析的奖励与拒绝采样方法，有效缓解奖励劫持与惰性优化并显著提升生成内核的速度与稳定性，且公开了环境、数据与模型。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.05885v1",
    "published": "2026-02-05",
    "update_time": "2026-02-05",
    "download_time": "2026-02-08 02:47:33"
  },
  {
    "id": "2602.06875",
    "title": "TraceCoder: A Trace-Driven Multi-Agent Framework for Automated Debugging of LLM-Generated Code",
    "abstract": "Large Language Models (LLMs) often generate code with subtle but critical bugs, especially for complex tasks. Existing automated repair methods typically rely on superficial pass/fail signals, offering limited visibility into program behavior and hindering precise error localization. In addition, without a way to learn from prior failures, repair processes often fall into repetitive and inefficient cycles. To overcome these challenges, we present TraceCoder, a collaborative multi-agent framework that emulates the observe-analyze-repair process of human experts. The framework first instruments the code with diagnostic probes to capture fine-grained runtime traces, enabling deep insight into its internal execution. It then conducts causal analysis on these traces to accurately identify the root cause of the failure. This process is further enhanced by a novel Historical Lesson Learning Mechanism (HLLM), which distills insights from prior failed repair attempts to inform subsequent correction strategies and prevent recurrence of similar mistakes. To ensure stable convergence, a Rollback Mechanism enforces that each repair iteration constitutes a strict improvement toward the correct solution. Comprehensive experiments across multiple benchmarks show that TraceCoder achieves up to a 34.43\\% relative improvement in Pass@1 accuracy over existing advanced baselines. Ablation studies verify the significance of each system component, with the iterative repair process alone contributing a 65.61\\% relative gain in accuracy. Furthermore, TraceCoder significantly outperforms leading iterative methods in terms of both accuracy and cost-efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2602.06875",
    "authors": [
      "Jiangping Huang",
      "Wenguang Ye",
      "Weisong Sun",
      "Jian Zhang",
      "Mingyue Zhang",
      "Yang Liu"
    ],
    "first_author": "Jiangping Huang",
    "category": [
      "Technical"
    ],
    "field": "Quality Management",
    "task": "Bug Repair",
    "tags": [
      "Program Repair",
      "Code Agents",
      "Runtime Instrumentation",
      "Causal Fault Localization",
      "Historical Lesson Learning",
      "Rollback Mechanism",
      "Iterative Repair",
      "Cost Efficiency"
    ],
    "summary": "本文提出TraceCoder——一个基于运行时跟踪的多智能体自动化调试框架，结合因果分析、历史经验学习与回滚机制对LLM生成代码进行迭代定位与修复，从而显著提升修复准确率与效率。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.06875v1",
    "published": "2026-02-06",
    "update_time": "2026-02-06",
    "download_time": "2026-02-09 02:31:04"
  },
  {
    "id": "2602.06751",
    "title": "Beyond Function-Level Analysis: Context-Aware Reasoning for Inter-Procedural Vulnerability Detection",
    "abstract": "Recent progress in ML and LLMs has improved vulnerability detection, and recent datasets have reduced label noise and unrelated code changes. However, most existing approaches still operate at the function level, where models are asked to predict whether a single function is vulnerable without inter-procedural context. In practice, vulnerability presence and root cause often depend on contextual information. Naively appending such context is not a reliable solution: real-world context is long, redundant, and noisy, and we find that unstructured context frequently degrades the performance of strong fine-tuned code models.   We present CPRVul, a context-aware vulnerability detection framework that couples Context Profiling and Selection with Structured Reasoning. CPRVul constructs a code property graph, and extracts candidate context. It then uses an LLM to generate security-focused profiles and assign relevance scores, selecting only high-impact contextual elements that fit within the model's context window. In the second phase, CPRVul integrates the target function, the selected context, and auxiliary vulnerability metadata to generate reasoning traces, which are used to fine-tune LLMs for reasoning-based vulnerability detection.   We evaluate CPRVul on three high-quality vulnerability datasets: PrimeVul, TitanVul, and CleanVul. Across all datasets, CPRVul consistently outperforms function-only baselines, achieving accuracies ranging from 64.94% to 73.76%, compared to 56.65% to 63.68% for UniXcoder. Specifically, on the challenging PrimeVul benchmark, CPRVul achieves 67.78% accuracy, outperforming prior state-of-the-art approaches, improving accuracy from 55.17% to 67.78% (22.9% improvement). Our ablations further show that neither raw context nor processed context alone benefits strong code models; gains emerge only when processed context is paired with structured reasoning.",
    "arxiv_url": "https://arxiv.org/abs/2602.06751",
    "authors": [
      "Yikun Li",
      "Ting Zhang",
      "Jieke Shi",
      "Chengran Yang",
      "Junda He",
      "Xin Zhou",
      "Jinfeng Jiang",
      "Huihui Huang",
      "Wen Bin Leow",
      "Yide Yin",
      "Eng Lieh Ouh",
      "Lwin Khin Shar",
      "David Lo"
    ],
    "first_author": "Yikun Li",
    "category": [
      "Technical",
      "Benchmark",
      "Empirical"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "Inter-procedural Reasoning",
      "Context Profiling",
      "Context Selection",
      "Code Property Graph",
      "Structured Reasoning Traces",
      "Repository-Level Analysis",
      "LLM-Guided Relevance Scoring",
      "Context-Enriched Benchmark"
    ],
    "summary": "本文提出CPRVUL，通过构建代码属性图进行仓库级上下文抽取、用LLM生成安全导向的上下文剖析与相关性评分以选择关键信息，并以结构化推理轨迹监督微调模型，从而显著提升跨过程漏洞检测性能并发布了带上下文的基准数据集。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.06751v1",
    "published": "2026-02-06",
    "update_time": "2026-02-06",
    "download_time": "2026-02-09 02:31:28"
  },
  {
    "id": "2602.06795",
    "title": "Generating Data-Driven Reasoning Rubrics for Domain-Adaptive Reward Modeling",
    "abstract": "An impediment to using Large Language Models (LLMs) for reasoning output verification is that LLMs struggle to reliably identify errors in thinking traces, particularly in long outputs, domains requiring expert knowledge, and problems without verifiable rewards. We propose a data-driven approach to automatically construct highly granular reasoning error taxonomies to enhance LLM-driven error detection on unseen reasoning traces. Our findings indicate that classification approaches that leverage these error taxonomies, or \"rubrics\", demonstrate strong error identification compared to baseline methods in technical domains like coding, math, and chemical engineering. These rubrics can be used to build stronger LLM-as-judge reward functions for reasoning model training via reinforcement learning. Experimental results show that these rewards have the potential to improve models' task accuracy on difficult domains over models trained by general LLMs-as-judges by +45%, and approach performance of models trained by verifiable rewards while using as little as 20% as many gold labels. Through our approach, we extend the usage of reward rubrics from assessing qualitative model behavior to assessing quantitative model correctness on tasks typically learned via RLVR rewards. This extension opens the door for teaching models to solve complex technical problems without a full dataset of gold labels, which are often highly costly to procure.",
    "arxiv_url": "https://arxiv.org/abs/2602.06795",
    "authors": [
      "Kate Sanders",
      "Nathaniel Weir",
      "Sapana Chaudhary",
      "Kaj Bostrom",
      "Huzefa Rangwala"
    ],
    "first_author": "Kate Sanders",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Model Training & Evaluation",
    "task": "Reward Modeling & Reasoning Trace Verification",
    "tags": [
      "Rubric Generation",
      "Error Taxonomy",
      "LLM-as-Judge",
      "Reward Modeling",
      "Reasoning Trace Classification",
      "Domain Adaptation",
      "Data-Efficient RL",
      "Process Supervision"
    ],
    "summary": "本文提出一种自动从模型失败实例中构建细粒度错误分类（rubrics）的方法，用以增强基于LLM的判别器作为训练时的奖励函数，从而在只用少量金标的情况下显著提高复杂技术性推理任务的错误识别与下游任务准确率。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.06795v1",
    "published": "2026-02-06",
    "update_time": "2026-02-06",
    "download_time": "2026-02-09 02:34:27"
  },
  {
    "id": "2602.06777",
    "title": "Next-generation cyberattack detection with large language models: anomaly analysis across heterogeneous logs",
    "abstract": "This project explores large language models (LLMs) for anomaly detection across heterogeneous log sources. Traditional intrusion detection systems suffer from high false positive rates, semantic blindness, and data scarcity, as logs are inherently sensitive, making clean datasets rare. We address these challenges through three contributions: (1) LogAtlas-Foundation-Sessions and LogAtlas-Defense-Set, balanced and heterogeneous log datasets with explicit attack annotations and privacy preservation; (2) empirical benchmarking revealing why standard metrics such as F1 and accuracy are misleading for security applications; and (3) a two phase training framework combining log understanding (Base-AMAN, 3B parameters) with real time detection (AMAN, 0.5B parameters via knowledge distillation). Results demonstrate practical feasibility, with inference times of 0.3-0.5 seconds per session and operational costs below 50 USD per day.",
    "arxiv_url": "https://arxiv.org/abs/2602.06777",
    "authors": [
      "Yassine Chagna",
      "Antal Goldschmidt"
    ],
    "first_author": "Yassine Chagna",
    "category": [
      "Technical",
      "Benchmark",
      "Empirical"
    ],
    "field": "AIOps",
    "task": "Log Anomaly Detection",
    "tags": [
      "Knowledge Distillation",
      "Soft Mixture-of-Experts",
      "Parameter-Efficient Fine-Tuning",
      "Heterogeneous Logs",
      "Privacy-Preserving Dataset",
      "Session-Level Aggregation",
      "Real-Time Detection",
      "Root-Cause Analysis",
      "Benchmarking Metrics",
      "Cross-Domain Generalization",
      "Inference Efficiency"
    ],
    "summary": "本文提出并公开了两个隐私保护的多源日志数据集，设计了基于LLM的两阶段训练（基础语义理解 + 蒸馏轻量检测器）与软专家混合等技术，实现对异构日志的高效实时异常检测并评估实际部署成本与评价指标缺陷。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.06777v1",
    "published": "2026-02-06",
    "update_time": "2026-02-06",
    "download_time": "2026-02-09 02:34:46"
  },
  {
    "id": "2602.07871",
    "title": "HerAgent: Rethinking the Automated Environment Deployment via Hierarchical Test Pyramid",
    "abstract": "Automated software environment setup is a prerequisite for testing, debugging, and reproducing failures, yet remains challenging in practice due to complex dependencies, heterogeneous build systems, and incomplete documentation. Recent work leverages large language models to automate this process, but typically evaluates success using weak signals such as dependency installation or partial test execution, which do not ensure that a project can actually run. In this paper, we argue that environment setup success should be evaluated through executable evidence rather than a single binary signal. We introduce the Environment Maturity Hierarchy, which defines three success levels based on progressively stronger execution requirements, culminating in successful execution of a project's main entry point. Guided by this hierarchy, we propose HerAgent, an automated environment setup approach that incrementally constructs executable environments through execution-based validation and repair. We evaluate HerAgent on four public benchmarks, where it outperforms all related work, achieving up to 79.6\\% improvement due to its holistic understanding of project structure and dependencies. On complex C/C++ projects, HerAgent surpasses prior approaches by 66.7\\%. In addition, HerAgent uniquely resolves 11-30 environment instances across the benchmarks that no prior method can configure.",
    "arxiv_url": "https://arxiv.org/abs/2602.07871",
    "authors": [
      "Xiang Li",
      "Siyu Lu",
      "Sarro Federica",
      "Claire Le Goues",
      "He Ye"
    ],
    "first_author": "Xiang Li",
    "category": [
      "Technical"
    ],
    "field": "Software Testing",
    "task": "Testing automation",
    "tags": [
      "Environment Maturity Hierarchy",
      "Environment Deployment",
      "Executable Validation",
      "Execution-based Repair",
      "Knowledge Graph",
      "Project Structure Analysis",
      "Repository-level Automation",
      "Cross-language Support"
    ],
    "summary": "本文提出HerAgent，一种基于环境成熟度层级（Installability、Testability、Runnability）的自动化项目环境部署框架，利用项目知识图谱进行整体依赖与结构推理并通过执行驱动的验证与修复策略，在四个公开基准上显著优于现有方法并提高复杂C/C++项目的可运行性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.07871v1",
    "published": "2026-02-08",
    "update_time": "2026-02-08",
    "download_time": "2026-02-10 02:37:59"
  },
  {
    "id": "2602.07783",
    "title": "Still Manual? Automated Linter Configuration via DSL-Based LLM Compilation of Coding Standards",
    "abstract": "Coding standards are essential for maintaining consistent and high-quality code across teams and projects. Linters help developers enforce these standards by detecting code violations. However, manual linter configuration is complex and expertise-intensive, and the diversity and evolution of programming languages, coding standards, and linters lead to repetitive and maintenance-intensive configuration work. To reduce manual effort, we propose LintCFG, a domain-specific language (DSL)-driven, LLM-based compilation approach to automate linter configuration generation for coding standards, independent of programming languages, coding standards, and linters. Inspired by compiler design, we first design a DSL to express coding rules in a tool-agnostic, structured, readable, and precise manner. Then, we build linter configurations into DSL configuration instructions. For a given natural language coding standard, the compilation process parses it into DSL coding standards, matches them with the DSL configuration instructions to set configuration names, option names and values, verifies consistency between the standards and configurations, and finally generates linter-specific configurations. Experiments with Checkstyle for Java coding standard show that our approach achieves over 90% precision and recall in DSL representation, with accuracy, precision, recall, and F1-scores close to 70% (with some exceeding 70%) in fine-grained linter configuration generation. Notably, our approach outperforms baselines by over 100% in precision. A user study further shows that our approach improves developers' efficiency in configuring linters for coding standards. Finally, we demonstrate the generality of the approach by generating ESLint configurations for JavaScript coding standards, showcasing its broad applicability across other programming languages, coding standards, and linters.",
    "arxiv_url": "https://arxiv.org/abs/2602.07783",
    "authors": [
      "Zejun Zhang",
      "Yixin Gan",
      "Zhenchang Xing",
      "Tian Zhang",
      "Yi Li",
      "Xiwei Xu",
      "Qinghua Lu",
      "Liming Zhu"
    ],
    "first_author": "Zejun Zhang",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Linting & Coding Standards",
    "task": "Linter Configuration Generation",
    "tags": [
      "Linter Configuration",
      "DSL for Coding Rules",
      "LLM Compilation",
      "NL-to-DSL Parsing",
      "Configuration Instruction Matching",
      "Semantic Consistency Verification",
      "Cross-Language Generality",
      "User Study"
    ],
    "summary": "该文提出 LintCFG：一种受编译器启发的 DSL 驱动且基于大模型的流水线，将自然语言编码规范解析为中间 DSL 表示并自动生成多种 linter 的配置，实验证明在 Checkstyle 和 ESLint 上具备高精度与通用性。",
    "quality": "High",
    "conference": "FSE 2026",
    "pdf_url": "https://arxiv.org/pdf/2602.07783v1",
    "published": "2026-02-08",
    "update_time": "2026-02-08",
    "download_time": "2026-02-10 02:38:25"
  },
  {
    "id": "2602.07832",
    "title": "rePIRL: Learn PRM with Inverse RL for LLM Reasoning",
    "abstract": "Process rewards have been widely used in deep reinforcement learning to improve training efficiency, reduce variance, and prevent reward hacking. In LLM reasoning, existing works also explore various solutions for learning effective process reward models (PRM) with or without the help of an expert policy. However, existing methods either rely on strong assumptions about the expert policies (e.g., requiring their reward functions) or suffer intrinsic limitations (e.g., entropy collapse), resulting in weak PRMs or limited generalizability. In this paper, we introduce rePIRL, an inverse RL-inspired framework that learns effective PRMs with minimal assumptions about expert policies. Specifically, we design a dual learning process that updates the policy and the PRM interchangeably. Our learning algorithm has customized techniques to address the challenges of scaling traditional inverse RL to LLMs. We theoretically show that our proposed learning framework can unify both online and offline PRM learning methods, justifying that rePIRL can learn PRMs with minimal assumptions. Empirical evaluations on standardized math and coding reasoning datasets demonstrate the effectiveness of rePIRL over existing methods. We further show the application of our trained PRM in test-time training, test-time scaling, and providing an early signal for training hard problems. Finally, we validate our training recipe and key design choices via a detailed ablation study.",
    "arxiv_url": "https://arxiv.org/abs/2602.07832",
    "authors": [
      "Xian Wu",
      "Kaijie Zhu",
      "Ying Zhang",
      "Lun Wang",
      "Wenbo Guo"
    ],
    "first_author": "Xian Wu",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Process Reward Modeling",
      "Inverse Reinforcement Learning",
      "Token-level MDP",
      "Maximum Entropy RL",
      "Policy-Reward Dual Learning",
      "Entropy Collapse Mitigation",
      "Test-time Training",
      "Test-time Scaling",
      "Early Hard-problem Signal",
      "Theoretical Unification"
    ],
    "summary": "本文提出 rePIRL，一种受逆向强化学习启发的可扩展框架，通过仅利用专家轨迹在 token 级别学习过程奖励模型（PRM）并交替训练策略与奖励，从理论上统一现有在线/离线方法，在数学与编码推理基准上展现出优于现有方法的性能并证明多种实际应用价值。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.07832v1",
    "published": "2026-02-08",
    "update_time": "2026-02-08",
    "download_time": "2026-02-10 02:39:27"
  },
  {
    "id": "2602.07787",
    "title": "Do Multi-Agents Dream of Electric Screens? Achieving Perfect Accuracy on AndroidWorld Through Task Decomposition",
    "abstract": "We present Minitap, a multi-agent system that achieves 100% success on the AndroidWorld benchmark, the first to fully solve all 116 tasks and surpassing human performance (80%). We first analyze why single-agent architectures fail: context pollution from mixed reasoning traces, silent text input failures undetected by the agent, and repetitive action loops without escape. Minitap addresses each failure through targeted mechanisms: cognitive separation across six specialized agents, deterministic post-validation of text input against device state, and meta-cognitive reasoning that detects cycles and triggers strategy changes. Ablations show multi-agent decomposition contributes +21 points over single-agent baselines; verified execution adds +7 points; meta-cognition adds +9 points. We release Minitap as open-source software. https://github.com/minitap-ai/mobile-use",
    "arxiv_url": "https://arxiv.org/abs/2602.07787",
    "authors": [
      "Pierre-Louis Favreau",
      "Jean-Pierre Lo",
      "Clement Guiguet",
      "Charles Simon-Meunier",
      "Nicolas Dehandschoewercker",
      "Allen G. Roush",
      "Judah Goldfeder",
      "Ravid Shwartz-Ziv"
    ],
    "first_author": "Pierre-Louis Favreau",
    "category": [
      "Technical"
    ],
    "field": "Software Testing",
    "task": "GUI test",
    "tags": [
      "Multi-Agent Decomposition",
      "Task Decomposition",
      "Verified Execution",
      "Meta-cognitive Reasoning",
      "Context Separation",
      "UI Automation",
      "Failure Mode Analysis",
      "Cycle Detection",
      "Action Sequencing",
      "Ablation Study"
    ],
    "summary": "本文提出Minitap的多代理移动UI自动化系统，通过六个专门代理、对文本输入的执行后验证和元认知循环检测，在AndroidWorld基准上实现100%成功率并超越人类表现。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.07787v1",
    "published": "2026-02-08",
    "update_time": "2026-02-08",
    "download_time": "2026-02-10 02:40:36"
  },
  {
    "id": "2602.09540",
    "title": "SWE-Bench Mobile: Can Large Language Model Agents Develop Industry-Level Mobile Applications?",
    "abstract": "Can large language model agents develop industry-level mobile applications? We introduce \\textbf{SWE-Bench Mobile}, a benchmark for evaluating coding agents on realistic software engineering tasks derived from a production iOS codebase. Unlike existing benchmarks that focus on isolated problems or bug fixes, SWE-Bench Mobile captures the full complexity of industrial development: multi-modal inputs (PRDs and Figma designs), a large-scale mixed Swift/Objective-C codebase, and comprehensive test suites. We evaluate 22 agent-model configurations across four coding agents -- three commercial (Cursor, Codex, Claude Code) and one open-source (OpenCode) -- and find that even the best configurations achieve only 12\\% task success rate. Our analysis reveals that (1) agent design matters as much as model capability -- the same model shows up to 6$\\times$ performance gap across agents, (2) commercial agents consistently outperform open-source alternatives, and (3) simple ``Defensive Programming'' prompts outperform complex ones by 7.4\\%. These findings highlight a significant gap between current agent capabilities and industrial requirements, while providing actionable insights for practitioners and researchers. We release SWE-Bench Mobile as a \\textit{hosted benchmark challenge} to prevent data contamination and ensure fair evaluation. The public leaderboard and development toolkit are available at https://swebenchmobile.com.",
    "arxiv_url": "https://arxiv.org/abs/2602.09540",
    "authors": [
      "Muxin Tian",
      "Zhe Wang",
      "Blair Yang",
      "Zhenwei Tang",
      "Kunlun Zhu",
      "Honghua Dong",
      "Hanchen Li",
      "Xinni Xie",
      "Guangjing Wang",
      "Jiaxuan You"
    ],
    "first_author": "Muxin Tian",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Repository-Level Coding",
    "tags": [
      "Repository-Level Coding",
      "Code Agents",
      "Multi-Modal Reasoning",
      "Mobile UI Implementation",
      "Cross-File Reasoning",
      "Patch-Level Evaluation",
      "Hosted Benchmark",
      "Failure Taxonomy"
    ],
    "summary": "本文提出SWE‑Bench Mobile——一个基于真实iOS生产代码、结合PRD与Figma的多模态移动开发基准，并评估多种编码代理（含商用与开源），发现当前代理在行业级移动应用开发上表现不足（最佳仅12%任务成功率）并系统分析失败模式与实用建议。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.09540v1",
    "published": "2026-02-10",
    "update_time": "2026-02-10",
    "download_time": "2026-02-11 02:35:36"
  },
  {
    "id": "2602.09467",
    "title": "Toward Linking Declined Proposals and Source Code: An Exploratory Study on the Go Repository",
    "abstract": "Traceability links are key information sources for software developers, connecting software artifacts (e.g., linking requirements to the corresponding source code). In open-source software (OSS) projects, such links play an important role, particularly between the contributions (e.g., GitHub issues) and the corresponding source code. Through these links, developers can trace the discussions in contributions and uncover design rationales, constraints, and security concerns. Previous studies have mainly examined accepted contributions, while those declined after discussion have been overlooked. The discussions behind declined contributions contain valuable design rationales and implicit knowledge about software decision-making, as the reasons behind the decline often reveal the criteria used to judge what should or should not be implemented. In this study, we present the first attempt to establish traceability links between declined contributions and related source code. We propose an initial linking approach and conduct an empirical analysis of the generated links to discuss factors affecting link generation. As our dataset, we use proposals from the official Go repository, which are GitHub issues used to propose new features or language changes. To link declined proposals to source code, we designed an LLM-driven pipeline. Our results showed that the pipeline selected the correct granularity for each declined proposal with an accuracy of 0.836, and generated correct links at that granularity with a mean precision of 0.643. To clarify the challenges of linking declined proposals, we performed a failure analysis. In the declined proposals where the pipeline failed to generate links, the discussions were often redundant and lacked concrete information (e.g., how the feature should be implemented).",
    "arxiv_url": "https://arxiv.org/abs/2602.09467",
    "authors": [
      "Sota Nakashima",
      "Masanari Kondo",
      "Mahmoud Alfadel",
      "Aly Ahmad",
      "Toshihiro Nakae",
      "Hidenori Matsuzaki"
    ],
    "first_author": "Sota Nakashima",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Requirements & Design",
    "task": "Specification & Validation",
    "tags": [
      "Proposals-to-code linking",
      "Granularity-aware linking",
      "LLM-based localization",
      "Code RAG",
      "Design rationale recovery",
      "Failure analysis",
      "Traceability link recovery",
      "Repository mining"
    ],
    "summary": "本文提出并评估了一个基于大型语言模型的流水线，用于将被拒绝的提案与 Go 仓库中的相应源代码建立粒度感知的可追溯性链接，并通过失败分析揭示影响链接生成的因子。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.09467v1",
    "published": "2026-02-10",
    "update_time": "2026-02-10",
    "download_time": "2026-02-11 02:35:59"
  },
  {
    "id": "2602.09856",
    "title": "Code2World: A GUI World Model via Renderable Code Generation",
    "abstract": "Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.",
    "arxiv_url": "https://arxiv.org/abs/2602.09856",
    "authors": [
      "Yuhao Zheng",
      "Li'an Zhong",
      "Yi Wang",
      "Rui Dai",
      "Kaikui Liu",
      "Xiangxiang Chu",
      "Linyuan Lv",
      "Philip Torr",
      "Kevin Qinghong Lin"
    ],
    "first_author": "Yuhao Zheng",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "GUI World Modeling",
    "task": "Renderable Code Generation for Next UI Prediction",
    "tags": [
      "Renderable Code Generation",
      "HTML UI Synthesis",
      "GUI World Model",
      "Render-Aware Reinforcement Learning",
      "Visual-feedback Revision",
      "Next-UI Prediction",
      "Virtual Sandbox",
      "Action-Conditional Prediction"
    ],
    "summary": "本文提出Code2World，通过生成可渲染的HTML代码作为GUI世界模型来预测动作条件下的下一界面，构建了80K+的AndroidCode数据集并引入渲染感知强化学习与可视化反馈修正以提升视觉保真与结构可控性，从而显著改进下游导航任务性能。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.09856v1",
    "published": "2026-02-10",
    "update_time": "2026-02-10",
    "download_time": "2026-02-11 02:36:24"
  },
  {
    "id": "2602.08887",
    "title": "DeepQuali: Initial results of a study on the use of large language models for assessing the quality of user stories",
    "abstract": "Generative artificial intelligence (GAI), specifically large language models (LLMs), are increasingly used in software engineering, mainly for coding tasks. However, requirements engineering - particularly requirements validation - has seen limited application of GAI. The current focus of using GAI for requirements is on eliciting, transforming, and classifying requirements, not on quality assessment. We propose and evaluate the LLM-based (GPT-4o) approach \"DeepQuali\", for assessing and improving requirements quality in agile software development. We applied it to projects in two small companies, where we compared LLM-based quality assessments with expert judgments. Experts also participated in walkthroughs of the solution, provided feedback, and rated their acceptance of the approach. Experts largely agreed with the LLM's quality assessments, especially regarding overall ratings and explanations. However, they did not always agree with the other experts on detailed ratings, suggesting that expertise and experience may influence judgments. Experts recognized the usefulness of the approach but criticized the lack of integration into their workflow. LLMs show potential in supporting software engineers with the quality assessment and improvement of requirements. The explicit use of quality models and explanatory feedback increases acceptance.",
    "arxiv_url": "https://arxiv.org/abs/2602.08887",
    "authors": [
      "Adam Trendowicz",
      "Daniel Seifert",
      "Andreas Jedlitschka",
      "Marcus Ciolkowski",
      "Anton Strahilov"
    ],
    "first_author": "Adam Trendowicz",
    "category": [
      "Empirical",
      "Technical"
    ],
    "field": "Requirements & Design",
    "task": "Specification & Validation",
    "tags": [
      "User stories",
      "Requirements quality",
      "Quality models",
      "Explainable feedback",
      "Human-LLM Interaction",
      "Agile workflows",
      "Expert agreement",
      "Tool acceptance"
    ],
    "summary": "本文提出并在两家小型公司中验证了基于GPT-4o的DeepQuali方法，通过显式质量模型对用户故事进行评分、识别问题并给出可解释的改进建议，专家总体认可其评估与解释但指出与现有工作流集成和细节评分上的差异性挑战。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.08887v1",
    "published": "2026-02-09",
    "update_time": "2026-02-09",
    "download_time": "2026-02-11 02:40:32"
  },
  {
    "id": "2602.10975",
    "title": "FeatureBench: Benchmarking Agentic Coding for Complex Feature Development",
    "abstract": "Agents powered by large language models (LLMs) are increasingly adopted in the software industry, contributing code as collaborators or even autonomous developers. As their presence grows, it becomes important to assess the current boundaries of their coding abilities. Existing agentic coding benchmarks, however, cover a limited task scope, e.g., bug fixing within a single pull request (PR), and often rely on non-executable evaluations or lack an automated approach for continually updating the evaluation coverage. To address such issues, we propose FeatureBench, a benchmark designed to evaluate agentic coding performance in end-to-end, feature-oriented software development. FeatureBench incorporates an execution-based evaluation protocol and a scalable test-driven method that automatically derives tasks from code repositories with minimal human effort. By tracing from unit tests along a dependency graph, our approach can identify feature-level coding tasks spanning multiple commits and PRs scattered across the development timeline, while ensuring the proper functioning of other features after the separation. Using this framework, we curated 200 challenging evaluation tasks and 3825 executable environments from 24 open-source repositories in the first version of our benchmark. Empirical evaluation reveals that the state-of-the-art agentic model, such as Claude 4.5 Opus, which achieves a 74.4% resolved rate on SWE-bench, succeeds on only 11.0% of tasks, opening new opportunities for advancing agentic coding. Moreover, benefiting from our automated task collection toolkit, FeatureBench can be easily scaled and updated over time to mitigate data leakage. The inherent verifiability of constructed environments also makes our method potentially valuable for agent training.",
    "arxiv_url": "https://arxiv.org/abs/2602.10975",
    "authors": [
      "Qixing Zhou",
      "Jiacheng Zhang",
      "Haiyang Wang",
      "Rui Hao",
      "Jiahe Wang",
      "Minghao Han",
      "Yuxue Yang",
      "Shuzhe Wu",
      "Feiyang Pan",
      "Lue Fan",
      "Dandan Tu",
      "Zhaoxiang Zhang"
    ],
    "first_author": "Qixing Zhou",
    "category": [
      "Benchmark",
      "Technical",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Editing",
    "tags": [
      "Feature-level Development",
      "Execution-based Evaluation",
      "Test-Driven Collection",
      "Dependency Graph Tracing",
      "Verifiable Environments",
      "Continual Updatable",
      "Repository-Level Coding",
      "Agentic Coding"
    ],
    "summary": "本文提出FeatureBench——一个面向特性级（feature）开发的可执行、可持续更新的agentic编码基准与自动化实例收集工具链，并基于24个开源仓库构建200个任务和3825个可运行环境来评估现有LLM代理的特性开发能力。",
    "quality": "High",
    "conference": "ICLR 2026",
    "pdf_url": "https://arxiv.org/pdf/2602.10975v1",
    "published": "2026-02-11",
    "update_time": "2026-02-11",
    "download_time": "2026-02-12 02:30:38"
  },
  {
    "id": "2602.10808",
    "title": "PELLI: Framework to effectively integrate LLMs for quality software generation",
    "abstract": "Recent studies have revealed that when LLMs are appropriately prompted and configured, they demonstrate mixed results. Such results often meet or exceed the baseline performance. However, these comparisons have two primary issues. First, they mostly considered only reliability as a comparison metric and selected a few LLMs (such as Codex and ChatGPT) for comparision. This paper proposes a comprehensive code quality assessment framework called Programmatic Excellence via LLM Iteration (PELLI). PELLI is an iterative analysis-based process that upholds high-quality code changes. We extended the state-of-the-art by performing a comprehensive evaluation that generates quantitative metrics for analyzing three primary nonfunctional requirements (such as maintainability, performance, and reliability) while selecting five popular LLMs. For PELLI's applicability, we selected three application domains while following Python coding standards. Following this framework, practitioners can ensure harmonious integration between LLMs and human developers, ensuring that their potential is fully realized. PELLI can serve as a practical guide for developers aiming to leverage LLMs while adhering to recognized quality standards. This study's outcomes are crucial for advancing LLM technologies in real-world applications, providing stakeholders with a clear understanding of where these LLMs excel and where they require further refinement. Overall, based on three nonfunctional requirements, we have found that GPT-4T and Gemini performed slightly better. We also found that prompt design can influence the overall code quality. In addition, each application domain demonstrated high and low scores across various metrics, and even within the same metrics across different prompts.",
    "arxiv_url": "https://arxiv.org/abs/2602.10808",
    "authors": [
      "Rasmus Krebs",
      "Somnath Mazumdar"
    ],
    "first_author": "Rasmus Krebs",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Prompting",
    "tags": [
      "Iterative Refinement",
      "Prompt Stratification",
      "Code Quality Assessment",
      "Maintainability",
      "Performance Evaluation",
      "Reliability Analysis",
      "Static Analysis",
      "Human-LLM Interaction",
      "Python-specific"
    ],
    "summary": "本文提出PELLI框架，通过分层提示、迭代生成与静态代码分析，比较五种主流大模型在Python三类应用上生成代码的可维护性、性能和可靠性，并讨论提示长度与模型特性对代码质量的影响，给出实用集成建议。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.10808v1",
    "published": "2026-02-11",
    "update_time": "2026-02-11",
    "download_time": "2026-02-12 02:31:06"
  },
  {
    "id": "2602.10063",
    "title": "Chain of Mindset: Reasoning with Adaptive Cognitive Modes",
    "abstract": "Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\\% and 4.72\\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at \\href{https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset}.",
    "arxiv_url": "https://arxiv.org/abs/2602.10063",
    "authors": [
      "Tianyi Jiang",
      "Arctanx An",
      "Hengyi Feng",
      "Naixin Zhai",
      "Haodong Li",
      "Xiaomin Yu",
      "Jiahui Liu",
      "Hanwen Du",
      "Shuo Zhang",
      "Zhi Yang",
      "Jie Huang",
      "Yuhua Li",
      "Yongxin Ni",
      "Huacan Wang",
      "Ronghao Chen"
    ],
    "first_author": "Tianyi Jiang",
    "category": [
      "Technical"
    ],
    "field": "Reasoning & Cognitive Control",
    "task": "Adaptive Mindset Orchestration",
    "tags": [
      "Adaptive Mindset",
      "Meta-Agent",
      "Context Gate",
      "Mindset Switching",
      "Algorithmic Thinking",
      "Spatial Reasoning",
      "Divergent Thinking",
      "Convergent Thinking",
      "Training-free Agent",
      "Step-level Orchestration",
      "Multi-domain Reasoning"
    ],
    "summary": "本文提出Chain of Mindset (CoM)，一种无需训练的智能体框架，通过在推理过程中按步自适应地在空间、聚合、发散与算法四种心智模式之间切换并引入双向Context Gate以过滤跨模块信息，从而在数学、代码生成、科学问答和空间推理等多领域基准上显著提升推理准确性与效率。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.10063v1",
    "published": "2026-02-10",
    "update_time": "2026-02-10",
    "download_time": "2026-02-12 02:32:30"
  },
  {
    "id": "2602.09937",
    "title": "Why Do AI Agents Systematically Fail at Cloud Root Cause Analysis?",
    "abstract": "Failures in large-scale cloud systems incur substantial financial losses, making automated Root Cause Analysis (RCA) essential for operational stability. Recent efforts leverage Large Language Model (LLM) agents to automate this task, yet existing systems exhibit low detection accuracy even with capable models, and current evaluation frameworks assess only final answer correctness without revealing why the agent's reasoning failed. This paper presents a process level failure analysis of LLM-based RCA agents. We execute the full OpenRCA benchmark across five LLM models, producing 1,675 agent runs, and classify observed failures into 12 pitfall types across intra-agent reasoning, inter-agent communication, and agent-environment interaction. Our analysis reveals that the most prevalent pitfalls, notably hallucinated data interpretation and incomplete exploration, persist across all models regardless of capability tier, indicating that these failures originate from the shared agent architecture rather than from individual model limitations. Controlled mitigation experiments further show that prompt engineering alone cannot resolve the dominant pitfalls, whereas enriching the inter-agent communication protocol reduces communication-related failures by up to 15 percentage points. The pitfall taxonomy and diagnostic methodology developed in this work provide a foundation for designing more reliable autonomous agents for cloud RCA.",
    "arxiv_url": "https://arxiv.org/abs/2602.09937",
    "authors": [
      "Taeyoon Kim",
      "Woohyeok Park",
      "Hoyeong Yun",
      "Kyungyong Lee"
    ],
    "first_author": "Taeyoon Kim",
    "category": [
      "Empirical"
    ],
    "field": "AIOps",
    "task": "Root Cause Analysis",
    "tags": [
      "Hallucination",
      "Incomplete Exploration",
      "Inter-Agent Communication",
      "Controller-Executor Architecture",
      "Telemetry Correlation",
      "Process-Level Analysis",
      "Pitfall Taxonomy",
      "Mitigation Experiments",
      "Human-in-the-Loop"
    ],
    "summary": "本文对基于大型语言模型的云故障根因分析（RCA）代理进行了过程级故障诊断，基于1,675次代理运行归纳出12类架构性陷阱，发现主要失败模式跨模型一致且源自代理架构，并通过加强代理间通信等结构性改动验证了可缓解部分问题。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.09937v1",
    "published": "2026-02-10",
    "update_time": "2026-02-10",
    "download_time": "2026-02-12 02:34:49"
  },
  {
    "id": "2602.12058",
    "title": "ModelWisdom: An Integrated Toolkit for TLA+ Model Visualization, Digest and Repair",
    "abstract": "Model checking in TLA+ provides strong correctness guarantees, yet practitioners continue to face significant challenges in interpreting counterexamples, understanding large state-transition graphs, and repairing faulty models. These difficulties stem from the limited explainability of raw model-checker output and the substantial manual effort required to trace violations back to source specifications. Although the TLA+ Toolbox includes a state diagram viewer, it offers only a static, fully expanded graph without folding, color highlighting, or semantic explanations, which limits its scalability and interpretability. We present ModelWisdom, an interactive environment that uses visualization and large language models to make TLA+ model checking more interpretable and actionable. ModelWisdom offers: (i) Model Visualization, with colorized violation highlighting, click-through links from transitions to TLA+ code, and mapping between violating states and broken properties; (ii) Graph Optimization, including tree-based structuring and node/edge folding to manage large models; (iii) Model Digest, which summarizes and explains subgraphs via large language models (LLMs) and performs preprocessing and partial explanations; and (iv) Model Repair, which extracts error information and supports iterative debugging. Together, these capabilities turn raw model-checker output into an interactive, explainable workflow, improving understanding and reducing debugging effort for nontrivial TLA+ specifications. The website to ModelWisdom is available: https://model-wisdom.pages.dev. A demonstrative video can be found at https://www.youtube.com/watch?v=plyZo30VShA.",
    "arxiv_url": "https://arxiv.org/abs/2602.12058",
    "authors": [
      "Zhiyong Chen",
      "Jialun Cao",
      "Chang Xu",
      "Shing-Chi Cheung"
    ],
    "first_author": "Zhiyong Chen",
    "category": [
      "Technical"
    ],
    "field": "Formal Verification & Specification",
    "task": "Model Visualization and Repair",
    "tags": [
      "TLA+ Visualization",
      "State Graph Folding",
      "Violation Highlighting",
      "Click-through Navigation",
      "Graph Compaction",
      "LLM-Guided Summarization",
      "Model Repair Assistance",
      "Interactive Debugging",
      "Subgraph Explanation",
      "Scalability"
    ],
    "summary": "ModelWisdom 是一个结合可视化与大语言模型的交互式工具，用于将 TLA+ 模型检查的状态转换图进行可视化、压缩与摘要，并提供基于 LLM 的子图解释与迭代修复支持，从而提升错误定位与调试效率。",
    "quality": "High",
    "conference": "FM (Formal Methods) Research Track (Tool) 2026",
    "pdf_url": "https://arxiv.org/pdf/2602.12058v1",
    "published": "2026-02-12",
    "update_time": "2026-02-12",
    "download_time": "2026-02-13 02:32:04"
  },
  {
    "id": "2602.11988",
    "title": "Evaluating AGENTS.md: Are Repository-Level Context Files Helpful for Coding Agents?",
    "abstract": "A widespread practice in software development is to tailor coding agents to repositories using context files, such as AGENTS.md, by either manually or automatically generating them. Although this practice is strongly encouraged by agent developers, there is currently no rigorous investigation into whether such context files are actually effective for real-world tasks. In this work, we study this question and evaluate coding agents' task completion performance in two complementary settings: established SWE-bench tasks from popular repositories, with LLM-generated context files following agent-developer recommendations, and a novel collection of issues from repositories containing developer-committed context files.   Across multiple coding agents and LLMs, we find that context files tend to reduce task success rates compared to providing no repository context, while also increasing inference cost by over 20%. Behaviorally, both LLM-generated and developer-provided context files encourage broader exploration (e.g., more thorough testing and file traversal), and coding agents tend to respect their instructions. Ultimately, we conclude that unnecessary requirements from context files make tasks harder, and human-written context files should describe only minimal requirements.",
    "arxiv_url": "https://arxiv.org/abs/2602.11988",
    "authors": [
      "Thibaud Gloaguen",
      "Niels Mündler",
      "Mark Müller",
      "Veselin Raychev",
      "Martin Vechev"
    ],
    "first_author": "Thibaud Gloaguen",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Prompting",
    "tags": [
      "Repository-Level Coding",
      "Context Files",
      "Agent Instructioning",
      "Issue Resolution",
      "Agent Behavior Analysis",
      "Inference Cost",
      "LLM-Generated Instructions",
      "Developer-Provided Instructions",
      "Trace Analysis",
      "Autonomous Task Solving"
    ],
    "summary": "本文构建了基于真实 GitHub 问题的新基准并在多种编码代理与设置下评估仓库级上下文文件的影响，结果表明自动生成的上下文文件通常降低成功率并提高推理成本，而开发者编写的上下文文件仅带来微小改进，建议仅保留最小必要说明。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.11988v1",
    "published": "2026-02-12",
    "update_time": "2026-02-12",
    "download_time": "2026-02-13 02:32:33"
  },
  {
    "id": "2602.12125",
    "title": "Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation",
    "abstract": "On-policy distillation (OPD), which aligns the student with the teacher's logit distribution on student-generated trajectories, has demonstrated strong empirical gains in improving student performance and often outperforms off-policy distillation and reinforcement learning (RL) paradigms. In this work, we first theoretically show that OPD is a special case of dense KL-constrained RL where the reward function and the KL regularization are always weighted equally and the reference model can by any model. Then, we propose the Generalized On-Policy Distillation (G-OPD) framework, which extends the standard OPD objective by introducing a flexible reference model and a reward scaling factor that controls the relative weight of the reward term against the KL regularization. Through comprehensive experiments on math reasoning and code generation tasks, we derive two novel insights: (1) Setting the reward scaling factor to be greater than 1 (i.e., reward extrapolation), which we term ExOPD, consistently improves over standard OPD across a range of teacher-student size pairings. In particular, in the setting where we merge the knowledge from different domain experts, obtained by applying domain-specific RL to the same student model, back into the original student, ExOPD enables the student to even surpass the teacher's performance boundary and outperform the domain teachers. (2) Building on ExOPD, we further find that in the strong-to-weak distillation setting (i.e., distilling a smaller student from a larger teacher), performing reward correction by choosing the reference model as the teacher's base model before RL yields a more accurate reward signal and further improves distillation performance. However, this choice assumes access to the teacher's pre-RL variant and incurs more computational overhead. We hope our work offers new insights for future research on OPD.",
    "arxiv_url": "https://arxiv.org/abs/2602.12125",
    "authors": [
      "Wenkai Yang",
      "Weijie Liu",
      "Ruobing Xie",
      "Kai Yang",
      "Saiyong Yang",
      "Yankai Lin"
    ],
    "first_author": "Wenkai Yang",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Alignment",
    "tags": [
      "On-Policy Distillation",
      "Reward Extrapolation",
      "Generalized OPD",
      "Reward Correction",
      "KL-Constrained RL",
      "Multi-Teacher Merging",
      "Strong-to-Weak Distillation",
      "Token-level Advantage"
    ],
    "summary": "本文提出广义在策略蒸馏框架G-OPD，通过引入奖励缩放（ExOPD）及可选的参考模型奖励校正，在数学推理与代码生成任务中提升蒸馏效果并在多教师合并与强对弱蒸馏设置下实现超越教师的性能。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2602.12125v1",
    "published": "2026-02-12",
    "update_time": "2026-02-12",
    "download_time": "2026-02-13 02:32:52"
  },
  {
    "id": "2602.12038",
    "title": "An Empirical Study of the Imbalance Issue in Software Vulnerability Detection",
    "abstract": "Vulnerability detection is crucial to protect software security. Nowadays, deep learning (DL) is the most promising technique to automate this detection task, leveraging its superior ability to extract patterns and representations within extensive code volumes. Despite its promise, DL-based vulnerability detection remains in its early stages, with model performance exhibiting variability across datasets. Drawing insights from other well-explored application areas like computer vision, we conjecture that the imbalance issue (the number of vulnerable code is extremely small) is at the core of the phenomenon. To validate this, we conduct a comprehensive empirical study involving nine open-source datasets and two state-of-the-art DL models. The results confirm our conjecture. We also obtain insightful findings on how existing imbalance solutions perform in vulnerability detection. It turns out that these solutions perform differently as well across datasets and evaluation metrics. Specifically: 1) Focal loss is more suitable to improve the precision, 2) mean false error and class-balanced loss encourages the recall, and 3) random over-sampling facilitates the F1-measure. However, none of them excels across all metrics. To delve deeper, we explore external influences on these solutions and offer insights for developing new solutions.",
    "arxiv_url": "https://arxiv.org/abs/2602.12038",
    "authors": [
      "Yuejun Guo",
      "Qiang Hu",
      "Qiang Tang",
      "Yves Le Traon"
    ],
    "first_author": "Yuejun Guo",
    "category": [
      "Empirical"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "Security and Vulnerabilities",
      "Class Imbalance",
      "Imbalanced Learning",
      "Focal Loss",
      "Class-Balanced Loss",
      "Mean False Error",
      "Random Over-Sampling",
      "Precision-Recall Evaluation",
      "False Negative Rate",
      "External Factors",
      "Reproducibility"
    ],
    "summary": "本文通过在九个开源数据集和两种深度学习模型上的系统实证研究，分析了软件漏洞检测中的类别不平衡问题、评估常见不平衡处理方法在不同数据和评价指标下的表现，并探讨了外部因素对这些方法有效性的影响。",
    "quality": "High",
    "conference": "European Symposium on Research in Computer Security (ESORICS) 2023",
    "pdf_url": "https://arxiv.org/pdf/2602.12038v1",
    "published": "2026-02-12",
    "update_time": "2026-02-12",
    "download_time": "2026-02-13 02:34:30"
  }
]